{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ae9554-e072-40e8-9506-847ed0d8977e",
   "metadata": {},
   "source": [
    "## Classification model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1c8c3-c382-45ab-a7b8-0f87eca181bd",
   "metadata": {},
   "source": [
    "3. Train a suitable NLP model for text classification (use Hugging Face)\n",
    "4. Evaluate the model's performance on the validation set using appropriate metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:43:25.030095Z",
     "start_time": "2025-02-06T13:43:19.747652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification, create_optimizer"
   ],
   "id": "37a8b2a291d19986",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:44:43.820324Z",
     "start_time": "2025-02-06T13:44:43.345621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\n",
    "    '/Users/mariapazoliva/PycharmProjects/ArticlesClassifier/jupyter_notebooks/data_arxiv_articles/final_arxiv_articles.csv')\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['clean_abstract'], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "train_encodings = tokenize_function(train_df.to_dict(orient='list'))\n",
    "val_encodings = tokenize_function(val_df.to_dict(orient='list'))\n",
    "\n",
    "# Convert categories into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df['category'])\n",
    "val_labels = label_encoder.transform(val_df['category'])"
   ],
   "id": "defd5024b3e55592",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:44:45.070358Z",
     "start_time": "2025-02-06T13:44:45.067292Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"LabelEncoder classes:\", label_encoder.classes_)",
   "id": "d34549067b259827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelEncoder classes: ['cs' 'econ' 'eess' 'math' 'phys' 'q-bio' 'q-fin' 'stat']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:57.496411Z",
     "start_time": "2025-02-05T15:42:57.035621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "\n",
    "# Adjust batch size according to your system's capabilities\n",
    "batch_size = 16\n",
    "train_dataset = train_dataset.shuffle(len(train_df)).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ],
   "id": "58e8f5e3c0e59320",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:21:30.026107Z",
     "start_time": "2025-02-05T15:43:01.997819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load BERT model pre-trained on uncased text\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                             num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Setting up the optimizer and loss\n",
    "num_train_steps = len(train_df) // batch_size * 3  # For 3 epochs\n",
    "optimizer, _ = create_optimizer(init_lr=5e-5, num_train_steps=num_train_steps, num_warmup_steps=0)\n",
    "\n",
    "# Compile the model explicitly specifying the loss function\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=3, validation_data=val_dataset)\n",
    "# Save the model to a directory\n",
    "model.save_pretrained('base_model')"
   ],
   "id": "1dde7e3458c98b37",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "45/45 [==============================] - 657s 15s/step - loss: 1.7246 - accuracy: 0.3639 - val_loss: 1.4372 - val_accuracy: 0.5125\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 806s 18s/step - loss: 1.1562 - accuracy: 0.6056 - val_loss: 1.2336 - val_accuracy: 0.6125\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 843s 19s/step - loss: 0.9244 - accuracy: 0.6917 - val_loss: 1.1602 - val_accuracy: 0.6375\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:30:50.674802Z",
     "start_time": "2025-02-05T16:30:29.300091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predict on validation dataset\n",
    "val_predictions = model.predict(val_dataset)\n",
    "\n",
    "# Get the class with the highest probability for each instance\n",
    "val_preds = np.argmax(val_predictions.logits, axis=1)"
   ],
   "id": "95b368035a98988b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 21s 4s/step\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:30:52.068332Z",
     "start_time": "2025-02-05T16:30:52.057599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(val_labels, val_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# For a more detailed report, use classification_report\n",
    "report = classification_report(val_labels, val_preds, target_names=label_encoder.classes_)\n",
    "print(report)"
   ],
   "id": "6c03aa640caade40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6375\n",
      "Precision: 0.6171\n",
      "Recall: 0.6375\n",
      "F1-Score: 0.6179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          cs       0.33      0.12      0.18         8\n",
      "        econ       0.88      0.88      0.88         8\n",
      "        eess       0.62      0.83      0.71        12\n",
      "        math       0.50      0.71      0.59         7\n",
      "        phys       0.64      0.58      0.61        12\n",
      "       q-bio       0.73      0.79      0.76        14\n",
      "       q-fin       0.86      0.86      0.86         7\n",
      "        stat       0.40      0.33      0.36        12\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.62      0.64      0.62        80\n",
      "weighted avg       0.62      0.64      0.62        80\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
