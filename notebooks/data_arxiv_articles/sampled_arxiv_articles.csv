category,title,abstract
phys,A High Robustness and Low Cost Model for Cascading Failures,"  We study numerically the cascading failure problem by using artificially
created scale-free networks and the real network structure of the power grid.
The capacity for a vertex is assigned as a monotonically increasing function of
the load (or the betweenness centrality). Through the use of a simple
functional form with two free parameters, revealed is that it is indeed
possible to make networks more robust while spending less cost. We suggest that
our method to prevent cascade by protecting less vertices is particularly
important for the design of more robust real-world networks to cascading
failures.
"
phys,Turbulent Diffusion of Lines and Circulations,"  We study material lines and passive vectors in a model of turbulent flow at
infinite-Reynolds number, the Kraichnan-Kazantsev ensemble of velocities that
are white-noise in time and rough (Hoelder continuous) in space. It is argued
that the phenomenon of ``spontaneous stochasticity'' generalizes to material
lines and that conservation of circulations generalizes to a ``martingale
property'' of the stochastic process of lines.
"
phys,Leaky modes of a left-handed slab,"  Using complex plane analysis we show that left-handed slab may support either
leaky slab waves, which are backward because of negative refraction, or leaky
surface waves, which are backward or forward depending on the propagation
direction of the surface wave itself. Moreover, there is a general connection
between the reflection coefficient of the left-handed slab and the one of the
corresponding right-handed slab (with opposite permittivity and permeability)
so that leaky slab modes are excited for the same angle of incidence of the
impinging beam for both structures. Many negative giant lateral shifts can be
explained by the excitation of these leaky modes.
"
phys,Universe Without Singularities. A Group Approach to De Sitter Cosmology,"  In the last years the traditional scenario of Big Bang has been deeply
modified by the study of the quantum features of the Universe evolution,
proposing again the problem of using local physical laws on cosmic scale, with
particular regard to the cosmological constant role. The group extention method
shows that the De Sitter group univocally generalizes the Poincare group,
formally justifies the cosmological constant use and suggests a new
interpretation for Hartle-Hawking boundary conditions in Quantum Cosmology.
"
phys,Quantifying social group evolution,"  The rich set of interactions between individuals in the society results in
complex community structure, capturing highly connected circles of friends,
families, or professional cliques in a social network. Thanks to frequent
changes in the activity and communication patterns of individuals, the
associated social and communication network is subject to constant evolution.
Our knowledge of the mechanisms governing the underlying community dynamics is
limited, but is essential for a deeper understanding of the development and
self-optimisation of the society as a whole. We have developed a new algorithm
based on clique percolation, that allows, for the first time, to investigate
the time dependence of overlapping communities on a large scale and as such, to
uncover basic relationships characterising community evolution. Our focus is on
networks capturing the collaboration between scientists and the calls between
mobile phone users. We find that large groups persist longer if they are
capable of dynamically altering their membership, suggesting that an ability to
change the composition results in better adaptability. The behaviour of small
groups displays the opposite tendency, the condition for stability being that
their composition remains unchanged. We also show that the knowledge of the
time commitment of the members to a given community can be used for estimating
the community's lifetime. These findings offer a new view on the fundamental
differences between the dynamics of small groups and large institutions.
"
phys,Eigen Equation of the Nonlinear Spinor,"  How to effectively solve the eigen solutions of the nonlinear spinor field
equation coupling with some other interaction fields is important to understand
the behavior of the elementary particles. In this paper, we derive a simplified
form of the eigen equation of the nonlinear spinor, and then propose a scheme
to solve their numerical solutions. This simplified equation has elegant and
neat structure, which is more convenient for both theoretical analysis and
numerical computation.
"
phys,Non-extensive thermodynamics of 1D systems with long-range interaction,"  A new approach to non-extensive thermodynamical systems with non-additive
energy and entropy is proposed. The main idea of the paper is based on the
statistical matching of the thermodynamical systems with the additive
multi-step Markov chains. This general approach is applied to the Ising spin
chain with long-range interaction between its elements. The asymptotical
expressions for the energy and entropy of the system are derived for the
limiting case of weak interaction. These thermodynamical quantities are found
to be non-proportional to the length of the system (number of its particle).
"
phys,Simulation of Robustness against Lesions of Cortical Networks,"  Structure entails function and thus a structural description of the brain
will help to understand its function and may provide insights into many
properties of brain systems, from their robustness and recovery from damage, to
their dynamics and even their evolution. Advances in the analysis of complex
networks provide useful new approaches to understanding structural and
functional properties of brain networks. Structural properties of networks
recently described allow their characterization as small-world, random
(exponential) and scale-free. They complement the set of other properties that
have been explored in the context of brain connectivity, such as topology,
hodology, clustering, and hierarchical organization. Here we apply new network
analysis methods to cortical inter-areal connectivity networks for the cat and
macaque brains. We compare these corticocortical fibre networks to benchmark
rewired, small-world, scale-free and random networks, using two analysis
strategies, in which we measure the effects of the removal of nodes and
connections on the structural properties of the cortical networks. The brain
networks' structural decay is in most respects similar to that of scale-free
networks. The results implicate highly connected hub-nodes and bottleneck
connections as structural basis for some of the conditional robustness of brain
systems. This informs the understanding of the development of brain networks'
connectivity.
"
phys,"Bibliometric statistical properties of the 100 largest European
  universities: prevalent scaling rules in the science system","  For the 100 largest European universities we studied the statistical
properties of bibliometric indicators related to research performance, field
citation density and journal impact. We find a size-dependent cumulative
advantage for the impact of universities in terms of total number of citations.
In previous work a similar scaling rule was found at the level of research
groups. Therefore we conjecture that this scaling rule is a prevalent property
of the science system. We observe that lower performance universities have a
larger size-dependent cumulative advantage for receiving citations than
top-performance universities. We also find that for the lower-performance
universities the fraction of not-cited publications decreases considerably with
size. Generally, the higher the average journal impact of the publications of a
university, the lower the number of not-cited publications. We find that the
average research performance does not dilute with size. Large top-performance
universities succeed in keeping a high performance over a broad range of
activities. This most probably is an indication of their scientific attractive
power. Next we find that particularly for the lower-performance universities
the field citation density provides a strong cumulative advantage in citations
per publication. The relation between number of citations and field citation
density found in this study can be considered as a second basic scaling rule of
the science system. Top-performance universities publish in journals with
significantly higher journal impact as compared to the lower performance
universities. We find a significant decrease of the fraction of self-citations
with increasing research performance, average field citation density, and
average journal impact.
"
phys,Nuclear Spin Effects in Optical Lattice Clocks,"  We present a detailed experimental and theoretical study of the effect of
nuclear spin on the performance of optical lattice clocks. With a state-mixing
theory including spin-orbit and hyperfine interactions, we describe the origin
of the $^1S_0$-$^3P_0$ clock transition and the differential g-factor between
the two clock states for alkaline-earth(-like) atoms, using $^{87}$Sr as an
example. Clock frequency shifts due to magnetic and optical fields are
discussed with an emphasis on those relating to nuclear structure. An
experimental determination of the differential g-factor in $^{87}$Sr is
performed and is in good agreement with theory. The magnitude of the tensor
light shift on the clock states is also explored experimentally. State specific
measurements with controlled nuclear spin polarization are discussed as a
method to reduce the nuclear spin-related systematic effects to below
10$^{-17}$ in lattice clocks.
"
phys,Statistical analysis of weighted networks,"  The purpose of this paper is to assess the statistical characterization of
weighted networks in terms of the generalization of the relevant parameters,
namely average path length, degree distribution and clustering coefficient.
Although the degree distribution and the average path length admit
straightforward generalizations, for the clustering coefficient several
different definitions have been proposed in the literature. We examined the
different definitions and identified the similarities and differences between
them. In order to elucidate the significance of different definitions of the
weighted clustering coefficient, we studied their dependence on the weights of
the connections. For this purpose, we introduce the relative perturbation norm
of the weights as an index to assess the weight distribution. This study
revealed new interesting statistical regularities in terms of the relative
perturbation norm useful for the statistical characterization of weighted
graphs.
"
phys,Genetic Optimization of Photonic Bandgap Structures,"  We investigate the use of a Genetic Algorithm (GA) to design a set of
photonic crystals (PCs) in one and two dimensions. Our flexible design
methodology allows us to optimize PC structures which are optimized for
specific objectives. In this paper, we report the results of several such
GA-based PC optimizations. We show that the GA performs well even in very
complex design spaces, and therefore has great potential for use as a robust
design tool in present and future applications.
"
phys,"Interference effects in above-threshold ionization from diatomic
  molecules: determining the internuclear separation","  We calculate angle-resolved above-threshold ionization spectra for diatomic
molecules in linearly polarized laser fields, employing the strong-field
approximation. The interference structure resulting from the individual
contributions of the different scattering scenarios is discussed in detail,
with respect to the dependence on the internuclear distance and molecular
orientation. We show that, in general, the contributions from the processes in
which the electron is freed at one center and rescatters off the other obscure
the interference maxima and minima obtained from single-center processes.
However, around the boundary of the energy regions for which rescattering has a
classical counterpart, such processes play a negligible role and very clear
interference patterns are observed. In such energy regions, one is able to
infer the internuclear distance from the energy difference between adjacent
interference minima.
"
phys,Three dimensional cooling and trapping with a narrow line,"  The intercombination line of Strontium at 689nm is successfully used in laser
cooling to reach the photon recoil limit with Doppler cooling in a
magneto-optical traps (MOT). In this paper we present a systematic study of the
loading efficiency of such a MOT. Comparing the experimental results to a
simple model allows us to discuss the actual limitation of our apparatus. We
also study in detail the final MOT regime emphasizing the role of gravity on
the position, size and temperature along the vertical and horizontal
directions. At large laser detuning, one finds an unusual situation where
cooling and trapping occur in the presence of a high bias magnetic field.
"
phys,"New version announcement for TaylUR, an arbitrary-order diagonal
  automatic differentiation package for Fortran 95","  We present a new version of TaylUR, a Fortran 95 module to automatically
compute the numerical values of a complex-valued function's derivatives with
respect to several variables up to an arbitrary order in each variable, but
excluding mixed derivatives. The new version fixes a potentially serious bug in
the code for exponential-related functions that could corrupt the imaginary
parts of derivatives, as well as being compatible with a wider range of
compilers.
"
phys,"Effective conservation of energy and momentum algorithm using switching
  potentials suitable for molecular dynamics simulation of thermodynamical
  systems","  During a crossover via a switching mechanism from one 2-body potential to
another as might be applied in modeling (chemical) reactions in the vicinity of
bond formation, energy violations would occur due to finite step size which
determines the trajectory of the particles relative to the potential
interactions of the unbonded state by numerical (e.g. Verlet) integration. This
problem is overcome by an algorithm which preserves the coordinates of the
system for each move, but corrects for energy discrepancies by ensuring both
energy and momentum conservation in the dynamics. The algorithm is tested for a
hysteresis loop reaction model with an without the implementation of the
algorithm. The tests involve checking the rate of energy flow out of the MD
simulation box; in the equilibrium state, no net rate of flows within
experimental error should be observed. The temperature and pressure of the box
should also be invariant within the range of fluctuation of these quantities.
It is demonstrated that the algorithm satisfies these criteria.
"
phys,"Lattice Boltzmann inverse kinetic approach for the incompressible
  Navier-Stokes equations","  In spite of the large number of papers appeared in the past which are devoted
to the lattice Boltzmann (LB) methods, basic aspects of the theory still remain
unchallenged. An unsolved theoretical issue is related to the construction of a
discrete kinetic theory which yields \textit{exactly} the fluid equations,
i.e., is non-asymptotic (here denoted as \textit{LB inverse kinetic theory}).
The purpose of this paper is theoretical and aims at developing an inverse
kinetic approach of this type. In principle infinite solutions exist to this
problem but the freedom can be exploited in order to meet important
requirements. In particular, the discrete kinetic theory can be defined so that
it yields exactly the fluid equation also for arbitrary non-equilibrium (but
suitably smooth) kinetic distribution functions and arbitrarily close to the
boundary of the fluid domain. Unlike previous entropic LB methods the theorem
can be obtained without functional constraints on the class of the initial
distribution functions. Possible realizations of the theory and asymptotic
approximations are provided which permit to determine the fluid equations
\textit{with prescribed accuracy.} As a result, asymptotic accuracy estimates
of customary LB approaches and comparisons with the Chorin artificial
compressibility method are discussed.
"
phys,Pairwise comparisons of typological profiles (of languages),"  No abstract given; compares pairs of languages from World Atlas of Language
Structures.
"
phys,"Convergence of the discrete dipole approximation. I. Theoretical
  analysis","  We performed a rigorous theoretical convergence analysis of the discrete
dipole approximation (DDA). We prove that errors in any measured quantity are
bounded by a sum of a linear and quadratic term in the size of a dipole d, when
the latter is in the range of DDA applicability. Moreover, the linear term is
significantly smaller for cubically than for non-cubically shaped scatterers.
Therefore, for small d errors for cubically shaped particles are much smaller
than for non-cubically shaped. The relative importance of the linear term
decreases with increasing size, hence convergence of DDA for large enough
scatterers is quadratic in the common range of d. Extensive numerical
simulations were carried out for a wide range of d. Finally we discuss a number
of new developments in DDA and their consequences for convergence.
"
phys,A Multiphilic Descriptor for Chemical Reactivity and Selectivity,"  In line with the local philicity concept proposed by Chattaraj et al.
(Chattaraj, P. K.; Maiti, B.; Sarkar, U. J. Phys. Chem. A. 2003, 107, 4973) and
a dual descriptor derived by Toro-Labbe and coworkers (Morell, C.; Grand, A.;
Toro-Labbe, A. J. Phys. Chem. A. 2005, 109, 205), we propose a multiphilic
descriptor. It is defined as the difference between nucleophilic (Wk+) and
electrophilic (Wk-) condensed philicity functions. This descriptor is capable
of simultaneously explaining the nucleophilicity and electrophilicity of the
given atomic sites in the molecule. Variation of these quantities along the
path of a soft reaction is also analyzed. Predictive ability of this descriptor
has been successfully tested on the selected systems and reactions.
Corresponding force profiles are also analyzed in some representative cases.
Also, to study the intra- and intermolecular reactivities another related
descriptor namely, the nucleophilicity excess (DelW-+) for a nucleophile, over
the electrophilicity in it has been defined and tested on all-metal aromatic
compounds.
"
phys,A critical theory of quantum entanglement for the Hydrogen molecule,"  In this paper we investigate some entanglement properties for the Hydrogen
molecule considered as a two interacting spin 1/2 (qubit) model. The
entanglement related to the $H_{2}$ molecule is evaluated both using the von
Neumann entropy and the Concurrence and it is compared with the corresponding
quantities for the two interacting spin system. Many aspects of these functions
are examinated employing in part analytical and, essentially, numerical
techniques. We have compared analogous results obtained by Huang and Kais a few
years ago. In this respect, some possible controversial situations are
presented and discussed.
"
phys,"Transient behavior of surface plasmon polaritons scattered at a
  subwavelength groove","  We present a numerical study and analytical model of the optical near-field
diffracted in the vicinity of subwavelength grooves milled in silver surfaces.
The Green's tensor approach permits computation of the phase and amplitude
dependence of the diffracted wave as a function of the groove geometry. It is
shown that the field diffracted along the interface by the groove is equivalent
to replacing the groove by an oscillating dipolar line source. An analytic
expression is derived from the Green's function formalism, that reproduces well
the asymptotic surface plasmon polariton (SPP) wave as well as the transient
surface wave in the near-zone close to the groove. The agreement between this
model and the full simulation is very good, showing that the transient
""near-zone"" regime does not depend on the precise shape of the groove. Finally,
it is shown that a composite diffractive evanescent wave model that includes
the asymptotic SPP can describe the wavelength evolution in this transient
near-zone. Such a semi-analytical model may be useful for the design and
optimization of more elaborate photonic circuits whose behavior in large part
will be controlled by surface waves.
"
phys,"Compounding Fields and Their Quantum Equations in the Trigintaduonion
  Space","  The 32-dimensional compounding fields and their quantum interplays in the
trigintaduonion space can be presented by analogy with octonion and sedenion
electromagnetic, gravitational, strong and weak interactions. In the
trigintaduonion fields which are associated with the electromagnetic,
gravitational, strong and weak interactions, the study deduces some conclusions
of field source particles (quarks and leptons) and intermediate particles which
are consistent with current some sorts of interaction theories. In the
trigintaduonion fields which are associated with the hyper-strong and
strong-weak fields, the paper draws some predicts and conclusions of the field
source particles (sub-quarks) and intermediate particles. The research results
show that there may exist some new particles in the nature.
"
phys,"New possible properties of atomic nuclei investigated by non linear
  methods: Fractal and recurrence quantification analysis","  For the first time we apply the methodologies of nonlinear analysis to
investigate atomic matter. We use these methods in the analysis of Atomic
Weights and of Mass Number of atomic nuclei. Using the AutoCorrelation Function
and Mutual Information we establish the presence of nonlinear effects in the
mechanism of increasing mass of atomic nuclei considered as a function of the
atomic number. We find that increasing mass is divergent, possibly chaotic. We
also investigate the possible existence of a Power Law for atomic nuclei and,
using also the technique of the variogram, we conclude that a fractal regime
could superintend to the mechanism of increasing mass for nuclei. Finally,
using the Hurst exponent, evidence is obtained that the mechanism of increasing
mass in atomic nuclei is in the fractional Brownian regime. The most
interesting results are obtained by using Recurrence Quantification Analysis
(RQA). New recurrences, psudoperiodicities, self-resemblance and class of
self-similarities are identified with values of determinism showing oscillating
values indicating the presence of more or less stability during the process of
increasing mass of atomic nuclei. In brief, new regimes of regularities are
identified for atomic nuclei that deserve to be studied by future researches.
In particular an accurate analysis of binding energy values by nonlinear
methods is further required.
"
phys,Daemons and DAMA: Their Celestial-Mechanics Interrelations,"  The assumption of the capture by the Solar System of the electrically charged
Planckian DM objects (daemons) from the galactic disk is confirmed not only by
the St.Petersburg (SPb) experiments detecting particles with V<30 km/s. Here
the daemon approach is analyzed considering the positive model independent
result of the DAMA/NaI experiment. We explain the maximum in DAMA signals
observed in the May-June period to be associated with the formation behind the
Sun of a trail of daemons that the Sun captures into elongated orbits as it
moves to the apex. The range of significant 2-6-keV DAMA signals fits well the
iodine nuclei elastically knocked out of the NaI(Tl) scintillator by particles
falling on the Earth with V=30-50 km/s from strongly elongated heliocentric
orbits. The half-year periodicity of the slower daemons observed in SPb
originates from the transfer of particles that are deflected through ~90 deg
into near-Earth orbits each time the particles cross the outer reaches of the
Sun which had captured them. Their multi-loop (cross-like) trajectories
traverse many times the Earth's orbit in March and September, which increases
the probability for the particles to enter near-Earth orbits during this time.
Corroboration of celestial mechanics calculations with observations yields
~1e-19 cm2 for the cross section of daemon interaction with the solar matter.
"
phys,"Polymerization Force Driven Buckling of Microtubule Bundles Determines
  the Wavelength of Patterns Formed in Tubulin Solutions","  We present a model for the spontaneous formation of a striated pattern in
polymerizing microtubule solutions. It describes the buckling of a single
microtubule (MT) bundle within an elastic network formed by other similarly
aligned and buckling bundles and unaligned MTs. Phase contrast and polarization
microscopy studies of the temporal evolution of the pattern imply that the
polymerization of MTs within the bundles creates the driving compressional
force. Using the measured rate of buckling, the established MT force-velocity
curve and the pattern wavelength, we obtain reasonable estimates for the MT
bundle bending rigidity and the elastic constant of the network. The analysis
implies that the bundles buckle as solid rods.
"
phys,Approaching the Heisenberg limit in an atom laser,"  We present experimental and theoretical results showing the improved beam
quality and reduced divergence of an atom laser produced by an optical Raman
transition, compared to one produced by an RF transition. We show that Raman
outcoupling can eliminate the diverging lens effect that the condensate has on
the outcoupled atoms. This substantially improves the beam quality of the atom
laser, and the improvement may be greater than a factor of ten for experiments
with tight trapping potentials. We show that Raman outcoupling can produce atom
lasers whose quality is only limited by the wavefunction shape of the
condensate that produces them, typically a factor of 1.3 above the Heisenberg
limit.
"
phys,"Protein and ionic surfactants - promoters and inhibitors of contact line
  pinning","  We report a new effect of surfactants in pinning a drop contact line,
specifically that lysozyme promotes while lauryl sulfate inhibits pinning. We
explain the pinning disparity assuming difference in wetting: the protein-laden
drop wets a ""clean"" surface and the surfactant-laden drop wets an
auto-precursored surface.
"
phys,"Thermal decomposition of norbornane (bicyclo[2.2.1]heptane) dissolved in
  benzene. Experimental study and mechanism investigation","  The thermal decomposition of norbornane (dissolved in benzene) has been
studied in a jet stirred reactor at temperatures between 873 and 973 K, at
residence times ranging from 1 to 4 s and at atmospheric pressure, leading to
conversions from 0.04 to 22.6%. 25 reaction products were identified and
quantified by gas chromatography, amongst which the main ones are hydrogen,
ethylene and 1,3-cyclopentadiene. A mechanism investigation of the thermal
decomposition of the norbornane - benzene binary mixture has been performed.
Reactions involved in the mechanism have been reviewed: unimolecular
initiations 1 by C-C bond scission of norbornane, fate of the generated
diradicals, reactions of transfer and propagation of norbornyl radicals,
reactions of benzene and cross-coupling reactions.
"
phys,"Monitoring spatially heterogeneous dynamics in a drying colloidal thin
  film","  We report on a new type of experiment that enables us to monitor spatially
and temporally heterogeneous dynamic properties in complex fluids. Our approach
is based on the analysis of near-field speckles produced by light diffusely
reflected from the superficial volume of a strongly scattering medium. By
periodic modulation of an incident speckle beam we obtain pixel-wise ensemble
averages of the structure function coefficient, a measure of the dynamic
activity. To illustrate the application of our approach we follow the different
stages in the drying process of a colloidal thin film. We show that we can
access ensemble averaged dynamic properties on length scales as small as ten
micrometers over the full field of view.
"
phys,The Genetic Programming Collaboration Network and its Communities,"  Useful information about scientific collaboration structures and patterns can
be inferred from computer databases of published papers. The genetic
programming bibliography is the most complete reference of papers on GP\@. In
addition to locating publications, it contains coauthor and coeditor
relationships from which a more complete picture of the field emerges. We treat
these relationships as undirected small world graphs whose study reveals the
community structure of the GP collaborative social network. Automatic analysis
discovers new communities and highlights new facets of them. The investigation
reveals many similarities between GP and coauthorship networks in other
scientific fields but also some subtle differences such as a smaller central
network component and a high clustering.
"
phys,Effect of node deleting on network structure,"  The ever-increasing knowledge of the structure of various real-world networks
has uncovered their complex multi-mechanism-governed evolution processes.
Therefore, a better understanding of the structure and evolution of these
networked complex systems requires us to describe such processes in a more
detailed and realistic manner. In this paper, we introduce a new type of
network growth rule which comprises addition and deletion of nodes, and propose
an evolving network model to investigate the effect of node deleting on network
structure. It is found that, with the introduction of node deleting, network
structure is significantly transformed. In particular, degree distribution of
the network undergoes a transition from scale-free to exponential forms as the
intensity of node deleting increases. At the same time, nontrivial
disassortative degree correlation develops spontaneously as a natural result of
network evolution in the model. We also demonstrate that node deleting
introduced in the model does not destroy the connectedness of a growing network
so long as the increasing rate of edges is not excessively small. In addition,
it is found that node deleting will weaken but not eliminate the small-world
effect of a growing network, and generally it will decrease the clustering
coefficient in a network.
"
phys,"On the over-barrier reflection in quantum mechanics with multiple
  degrees of freedom","  We present an analytic example of two dimensional quantum mechanical system,
where the exponential suppression of the probability of over-barrier reflection
changes non-monotonically with energy. The suppression is minimal at certain
""optimal"" energies where reflection occurs with exponentially larger
probability than at other energies.
"
phys,Scalar potential model progress,"  Because observations of galaxies and clusters have been found inconsistent
with General Relativity (GR), the focus of effort in developing a Scalar
Potential Model (SPM) has been on the examination of galaxies and clusters. The
SPM has been found to be consistent with cluster cellular structure, the flow
of IGM from spiral galaxies to elliptical galaxies, intergalactic redshift
without an expanding universe, discrete redshift, rotation curve (RC) data
without dark matter, asymmetric RCs, galaxy central mass, galaxy central
velocity dispersion, and the Pioneer Anomaly. In addition, the SPM suggests a
model of past expansion, past contraction, and current expansion of the
universe. GR corresponds to the SPM in the limit in which the effect of the
Sources and Sinks approximate a flat scalar potential field such as between
clusters and on the solar system scale, which is small relative to the distance
to a Source.
"
phys,"Alternative Approaches to the Equilibrium Properties of Hard-Sphere
  Liquids","  An overview of some analytical approaches to the computation of the
structural and thermodynamic properties of single component and multicomponent
hard-sphere fluids is provided. For the structural properties, they yield a
thermodynamically consistent formulation, thus improving and extending the
known analytical results of the Percus-Yevick theory. Approximate expressions
for the contact values of the radial distribution functions and the
corresponding analytical equations of state are also discussed. Extensions of
this methodology to related systems, such as sticky hard spheres and
square-well fluids, as well as its use in connection with the perturbation
theory of fluids are briefly addressed.
"
phys,"Measurement of the Aerosol Phase Function at the Pierre Auger
  Observatory","  Air fluorescence detectors measure the energy of ultra-high energy cosmic
rays by collecting fluorescence light emitted from nitrogen molecules along the
extensive air shower cascade. To ensure a reliable energy determination, the
light signal needs to be corrected for atmospheric effects, which not only
attenuate the signal, but also produce a non-negligible background component
due to scattered Cherenkov light and multiple-scattered light. The correction
requires regular measurements of the aerosol attenuation length and the aerosol
phase function, defined as the probability of light scattered in a given
direction. At the Pierre Auger Observatory in Malargue, Argentina, the phase
function is measured on an hourly basis using two Aerosol Phase Function (APF)
light sources. These sources direct a UV light beam across the field of view of
the fluorescence detectors; the phase function can be extracted from the image
of the shots in the fluorescence detector cameras. This paper describes the
design, current status, standard operation procedure, and performance of the
APF system at the Pierre Auger Observatory.
"
phys,"Coupling of whispering-gallery modes in size-mismatched microdisk
  photonic molecules","  Mechanisms of whispering-gallery (WG) modes coupling in microdisk photonic
molecules (PMs) with slight and significant size mismatch are numerically
investigated. The results reveal two different scenarios of modes interaction
depending on the degree of this mismatch and offer new insight into how PM
parameters can be tuned to control and modify WG-modes wavelengths and
Q-factors. From a practical point of view, these findings offer a way to
fabricate PM microlaser structures that exhibit low thresholds and directional
emission, and at the same time are more tolerant to fabrication errors than
previously explored coupled-cavity structures composed of identical
microresonators.
"
phys,"Laser spectroscopy of hyperfine structure in highly-charged ions: a test
  of QED at high fields","  An overview is presented of laser spectroscopy experiments with cold,
trapped, highly-charged ions, which will be performed at the HITRAP facility at
GSI in Darmstadt (Germany). These high-resolution measurements of ground state
hyperfine splittings will be three orders of magnitude more precise than
previous measurements. Moreover, from a comparison of measurements of the
hyperfine splittings in hydrogen- and lithium-like ions of the same isotope,
QED effects at high electromagnetic fields can be determined within a few
percent. Several candidate ions suited for these laser spectroscopy studies are
presented.
"
phys,The Einstein-Varicak Correspondence on Relativistic Rigid Rotation,"  The historical significance of the problem of relativistic rigid rotation is
reviewed in light of recently published correspondence between Einstein and the
mathematician Vladimir Varicak from the years 1909 to 1913.
"
phys,Real Options for Project Schedules (ROPS),"  Real Options for Project Schedules (ROPS) has three recursive
sampling/optimization shells. An outer Adaptive Simulated Annealing (ASA)
optimization shell optimizes parameters of strategic Plans containing multiple
Projects containing ordered Tasks. A middle shell samples probability
distributions of durations of Tasks. An inner shell samples probability
distributions of costs of Tasks. PATHTREE is used to develop options on
schedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to
develop a relative risk analysis among projects.
"
phys,Failure of the work-Hamiltonian connection for free energy calculations,"  Extensions of statistical mechanics are routinely being used to infer free
energies from the work performed over single-molecule nonequilibrium
trajectories. A key element of this approach is the ubiquitous expression
dW/dt=\partial H(x,t)/ \partial t which connects the microscopic work W
performed by a time-dependent force on the coordinate x with the corresponding
Hamiltonian H(x,t) at time t. Here we show that this connection, as pivotal as
it is, cannot be used to estimate free energy changes. We discuss the
implications of this result for single-molecule experiments and atomistic
molecular simulations and point out possible avenues to overcome these
limitations.
"
phys,"Time and motion in physics: the Reciprocity Principle, relativistic
  invariance of the lengths of rulers and time dilatation","  Ponderable objects moving in free space according to Newton's First Law
constitute both rulers and clocks when one such object is viewed from the rest
frame of another. Together with the Reciprocity Principle this is used to
demonstrate, in both Galilean and special relativity, the invariance of the
measured length of a ruler in motion. The different times: `proper', `improper'
and `apparent' appearing in different formulations of the relativistic time
dilatation relation are discussed and exemplified by experimental applications.
A non-intuitive `length expansion' effect predicted by the Reciprocity
Principle as a necessary consequence of time dilatation is pointed out
"
phys,"Proposal for an Enhanced Optical Cooling System Test in an Electron
  Storage Ring","  We are proposing to test experimentally the new idea of Enhanced Optical
Cooling (EOC) in an electron storage ring. This experiment will confirm new
fundamental processes in beam physics and will demonstrate new unique
possibilities with this cooling technique. It will open important applications
of EOC in nuclear physics, elementary particle physics and in Light Sources
(LS) based on high brightness electron and ion beams.
"
phys,Equation of state for dense hydrogen and plasma phase transition,"  We calculate the equation of state of dense hydrogen within the chemical
picture. Fluid variational theory is generalized for a multi-component system
of molecules, atoms, electrons, and protons. Chemical equilibrium is supposed
for the reactions dissociation and ionization. We identify the region of
thermodynamic instability which is related to the plasma phase transition. The
reflectivity is calculated along the Hugoniot curve and compared with
experimental results. The equation-of-state data is used to calculate the
pressure and temperature profiles for the interior of Jupiter.
"
phys,Nova Geminorum 1912 and the Origin of the Idea of Gravitational Lensing,"  Einstein's early calculations of gravitational lensing, contained in a
scratch notebook and dated to the spring of 1912, are reexamined. A hitherto
unknown letter by Einstein suggests that he entertained the idea of explaining
the phenomenon of new stars by gravitational lensing in the fall of 1915 much
more seriously than was previously assumed. A reexamination of the relevant
calculations by Einstein shows that, indeed, at least some of them most likely
date from early October 1915. But in support of earlier historical
interpretation of Einstein's notes, it is argued that the appearance of Nova
Geminorum 1912 (DN Gem) in March 1912 may, in fact, provide a relevant context
and motivation for Einstein's lensing calculations on the occasion of his first
meeting with Erwin Freundlich during a visit in Berlin in April 1912. We also
comment on the significance of Einstein's consideration of gravitational
lensing in the fall of 1915 for the reconstruction of Einstein's final steps in
his path towards general relativity.
"
phys,"Rich methane premixed laminar flames doped by light unsaturated
  hydrocarbons - Part I : allene and propyne","  The structure of three laminar premixed rich flames has been investigated: a
pure methane flame and two methane flames doped by allene and propyne,
respectively. The gases of the three flames contain 20.9% (molar) of methane
and 33.4% of oxygen, corresponding to an equivalence ratio of 1.25 for the pure
methane flame. In both doped flames, 2.49% of C3H4 was added, corresponding to
a ratio C3H4/CH4 of 12% and an equivalence ratio of 1.55. The three flames have
been stabilized on a burner at a pressure of 6.7 kPa using argon as dilutant,
with a gas velocity at the burner of 36 cm/s at 333 K. The concentration
profiles of stable species were measured by gas chromatography after sampling
with a quartz microprobe. Quantified species included carbon monoxide and
dioxide, methane, oxygen, hydrogen, ethane, ethylene, acetylene, propyne,
allene, propene, propane, 1,2-butadiene, 1,3-butadiene, 1-butene, isobutene,
1-butyne, vinylacetylene, and benzene. The temperature was measured using a
PtRh (6%)-PtRh (30%) thermocouple settled inside the enclosure and ranged from
700 K close to the burner up to 1850 K. In order to model these new results,
some improvements have been made to a mechanism previously developed in our
laboratory for the reactions of C3-C4 unsaturated hydrocarbons. The main
reaction pathways of consumption of allene and propyne and of formation of C6
aromatic species have been derived from flow rate analyses.
"
phys,A non-perturbative proof of Bertrand's theorem,"  We discuss an alternative non-perturbative proof of Bertrand's theorem that
leads in a concise way directly to the two allowed fields: the newtonian and
the isotropic harmonic oscillator central fields.
"
phys,"The discrete dipole approximation for simulation of light scattering by
  particles much larger than the wavelength","  In this manuscript we investigate the capabilities of the Discrete Dipole
Approximation (DDA) to simulate scattering from particles that are much larger
than the wavelength of the incident light, and describe an optimized publicly
available DDA computer program that processes the large number of dipoles
required for such simulations. Numerical simulations of light scattering by
spheres with size parameters x up to 160 and 40 for refractive index m=1.05 and
2 respectively are presented and compared with exact results of the Mie theory.
Errors of both integral and angle-resolved scattering quantities generally
increase with m and show no systematic dependence on x. Computational times
increase steeply with both x and m, reaching values of more than 2 weeks on a
cluster of 64 processors. The main distinctive feature of the computer program
is the ability to parallelize a single DDA simulation over a cluster of
computers, which allows it to simulate light scattering by very large
particles, like the ones that are considered in this manuscript. Current
limitations and possible ways for improvement are discussed.
"
phys,Stock market return distributions: from past to present,"  We show that recent stock market fluctuations are characterized by the
cumulative distributions whose tails on short, minute time scales exhibit power
scaling with the scaling index alpha > 3 and this index tends to increase
quickly with decreasing sampling frequency. Our study is based on
high-frequency recordings of the S&P500, DAX and WIG20 indices over the
interval May 2004 - May 2006. Our findings suggest that dynamics of the
contemporary market may differ from the one observed in the past. This effect
indicates a constantly increasing efficiency of world markets.
"
phys,Extraction of physical laws from joint experimental data,"  The extraction of a physical law y=yo(x) from joint experimental data about x
and y is treated. The joint, the marginal and the conditional probability
density functions (PDF) are expressed by given data over an estimator whose
kernel is the instrument scattering function. As an optimal estimator of yo(x)
the conditional average is proposed. The analysis of its properties is based
upon a new definition of prediction quality. The joint experimental information
and the redundancy of joint measurements are expressed by the relative entropy.
With the number of experiments the redundancy on average increases, while the
experimental information converges to a certain limit value. The difference
between this limit value and the experimental information at a finite number of
data represents the discrepancy between the experimentally determined and the
true properties of the phenomenon. The sum of the discrepancy measure and the
redundancy is utilized as a cost function. By its minimum a reasonable number
of data for the extraction of the law yo(x) is specified. The mutual
information is defined by the marginal and the conditional PDFs of the
variables. The ratio between mutual information and marginal information is
used to indicate which variable is the independent one. The properties of the
introduced statistics are demonstrated on deterministically and randomly
related variables.
"
phys,Semi-spheroidal Quantum Harmonic Oscillator,"  A new single-particle shell model is derived by solving the Schr\""odinger
equation for a semi-spheroidal potential well. Only the negative parity states
of the $Z(z)$ component of the wave function are allowed, so that new magic
numbers are obtained for oblate semi-spheroids, semi-sphere and prolate
semi-spheroids. The semi-spherical magic numbers are identical with those
obtained at the oblate spheroidal superdeformed shape: 2, 6, 14, 26, 44, 68,
100, 140, ... The superdeformed prolate magic numbers of the semi-spheroidal
shape are identical with those obtained at the spherical shape of the
spheroidal harmonic oscillator: 2, 8, 20, 40, 70, 112, 168 ...
"
phys,"Convergence of the discrete dipole approximation. II. An extrapolation
  technique to increase the accuracy","  We propose an extrapolation technique that allows accuracy improvement of the
discrete dipole approximation computations. The performance of this technique
was studied empirically based on extensive simulations for 5 test cases using
many different discretizations. The quality of the extrapolation improves with
refining discretization reaching extraordinary performance especially for
cubically shaped particles. A two order of magnitude decrease of error was
demonstrated. We also propose estimates of the extrapolation error, which were
proven to be reliable. Finally we propose a simple method to directly separate
shape and discretization errors and illustrated this for one test case.
"
phys,Photon splitting in a laser field,"  Photon splitting due to vacuum polarization in a laser field is considered.
Using an operator technique, we derive the amplitudes for arbitrary strength,
spectral content and polarization of the laser field. The case of a
monochromatic circularly polarized laser field is studied in detail and the
amplitudes are obtained as three-fold integrals. The asymptotic behavior of the
amplitudes for various limits of interest are investigated also in the case of
a linearly polarized laser field. Using the obtained results, the possibility
of experimental observation of the process is discussed.
"
phys,"A Rigorous Time-Domain Analysis of Full--Wave Electromagnetic Cloaking
  (Invisibility)","  There is currently a great deal of interest in the theoretical and practical
possibility of cloaking objects from the observation by electromagnetic waves.
The basic idea of these invisibility devices \cite{glu1, glu2, le},\cite{pss1}
is to use anisotropic {\it transformation media} whose permittivity and
permeability $\var^{\lambda\nu}, \mu^{\lambda\nu}$, are obtained from the ones,
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$, of isotropic media, by singular
transformations of coordinates. In this paper we study electromagnetic cloaking
in the time-domain using the formalism of time-dependent scattering theory.
This formalism allows us to settle in an unambiguous way the mathematical
problems posed by the singularities of the inverse of the permittivity and the
permeability of the {\it transformation media} on the boundary of the cloaked
objects. We write Maxwell's equations in Schr\""odinger form with the
electromagnetic propagator playing the role of the Hamiltonian. We prove that
the electromagnetic propagator outside of the cloaked objects is essentially
self-adjoint. Moreover, the unique self-adjoint extension is unitarily
equivalent to the electromagnetic propagator in the medium
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$. Using this fact, and since the
coordinate transformation is the identity outside of a ball, we prove that the
scattering operator is the identity. Our results give a rigorous proof that the
construction of \cite{glu1, glu2, le}, \cite{pss1} perfectly cloaks passive and
active devices from observation by electromagnetic waves. Furthermore, we prove
cloaking for general anisotropic materials. In particular, our results prove
that it is possible to cloak objects inside general crystals.
"
phys,"Evolutionary Neural Gas (ENG): A Model of Self Organizing Network from
  Input Categorization","  Despite their claimed biological plausibility, most self organizing networks
have strict topological constraints and consequently they cannot take into
account a wide range of external stimuli. Furthermore their evolution is
conditioned by deterministic laws which often are not correlated with the
structural parameters and the global status of the network, as it should happen
in a real biological system. In nature the environmental inputs are noise
affected and fuzzy. Which thing sets the problem to investigate the possibility
of emergent behaviour in a not strictly constrained net and subjected to
different inputs. It is here presented a new model of Evolutionary Neural Gas
(ENG) with any topological constraints, trained by probabilistic laws depending
on the local distortion errors and the network dimension. The network is
considered as a population of nodes that coexist in an ecosystem sharing local
and global resources. Those particular features allow the network to quickly
adapt to the environment, according to its dimensions. The ENG model analysis
shows that the net evolves as a scale-free graph, and justifies in a deeply
physical sense- the term gas here used.
"
phys,"Gravity-induced electric polarization of matter and planetary magnetic
  fields","  This paper has been withdrawn due to copyright reasons.
"
phys,Polarization conversion in a silica microsphere,"  We experimentally demonstrate controlled polarization-selective phenomena in
a whispering gallery mode resonator. We observed efficient ($\approx 75 %$)
polarization conversion of light in a silica microsphere coupled to a tapered
optical fiber with proper optimization of the polarization of the propagating
light. A simple model treating the microsphere as a ring resonator provides a
good fit to the observed behavior.
"
phys,Search for Chaotic Behavior in a Flapping Flag,"  We measured the correlation of the times between successive flaps of a flag
for a variety of wind speeds and found no evidence of low dimensional chaotic
behavior in the return maps of these times. We instead observed what is best
modeled as random times determined by an exponential distribution. This study
was done as an undergraduate experiment and illustrates the differences between
low dimensional chaotic and possibly higher dimensional chaotic systems.
"
phys,"Polarization properties of subwavelength hole arrays consisting of
  rectangular holes","  Influence of hole shape on extraordinary optical transmission was
investigated using hole arrays consisting of rectangular holes with different
aspect ratio. It was found that the transmission could be tuned continuously by
rotating the hole array. Further more, a phase was generated in this process,
and linear polarization states could be changed to elliptical polarization
states. This phase was correlated with the aspect ratio of the holes. An
intuitional model was presented to explain these results.
"
phys,Estimation of experimental data redundancy and related statistics,"  Redundancy of experimental data is the basic statistic from which the
complexity of a natural phenomenon and the proper number of experiments needed
for its exploration can be estimated. The redundancy is expressed by the
entropy of information pertaining to the probability density function of
experimental variables. Since the calculation of entropy is inconvenient due to
integration over a range of variables, an approximate expression for redundancy
is derived that includes only a sum over the set of experimental data about
these variables. The approximation makes feasible an efficient estimation of
the redundancy of data along with the related experimental information and
information cost function. From the experimental information the complexity of
the phenomenon can be simply estimated, while the proper number of experiments
needed for its exploration can be determined from the minimum of the cost
function. The performance of the approximate estimation of these statistics is
demonstrated on two-dimensional normally distributed random data.
"
phys,Visualizing Teleportation,"  A novel way of picturing the processing of quantum information is described,
allowing a direct visualization of teleportation of quantum states and
providing a simple and intuitive understanding of this fascinating phenomenon.
The discussion is aimed at providing physicists a method of explaining
teleportation to non-scientists. The basic ideas of quantum physics are first
explained in lay terms, after which these ideas are used with a graphical
description, out of which teleportation arises naturally.
"
phys,"Nonlinear Dynamics of the Phonon Stimulated Emission in Microwave
  Solid-State Resonator of the Nonautonomous Phaser Generator","  The microwave phonon stimulated emission (SE) has been experimentally and
numerically investigated in a nonautonomous microwave acoustic quantum
generator, called also microwave phonon laser or phaser (see previous works
arXiv:cond-mat/0303188 ; arXiv:cond-mat/0402640 ; arXiv:nlin.CG/0703050)
Phenomena of branching and long-time refractority (absence of the reaction on
the external pulses) for deterministic chaotic and regular processes of SE were
observed in experiments with various levels of electromagnetic pumping. At the
pumping level growth, the clearly depined increasing of the number of
coexisting SE states has been observed both in real physical experiments and in
computer simulations. This confirms the analytical estimations of the branching
density in the phase space. The nature of the refractority of SE pulses is
closely connected with the pointed branching and reflects the crises of strange
attractors, i.e. their collisions with unstable periodic components of the
higher branches of SE states in the nonautonomous microwave phonon laser.
"
phys,"Analysis of the real estate market in Las Vegas: Bubble, seasonal
  patterns, and prediction of the CSW indexes","  We analyze 27 house price indexes of Las Vegas from Jun. 1983 to Mar. 2005,
corresponding to 27 different zip codes. These analyses confirm the existence
of a real-estate bubble, defined as a price acceleration faster than
exponential, which is found however to be confined to a rather limited time
interval in the recent past from approximately 2003 to mid-2004 and has
progressively transformed into a more normal growth rate comparable to
pre-bubble levels in 2005. There has been no bubble till 2002 except for a
medium-sized surge in 1990. In addition, we have identified a strong yearly
periodicity which provides a good potential for fine-tuned prediction from
month to month. A monthly monitoring using a model that we have developed could
confirm, by testing the intra-year structure, if indeed the market has returned
to ``normal'' or if more turbulence is expected ahead. We predict the evolution
of the indexes one year ahead, which is validated with new data up to Sep.
2006. The present analysis demonstrates the existence of very significant
variations at the local scale, in the sense that the bubble in Las Vegas seems
to have preceded the more global USA bubble and has ended approximately two
years earlier (mid 2004 for Las Vegas compared with mid-2006 for the whole of
the USA).
"
phys,"The evolution of the Earth-Moon system based on the dark matter field
  fluid model","  The evolution of Earth-Moon system is described by the dark matter field
fluid model proposed in the Meeting of Division of Particle and Field 2004,
American Physical Society. The current behavior of the Earth-Moon system agrees
with this model very well and the general pattern of the evolution of the
Moon-Earth system described by this model agrees with geological and fossil
evidence. The closest distance of the Moon to Earth was about 259000 km at 4.5
billion years ago, which is far beyond the Roche's limit. The result suggests
that the tidal friction may not be the primary cause for the evolution of the
Earth-Moon system. The average dark matter field fluid constant derived from
Earth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts
that the Mars's rotation is also slowing with the angular acceleration rate
about -4.38 x 10^(-22) rad s^(-2).
"
phys,"Approximate Selection Rule for Orbital Angular Momentum in Atomic
  Radiative Transitions","  We demonstrate that radiative transitions with \Delta l = - 1 are strongly
dominating for all values of n and l, except small region where l << n.
"
phys,Frequency modulation Fourier transform spectroscopy,"  A new method, FM-FTS, combining Frequency Modulation heterodyne laser
spectroscopy and Fourier Transform Spectroscopy is presented. It provides
simultaneous sensitive measurement of absorption and dispersion profiles with
broadband spectral coverage capabilities. Experimental demonstration is made on
the overtone spectrum of C2H2 in the 1.5 $\mu$m region.
"
phys,"Reciprocal Symmetry and Classical Discrete Oscillator Incorporating
  Half-Integral Energy Levels","  Classical oscillator differential equation is replaced by the corresponding
(finite time) difference equation. The equation is, then, symmetrized so that
it remains invariant under the change d going to -d, where d is the smallest
span of time. This symmetric equation has solutions, which come in reciprocally
related pairs. One member of a pair agrees with the classical solution and the
other is an oscillating solution and does not converge to a limit as d goes to
0. This solution contributes to oscillator energy a term which is a multiple of
half-integers.
"
phys,"Local-field effects in radiatively broadened magneto-dielectric media:
  negative refraction and absorption reduction","  We give a microscopic derivation of the Clausius-Mossotti relations for a
homogeneous and isotropic magneto-dielectric medium consisting of radiatively
broadened atomic oscillators. To this end the diagram series of electromagnetic
propagators is calculated exactly for an infinite bi-cubic lattice of
dielectric and magnetic dipoles for a lattice constant small compared to the
resonance wavelength $\lambda$. Modifications of transition frequencies and
linewidth of the elementary oscillators are taken into account in a
selfconsistent way by a proper incorporation of the singular self-interaction
terms. We show that in radiatively broadened media sufficiently close to the
free-space resonance the real part of the index of refraction approaches the
value -2 in the limit of $\rho \lambda^3 \gg 1$, where $\rho$ is the number
density of scatterers. Since at the same time the imaginary part vanishes as
$1/\rho$ local field effects can have important consequences for realizing
low-loss negative index materials.
"
phys,Shocks in nonlocal media,"  We investigate the formation of collisionless shocks along the spatial
profile of a gaussian laser beam propagating in nonlocal nonlinear media. For
defocusing nonlinearity the shock survives the smoothing effect of the nonlocal
response, though its dynamics is qualitatively affected by the latter, whereas
for focusing nonlinearity it dominates over filamentation. The patterns
observed in a thermal defocusing medium are interpreted in the framework of our
theory.
"
phys,"Astrophysical gyrokinetics: kinetic and fluid turbulent cascades in
  magnetized weakly collisional plasmas","  We present a theoretical framework for plasma turbulence in astrophysical
plasmas (solar wind, interstellar medium, galaxy clusters, accretion disks).
The key assumptions are that the turbulence is anisotropic with respect to the
mean magnetic field and frequencies are low compared to the ion cyclotron
frequency. The energy injected at the outer scale scale has to be converted
into heat, which ultimately cannot be done without collisions. A KINETIC
CASCADE develops that brings the energy to collisional scales both in space and
velocity. Its nature depends on the physics of plasma fluctuations. In each of
the physically distinct scale ranges, the kinetic problem is systematically
reduced to a more tractable set of equations. In the ""inertial range"" above the
ion gyroscale, the kinetic cascade splits into a cascade of Alfvenic
fluctuations, which are governed by the RMHD equations at both the collisional
and collisionless scales, and a passive cascade of compressive fluctuations,
which obey a linear kinetic equation along the moving field lines associated
with the Alfvenic component. In the ""dissipation range"" between the ion and
electron gyroscales, there are again two cascades: the kinetic-Alfven-wave
(KAW) cascade governed by two fluid-like Electron RMHD equations and a passive
phase-space cascade of ion entropy fluctuations. The latter cascade brings the
energy of the inertial-range fluctuations that was damped by collisionless
wave-particle interaction at the ion gyroscale to collisional scales in the
phase space and leads to ion heating. The KAW energy is similarly damped at the
electron gyroscale and converted into electron heat. Kolmogorov-style scaling
relations are derived for these cascades. Astrophysical and space-physical
applications are discussed in detail.
"
phys,Electromagnetic wormholes via handlebody constructions,"  Cloaking devices are prescriptions of electrostatic, optical or
electromagnetic parameter fields (conductivity $\sigma(x)$, index of refraction
$n(x)$, or electric permittivity $\epsilon(x)$ and magnetic permeability
$\mu(x)$) which are piecewise smooth on $\mathbb R^3$ and singular on a
hypersurface $\Sigma$, and such that objects in the region enclosed by $\Sigma$
are not detectable to external observation by waves. Here, we give related
constructions of invisible tunnels, which allow electromagnetic waves to pass
between possibly distant points, but with only the ends of the tunnels visible
to electromagnetic imaging. Effectively, these change the topology of space
with respect to solutions of Maxwell's equations, corresponding to attaching a
handlebody to $\mathbb R^3$. The resulting devices thus function as
electromagnetic wormholes.
"
phys,Some new experimental photonic flame effect features,"  The results of the spectral, energetical and temporal characteristics of
radiation in the presence of the photonic flame effect are presented.
Artificial opal posed on Cu plate at the temperature of liquid nitrogen boiling
point (77 K) being irradiated by nanosecond ruby laser pulse produces long-
term luminiscence with a duration till ten seconds with a finely structured
spectrum in the the antistocks part of the spectrum. Analogous visible
luminescence manifesting time delay appeared in other samples of the artificial
opals posed on the same plate. In the case of the opal infiltrated with
different nonlinear liquids the threshold of the luminiscence is reduced and
the spatial disribution of the bright emmiting area on the opal surface is
being changed. In the case of the putting the frozen nonlinear liquids on the
Cu plate long-term blue bright luminiscence took place in the frozen species of
the liquids. Temporal characteristics of this luminiscence are nearly the same
as in opal matrixes.
"
phys,"Birth, survival and death of languages by Monte Carlo simulation","  Simulations of physicists for the competition between adult languages since
2003 are reviewed. How many languages are spoken by how many people? How many
languages are contained in various language families? How do language
similarities decay with geographical distance, and what effects do natural
boundaries have? New simulations of bilinguality are given in an appendix.
"
phys,"Two-scale structure of the electron dissipation region during
  collisionless magnetic reconnection","  Particle in cell (PIC) simulations of collisionless magnetic reconnection are
presented that demonstrate that the electron dissipation region develops a
distinct two-scale structure along the outflow direction. The length of the
electron current layer is found to decrease with decreasing electron mass,
approaching the ion inertial length for a proton-electron plasma. A surprise,
however, is that the electrons form a high-velocity outflow jet that remains
decoupled from the magnetic field and extends large distances downstream from
the x-line. The rate of reconnection remains fast in very large systems,
independent of boundary conditions and the mass of electrons.
"
phys,"A general approach to statistical modeling of physical laws:
  nonparametric regression","  Statistical modeling of experimental physical laws is based on the
probability density function of measured variables. It is expressed by
experimental data via a kernel estimator. The kernel is determined objectively
by the scattering of data during calibration of experimental setup. A physical
law, which relates measured variables, is optimally extracted from experimental
data by the conditional average estimator. It is derived directly from the
kernel estimator and corresponds to a general nonparametric regression. The
proposed method is demonstrated by the modeling of a return map of noisy
chaotic data. In this example, the nonparametric regression is used to predict
a future value of chaotic time series from the present one. The mean predictor
error is used in the definition of predictor quality, while the redundancy is
expressed by the mean square distance between data points. Both statistics are
used in a new definition of predictor cost function. From the minimum of the
predictor cost function, a proper number of data in the model is estimated.
"
phys,Resonant activation in bistable semiconductor lasers,"  We theoretically investigate the possibility of observing resonant activation
in the hopping dynamics of two-mode semiconductor lasers. We present a series
of simulations of a rate-equations model under random and periodic modulation
of the bias current. In both cases, for an optimal choice of the modulation
time-scale, the hopping times between the stable lasing modes attain a minimum.
The simulation data are understood by means of an effective one-dimensional
Langevin equation with multiplicative fluctuations. Our conclusions apply to
both Edge Emitting and Vertical Cavity Lasers, thus opening the way to several
experimental tests in such optical systems.
"
phys,Intricate Knots in Proteins: Function and Evolution,"  A number of recently discovered protein structures incorporate a rather
unexpected structural feature: a knot in the polypeptide backbone. These knots
are extremely rare, but their occurrence is likely connected to protein
function in as yet unexplored fashion. Our analysis of the complete Protein
Data Bank reveals several new knots which, along with previously discovered
ones, can shed light on such connections. In particular, we identify the most
complex knot discovered to date in human ubiquitin hydrolase, and suggest that
its entangled topology protects it against unfolding and degradation by the
proteasome. Knots in proteins are typically preserved across species and
sometimes even across kingdoms. However, we also identify a knot which only
appears in some transcarbamylases while being absent in homologous proteins of
similar structure. The emergence of the knot is accompanied by a shift in the
enzymatic function of the protein. We suggest that the simple insertion of a
short DNA fragment into the gene may suffice to turn an unknotted into a
knotted structure in this protein.
"
phys,Universal Forces and the Dark Energy Problem,"  The Dark Energy problem is forcing us to re-examine our models and our
understanding of relativity and space-time. Here a novel idea of Fundamental
Forces is introduced. This allows us to perceive the General Theory of
Relativity and Einstein's Equation from a new pesrpective. In addition to
providing us with an improved understanding of space and time, it will be shown
how it leads to a resolution of the Dark Energy problem.
"
phys,Vacuum Structure and Potential,"  Based on overall experimental observations, especially the pair processes, I
developed a model structure of the vacuum along with a basic-particle formation
scheme begun in 2000 (with collaborator P-I Johansson). The model consists in
that the vacuum is, briefly, filled of neutral but polarizable vacuuons,
consisting each of a p-vaculeon and n- vaculeon of charges $+e$ and $-e$ of
zero rest masses but with spin motions, assumed interacting each other with a
Coulomb force. The model has been introduced in full in a book (Nova Sci, 2005)
and referred to in a number of journal/E-print papers. I outline in this easier
accessible paper the detailed derivation of the model and a corresponding
quantitative determination of the vacuuon size.
"
phys,A POVM view of the ensemble approach to polarization optics,"  Statistical ensemble formalism of Kim, Mandel and Wolf (J. Opt. Soc. Am. A 4,
433 (1987)) offers a realistic model for characterizing the effect of
stochastic non-image forming optical media on the state of polarization of
transmittedlight. With suitable choice of the Jones ensemble, various Mueller
transformations - some of which have been unknown so far - are deduced. It is
observed that the ensemble approach is formally identical to the positive
operator valued measures (POVM) on the quantum density matrix. This
observation, in combination with the recent suggestion by Ahnert and Payne
(Phys. Rev. A 71, 012330, (2005)) - in the context of generalized quantum
measurement on single photon polarization states - that linear optics elements
can be employed in setting up all possible POVMs, enables us to propose a way
of realizing different types of Mueller devices.
"
phys,Quantum electromagnetic X-waves,"  We show that two distinct quantum states of the electromagnetic field can be
associated to a classical vector X wave or a propagation-invariant solution of
Maxwell equations. The difference between the two states is of pure quantum
mechanical origin since they are internally entangled and disentangled,
respectively and can be generated by different linear or nonlinear processes.
Detection and generation of Schr\""odinger-cat states comprising two entangled
X-waves and their possible applications are discussed.
"
phys,"Robust manipulation of electron spin coherence in an ensemble of singly
  charged quantum dots","  Using the recently reported mode locking effect we demonstrate a highly
robust control of electron spin coherence in an ensemble of (In,Ga)As quantum
dots during the single spin coherence time. The spin precession in a transverse
magnetic field can be fully controlled up to 25 K by the parameters of the
exciting pulsed laser protocol such as the pulse train sequence, leading to
adjustable quantum beat bursts in Faraday rotation. Flipping of the electron
spin precession phase was demonstrated by inverting the polarization within a
pulse doublet sequence.
"
phys,"General System theory, Like-Quantum Semantics and Fuzzy Sets","  It is outlined the possibility to extend the quantum formalism in relation to
the requirements of the general systems theory. It can be done by using a
quantum semantics arising from the deep logical structure of quantum theory. It
is so possible taking into account the logical openness relationship between
observer and system. We are going to show how considering the truth-values of
quantum propositions within the context of the fuzzy sets is here more useful
for systemics . In conclusion we propose an example of formal quantum
coherence.
"
phys,"Detailed kinetic study of the ring opening of cycloalkanes by CBS-QB3
  calculations","  This work reports a theoretical study of the gas phase unimolecular
decomposition of cyclobutane, cyclopentane and cyclohexane by means of quantum
chemical calculations. A biradical mechanism has been envisaged for each
cycloalkane, and the main routes for the decomposition of the biradicals formed
have been investigated at the CBS-QB3 level of theory. Thermochemical data
(\delta H^0_f, S^0, C^0_p) for all the involved species have been obtained by
means of isodesmic reactions. The contribution of hindered rotors has also been
included. Activation barriers of each reaction have been analyzed to assess the
1 energetically most favorable pathways for the decomposition of biradicals.
Rate constants have been derived for all elementary reactions using transition
state theory at 1 atm and temperatures ranging from 600 to 2000 K. Global rate
constant for the decomposition of the cyclic alkanes in molecular products have
been calculated. Comparison between calculated and experimental results allowed
to validate the theoretical approach. An important result is that the
rotational barriers between the conformers, which are usually neglected, are of
importance in decomposition rate of the largest biradicals. Ring strain
energies (RSE) in transition states for ring opening have been estimated and
show that the main part of RSE contained in the cyclic reactants is removed
upon the activation process.
"
phys,Long-range correlation and multifractality in Bach's Inventions pitches,"  We show that it can be considered some of Bach pitches series as a stochastic
process with scaling behavior. Using multifractal deterend fluctuation analysis
(MF-DFA) method, frequency series of Bach pitches have been analyzed. In this
view we find same second moment exponents (after double profiling) in ranges
(1.7-1.8) in his works. Comparing MF-DFA results of original series to those
for shuffled and surrogate series we can distinguish multifractality due to
long-range correlations and a broad probability density function. Finally we
determine the scaling exponents and singularity spectrum. We conclude fat tail
has more effect in its multifractality nature than long-range correlations.
"
phys,"Molecular Synchronization Waves in Arrays of Allosterically Regulated
  Enzymes","  Spatiotemporal pattern formation in a product-activated enzymic reaction at
high enzyme concentrations is investigated. Stochastic simulations show that
catalytic turnover cycles of individual enzymes can become coherent and that
complex wave patterns of molecular synchronization can develop. The analysis
based on the mean-field approximation indicates that the observed patterns
result from the presence of Hopf and wave bifurcations in the considered
system.
"
phys,Experimental modeling of physical laws,"  A physical law is represented by the probability distribution of a measured
variable. The probability density is described by measured data using an
estimator whose kernel is the instrument scattering function. The experimental
information and data redundancy are defined in terms of information entropy.
The model cost function, comprised of data redundancy and estimation error, is
minimized by the creation-annihilation process.
"
phys,Modeling the field of laser welding melt pool by RBFNN,"  Efficient control of a laser welding process requires the reliable prediction
of process behavior. A statistical method of field modeling, based on
normalized RBFNN, can be successfully used to predict the spatiotemporal
dynamics of surface optical activity in the laser welding process. In this
article we demonstrate how to optimize RBFNN to maximize prediction quality.
Special attention is paid to the structure of sample vectors, which represent
the bridge between the field distributions in the past and future.
"
phys,Collective behavior of stock price movements in an emerging market,"  To investigate the universality of the structure of interactions in different
markets, we analyze the cross-correlation matrix C of stock price fluctuations
in the National Stock Exchange (NSE) of India. We find that this emerging
market exhibits strong correlations in the movement of stock prices compared to
developed markets, such as the New York Stock Exchange (NYSE). This is shown to
be due to the dominant influence of a common market mode on the stock prices.
By comparison, interactions between related stocks, e.g., those belonging to
the same business sector, are much weaker. This lack of distinct sector
identity in emerging markets is explicitly shown by reconstructing the network
of mutually interacting stocks. Spectral analysis of C for NSE reveals that,
the few largest eigenvalues deviate from the bulk of the spectrum predicted by
random matrix theory, but they are far fewer in number compared to, e.g., NYSE.
We show this to be due to the relative weakness of intra-sector interactions
between stocks, compared to the market mode, by modeling stock price dynamics
with a two-factor model. Our results suggest that the emergence of an internal
structure comprising multiple groups of strongly coupled components is a
signature of market development.
"
phys,The discrete dipole approximation: an overview and recent developments,"  We present a review of the discrete dipole approximation (DDA), which is a
general method to simulate light scattering by arbitrarily shaped particles. We
put the method in historical context and discuss recent developments, taking
the viewpoint of a general framework based on the integral equations for the
electric field. We review both the theory of the DDA and its numerical aspects,
the latter being of critical importance for any practical application of the
method. Finally, the position of the DDA among other methods of light
scattering simulation is shown and possible future developments are discussed.
"
phys,On the dragging of light by a rotating medium,"  When light is passing through a rotating medium the optical polarisation is
rotated. Recently it has been reasoned that this rotation applies also to the
transmitted image (Padgett et al. 2006). We examine these two phenomena by
extending an analysis of Player (1976) to general electromagnetic fields. We
find that in this more general case the wave equation inside the rotating
medium has to be amended by a term which is connected to the orbital angular
momentum of the light. We show that optical spin and orbital angular momentum
account respectively for the rotation of the polarisation and the rotation of
the transmitted image.
"
phys,Intelligent Life in Cosmology,"  I shall present three arguments for the proposition that intelligent life is
very rare in the universe. First, I shall summarize the consensus opinion of
the founders of the Modern Synthesis (Simpson, Dobzhanski, and Mayr) that the
evolution of intelligent life is exceedingly improbable. Second, I shall
develop the Fermi Paradox: if they existed they'd be here. Third, I shall show
that if intelligent life were too common, it would use up all available
resources and die out. But I shall show that the quantum mechanical principle
of unitarity (actually a form of teleology!) requires intelligent life to
survive to the end of time. Finally, I shall argue that, if the universe is
indeed accelerating, then survival to the end of time requires that intelligent
life, though rare, to have evolved several times in the visible universe. I
shall argue that the acceleration is a consequence of the excess of matter over
antimatter in the universe. I shall suggest experiments to test these claims.
"
phys,"On thermal effects in solid state lasers: the case of ytterbium-doped
  materials","  A review of theoretical and experimental studies of thermal effects in
solid-state lasers is presented, with a special focus on diode-pumped
ytterbium-doped materials. A large part of this review provides however general
information applicable to any kind of solid-state laser. Our aim here is not to
make a list of the techniques that have been used to minimize thermal effects,
but instead to give an overview of the theoretical aspects underneath, and give
a state-of-the-art of the tools at the disposal of the laser scientist to
measure thermal effects. After a presentation of some general properties of
Yb-doped materials, we address the issue of evaluating the temperature map in
Yb-doped laser crystals, both theoretically and experimentally. This is the
first step before studying the complex problem of thermal lensing (part III).
We will focus on some newly discussed aspects, like the definition of the
thermo-optic coefficient: we will highlight some misleading interpretations of
thermal lensing experiments due to the use of the dn/dT parameter in a context
where it is not relevant. Part IV will be devoted to a state-of-the-art of
experimental techniques used to measure thermal lensing. Eventually, in part V,
we will give some concrete examples in Yb-doped materials, where their
peculiarities will be pointed out.
"
phys,"Formation of density singularities in ideal hydrodynamics of freely
  cooling inelastic gases: a family of exact solutions","  We employ granular hydrodynamics to investigate a paradigmatic problem of
clustering of particles in a freely cooling dilute granular gas. We consider
large-scale hydrodynamic motions where the viscosity and heat conduction can be
neglected, and one arrives at the equations of ideal gas dynamics with an
additional term describing bulk energy losses due to inelastic collisions. We
employ Lagrangian coordinates and derive a broad family of exact non-stationary
analytical solutions that depend only on one spatial coordinate. These
solutions exhibit a new type of singularity, where the gas density blows up in
a finite time when starting from smooth initial conditions. The density blowups
signal formation of close-packed clusters of particles. As the density blow-up
time $t_c$ is approached, the maximum density exhibits a power law $\sim
(t_c-t)^{-2}$. The velocity gradient blows up as $\sim - (t_c-t)^{-1}$ while
the velocity itself remains continuous and develops a cusp (rather than a shock
discontinuity) at the singularity. The gas temperature vanishes at the
singularity, and the singularity follows the isobaric scenario: the gas
pressure remains finite and approximately uniform in space and constant in time
close to the singularity. An additional exact solution shows that the density
blowup, of the same type, may coexist with an ""ordinary"" shock, at which the
hydrodynamic fields are discontinuous but finite. We confirm stability of the
exact solutions with respect to small one-dimensional perturbations by solving
the ideal hydrodynamic equations numerically. Furthermore, numerical solutions
show that the local features of the density blowup hold universally,
independently of details of the initial and boundary conditions.
"
phys,"Fluctuation-dissipation relation on a Melde string in a turbulent flow,
  considerations on a ""dynamical temperature""","  We report on measurements of the transverse fluctuations of a string in a
turbulent air jet flow. Harmonic modes are excited by the fluctuating drag
force, at different wave-numbers. This simple mechanical probe makes it
possible to measure excitations of the flow at specific scales, averaged over
space and time: it is a scale-resolved, global measurement. We also measure the
dissipation associated to the string motion, and we consider the ratio of the
fluctuations over dissipation (FDR). In an exploratory approach, we investigate
the concept of {\it effective temperature} defined through the FDR. We compare
our observations with other definitions of temperature in turbulence. From the
theory of Kolmogorov (1941), we derive the exponent -11/3 expected for the
spectrum of the fluctuations. This simple model and our experimental results
are in good agreement, over the range of wave-numbers, and Reynolds number
accessible ($74000 \leq Re \leq 170000$).
"
phys,"Experimental and theoretical study of light scattering by individual
  mature red blood cells by use of scanning flow cytometry and discrete dipole
  approximation","  Elastic light scattering by mature red blood cells (RBCs) was theoretically
and experimentally analyzed with the discrete dipole approximation (DDA) and
the scanning flow cytometry (SFC), respectively. SFC permits measurement of
angular dependence of light-scattering intensity (indicatrix) of single
particles. A mature RBC is modeled as a biconcave disk in DDA simulations of
light scattering. We have studied the effect of RBC orientation related to the
direction of the incident light upon the indicatrix. Numerical calculations of
indicatrices for several aspect ratios and volumes of RBC have been carried
out. Comparison of the simulated indicatrices and indicatrices measured by SFC
showed good agreement, validating the biconcave disk model for a mature RBC. We
simulated the light-scattering output signals from the SFC with the DDA for
RBCs modeled as a disk-sphere and as an oblate spheroid. The biconcave disk,
the disk-sphere, and the oblate spheroid models have been compared for two
orientations, i.e. face-on and rim-on incidence. Only the oblate spheroid model
for rim-on incidence gives results similar to the rigorous biconcave disk
model.
"
phys,"Growing Networks: Limit in-degree distribution for arbitrary out-degree
  one","  We compute the stationary in-degree probability, $P_{in}(k)$, for a growing
network model with directed edges and arbitrary out-degree probability. In
particular, under preferential linking, we find that if the nodes have a light
tail (finite variance) out-degree distribution, then the corresponding
in-degree one behaves as $k^{-3}$. Moreover, for an out-degree distribution
with a scale invariant tail, $P_{out}(k)\sim k^{-\alpha}$, the corresponding
in-degree distribution has exactly the same asymptotic behavior only if
$2<\alpha<3$ (infinite variance). Similar results are obtained when
attractiveness is included. We also present some results on descriptive
statistics measures %descriptive statistics such as the correlation between the
number of in-going links, $D_{in}$, and outgoing links, $D_{out}$, and the
conditional expectation of $D_{in}$ given $D_{out}$, and we calculate these
measures for the WWW network. Finally, we present an application to the
scientific publications network. The results presented here can explain the
tail behavior of in/out-degree distribution observed in many real networks.
"
phys,"On axisymmetric MHD equilibria with incompressible flows under side
  conditions","  Axisymmetric equilibria with incompressible flows of arbitrary direction are
studied in the framework of magnetohydrodynamics under a variety of physically
relevant side conditions. To this end a set of pertinent non-linear ODEs are
transformed to quasilinear ones and the respective initial value problem is
solved numerically with appropriately determined initial values near the
magnetic axis. Several equilibria are then constructed surface by surface. The
non field aligned flow results in novel configurations with a single magnetic
axis, toroidal shell configurations in which the plasma is confined within a
couple of magnetic surfaces and double shell-like configurations. In addition,
the flow affects the elongation and triangularity of the magnetic surfaces.
"
phys,"Rounding of first-order phase transitions and optimal cooperation in
  scale-free networks","  We consider the ferromagnetic large-$q$ state Potts model in complex evolving
networks, which is equivalent to an optimal cooperation problem, in which the
agents try to optimize the total sum of pair cooperation benefits and the
supports of independent projects. The agents are found to be typically of two
kinds: a fraction of $m$ (being the magnetization of the Potts model) belongs
to a large cooperating cluster, whereas the others are isolated one man's
projects. It is shown rigorously that the homogeneous model has a strongly
first-order phase transition, which turns to second-order for random
interactions (benefits), the properties of which are studied numerically on the
Barab\'asi-Albert network. The distribution of finite-size transition points is
characterized by a shift exponent, $1/\tilde{\nu}'=.26(1)$, and by a different
width exponent, $1/\nu'=.18(1)$, whereas the magnetization at the transition
point scales with the size of the network, $N$, as: $m\sim N^{-x}$, with
$x=.66(1)$.
"
phys,"Query on Negative Temperature, Internal Interactions and Decrease of
  Entropy","  After negative temperature is restated, we find that it will derive
necessarily decrease of entropy. Negative temperature is based on the Kelvin
scale and the condition dU>0 and dS<0. Conversely, there is also negative
temperature for dU<0 and dS>0. But, negative temperature is contradiction with
usual meaning of temperature and with some basic concepts of physics and
mathematics. It is a question in nonequilibrium thermodynamics. We proposed a
possibility of decrease of entropy due to fluctuation magnified and internal
interactions in some isolated systems. From this we discuss some possible
examples and theories.
"
math,Spectral perturbation bounds for selfadjoint operators,"  We give general spectral and eigenvalue perturbation bounds for a selfadjoint
operator perturbed in the sense of the pseudo-Friedrichs extension. We also
give several generalisations of the aforementioned extension. The spectral
bounds for finite eigenvalues are obtained by using analyticity and
monotonicity properties (rather than variational principles) and they are
general enough to include eigenvalues in gaps of the essential spectrum.
"
math,"Direct Theorems in the Theory of Approximation of the Banach Space
  Vectors by Entire Vectors of Exponential Type","  For an arbitrary operator A on a Banach space X which is a generator of
C_0-group with certain growth condition at the infinity, the direct theorems on
connection between the smoothness degree of a vector $x\in X$ with respect to
the operator A, the order of convergence to zero of the best approximation of x
by exponential type entire vectors for the operator A, and the k-module of
continuity are given. Obtained results allows to acquire Jackson-type
inequalities in many classic spaces of periodic functions and weighted $L_p$
spaces.
"
math,Generic character sheaves on disconnected groups and character values,"  We relate a generic character sheaf on a disconnected reductive group with a
character of a representation of the rational points of the group over a finite
field extending a result known in the connected case.
"
math,"Placeholder Substructures III: A Bit-String-Driven ''Recipe Theory'' for
  Infinite-Dimensional Zero-Divisor Spaces","  Zero-divisors (ZDs) derived by Cayley-Dickson Process (CDP) from
N-dimensional hypercomplex numbers (N a power of 2, at least 4) can represent
singularities and, as N approaches infinite, fractals -- and thereby,scale-free
networks. Any integer greater than 8 and not a power of 2 generates a
meta-fractal or ""Sky"" when it is interpreted as the ""strut constant"" (S) of an
ensemble of octahedral vertex figures called ""Box-Kites"" (the fundamental
building blocks of ZDs). Remarkably simple bit-manipulation rules or ""recipes""
provide tools for transforming one fractal genus into others within the context
of Wolfram's Class 4 complexity.
"
math,The local structure of conformally symmetric manifolds,"  This is a final step in a local classification of pseudo-Riemannian manifolds
with parallel Weyl tensor that are not conformally flat or locally symmetric.
"
math,K_0-theory of n-potents in rings and algebras,"  Let $n \geq 2$ be an integer. An \emph{$n$-potent} is an element $e$ of a
ring $R$ such that $e^n = e$. In this paper, we study $n$-potents in matrices
over $R$ and use them to construct an abelian group $K_0^n(R)$. If $A$ is a
complex algebra, there is a group isomorphism $K_0^n(A) \cong
\bigl(K_0(A)\bigr)^{n-1}$ for all $n \geq 2$. However, for algebras over
cyclotomic fields, this is not true in general. We consider $K_0^n$ as a
covariant functor, and show that it is also functorial for a generalization of
homomorphism called an \emph{$n$-homomorphism}.
"
math,"Renewals for exponentially increasing lifetimes, with an application to
  digital search trees","  We show that the number of renewals up to time $t$ exhibits distributional
fluctuations as $t\to\infty$ if the underlying lifetimes increase at an
exponential rate in a distributional sense. This provides a probabilistic
explanation for the asymptotics of insertion depth in random trees generated by
a bit-comparison strategy from uniform input; we also obtain a representation
for the resulting family of limit laws along subsequences. Our approach can
also be used to obtain rates of convergence.
"
math,Mathematics of thermoacoustic tomography,"  The paper presents a survey of mathematical problems, techniques, and
challenges arising in the Thermoacoustic and Photoacoustic Tomography.
"
math,Fractional WKB Approximation,"  Wentzel, Kramers, Brillouin (WKB) approximation for fractional systems is
investigated in this paper using the fractional calculus. In the fractional
case the wave function is constructed such that the phase factor is the same as
the Hamilton's principle function ""S"". To demonstrate our proposed approach two
examples are investigated in details.
"
math,Complete Shrinking Ricci Solitons have Finite Fundamental Group,"  We show that if a complete Riemannian manifold supports a vector field such
that the Ricci tensor plus the Lie derivative of the metric with respect to the
vector field has a positive lower bound, then the fundamental group is finite.
In particular, it follows that complete shrinking Ricci solitons and complete
smooth metric measure spaces with a positive lower bound on the Bakry-Emery
tensor have finite fundamental group. The method of proof is to generalize
arguments of Garcia-Rio and Fernandez-Lopez in the compact case.
"
math,Algebraic geometry of Gaussian Bayesian networks,"  Conditional independence models in the Gaussian case are algebraic varieties
in the cone of positive definite covariance matrices. We study these varieties
in the case of Bayesian networks, with a view towards generalizing the
recursive factorization theorem to situations with hidden variables. In the
case when the underlying graph is a tree, we show that the vanishing ideal of
the model is generated by the conditional independence statements implied by
graph. We also show that the ideal of any Bayesian network is homogeneous with
respect to a multigrading induced by a collection of upstream random variables.
This has a number of important consequences for hidden variable models.
Finally, we relate the ideals of Bayesian networks to a number of classical
constructions in algebraic geometry including toric degenerations of the
Grassmannian, matrix Schubert varieties, and secant varieties.
"
math,"Local well-posedness of nonlinear dispersive equations on modulation
  spaces","  By using tools of time-frequency analysis, we obtain some improved local
well-posedness results for the NLS, NLW and NLKG equations with Cauchy data in
modulation spaces $M{p, 1}_{0,s}$.
"
math,"Nonimmersions of RP^n implied by tmf, revisited","  In a 2002 paper, the authors and Bruner used the new spectrum tmf to obtain
some new nonimmersions of real projective spaces. In this note, we
complete/correct two oversights in that paper.
  The first is to note that in that paper a general nonimmersion result was
stated which yielded new nonimmersions for RP^n with n as small as 48, and yet
it was stated there that the first new result occurred when n=1536. Here we
give a simple proof of those overlooked results.
  Secondly, we fill in a gap in the proof of the 2002 paper. There it was
claimed that an axial map f must satisfy f^*(X)=X_1+X_2. We realized recently
that this is not clear. However, here we show that it is true up multiplication
by a unit in the appropriate ring, and so we retrieve all the nonimmersion
results claimed in the original paper.
  Finally, we present a complete determination of tmf^{8*}(RP^\infty\times
RP^\infty) and tmf^*(CP^\infty\times CP^\infty) in positive dimensions.
"
math,Hamilton-Jacobi Fractional Sequential Mechanics,"  As a continuation of Rabei et al. work [11], the Hamilton- Jacobi partial
differential equation is generalized to be applicable for systems containing
fractional derivatives. The Hamilton- Jacobi function in configuration space is
obtained in a similar manner to the usual mechanics. Two problems are
considered to demonstrate the application of the formalism. The result found to
be in exact agreement with Agrawal's formalism.
"
math,"An equilibrium problem for the limiting eigenvalue distribution of
  banded Toeplitz matrices","  We study the limiting eigenvalue distribution of $n\times n$ banded Toeplitz
matrices as $n\to \infty$. From classical results of Schmidt-Spitzer and
Hirschman it is known that the eigenvalues accumulate on a special curve in the
complex plane and the normalized eigenvalue counting measure converges weakly
to a measure on this curve as $n\to\infty$. In this paper, we characterize the
limiting measure in terms of an equilibrium problem. The limiting measure is
one component of the unique vector of measures that minimes an energy
functional defined on admissible vectors of measures. In addition, we show that
each of the other components is the limiting measure of the normalized counting
measure on certain generalized eigenvalues.
"
math,"Computing genus 2 Hilbert-Siegel modular forms over $\Q(\sqrt{5})$ via
  the Jacquet-Langlands correspondence","  In this paper we present an algorithm for computing Hecke eigensystems of
Hilbert-Siegel cusp forms over real quadratic fields of narrow class number
one. We give some illustrative examples using the quadratic field
$\Q(\sqrt{5})$. In those examples, we identify Hilbert-Siegel eigenforms that
are possible lifts from Hilbert eigenforms.
"
math,Stable algebras of entire functions,"  Suppose that $h$ and $g$ belong to the algebra $\B$ generated by the rational
functions and an entire function $f$ of finite order on ${\Bbb C}^n$ and that
$h/g$ has algebraic polar variety. We show that either $h/g\in\B$ or
$f=q_1e^p+q_2$, where $p$ is a polynomial and $q_1,q_2$ are rational functions.
In the latter case, $h/g$ belongs to the algebra generated by the rational
functions, $e^p$ and $e^{-p}$.
"
math,Average optimality for risk-sensitive control with general state space,"  This paper deals with discrete-time Markov control processes on a general
state space. A long-run risk-sensitive average cost criterion is used as a
performance measure. The one-step cost function is nonnegative and possibly
unbounded. Using the vanishing discount factor approach, the optimality
inequality and an optimal stationary strategy for the decision maker are
established.
"
math,Quivers with potentials and their representations I: Mutations,"  We study quivers with relations given by non-commutative analogs of Jacobian
ideals in the complete path algebra. This framework allows us to give a
representation-theoretic interpretation of quiver mutations at arbitrary
vertices. This gives a far-reaching generalization of
Bernstein-Gelfand-Ponomarev reflection functors. The motivations for this work
come from several sources: superpotentials in physics, Calabi-Yau algebras,
cluster algebras.
"
math,Hyperbolicity in unbounded convex domains,"  We provide several equivalent characterizations of Kobayashi hyperbolicity in
unbounded convex domains in terms of peak and anti-peak functions at infinity,
affine lines, Bergman metric and iteration theory.
"
math,"On the Nonexistence of Nontrivial Involutive n-Homomorphisms of
  C*-algebras","  An n-homomorphism between algebras is a linear map $\phi : A \to B$ such that
$\phi(a_1 ... a_n) = \phi(a_1)... \phi(a_n)$ for all elements $a_1, >..., a_n
\in A.$ Every homomorphism is an n-homomorphism, for all n >= 2, but the
converse is false, in general. Hejazian et al. [7] ask: Is every *-preserving
n-homomorphism between C*-algebras continuous? We answer their question in the
affirmative, but the even and odd n arguments are surprisingly disjoint. We
then use these results to prove stronger ones: If n >2 is even, then $\phi$ is
just an ordinary *-homomorphism. If n >= 3 is odd, then $\phi$ is a difference
of two orthogonal *-homomorphisms. Thus, there are no nontrivial *-linear
n-homomorphisms between C*-algebras.
"
math,"The classification of surfaces with p_g=q=1 isogenous to a product of
  curves","  A projective surface S is said to be isogenous to a product if there exist
two smooth curves C, F and a finite group G acting freely on C \times F so that
S=(C \times F)/G. In this paper we classify all surfaces with p_g=q=1 which are
isogenous to a product.
"
math,Duality and Tameness,"  We prove a duality theorem for certain graded algebras and show by various
examples different kinds of failure of tameness of local cohomology.
"
math,M-regularity of the Fano surface,"  Let $(A,\Theta)$ be a principally polarised abelian variety, and let Y be a
subvariety. Pareschi and Popa conjectured that Y has minimal cohomology class
if and only if the structure sheaf of Y satisfies a property that they call
M-regularity.
  Let now X be a smooth cubic threefold. By a classical result due to Clemens
and Griffiths, its intermediate Jacobian J(X) is a principally polarised
abelian variety; furthermore the Fano surface of lines on X can be embedded in
J(X) and has minimal cohomology class. In this short note we show that its
structure sheaf is M-regular.
"
math,Invariance and the twisted Chern character : a case study,"  We give details of the proof of the remark made in \cite{G2} that the Chern
characters of the canonical generators on the K homology of the quantum group
$SU_q(2)$ are not invariant under the natural $SU_q(2)$ coaction. Furthermore,
the conjecture made in \cite{G2} about the nontriviality of the twisted Chern
character coming from an odd equivariant spectral triple on $SU_q(2)$ is
settled in the affirmative.
"
math,Stability of a finite volume scheme for the incompressible fluids,"  We introduce a finite volume scheme for the two-dimensional incompressible
Navier-Stokes equations. We use a triangular mesh. The unknowns for the
velocity and pressure are respectively piecewise constant and affine. We use a
projection method to deal with the incompressibility constraint. We show that
the differential operators in the Navier-Stokes equations and their discrete
counterparts share similar properties. In particular we state an inf-sup
(Babuska-Brezzi) condition. Using these properties we infer the stability of
the scheme.
"
math,Energy conservation and Onsager's conjecture for the Euler equations,"  Onsager conjectured that weak solutions of the Euler equations for
incompressible fluids in 3D conserve energy only if they have a certain minimal
smoothness, (of order of 1/3 fractional derivatives) and that they dissipate
energy if they are rougher. In this paper we prove that energy is conserved for
velocities in the function space $B^{1/3}_{3,c(\NN)}$. We show that this space
is sharp in a natural sense. We phrase the energy spectrum in terms of the
Littlewood-Paley decomposition and show that the energy flux is controlled by
local interactions. This locality is shown to hold also for the helicity flux;
moreover, every weak solution of the Euler equations that belongs to
$B^{2/3}_{3,c(\NN)}$ conserves helicity. In contrast, in two dimensions, the
strong locality of the enstrophy holds only in the ultraviolet range.
"
math,"Penalization approach for mixed hyperbolic systems with constant
  coefficients satisfying a Uniform Lopatinski Condition","  In this paper, we describe a new, systematic and explicit way of
approximating solutions of mixed hyperbolic systems with constant coefficients
satisfying a Uniform Lopatinski Condition via different Penalization
approaches.
"
math,"Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian
  Vector Broadcast Channels","  MIMO technology is one of the most significant advances in the past decade to
increase channel capacity and has a great potential to improve network capacity
for mesh networks. In a MIMO-based mesh network, the links outgoing from each
node sharing the common communication spectrum can be modeled as a Gaussian
vector broadcast channel. Recently, researchers showed that ``dirty paper
coding'' (DPC) is the optimal transmission strategy for Gaussian vector
broadcast channels. So far, there has been little study on how this fundamental
result will impact the cross-layer design for MIMO-based mesh networks. To fill
this gap, we consider the problem of jointly optimizing DPC power allocation in
the link layer at each node and multihop/multipath routing in a MIMO-based mesh
networks. It turns out that this optimization problem is a very challenging
non-convex problem. To address this difficulty, we transform the original
problem to an equivalent problem by exploiting the channel duality. For the
transformed problem, we develop an efficient solution procedure that integrates
Lagrangian dual decomposition method, conjugate gradient projection method
based on matrix differential calculus, cutting-plane method, and subgradient
method. In our numerical example, it is shown that we can achieve a network
performance gain of 34.4% by using DPC.
"
math,"Transfinite diameter, Chebyshev constant and energy on locally compact
  spaces","  We study the relationship between transfinite diameter, Chebyshev constant
and Wiener energy in the abstract linear potential analytic setting pioneered
by Choquet, Fuglede and Ohtsuka. It turns out that, whenever the potential
theoretic kernel has the maximum principle, then all these quantities are equal
for all compact sets. For continuous kernels even the converse statement is
true: if the Chebyshev constant of any compact set coincides with its
transfinite diameter, the kernel must satisfy the maximum principle. An
abundance of examples is provided to show the sharpness of the results.
"
math,"Contrasting Two Transformation-Based Methods for Obtaining Absolute
  Extrema","  In this note we contrast two transformation-based methods to deduce absolute
extrema and the corresponding extremizers. Unlike variation-based methods, the
transformation-based ones of Carlson and Leitmann and the recent one of Silva
and Torres are direct in that they permit obtaining solutions by inspection.
"
math,"A Direct Method for Solving Optimal Switching Problems of
  One-Dimensional Diffusions","  In this paper, we propose a direct solution method for optimal switching
problems of one-dimensional diffusions. This method is free from conjectures
about the form of the value function and switching strategies, or does not
require the proof of optimality through quasi-variational inequalities. The
direct method uses a general theory of optimal stopping problems for
one-dimensional diffusions and characterizes the value function as sets of the
smallest linear majorants in their respective transformed spaces.
"
math,Electromagnetic wormholes via handlebody constructions,"  Cloaking devices are prescriptions of electrostatic, optical or
electromagnetic parameter fields (conductivity $\sigma(x)$, index of refraction
$n(x)$, or electric permittivity $\epsilon(x)$ and magnetic permeability
$\mu(x)$) which are piecewise smooth on $\mathbb R^3$ and singular on a
hypersurface $\Sigma$, and such that objects in the region enclosed by $\Sigma$
are not detectable to external observation by waves. Here, we give related
constructions of invisible tunnels, which allow electromagnetic waves to pass
between possibly distant points, but with only the ends of the tunnels visible
to electromagnetic imaging. Effectively, these change the topology of space
with respect to solutions of Maxwell's equations, corresponding to attaching a
handlebody to $\mathbb R^3$. The resulting devices thus function as
electromagnetic wormholes.
"
math,Conformal Field Theory and Operator Algebras,"  We review recent progress in operator algebraic approach to conformal quantum
field theory. Our emphasis is on use of representation theory in classification
theory. This is based on a series of joint works with R. Longo.
"
math,Intersection Bodies and Generalized Cosine Transforms,"  Intersection bodies represent a remarkable class of geometric objects
associated with sections of star bodies and invoking
  Radon transforms, generalized cosine transforms, and the relevant Fourier
analysis. The main focus of this article is interrelation between generalized
cosine transforms of different kinds in the context of their application to
investigation of a certain family of intersection bodies, which we call
$\lam$-intersection bodies. The latter include $k$-intersection bodies (in the
sense of A. Koldobsky) and unit balls of finite-dimensional subspaces of
$L_p$-spaces. In particular, we show that restrictions onto lower dimensional
subspaces of the spherical Radon transforms and the generalized cosine
transforms preserve their integral-geometric structure. We apply this result to
the study of sections of $\lam$-intersection bodies. New characterizations of
this class of bodies are obtained and examples are given. We also review some
known facts and give them new proofs.
"
math,"Hecke-Clifford algebras and spin Hecke algebras I: the classical affine
  type","  Associated to the classical Weyl groups, we introduce the notion of
degenerate spin affine Hecke algebras and affine Hecke-Clifford algebras. For
these algebras, we establish the PBW properties, formulate the intertwiners,
and describe the centers. We further develop connections of these algebras with
the usual degenerate (i.e. graded) affine Hecke algebras of Lusztig by
introducing a notion of degenerate covering affine Hecke algebras.
"
math,A unified approach to SIC-POVMs and MUBs,"  A unified approach to (symmetric informationally complete) positive operator
valued measures and mutually unbiased bases is developed in this article. The
approach is based on the use of operator equivalents expanded in the enveloping
algebra of SU(2). Emphasis is put on similarities and differences between
SIC-POVMs and MUBs.
"
math,Littlewood-Richardson polynomials,"  We introduce a family of rings of symmetric functions depending on an
infinite sequence of parameters. A distinguished basis of such a ring is
comprised by analogues of the Schur functions. The corresponding structure
coefficients are polynomials in the parameters which we call the
Littlewood-Richardson polynomials. We give a combinatorial rule for their
calculation by modifying an earlier result of B. Sagan and the author. The new
rule provides a formula for these polynomials which is manifestly positive in
the sense of W. Graham. We apply this formula for the calculation of the
product of equivariant Schubert classes on Grassmannians which implies a
stability property of the structure coefficients. The first manifestly positive
formula for such an expansion was given by A. Knutson and T. Tao by using
combinatorics of puzzles while the stability property was not apparent from
that formula. We also use the Littlewood-Richardson polynomials to describe the
multiplication rule in the algebra of the Casimir elements for the general
linear Lie algebra in the basis of the quantum immanants constructed by A.
Okounkov and G. Olshanski.
"
math,"Some combinatorial aspects of differential operation compositions on
  space $R^n$","  In this paper we present a recurrent relation for counting meaningful
compositions of the higher-order differential operations on the space $R^{n}$
(n=3,4,...) and extract the non-trivial compositions of order higher than two.
"
math,Curvature flows in semi-Riemannian manifolds,"  We prove that the limit hypersurfaces of converging curvature flows are
stable, if the initial velocity has a weak sign, and give a survey of the
existence and regularity results.
"
math,On Ando's inequalities for convex and concave functions,"  For positive semidefinite matrices $A$ and $B$, Ando and Zhan proved the
inequalities $||| f(A)+f(B) ||| \ge ||| f(A+B) |||$ and $||| g(A)+g(B) ||| \le
||| g(A+B) |||$, for any unitarily invariant norm, and for any non-negative
operator monotone $f$ on $[0,\infty)$ with inverse function $g$. These
inequalities have very recently been generalised to non-negative concave
functions $f$ and non-negative convex functions $g$, by Bourin and Uchiyama,
and Kosem, respectively.
  In this paper we consider the related question whether the inequalities $|||
f(A)-f(B) ||| \le ||| f(|A-B|) |||$, and $||| g(A)-g(B) ||| \ge ||| g(|A-B|)
|||$, obtained by Ando, for operator monotone $f$ with inverse $g$, also have a
similar generalisation to non-negative concave $f$ and convex $g$. We answer
exactly this question, in the negative for general matrices, and affirmatively
in the special case when $A\ge ||B||$.
  In the course of this work, we introduce the novel notion of $Y$-dominated
majorisation between the spectra of two Hermitian matrices, where $Y$ is itself
a Hermitian matrix, and prove a certain property of this relation that allows
to strengthen the results of Bourin-Uchiyama and Kosem, mentioned above.
"
math,On the total disconnectedness of the quotient Aubry set,"  In this paper we show that the quotient Aubry set associated to certain
Lagrangians is totally disconnected (i.e., every connected component consists
of a single point). Moreover, we discuss the relation between this problem and
a Morse-Sard type property for (difference of) critical subsolutions of
Hamilton-Jacobi equations.
"
math,"A geometric realization of sl(6,C)","  Given an orientable weakly self-dual manifold X of rank two, we build a
geometric realization of the Lie algebra sl(6,C) as a naturally defined algebra
L of endomorphisms of the space of differential forms of X. We provide an
explicit description of Serre generators in terms of natural generators of L.
This construction gives a bundle on X which is related to the search for a
natural Gauge theory on X. We consider this paper as a first step in the study
of a rich and interesting algebraic structure.
"
math,"Pfaffians, hafnians and products of real linear functionals","  We prove pfaffian and hafnian versions of Lieb's inequalities on determinants
and permanents of positive semi-definite matrices. We use the hafnian
inequality to improve the lower bound of R\'ev\'esz and Sarantopoulos on the
norm of a product of linear functionals on a real Euclidean space (this subject
is sometimes called the `real linear polarization constant' problem).
"
math,Groups with finitely many conjugacy classes and their automorphisms,"  We combine classical methods of combinatorial group theory with the theory of
small cancellations over relatively hyperbolic groups to construct finitely
generated torsion-free groups that have only finitely many classes of conjugate
elements. Moreover, we present several results concerning embeddings into such
groups.
  As another application of these techniques, we prove that every countable
group $C$ can be realized as a group of outer automorphisms of a group $N$,
where $N$ is a finitely generated group having Kazhdan's property (T) and
containing exactly two conjugacy classes.
"
math,Sparsity-certifying Graph Decompositions,"  We describe a new algorithm, the $(k,\ell)$-pebble game with colors, and use
it obtain a characterization of the family of $(k,\ell)$-sparse graphs and
algorithmic solutions to a family of problems concerning tree decompositions of
graphs. Special instances of sparse graphs appear in rigidity theory and have
received increased attention in recent years. In particular, our colored
pebbles generalize and strengthen the previous results of Lee and Streinu and
give a new proof of the Tutte-Nash-Williams characterization of arboricity. We
also present a new decomposition that certifies sparsity based on the
$(k,\ell)$-pebble game with colors. Our work also exposes connections between
pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and
Westermann and Hendrickson.
"
math,"Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless
  Networks","  This paper investigates the many-to-one throughput capacity (and by symmetry,
one-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has
generally been assumed in prior studies that the many-to-one throughput
capacity is upper-bounded by the link capacity L. Throughput capacity L is not
achievable under 802.11. This paper introduces the notion of ""canonical
networks"", which is a class of regularly-structured networks whose capacities
can be analyzed more easily than unstructured networks. We show that the
throughput capacity of canonical networks under 802.11 has an analytical upper
bound of 3L/4 when the source nodes are two or more hops away from the sink;
and simulated throughputs of 0.690L (0.740L) when the source nodes are many
hops away. We conjecture that 3L/4 is also the upper bound for general
networks. When all links have equal length, 2L/3 can be shown to be the upper
bound for general networks. Our simulations show that 802.11 networks with
random topologies operated with AODV routing can only achieve throughputs far
below the upper bounds. Fortunately, by properly selecting routes near the
gateway (or by properly positioning the relay nodes leading to the gateway) to
fashion after the structure of canonical networks, the throughput can be
improved significantly by more than 150%. Indeed, in a dense network, it is
worthwhile to deactivate some of the relay nodes near the sink judiciously.
"
math,"Glueball Masses in (2+1)-Dimensional Anisotropic Weakly-Coupled
  Yang-Mills Theory","  The confinement problem has been solved in the anisotropic (2+1)-dimensional
SU(N) Yang-Mills theory at weak coupling. In this paper, we find the low-lying
spectrum for N=2. The lightest excitations are pairs of fundamental particles
of the (1+1)-dimensional SU(2)XSU(2) principal chiral sigma model bound in a
linear potential, with a specified matching condition where the particles
overlap. This matching condition can be determined from the exactly-known
S-matrix for the sigma model.
"
math,Skew-Hadamard matrices of orders 188 and 388 exist,"  We construct several difference families on cyclic groups of orders 47 and
97, and use them to construct skew-Hadamard matrices of orders 188 and 388.
Such difference families and matrices are constructed here for the first time.
The matrices are constructed by using the Goethals-Seidel array.
"
math,A note on higher-order differential operations,"  In this paper we consider successive iterations of the first-order
differential operations in space ${\bf R}^3.$
"
math,"Dual billiards, Fagnano orbits and regular polygons","  We study the notion of Fagnano orbits for dual polygonal billiards. We used
them to characterize regular polygons and we study the iteration of the
developing map.
"
math,"Approximate solutions to the Dirichlet problem for harmonic maps between
  hyperbolic spaces","  Our main result in this paper is the following: Given $H^m, H^n$ hyperbolic
spaces of dimensional $m$ and $n$ corresponding, and given a Holder function
$f=(s^1,...,f^{n-1}):\partial H^m\to \partial H^n$ between geometric boundaries
of $H^m$ and $H^n$. Then for each $\epsilon >0$ there exists a harmonic map
$u:H^m\to H^n$ which is continuous up to the boundary (in the sense of
Euclidean) and $u|_{\partial H^m}=(f^1,...,f^{n-1},\epsilon)$.
"
math,A Global Approach to the Theory of Special Finsler Manifolds,"  The aim of the present paper is to provide a global presentation of the
theory of special Finsler manifolds. We introduce and investigate globally (or
intrinsically, free from local coordinates) many of the most important and most
commonly used special Finsler manifolds: locally Minkowskian, Berwald,
Landesberg, general Landesberg, $P$-reducible, $C$-reducible,
semi-$C$-reducible, quasi-$C$-reducible, $P^{*}$-Finsler, $C^{h}$-recurrent,
$C^{v}$-recurrent, $C^{0}$-recurrent, $S^{v}$-recurrent, $S^{v}$-recurrent of
the second order, $C_{2}$-like, $S_{3}$-like, $S_{4}$-like, $P_{2}$-like,
$R_{3}$-like, $P$-symmetric, $h$-isotropic, of scalar curvature, of constant
curvature, of $p$-scalar curvature, of $s$-$ps$-curvature. The global
definitions of these special Finsler manifolds are introduced. Various
relationships between the different types of the considered special Finsler
manifolds are found. Many local results, known in the literature, are proved
globally and several new results are obtained. As a by-product, interesting
identities and properties concerning the torsion tensor fields and the
curvature tensor fields are deduced. Although our investigation is entirely
global, we provide; for comparison reasons, an appendix presenting a local
counterpart of our global approach and the local definitions of the special
Finsler spaces considered.
"
math,"Construction of Complete Embedded Self-Similar Surfaces under Mean
  Curvature Flow. Part II","  We study the Dirichlet problem associated to the equation for self-similar
surfaces for graphs over the Euclidean plane with a disk removed. We show the
existence of a solution provided the boundary conditions on the boundary circle
are small enough and satisfy some symmetries. This is the second step towards
the construction of new examples of complete embedded self similar surfaces
under mean curvature flow.
"
math,One-dimensional Brownian particle systems with rank dependent drifts,"  We study interacting systems of linear Brownian motions whose drift vector at
every time point is determined by the relative ranks of the coordinate
processes at that time. Our main objective has been to study the long range
behavior of the spacings between the Brownian motions arranged in increasing
order. For finitely many Brownian motions interacting in this manner, we
characterize drifts for which the family of laws of the vector of spacings is
tight, and show its convergence to a unique stationary joint distribution given
by independent exponential distributions with varying means. We also study one
particular countably infinite system, where only the minimum Brownian particle
gets a constant upward drift, and prove that independent and identically
distributed exponential spacings remain stationary under the dynamics of such a
process. Some related conjectures in this direction have also been discussed.
"
math,"Strong Spherical Asymptotics for Rotor-Router Aggregation and the
  Divisible Sandpile","  The rotor-router model is a deterministic analogue of random walk. It can be
used to define a deterministic growth model analogous to internal DLA. We prove
that the asymptotic shape of this model is a Euclidean ball, in a sense which
is stronger than our earlier work. For the shape consisting of $n=\omega_d r^d$
sites, where $\omega_d$ is the volume of the unit ball in $\R^d$, we show that
the inradius of the set of occupied sites is at least $r-O(\log r)$, while the
outradius is at most $r+O(r^\alpha)$ for any $\alpha > 1-1/d$. For a related
model, the divisible sandpile, we show that the domain of occupied sites is a
Euclidean ball with error in the radius a constant independent of the total
mass. For the classical abelian sandpile model in two dimensions, with $n=\pi
r^2$ particles, we show that the inradius is at least $r/\sqrt{3}$, and the
outradius is at most $(r+o(r))/\sqrt{2}$. This improves on bounds of Le Borgne
and Rossin. Similar bounds apply in higher dimensions.
"
math,"Late-time tails of a Yang-Mills field on Minkowski and Schwarzschild
  backgrounds","  We study the late-time behavior of spherically symmetric solutions of the
Yang-Mills equations on Minkowski and Schwarzschild backgrounds. Using
nonlinear perturbation theory we show in both cases that solutions having
smooth compactly supported initial data posses tails which decay as $t^{-4}$ at
timelike infinity. Moreover, for small initial data on Minkowski background we
derive the third-order formula for the amplitude of the tail and confirm
numerically its accuracy.
"
math,Dimers on surface graphs and spin structures. II,"  In a previous paper, we showed how certain orientations of the edges of a
graph G embedded in a closed oriented surface S can be understood as discrete
spin structures on S. We then used this correspondence to give a geometric
proof of the Pfaffian formula for the partition function of the dimer model on
G. In the present article, we generalize these results to the case of compact
oriented surfaces with boundary. We also show how the operations of cutting and
gluing act on discrete spin structures and how they change the partition
function. These operations allow to reformulate the dimer model as a quantum
field theory on surface graphs.
"
math,Frobenius-Schur indicators for semisimple Lie algebras,"  Let g be a finite dimensional complex semisimple Lie algebra, and let V be a
finite dimensional represenation of g. We give a closed formula for the mth
Frobenius-Schur indicator, m>1, of V in representation-theoretic terms. We
deduce that the indicators take integer values, and that for a large enough m,
the mth indicator of V equals the dimension of the zero weight space of V. For
the classical Lie algebras sl(n), so(2n), so(2n+1) and sp(2n), this is the case
for m greater or equal to 2n-1, 4n-5, 4n-3 and 2n+1, respectively.
"
math,"Generic representations of orthogonal groups: projective functors in the
  category Fquad","  In this paper, we continue the study of the category of functors Fquad,
associated to F_2-vector spaces equipped with a nondegenerate quadratic form,
initiated in two previous papers of the author. We define a filtration of the
standard projective objects in Fquad; this refines to give a decomposition into
indecomposable factors of the two first standard projective objects in Fquad.
As an application of these two decompositions, we give a complete description
of the polynomial functors of the category Fquad.
"
math,"On packet lengths and overhead for random linear coding over the erasure
  channel","  We assess the practicality of random network coding by illuminating the issue
of overhead and considering it in conjunction with increasingly long packets
sent over the erasure channel. We show that the transmission of increasingly
long packets, consisting of either of an increasing number of symbols per
packet or an increasing symbol alphabet size, results in a data rate
approaching zero over the erasure channel. This result is due to an erasure
probability that increases with packet length. Numerical results for a
particular modulation scheme demonstrate a data rate of approximately zero for
a large, but finite-length packet. Our results suggest a reduction in the
performance gains offered by random network coding.
"
math,Clustering in a stochastic model of one-dimensional gas,"  We give a quantitative analysis of clustering in a stochastic model of
one-dimensional gas. At time zero, the gas consists of $n$ identical particles
that are randomly distributed on the real line and have zero initial speeds.
Particles begin to move under the forces of mutual attraction. When particles
collide, they stick together forming a new particle, called cluster, whose mass
and speed are defined by the laws of conservation. We are interested in the
asymptotic behavior of $K_n(t)$ as $n\to \infty$, where $K_n(t)$ denotes the
number of clusters at time $t$ in the system with $n$ initial particles. Our
main result is a functional limit theorem for $K_n(t)$. Its proof is based on
the discovered localization property of the aggregation process, which states
that the behavior of each particle is essentially defined by the motion of
neighbor particles.
"
math,Gorenstein locus of minuscule Schubert varieties,"  In this article, we describe explicitely the Gorenstein locus of all
minuscule Schubert varieties. This proves a special case of a conjecture of A.
Woo and A. Yong (see math.AG/0603273) on the Gorenstein locus of Schubert
varieties.
"
math,Smooth maps with singularities of bounded K-codimensions,"  We will prove the relative homotopy principle for smooth maps with
singularities of a given {\cal K}-invariant class with a mild condition. We
next study a filtration of the group of homotopy self-equivalences of a given
manifold P by considering singularities of non-negative {\cal K}-codimensions.
"
math,Bursting Dynamics of the 3D Euler Equations in Cylindrical Domains,"  A class of three-dimensional initial data characterized by uniformly large
vorticity is considered for the Euler equations of incompressible fluids. The
fast singular oscillating limits of the Euler equations are studied for
parametrically resonant cylinders. Resonances of fast swirling Beltrami waves
deplete the Euler nonlinearity. The resonant Euler equations are systems of
three-dimensional rigid body equations, coupled or not. Some cases of these
resonant systems have homoclinic cycles, and orbits in the vicinity of these
homoclinic cycles lead to bursts of the Euler solution measured in Sobolev
norms of order higher than that corresponding to the enstrophy.
"
math,Counting characters in linear group actions,"  Let $G$ be a finite group and $V$ be a finite $G$--module. We present upper
bounds for the cardinalities of certain subsets of $\Irr(GV)$, such as the set
of those $\chi\in\Irr(GV)$ such that, for a fixed $v\in V$, the restriction of
$\chi$ to $<v>$ is not a multiple of the regular character of $<v>$. These
results might be useful in attacking the non--coprime $k(GV)$--problem.
"
math,Rigorous Results for the Periodic Oscillation of an Adiabatic Piston,"  We study a heavy piston of mass $M$ that moves in one dimension. The piston
separates two gas chambers, each of which contains finitely many ideal, unit
mass gas particles moving in $d$ dimensions, where $ d\geq 1$. Using averaging
techniques, we prove that the actual motions of the piston converge in
probability to the predicted averaged behavior on the time scale $M^ {1/2} $
when $M$ tends to infinity while the total energy of the system is bounded and
the number of gas particles is fixed. Neishtadt and Sinai previously pointed
out that an averaging theorem due to Anosov should extend to this situation.
  When $ d=1$, the gas particles move in just one dimension, and we prove that
the rate of convergence of the actual motions of the piston to its averaged
behavior is $\mathcal{O} (M^ {-1/2}) $ on the time scale $M^ {1/2} $. The
convergence is uniform over all initial conditions in a compact set. We also
investigate the piston system when the particle interactions have been
smoothed. The convergence to the averaged behavior again takes place uniformly,
both over initial conditions and over the amount of smoothing.
  In addition, we prove generalizations of our results to $N$ pistons
separating $N+1$ gas chambers. We also provide a general discussion of
averaging theory and the proofs of a number of previously known averaging
results. In particular, we include a new proof of Anosov's averaging theorem
for smooth systems that is primarily due to Dolgopyat.
"
math,Thermodynamic Stability - A note on a footnote in Ruelle's book,"  Thermodynamic stable interaction pair potentials which are not of the form
``positive function + real continuous function of positive type'' are presented
in dimension one. Construction of such a potential in dimension two is
sketched. These constructions use only elementary calculations. The
mathematical background is discussed separately.
"
math,"Computation of Power Loss in Likelihood Ratio Tests for Probability
  Densities Extended by Lehmann Alternatives","  We compute the loss of power in likelihood ratio tests when we test the
original parameter of a probability density extended by the first Lehmann
alternative.
"
math,On the polynomial automorphisms of a group,"  We prove that if a group is nilpotent (resp. metabelian), then so is the
subgroup of its automorphism group generated by all polynomial automorphisms.
"
math,"Locating the peaks of least-energy solutions to a quasilinear elliptic
  Neumann problem","  In this paper we study the shape of least-energy solutions to a singularly
perturbed quasilinear problem with homogeneous Neumann boundary condition. We
use an intrinsic variation method to show that at limit, the global maximum
point of least-energy solutions goes to a point on the boundary faster than the
linear rate and this point on the boundary approaches to a point where the mean
curvature of the boundary achieves its maximum. We also give a complete proof
of exponential decay of least-energy solutions.
"
math,"A Rigorous Time-Domain Analysis of Full--Wave Electromagnetic Cloaking
  (Invisibility)","  There is currently a great deal of interest in the theoretical and practical
possibility of cloaking objects from the observation by electromagnetic waves.
The basic idea of these invisibility devices \cite{glu1, glu2, le},\cite{pss1}
is to use anisotropic {\it transformation media} whose permittivity and
permeability $\var^{\lambda\nu}, \mu^{\lambda\nu}$, are obtained from the ones,
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$, of isotropic media, by singular
transformations of coordinates. In this paper we study electromagnetic cloaking
in the time-domain using the formalism of time-dependent scattering theory.
This formalism allows us to settle in an unambiguous way the mathematical
problems posed by the singularities of the inverse of the permittivity and the
permeability of the {\it transformation media} on the boundary of the cloaked
objects. We write Maxwell's equations in Schr\""odinger form with the
electromagnetic propagator playing the role of the Hamiltonian. We prove that
the electromagnetic propagator outside of the cloaked objects is essentially
self-adjoint. Moreover, the unique self-adjoint extension is unitarily
equivalent to the electromagnetic propagator in the medium
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$. Using this fact, and since the
coordinate transformation is the identity outside of a ball, we prove that the
scattering operator is the identity. Our results give a rigorous proof that the
construction of \cite{glu1, glu2, le}, \cite{pss1} perfectly cloaks passive and
active devices from observation by electromagnetic waves. Furthermore, we prove
cloaking for general anisotropic materials. In particular, our results prove
that it is possible to cloak objects inside general crystals.
"
math,Enumerating limit groups,"  We prove that the set of limit groups is recursive, answering a question of
Delzant. One ingredient of the proof is the observation that a finitely
presented group with local retractions (a la Long and Reid) is coherent and,
furthermore, there exists an algorithm that computes presentations for finitely
generated subgroups. The other main ingredient is the ability to
algorithmically calculate centralizers in relatively hyperbolic groups.
Applications include the existence of recognition algorithms for limit groups
and free groups.
"
math,Learning from compressed observations,"  The problem of statistical learning is to construct a predictor of a random
variable $Y$ as a function of a related random variable $X$ on the basis of an
i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable
predictors are drawn from some specified class, and the goal is to approach
asymptotically the performance (expected loss) of the best predictor in the
class. We consider the setting in which one has perfect observation of the
$X$-part of the sample, while the $Y$-part has to be communicated at some
finite bit rate. The encoding of the $Y$-values is allowed to depend on the
$X$-values. Under suitable regularity conditions on the admissible predictors,
the underlying family of probability distributions and the loss function, we
give an information-theoretic characterization of achievable predictor
performance in terms of conditional distortion-rate functions. The ideas are
illustrated on the example of nonparametric regression in Gaussian noise.
"
math,A procedure for finding the k-th power of a matrix,"  We give a new procedure in Maple for finding the k-th power of a martix. The
algorithm is based on the article [1].
"
math,The Arctic Circle Revisited,"  The problem of limit shapes in the six-vertex model with domain wall boundary
conditions is addressed by considering a specially tailored bulk correlation
function, the emptiness formation probability. A closed expression of this
correlation function is given, both in terms of certain determinant and
multiple integral, which allows for a systematic treatment of the limit shapes
of the model for full range of values of vertex weights. Specifically, we show
that for vertex weights corresponding to the free-fermion line on the phase
diagram, the emptiness formation probability is related to a one-matrix model
with a triple logarithmic singularity, or Triple Penner model. The saddle-point
analysis of this model leads to the Arctic Circle Theorem, and its
generalization to the Arctic Ellipses, known previously from domino tilings.
"
math,A limit relation for entropy and channel capacity per unit cost,"  In a quantum mechanical model, Diosi, Feldmann and Kosloff arrived at a
conjecture stating that the limit of the entropy of certain mixtures is the
relative entropy as system size goes to infinity. The conjecture is proven in
this paper for density matrices. The first proof is analytic and uses the
quantum law of large numbers. The second one clarifies the relation to channel
capacity per unit cost for classical-quantum channels. Both proofs lead to
generalization of the conjecture.
"
math,Topological Free Entropy Dimension of in Unital C^*-algebras,"  The notion of topological free entropy dimension of $n-$tuples of elements in
a unital C$^*$ algebra was introduced by Voiculescu. In the paper, we compute
topological free entropy dimension of one self-adjoint element and topological
orbit dimension of one self-adjoint element in a unital C$^*$ algebra.
Moreover, we calculate the values of topological free entropy dimensions of
families of generators of some unital C$^*$ algebras (for example: irrational
rotation C$^*$ algebras or minimal tensor product of two reduced C$^*$ algebras
of free groups).
"
math,Dynamical Objects for Cohomologically Expanding Maps,"  The goal of this paper is to construct invariant dynamical objects for a (not
necessarily invertible) smooth self map of a compact manifold. We prove a
result that takes advantage of differences in rates of expansion in the terms
of a sheaf cohomological long exact sequence to create unique lifts of finite
dimensional invariant subspaces of one term of the sequence to invariant
subspaces of the preceding term. This allows us to take invariant cohomological
classes and under the right circumstances construct unique currents of a given
type, including unique measures of a given type, that represent those classes
and are invariant under pullback. A dynamically interesting self map may have a
plethora of invariant measures, so the uniquess of the constructed currents is
important. It means that if local growth is not too big compared to the growth
rate of the cohomological class then the expanding cohomological class gives
sufficient ""marching orders"" to the system to prohibit the formation of any
other such invariant current of the same type (say from some local dynamical
subsystem). Because we use subsheaves of the sheaf of currents we give
conditions under which a subsheaf will have the same cohomology as the sheaf
containing it. Using a smoothing argument this allows us to show that the sheaf
cohomology of the currents under consideration can be canonically identified
with the deRham cohomology groups. Our main theorem can be applied in both the
smooth and holomorphic setting.
"
math,"Capacity of a Multiple-Antenna Fading Channel with a Quantized Precoding
  Matrix","  Given a multiple-input multiple-output (MIMO) channel, feedback from the
receiver can be used to specify a transmit precoding matrix, which selectively
activates the strongest channel modes. Here we analyze the performance of
Random Vector Quantization (RVQ), in which the precoding matrix is selected
from a random codebook containing independent, isotropically distributed
entries. We assume that channel elements are i.i.d. and known to the receiver,
which relays the optimal (rate-maximizing) precoder codebook index to the
transmitter using B bits. We first derive the large system capacity of
beamforming (rank-one precoding matrix) as a function of B, where large system
refers to the limit as B and the number of transmit and receive antennas all go
to infinity with fixed ratios. With beamforming RVQ is asymptotically optimal,
i.e., no other quantization scheme can achieve a larger asymptotic rate. The
performance of RVQ is also compared with that of a simpler reduced-rank scalar
quantization scheme in which the beamformer is constrained to lie in a random
subspace. We subsequently consider a precoding matrix with arbitrary rank, and
approximate the asymptotic RVQ performance with optimal and linear receivers
(matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show
that these approximations accurately predict the performance of finite-size
systems of interest. Given a target spectral efficiency, numerical examples
show that the amount of feedback required by the linear MMSE receiver is only
slightly more than that required by the optimal receiver, whereas the matched
filter can require significantly more feedback.
"
math,Non-monotone convergence in the quadratic Wasserstein distance,"  We give an easy counter-example to Problem 7.20 from C. Villani's book on
mass transport: in general, the quadratic Wasserstein distance between $n$-fold
normalized convolutions of two given measures fails to decrease monotonically.
"
math,"The exact asymptotic of the collision time tail distribution for
  independent Brownian particles with different drifts","  In this note we consider the time of the collision $\tau$ for $n$ independent
Brownian motions $X^1_t,...,X_t^n$ with drifts $a_1,...,a_n$, each starting
from $x=(x_1,...,x_n)$, where $x_1<...<x_n$. We show the exact asymptotics of
$P_x(\tau>t) = C h(x)t^{-\alpha}e^{-\gamma t}(1 + o(1))$ as $t\to\infty$ and
identify $C,h(x),\alpha,\gamma$ in terms of the drifts.
"
math,A Symplectic Test of the L-Functions Ratios Conjecture,"  Recently Conrey, Farmer and Zirnbauer conjectured formulas for the averages
over a family of ratios of products of shifted L-functions. Their L-functions
Ratios Conjecture predicts both the main and lower order terms for many
problems, ranging from n-level correlations and densities to mollifiers and
moments to vanishing at the central point. There are now many results showing
agreement between the main terms of number theory and random matrix theory;
however, there are very few families where the lower order terms are known.
These terms often depend on subtle arithmetic properties of the family, and
provide a way to break the universality of behavior. The L-functions Ratios
Conjecture provides a powerful and tractable way to predict these terms. We
test a specific case here, that of the 1-level density for the symplectic
family of quadratic Dirichlet characters arising from even fundamental
discriminants d \le X. For test functions supported in (-1/3, 1/3) we calculate
all the lower order terms up to size O(X^{-1/2+epsilon}) and observe perfect
agreement with the conjecture (for test functions supported in (-1, 1) we show
agreement up to errors of size O(X^{-epsilon}) for any epsilon). Thus for this
family and suitably restricted test functions, we completely verify the Ratios
Conjecture's prediction for the 1-level density.
"
math,Classification of superpotentials,"  We extend our previous classification of superpotentials of ``scalar
curvature type"" for the cohomogeneity one Ricci-flat equations. We now consider
the case not covered in our previous paper, i.e., when some weight vector of
the superpotential lies outside (a scaled translate of) the convex hull of the
weight vectors associated with the scalar curvature function of the principal
orbit. In this situation we show that either the isotropy representation has at
most 3 irreducible summands or the first order subsystem associated to the
superpotential is of the same form as the Calabi-Yau condition for submersion
type metrics on complex line bundles over a Fano K\""ahler-Einstein product.
"
math,"On the Achievable Rate Regions for Interference Channels with Degraded
  Message Sets","  The interference channel with degraded message sets (IC-DMS) refers to a
communication model in which two senders attempt to communicate with their
respective receivers simultaneously through a common medium, and one of the
senders has complete and a priori (non-causal) knowledge about the message
being transmitted by the other. A coding scheme that collectively has
advantages of cooperative coding, collaborative coding, and dirty paper coding,
is developed for such a channel. With resorting to this coding scheme,
achievable rate regions of the IC-DMS in both discrete memoryless and Gaussian
cases are derived, which, in general, include several previously known rate
regions. Numerical examples for the Gaussian case demonstrate that in the
high-interference-gain regime, the derived achievable rate regions offer
considerable improvements over these existing results.
"
math,"Weak and Strong Taylor methods for numerical solutions of stochastic
  differential equations","  We apply results of Malliavin-Thalmaier-Watanabe for strong and weak Taylor
expansions of solutions of perturbed stochastic differential equations (SDEs).
In particular, we work out weight expressions for the Taylor coefficients of
the expansion. The results are applied to LIBOR market models in order to deal
with the typical stochastic drift and with stochastic volatility. In contrast
to other accurate methods like numerical schemes for the full SDE, we obtain
easily tractable expressions for accurate pricing. In particular, we present an
easily tractable alternative to ``freezing the drift'' in LIBOR market models,
which has an accuracy similar to the full numerical scheme. Numerical examples
underline the results.
"
math,"Unit groups of integral finite group rings with no noncyclic abelian
  finite subgroups","  It is shown that in the units of augmentation one of an integral group ring
$\mathbb{Z} G$ of a finite group $G$, a noncyclic subgroup of order $p^{2}$,
for some odd prime $p$, exists only if such a subgroup exists in $G$. The
corresponding statement for $p=2$ holds by the Brauer--Suzuki theorem, as
recently observed by W. Kimmerle.
"
math,Convergence of a finite volume scheme for the incompressible fluids,"  We consider a finite volume scheme for the two-dimensional incompressible
Navier-Stokes equations. We use a triangular mesh. The unknowns for the
velocity and pressure are respectively piecewise constant and affine. We use a
projection method to deal with the incompressibility constraint. In a former
paper, the stability of the scheme has been proven. We infer from it its
convergence.
"
math,Proper J-holomorphic discs in Stein domains of dimension 2,"  We prove the existence of global Bishop discs in a strictly pseudoconvex
Stein domain in an almost complex manifold of complex dimension 2.
"
math,"The Hardy-Lorentz Spaces $H^{p,q}(R^n)$","  In this paper we consider the Hardy-Lorentz spaces $H^{p,q}(R^n)$, with
$0<p\le 1$, $0<q\le \infty$. We discuss the atomic decomposition of the
elements in these spaces, their interpolation properties, and the behavior of
singular integrals and other operators acting on them.
"
math,"Distribution of integral Fourier Coefficients of a Modular Form of Half
  Integral Weight Modulo Primes","  Recently, Bruinier and Ono classified cusp forms $f(z) := \sum_{n=0}^{\infty}
a_f(n)q ^n \in S_{\lambda+1/2}(\Gamma_0(N),\chi)\cap \mathbb{Z}[[q]]$ that does
not satisfy a certain distribution property for modulo odd primes $p$. In this
paper, using Rankin-Cohen Bracket, we extend this result to modular forms of
half integral weight for primes $p \geq 5$. As applications of our main theorem
we derive distribution properties, for modulo primes $p\geq5$, of traces of
singular moduli and Hurwitz class number. We also study an analogue of Newman's
conjecture for overpartitions.
"
math,Growing Perfect Decagonal Quasicrystals by Local Rules,"  A local growth algorithm for a decagonal quasicrystal is presented. We show
that a perfect Penrose tiling (PPT) layer can be grown on a decapod tiling
layer by a three dimensional (3D) local rule growth. Once a PPT layer begins to
form on the upper layer, successive 2D PPT layers can be added on top resulting
in a perfect decagonal quasicrystalline structure in bulk with a point defect
only on the bottom surface layer. Our growth rule shows that an ideal
quasicrystal structure can be constructed by a local growth algorithm in 3D,
contrary to the necessity of non-local information for a 2D PPT growth.
"
math,Maximum solutions of normalized Ricci flows on 4-manifolds,"  We consider maximum solution $g(t)$, $t\in [0, +\infty)$, to the normalized
Ricci flow. Among other things, we prove that, if $(M, \omega) $ is a smooth
compact symplectic 4-manifold such that $b_2^+(M)>1$ and let
$g(t),t\in[0,\infty)$, be a solution to (1.3) on $M$ whose Ricci curvature
satisfies that $|\text{Ric}(g(t))|\leq 3$ and additionally $\chi(M)=3 \tau
(M)>0$, then there exists an $m\in \mathbb{N}$, and a sequence of points
$\{x_{j,k}\in M\}$, $j=1, ..., m$, satisfying that, by passing to a
subsequence, $$(M, g(t_{k}+t), x_{1,k},..., x_{m,k})
\stackrel{d_{GH}}\longrightarrow (\coprod_{j=1}^m N_j, g_{\infty},
x_{1,\infty}, ...,, x_{m,\infty}),$$ $t\in [0, \infty)$, in the $m$-pointed
Gromov-Hausdorff sense for any sequence $t_{k}\longrightarrow \infty$, where
$(N_{j}, g_{\infty})$, $j=1,..., m$, are complete complex hyperbolic orbifolds
of complex dimension 2 with at most finitely many isolated orbifold points.
Moreover, the convergence is $C^{\infty}$ in the non-singular part of
$\coprod_1^m N_{j}$ and
$\text{Vol}_{g_{0}}(M)=\sum_{j=1}^{m}\text{Vol}_{g_{\infty}}(N_{j})$, where
$\chi(M)$ (resp. $\tau(M)$) is the Euler characteristic (resp. signature) of
$M$.
"
math,Monoid generalizations of the Richard Thompson groups,"  The groups G_{k,1} of Richard Thompson and Graham Higman can be generalized
in a natural way to monoids, that we call M_{k,1}, and to inverse monoids,
called Inv_{k,1}; this is done by simply generalizing bijections to partial
functions or partial injective functions. The monoids M_{k,1} have connections
with circuit complexity (studied in another paper). Here we prove that M_{k,1}
and Inv_{k,1} are congruence-simple for all k. Their Green relations J and D
are characterized: M_{k,1} and Inv_{k,1} are J-0-simple, and they have k-1
non-zero D-classes. They are submonoids of the multiplicative part of the Cuntz
algebra O_k. They are finitely generated, and their word problem over any
finite generating set is in P. Their word problem is coNP-complete over certain
infinite generating sets.
  Changes in this version: Section 4 has been thoroughly revised, and errors
have been corrected; however, the main results of Section 4 do not change.
Sections 1, 2, and 3 are unchanged, except for the proof of Theorem 2.3, which
was incomplete; a complete proof was published in the Appendix of reference
[6], and is also given here.
"
math,"Universal Source Coding for Monotonic and Fast Decaying Monotonic
  Distributions","  We study universal compression of sequences generated by monotonic
distributions. We show that for a monotonic distribution over an alphabet of
size $k$, each probability parameter costs essentially $0.5 \log (n/k^3)$ bits,
where $n$ is the coded sequence length, as long as $k = o(n^{1/3})$. Otherwise,
for $k = O(n)$, the total average sequence redundancy is $O(n^{1/3+\epsilon})$
bits overall. We then show that there exists a sub-class of monotonic
distributions over infinite alphabets for which redundancy of
$O(n^{1/3+\epsilon})$ bits overall is still achievable. This class contains
fast decaying distributions, including many distributions over the integers and
geometric distributions. For some slower decays, including other distributions
over the integers, redundancy of $o(n)$ bits overall is achievable, where a
method to compute specific redundancy rates for such distributions is derived.
The results are specifically true for finite entropy monotonic distributions.
Finally, we study individual sequence redundancy behavior assuming a sequence
is governed by a monotonic distribution. We show that for sequences whose
empirical distributions are monotonic, individual redundancy bounds similar to
those in the average case can be obtained. However, even if the monotonicity in
the empirical distribution is violated, diminishing per symbol individual
sequence redundancies with respect to the monotonic maximum likelihood
description length may still be achievable.
"
math,On the Markov trace for Temperley--Lieb algebras of type $E_n$,"  We show that there is a unique Markov trace on the tower of Temperley--Lieb
type quotients of Hecke algebras of Coxeter type $E_n$ (for all $n \geq 6$). We
explain in detail how this trace may be computed easily using tom Dieck's
calculus of diagrams. As applications, we show how to use the trace to show
that the diagram representation is faithful, and to compute leading
coefficients of certain Kazhdan--Lusztig polynomials.
"
math,Spline Single-Index Prediction Model,"  For the past two decades, single-index model, a special case of projection
pursuit regression, has proven to be an efficient way of coping with the high
dimensional problem in nonparametric regression. In this paper, based on weakly
dependent sample, we investigate the single-index prediction (SIP) model which
is robust against deviation from the single-index model. The single-index is
identified by the best approximation to the multivariate prediction function of
the response variable, regardless of whether the prediction function is a
genuine single-index function. A polynomial spline estimator is proposed for
the single-index prediction coefficients, and is shown to be root-n consistent
and asymptotically normal. An iterative optimization routine is used which is
sufficiently fast for the user to analyze large data of high dimension within
seconds. Simulation experiments have provided strong evidence that corroborates
with the asymptotic theory. Application of the proposed procedure to the rive
flow data of Iceland has yielded superior out-of-sample rolling forecasts.
"
math,"Metropolis algorithm and equienergy sampling for two mean field spin
  systems","  In this paper we study the Metropolis algorithm in connection with two
mean--field spin systems, the so called mean--field Ising model and the
Blume--Emery--Griffiths model. In both this examples the naive choice of
proposal chain gives rise, for some parameters, to a slowly mixing Metropolis
chain, that is a chain whose spectral gap decreases exponentially fast (in the
dimension $N$ of the problem). Here we show how a slight variant in the
proposal chain can avoid this problem, keeping the mean computational cost
similar to the cost of the usual Metropolis. More precisely we prove that, with
a suitable variant in the proposal, the Metropolis chain has a spectral gap
which decreases polynomially in 1/N. Using some symmetry structure of the
energy, the method rests on allowing appropriate jumps within the energy level
of the starting state.
"
math,"Optimal Routing for Decode-and-Forward based Cooperation in Wireless
  Networks","  We investigate cooperative wireless relay networks in which the nodes can
help each other in data transmission. We study different coding strategies in
the single-source single-destination network with many relay nodes. Given the
myriad of ways in which nodes can cooperate, there is a natural routing
problem, i.e., determining an ordered set of nodes to relay the data from the
source to the destination. We find that for a given route, the
decode-and-forward strategy, which is an information theoretic cooperative
coding strategy, achieves rates significantly higher than that achievable by
the usual multi-hop coding strategy, which is a point-to-point non-cooperative
coding strategy. We construct an algorithm to find an optimal route (in terms
of rate maximizing) for the decode-and-forward strategy. Since the algorithm
runs in factorial time in the worst case, we propose a heuristic algorithm that
runs in polynomial time. The heuristic algorithm outputs an optimal route when
the nodes transmit independent codewords. We implement these coding strategies
using practical low density parity check codes to compare the performance of
the strategies on different routes.
"
math,$p$-Adic Haar multiresolution analysis,"  In this paper, the notion of {\em $p$-adic multiresolution analysis (MRA)} is
introduced. We use a ``natural'' refinement equation whose solution (a
refinable function) is the characteristic function of the unit disc. This
equation reflects the fact that the characteristic function of the unit disc is
the sum of $p$ characteristic functions of disjoint discs of radius $p^{-1}$.
The case $p=2$ is studied in detail. Our MRA is a 2-adic analog of the real
Haar MRA. But in contrast to the real setting, the refinable function
generating our Haar MRA is periodic with period 1, which never holds for real
refinable functions. This fact implies that there exist infinity many different
2-adic orthonormal wavelet bases in ${\cL}^2(\bQ_2)$ generated by the same Haar
MRA. All of these bases are constructed. Since $p$-adic pseudo-differential
operators are closely related to wavelet-type bases, our bases can be
intensively used for applications.
"
cs,The Genetic Programming Collaboration Network and its Communities,"  Useful information about scientific collaboration structures and patterns can
be inferred from computer databases of published papers. The genetic
programming bibliography is the most complete reference of papers on GP\@. In
addition to locating publications, it contains coauthor and coeditor
relationships from which a more complete picture of the field emerges. We treat
these relationships as undirected small world graphs whose study reveals the
community structure of the GP collaborative social network. Automatic analysis
discovers new communities and highlights new facets of them. The investigation
reveals many similarities between GP and coauthorship networks in other
scientific fields but also some subtle differences such as a smaller central
network component and a high clustering.
"
cs,Intricate Knots in Proteins: Function and Evolution,"  A number of recently discovered protein structures incorporate a rather
unexpected structural feature: a knot in the polypeptide backbone. These knots
are extremely rare, but their occurrence is likely connected to protein
function in as yet unexplored fashion. Our analysis of the complete Protein
Data Bank reveals several new knots which, along with previously discovered
ones, can shed light on such connections. In particular, we identify the most
complex knot discovered to date in human ubiquitin hydrolase, and suggest that
its entangled topology protects it against unfolding and degradation by the
proteasome. Knots in proteins are typically preserved across species and
sometimes even across kingdoms. However, we also identify a knot which only
appears in some transcarbamylases while being absent in homologous proteins of
similar structure. The emergence of the knot is accompanied by a shift in the
enzymatic function of the protein. We suggest that the simple insertion of a
short DNA fragment into the gene may suffice to turn an unknotted into a
knotted structure in this protein.
"
cs,The Complexity of HCP in Digraps with Degree Bound Two,"  The Hamiltonian cycle problem (HCP) in digraphs D with degree bound two is
solved by two mappings in this paper. The first bijection is between an
incidence matrix C_{nm} of simple digraph and an incidence matrix F of balanced
bipartite undirected graph G; The second mapping is from a perfect matching of
G to a cycle of D. It proves that the complexity of HCP in D is polynomial, and
finding a second non-isomorphism Hamiltonian cycle from a given Hamiltonian
digraph with degree bound two is also polynomial. Lastly it deduces P=NP base
on the results.
"
cs,"The discrete dipole approximation for simulation of light scattering by
  particles much larger than the wavelength","  In this manuscript we investigate the capabilities of the Discrete Dipole
Approximation (DDA) to simulate scattering from particles that are much larger
than the wavelength of the incident light, and describe an optimized publicly
available DDA computer program that processes the large number of dipoles
required for such simulations. Numerical simulations of light scattering by
spheres with size parameters x up to 160 and 40 for refractive index m=1.05 and
2 respectively are presented and compared with exact results of the Mie theory.
Errors of both integral and angle-resolved scattering quantities generally
increase with m and show no systematic dependence on x. Computational times
increase steeply with both x and m, reaching values of more than 2 weeks on a
cluster of 64 processors. The main distinctive feature of the computer program
is the ability to parallelize a single DDA simulation over a cluster of
computers, which allows it to simulate light scattering by very large
particles, like the ones that are considered in this manuscript. Current
limitations and possible ways for improvement are discussed.
"
cs,Resonant activation in bistable semiconductor lasers,"  We theoretically investigate the possibility of observing resonant activation
in the hopping dynamics of two-mode semiconductor lasers. We present a series
of simulations of a rate-equations model under random and periodic modulation
of the bias current. In both cases, for an optimal choice of the modulation
time-scale, the hopping times between the stable lasing modes attain a minimum.
The simulation data are understood by means of an effective one-dimensional
Langevin equation with multiplicative fluctuations. Our conclusions apply to
both Edge Emitting and Vertical Cavity Lasers, thus opening the way to several
experimental tests in such optical systems.
"
cs,"Proposal for an Enhanced Optical Cooling System Test in an Electron
  Storage Ring","  We are proposing to test experimentally the new idea of Enhanced Optical
Cooling (EOC) in an electron storage ring. This experiment will confirm new
fundamental processes in beam physics and will demonstrate new unique
possibilities with this cooling technique. It will open important applications
of EOC in nuclear physics, elementary particle physics and in Light Sources
(LS) based on high brightness electron and ion beams.
"
cs,"Lattice Boltzmann inverse kinetic approach for the incompressible
  Navier-Stokes equations","  In spite of the large number of papers appeared in the past which are devoted
to the lattice Boltzmann (LB) methods, basic aspects of the theory still remain
unchallenged. An unsolved theoretical issue is related to the construction of a
discrete kinetic theory which yields \textit{exactly} the fluid equations,
i.e., is non-asymptotic (here denoted as \textit{LB inverse kinetic theory}).
The purpose of this paper is theoretical and aims at developing an inverse
kinetic approach of this type. In principle infinite solutions exist to this
problem but the freedom can be exploited in order to meet important
requirements. In particular, the discrete kinetic theory can be defined so that
it yields exactly the fluid equation also for arbitrary non-equilibrium (but
suitably smooth) kinetic distribution functions and arbitrarily close to the
boundary of the fluid domain. Unlike previous entropic LB methods the theorem
can be obtained without functional constraints on the class of the initial
distribution functions. Possible realizations of the theory and asymptotic
approximations are provided which permit to determine the fluid equations
\textit{with prescribed accuracy.} As a result, asymptotic accuracy estimates
of customary LB approaches and comparisons with the Chorin artificial
compressibility method are discussed.
"
cs,"Formation of density singularities in ideal hydrodynamics of freely
  cooling inelastic gases: a family of exact solutions","  We employ granular hydrodynamics to investigate a paradigmatic problem of
clustering of particles in a freely cooling dilute granular gas. We consider
large-scale hydrodynamic motions where the viscosity and heat conduction can be
neglected, and one arrives at the equations of ideal gas dynamics with an
additional term describing bulk energy losses due to inelastic collisions. We
employ Lagrangian coordinates and derive a broad family of exact non-stationary
analytical solutions that depend only on one spatial coordinate. These
solutions exhibit a new type of singularity, where the gas density blows up in
a finite time when starting from smooth initial conditions. The density blowups
signal formation of close-packed clusters of particles. As the density blow-up
time $t_c$ is approached, the maximum density exhibits a power law $\sim
(t_c-t)^{-2}$. The velocity gradient blows up as $\sim - (t_c-t)^{-1}$ while
the velocity itself remains continuous and develops a cusp (rather than a shock
discontinuity) at the singularity. The gas temperature vanishes at the
singularity, and the singularity follows the isobaric scenario: the gas
pressure remains finite and approximately uniform in space and constant in time
close to the singularity. An additional exact solution shows that the density
blowup, of the same type, may coexist with an ""ordinary"" shock, at which the
hydrodynamic fields are discontinuous but finite. We confirm stability of the
exact solutions with respect to small one-dimensional perturbations by solving
the ideal hydrodynamic equations numerically. Furthermore, numerical solutions
show that the local features of the density blowup hold universally,
independently of details of the initial and boundary conditions.
"
cs,"A Hierarchical Approach for Dependability Analysis of a Commercial
  Cache-Based RAID Storage Architecture","  We present a hierarchical simulation approach for the dependability analysis
and evaluation of a highly available commercial cache-based RAID storage
system. The archi-tecture is complex and includes several layers of
overlap-ping error detection and recovery mechanisms. Three ab-straction levels
have been developed to model the cache architecture, cache operations, and
error detection and recovery mechanism. The impact of faults and errors
oc-curring in the cache and in the disks is analyzed at each level of the
hierarchy. A simulation submodel is associated with each abstraction level. The
models have been devel-oped using DEPEND, a simulation-based environment for
system-level dependability analysis, which provides facili-ties to inject
faults into a functional behavior model, to simulate error detection and
recovery mechanisms, and to evaluate quantitative measures. Several fault
models are defined for each submodel to simulate cache component failures, disk
failures, transmission errors, and data errors in the cache memory and in the
disks. Some of the parame-ters characterizing fault injection in a given
submodel cor-respond to probabilities evaluated from the simulation of the
lower-level submodel. Based on the proposed method-ology, we evaluate and
analyze 1) the system behavior un-der a real workload and high error rate
(focusing on error bursts), 2) the coverage of the error detection mechanisms
implemented in the system and the error latency distribu-tions, and 3) the
accumulation of errors in the cache and in the disks.
"
cs,An architecture-based dependability modeling framework using AADL,"  For efficiency reasons, the software system designers' will is to use an
integrated set of methods and tools to describe specifications and designs, and
also to perform analyses such as dependability, schedulability and performance.
AADL (Architecture Analysis and Design Language) has proved to be efficient for
software architecture modeling. In addition, AADL was designed to accommodate
several types of analyses. This paper presents an iterative dependency-driven
approach for dependability modeling using AADL. It is illustrated on a small
example. This approach is part of a complete framework that allows the
generation of dependability analysis and evaluation models from AADL models to
support the analysis of software and system architectures, in critical
application domains.
"
cs,Quantum electromagnetic X-waves,"  We show that two distinct quantum states of the electromagnetic field can be
associated to a classical vector X wave or a propagation-invariant solution of
Maxwell equations. The difference between the two states is of pure quantum
mechanical origin since they are internally entangled and disentangled,
respectively and can be generated by different linear or nonlinear processes.
Detection and generation of Schr\""odinger-cat states comprising two entangled
X-waves and their possible applications are discussed.
"
cs,Three dimensional cooling and trapping with a narrow line,"  The intercombination line of Strontium at 689nm is successfully used in laser
cooling to reach the photon recoil limit with Doppler cooling in a
magneto-optical traps (MOT). In this paper we present a systematic study of the
loading efficiency of such a MOT. Comparing the experimental results to a
simple model allows us to discuss the actual limitation of our apparatus. We
also study in detail the final MOT regime emphasizing the role of gravity on
the position, size and temperature along the vertical and horizontal
directions. At large laser detuning, one finds an unusual situation where
cooling and trapping occur in the presence of a high bias magnetic field.
"
cs,"Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian
  Vector Broadcast Channels","  MIMO technology is one of the most significant advances in the past decade to
increase channel capacity and has a great potential to improve network capacity
for mesh networks. In a MIMO-based mesh network, the links outgoing from each
node sharing the common communication spectrum can be modeled as a Gaussian
vector broadcast channel. Recently, researchers showed that ``dirty paper
coding'' (DPC) is the optimal transmission strategy for Gaussian vector
broadcast channels. So far, there has been little study on how this fundamental
result will impact the cross-layer design for MIMO-based mesh networks. To fill
this gap, we consider the problem of jointly optimizing DPC power allocation in
the link layer at each node and multihop/multipath routing in a MIMO-based mesh
networks. It turns out that this optimization problem is a very challenging
non-convex problem. To address this difficulty, we transform the original
problem to an equivalent problem by exploiting the channel duality. For the
transformed problem, we develop an efficient solution procedure that integrates
Lagrangian dual decomposition method, conjugate gradient projection method
based on matrix differential calculus, cutting-plane method, and subgradient
method. In our numerical example, it is shown that we can achieve a network
performance gain of 34.4% by using DPC.
"
cs,Scalar potential model progress,"  Because observations of galaxies and clusters have been found inconsistent
with General Relativity (GR), the focus of effort in developing a Scalar
Potential Model (SPM) has been on the examination of galaxies and clusters. The
SPM has been found to be consistent with cluster cellular structure, the flow
of IGM from spiral galaxies to elliptical galaxies, intergalactic redshift
without an expanding universe, discrete redshift, rotation curve (RC) data
without dark matter, asymmetric RCs, galaxy central mass, galaxy central
velocity dispersion, and the Pioneer Anomaly. In addition, the SPM suggests a
model of past expansion, past contraction, and current expansion of the
universe. GR corresponds to the SPM in the limit in which the effect of the
Sources and Sinks approximate a flat scalar potential field such as between
clusters and on the solar system scale, which is small relative to the distance
to a Source.
"
cs,A Multiphilic Descriptor for Chemical Reactivity and Selectivity,"  In line with the local philicity concept proposed by Chattaraj et al.
(Chattaraj, P. K.; Maiti, B.; Sarkar, U. J. Phys. Chem. A. 2003, 107, 4973) and
a dual descriptor derived by Toro-Labbe and coworkers (Morell, C.; Grand, A.;
Toro-Labbe, A. J. Phys. Chem. A. 2005, 109, 205), we propose a multiphilic
descriptor. It is defined as the difference between nucleophilic (Wk+) and
electrophilic (Wk-) condensed philicity functions. This descriptor is capable
of simultaneously explaining the nucleophilicity and electrophilicity of the
given atomic sites in the molecule. Variation of these quantities along the
path of a soft reaction is also analyzed. Predictive ability of this descriptor
has been successfully tested on the selected systems and reactions.
Corresponding force profiles are also analyzed in some representative cases.
Also, to study the intra- and intermolecular reactivities another related
descriptor namely, the nucleophilicity excess (DelW-+) for a nucleophile, over
the electrophilicity in it has been defined and tested on all-metal aromatic
compounds.
"
cs,The World as Evolving Information,"  This paper discusses the benefits of describing the world as information,
especially in the study of the evolution of life and cognition. Traditional
studies encounter problems because it is difficult to describe life and
cognition in terms of matter and energy, since their laws are valid only at the
physical scale. However, if matter and energy, as well as life and cognition,
are described in terms of information, evolution can be described consistently
as information becoming more complex.
  The paper presents eight tentative laws of information, valid at multiple
scales, which are generalizations of Darwinian, cybernetic, thermodynamic,
psychological, philosophical, and complexity principles. These are further used
to discuss the notions of life, cognition and their evolution.
"
cs,Approaching the Heisenberg limit in an atom laser,"  We present experimental and theoretical results showing the improved beam
quality and reduced divergence of an atom laser produced by an optical Raman
transition, compared to one produced by an RF transition. We show that Raman
outcoupling can eliminate the diverging lens effect that the condensate has on
the outcoupled atoms. This substantially improves the beam quality of the atom
laser, and the improvement may be greater than a factor of ten for experiments
with tight trapping potentials. We show that Raman outcoupling can produce atom
lasers whose quality is only limited by the wavefunction shape of the
condensate that produces them, typically a factor of 1.3 above the Heisenberg
limit.
"
cs,"Geometric Complexity Theory VI: the flip via saturated and positive
  integer programming in representation theory and algebraic geometry","  This article belongs to a series on geometric complexity theory (GCT), an
approach to the P vs. NP and related problems through algebraic geometry and
representation theory. The basic principle behind this approach is called the
flip. In essence, it reduces the negative hypothesis in complexity theory (the
lower bound problems), such as the P vs. NP problem in characteristic zero, to
the positive hypothesis in complexity theory (the upper bound problems):
specifically, to showing that the problems of deciding nonvanishing of the
fundamental structural constants in representation theory and algebraic
geometry, such as the well known plethysm constants--or rather certain relaxed
forms of these decision probelms--belong to the complexity class P. In this
article, we suggest a plan for implementing the flip, i.e., for showing that
these relaxed decision problems belong to P. This is based on the reduction of
the preceding complexity-theoretic positive hypotheses to mathematical
positivity hypotheses: specifically, to showing that there exist positive
formulae--i.e. formulae with nonnegative coefficients--for the structural
constants under consideration and certain functions associated with them. These
turn out be intimately related to the similar positivity properties of the
Kazhdan-Lusztig polynomials and the multiplicative structural constants of the
canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum
groups. The known proofs of these positivity properties depend on the Riemann
hypothesis over finite fields and the related results. Thus the reduction here,
in conjunction with the flip, in essence, says that the validity of the P vs.
NP conjecture in characteristic zero is intimately linked to the Riemann
hypothesis over finite fields and related problems.
"
cs,Universal Forces and the Dark Energy Problem,"  The Dark Energy problem is forcing us to re-examine our models and our
understanding of relativity and space-time. Here a novel idea of Fundamental
Forces is introduced. This allows us to perceive the General Theory of
Relativity and Einstein's Equation from a new pesrpective. In addition to
providing us with an improved understanding of space and time, it will be shown
how it leads to a resolution of the Dark Energy problem.
"
cs,On Punctured Pragmatic Space-Time Codes in Block Fading Channel,"  This paper considers the use of punctured convolutional codes to obtain
pragmatic space-time trellis codes over block-fading channel. We show that good
performance can be achieved even when puncturation is adopted and that we can
still employ the same Viterbi decoder of the convolutional mother code by using
approximated metrics without increasing the complexity of the decoding
operations.
"
cs,"Approximate Selection Rule for Orbital Angular Momentum in Atomic
  Radiative Transitions","  We demonstrate that radiative transitions with \Delta l = - 1 are strongly
dominating for all values of n and l, except small region where l << n.
"
cs,Inapproximability of Maximum Weighted Edge Biclique and Its Applications,"  Given a bipartite graph $G = (V_1,V_2,E)$ where edges take on {\it both}
positive and negative weights from set $\mathcal{S}$, the {\it maximum weighted
edge biclique} problem, or $\mathcal{S}$-MWEB for short, asks to find a
bipartite subgraph whose sum of edge weights is maximized. This problem has
various applications in bioinformatics, machine learning and databases and its
(in)approximability remains open. In this paper, we show that for a wide range
of choices of $\mathcal{S}$, specifically when $| \frac{\min\mathcal{S}} {\max
\mathcal{S}} | \in \Omega(\eta^{\delta-1/2}) \cap O(\eta^{1/2-\delta})$ (where
$\eta = \max\{|V_1|, |V_2|\}$, and $\delta \in (0,1/2]$), no polynomial time
algorithm can approximate $\mathcal{S}$-MWEB within a factor of $n^{\epsilon}$
for some $\epsilon > 0$ unless $\mathsf{RP = NP}$. This hardness result gives
justification of the heuristic approaches adopted for various applied problems
in the aforementioned areas, and indicates that good approximation algorithms
are unlikely to exist. Specifically, we give two applications by showing that:
1) finding statistically significant biclusters in the SAMBA model, proposed in
\cite{Tan02} for the analysis of microarray data, is
$n^{\epsilon}$-inapproximable; and 2) no polynomial time algorithm exists for
the Minimum Description Length with Holes problem \cite{Bu05} unless
$\mathsf{RP=NP}$.
"
cs,"Monitoring spatially heterogeneous dynamics in a drying colloidal thin
  film","  We report on a new type of experiment that enables us to monitor spatially
and temporally heterogeneous dynamic properties in complex fluids. Our approach
is based on the analysis of near-field speckles produced by light diffusely
reflected from the superficial volume of a strongly scattering medium. By
periodic modulation of an incident speckle beam we obtain pixel-wise ensemble
averages of the structure function coefficient, a measure of the dynamic
activity. To illustrate the application of our approach we follow the different
stages in the drying process of a colloidal thin film. We show that we can
access ensemble averaged dynamic properties on length scales as small as ten
micrometers over the full field of view.
"
cs,"Convergence of the discrete dipole approximation. II. An extrapolation
  technique to increase the accuracy","  We propose an extrapolation technique that allows accuracy improvement of the
discrete dipole approximation computations. The performance of this technique
was studied empirically based on extensive simulations for 5 test cases using
many different discretizations. The quality of the extrapolation improves with
refining discretization reaching extraordinary performance especially for
cubically shaped particles. A two order of magnitude decrease of error was
demonstrated. We also propose estimates of the extrapolation error, which were
proven to be reliable. Finally we propose a simple method to directly separate
shape and discretization errors and illustrated this for one test case.
"
cs,Non-extensive thermodynamics of 1D systems with long-range interaction,"  A new approach to non-extensive thermodynamical systems with non-additive
energy and entropy is proposed. The main idea of the paper is based on the
statistical matching of the thermodynamical systems with the additive
multi-step Markov chains. This general approach is applied to the Ising spin
chain with long-range interaction between its elements. The asymptotical
expressions for the energy and entropy of the system are derived for the
limiting case of weak interaction. These thermodynamical quantities are found
to be non-proportional to the length of the system (number of its particle).
"
cs,Equation of state for dense hydrogen and plasma phase transition,"  We calculate the equation of state of dense hydrogen within the chemical
picture. Fluid variational theory is generalized for a multi-component system
of molecules, atoms, electrons, and protons. Chemical equilibrium is supposed
for the reactions dissociation and ionization. We identify the region of
thermodynamic instability which is related to the plasma phase transition. The
reflectivity is calculated along the Hugoniot curve and compared with
experimental results. The equation-of-state data is used to calculate the
pressure and temperature profiles for the interior of Jupiter.
"
cs,Leaky modes of a left-handed slab,"  Using complex plane analysis we show that left-handed slab may support either
leaky slab waves, which are backward because of negative refraction, or leaky
surface waves, which are backward or forward depending on the propagation
direction of the surface wave itself. Moreover, there is a general connection
between the reflection coefficient of the left-handed slab and the one of the
corresponding right-handed slab (with opposite permittivity and permeability)
so that leaky slab modes are excited for the same angle of incidence of the
impinging beam for both structures. Many negative giant lateral shifts can be
explained by the excitation of these leaky modes.
"
cs,Estimation of experimental data redundancy and related statistics,"  Redundancy of experimental data is the basic statistic from which the
complexity of a natural phenomenon and the proper number of experiments needed
for its exploration can be estimated. The redundancy is expressed by the
entropy of information pertaining to the probability density function of
experimental variables. Since the calculation of entropy is inconvenient due to
integration over a range of variables, an approximate expression for redundancy
is derived that includes only a sum over the set of experimental data about
these variables. The approximation makes feasible an efficient estimation of
the redundancy of data along with the related experimental information and
information cost function. From the experimental information the complexity of
the phenomenon can be simply estimated, while the proper number of experiments
needed for its exploration can be determined from the minimum of the cost
function. The performance of the approximate estimation of these statistics is
demonstrated on two-dimensional normally distributed random data.
"
cs,"Rich methane premixed laminar flames doped by light unsaturated
  hydrocarbons - Part I : allene and propyne","  The structure of three laminar premixed rich flames has been investigated: a
pure methane flame and two methane flames doped by allene and propyne,
respectively. The gases of the three flames contain 20.9% (molar) of methane
and 33.4% of oxygen, corresponding to an equivalence ratio of 1.25 for the pure
methane flame. In both doped flames, 2.49% of C3H4 was added, corresponding to
a ratio C3H4/CH4 of 12% and an equivalence ratio of 1.55. The three flames have
been stabilized on a burner at a pressure of 6.7 kPa using argon as dilutant,
with a gas velocity at the burner of 36 cm/s at 333 K. The concentration
profiles of stable species were measured by gas chromatography after sampling
with a quartz microprobe. Quantified species included carbon monoxide and
dioxide, methane, oxygen, hydrogen, ethane, ethylene, acetylene, propyne,
allene, propene, propane, 1,2-butadiene, 1,3-butadiene, 1-butene, isobutene,
1-butyne, vinylacetylene, and benzene. The temperature was measured using a
PtRh (6%)-PtRh (30%) thermocouple settled inside the enclosure and ranged from
700 K close to the burner up to 1850 K. In order to model these new results,
some improvements have been made to a mechanism previously developed in our
laboratory for the reactions of C3-C4 unsaturated hydrocarbons. The main
reaction pathways of consumption of allene and propyne and of formation of C6
aromatic species have been derived from flow rate analyses.
"
cs,"On Almost Periodicity Criteria for Morphic Sequences in Some Particular
  Cases","  In some particular cases we give criteria for morphic sequences to be almost
periodic (=uniformly recurrent). Namely, we deal with fixed points of
non-erasing morphisms and with automatic sequences. In both cases a
polynomial-time algorithm solving the problem is found. A result more or less
supporting the conjecture of decidability of the general problem is given.
"
cs,"Polarization properties of subwavelength hole arrays consisting of
  rectangular holes","  Influence of hole shape on extraordinary optical transmission was
investigated using hole arrays consisting of rectangular holes with different
aspect ratio. It was found that the transmission could be tuned continuously by
rotating the hole array. Further more, a phase was generated in this process,
and linear polarization states could be changed to elliptical polarization
states. This phase was correlated with the aspect ratio of the holes. An
intuitional model was presented to explain these results.
"
cs,"Empirical analysis and statistical modeling of attack processes based on
  honeypots","  Honeypots are more and more used to collect data on malicious activities on
the Internet and to better understand the strategies and techniques used by
attackers to compromise target systems. Analysis and modeling methodologies are
needed to support the characterization of attack processes based on the data
collected from the honeypots. This paper presents some empirical analyses based
on the data collected from the Leurr{\'e}.com honeypot platforms deployed on
the Internet and presents some preliminary modeling studies aimed at fulfilling
such objectives.
"
cs,Failure of the work-Hamiltonian connection for free energy calculations,"  Extensions of statistical mechanics are routinely being used to infer free
energies from the work performed over single-molecule nonequilibrium
trajectories. A key element of this approach is the ubiquitous expression
dW/dt=\partial H(x,t)/ \partial t which connects the microscopic work W
performed by a time-dependent force on the coordinate x with the corresponding
Hamiltonian H(x,t) at time t. Here we show that this connection, as pivotal as
it is, cannot be used to estimate free energy changes. We discuss the
implications of this result for single-molecule experiments and atomistic
molecular simulations and point out possible avenues to overcome these
limitations.
"
cs,"Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless
  Networks","  This paper investigates the many-to-one throughput capacity (and by symmetry,
one-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has
generally been assumed in prior studies that the many-to-one throughput
capacity is upper-bounded by the link capacity L. Throughput capacity L is not
achievable under 802.11. This paper introduces the notion of ""canonical
networks"", which is a class of regularly-structured networks whose capacities
can be analyzed more easily than unstructured networks. We show that the
throughput capacity of canonical networks under 802.11 has an analytical upper
bound of 3L/4 when the source nodes are two or more hops away from the sink;
and simulated throughputs of 0.690L (0.740L) when the source nodes are many
hops away. We conjecture that 3L/4 is also the upper bound for general
networks. When all links have equal length, 2L/3 can be shown to be the upper
bound for general networks. Our simulations show that 802.11 networks with
random topologies operated with AODV routing can only achieve throughputs far
below the upper bounds. Fortunately, by properly selecting routes near the
gateway (or by properly positioning the relay nodes leading to the gateway) to
fashion after the structure of canonical networks, the throughput can be
improved significantly by more than 150%. Indeed, in a dense network, it is
worthwhile to deactivate some of the relay nodes near the sink judiciously.
"
cs,Visualizing Teleportation,"  A novel way of picturing the processing of quantum information is described,
allowing a direct visualization of teleportation of quantum states and
providing a simple and intuitive understanding of this fascinating phenomenon.
The discussion is aimed at providing physicists a method of explaining
teleportation to non-scientists. The basic ideas of quantum physics are first
explained in lay terms, after which these ideas are used with a graphical
description, out of which teleportation arises naturally.
"
cs,"Molecular Synchronization Waves in Arrays of Allosterically Regulated
  Enzymes","  Spatiotemporal pattern formation in a product-activated enzymic reaction at
high enzyme concentrations is investigated. Stochastic simulations show that
catalytic turnover cycles of individual enzymes can become coherent and that
complex wave patterns of molecular synchronization can develop. The analysis
based on the mean-field approximation indicates that the observed patterns
result from the presence of Hopf and wave bifurcations in the considered
system.
"
cs,"Two-scale structure of the electron dissipation region during
  collisionless magnetic reconnection","  Particle in cell (PIC) simulations of collisionless magnetic reconnection are
presented that demonstrate that the electron dissipation region develops a
distinct two-scale structure along the outflow direction. The length of the
electron current layer is found to decrease with decreasing electron mass,
approaching the ion inertial length for a proton-electron plasma. A surprise,
however, is that the electrons form a high-velocity outflow jet that remains
decoupled from the magnetic field and extends large distances downstream from
the x-line. The rate of reconnection remains fast in very large systems,
independent of boundary conditions and the mass of electrons.
"
cs,Shocks in nonlocal media,"  We investigate the formation of collisionless shocks along the spatial
profile of a gaussian laser beam propagating in nonlocal nonlinear media. For
defocusing nonlinearity the shock survives the smoothing effect of the nonlocal
response, though its dynamics is qualitatively affected by the latter, whereas
for focusing nonlinearity it dominates over filamentation. The patterns
observed in a thermal defocusing medium are interpreted in the framework of our
theory.
"
cs,Nuclear Spin Effects in Optical Lattice Clocks,"  We present a detailed experimental and theoretical study of the effect of
nuclear spin on the performance of optical lattice clocks. With a state-mixing
theory including spin-orbit and hyperfine interactions, we describe the origin
of the $^1S_0$-$^3P_0$ clock transition and the differential g-factor between
the two clock states for alkaline-earth(-like) atoms, using $^{87}$Sr as an
example. Clock frequency shifts due to magnetic and optical fields are
discussed with an emphasis on those relating to nuclear structure. An
experimental determination of the differential g-factor in $^{87}$Sr is
performed and is in good agreement with theory. The magnitude of the tensor
light shift on the clock states is also explored experimentally. State specific
measurements with controlled nuclear spin polarization are discussed as a
method to reduce the nuclear spin-related systematic effects to below
10$^{-17}$ in lattice clocks.
"
cs,Turbulent Diffusion of Lines and Circulations,"  We study material lines and passive vectors in a model of turbulent flow at
infinite-Reynolds number, the Kraichnan-Kazantsev ensemble of velocities that
are white-noise in time and rough (Hoelder continuous) in space. It is argued
that the phenomenon of ``spontaneous stochasticity'' generalizes to material
lines and that conservation of circulations generalizes to a ``martingale
property'' of the stochastic process of lines.
"
cs,"Interference effects in above-threshold ionization from diatomic
  molecules: determining the internuclear separation","  We calculate angle-resolved above-threshold ionization spectra for diatomic
molecules in linearly polarized laser fields, employing the strong-field
approximation. The interference structure resulting from the individual
contributions of the different scattering scenarios is discussed in detail,
with respect to the dependence on the internuclear distance and molecular
orientation. We show that, in general, the contributions from the processes in
which the electron is freed at one center and rescatters off the other obscure
the interference maxima and minima obtained from single-center processes.
However, around the boundary of the energy regions for which rescattering has a
classical counterpart, such processes play a negligible role and very clear
interference patterns are observed. In such energy regions, one is able to
infer the internuclear distance from the energy difference between adjacent
interference minima.
"
cs,"A Low Complexity Algorithm and Architecture for Systematic Encoding of
  Hermitian Codes","  We present an algorithm for systematic encoding of Hermitian codes. For a
Hermitian code defined over GF(q^2), the proposed algorithm achieves a run time
complexity of O(q^2) and is suitable for VLSI implementation. The encoder
architecture uses as main blocks q varying-rate Reed-Solomon encoders and
achieves a space complexity of O(q^2) in terms of finite field multipliers and
memory elements.
"
cs,Photon splitting in a laser field,"  Photon splitting due to vacuum polarization in a laser field is considered.
Using an operator technique, we derive the amplitudes for arbitrary strength,
spectral content and polarization of the laser field. The case of a
monochromatic circularly polarized laser field is studied in detail and the
amplitudes are obtained as three-fold integrals. The asymptotic behavior of the
amplitudes for various limits of interest are investigated also in the case of
a linearly polarized laser field. Using the obtained results, the possibility
of experimental observation of the process is discussed.
"
cs,On the dragging of light by a rotating medium,"  When light is passing through a rotating medium the optical polarisation is
rotated. Recently it has been reasoned that this rotation applies also to the
transmitted image (Padgett et al. 2006). We examine these two phenomena by
extending an analysis of Player (1976) to general electromagnetic fields. We
find that in this more general case the wave equation inside the rotating
medium has to be amended by a term which is connected to the orbital angular
momentum of the light. We show that optical spin and orbital angular momentum
account respectively for the rotation of the polarisation and the rotation of
the transmitted image.
"
cs,Frequency modulation Fourier transform spectroscopy,"  A new method, FM-FTS, combining Frequency Modulation heterodyne laser
spectroscopy and Fourier Transform Spectroscopy is presented. It provides
simultaneous sensitive measurement of absorption and dispersion profiles with
broadband spectral coverage capabilities. Experimental demonstration is made on
the overtone spectrum of C2H2 in the 1.5 $\mu$m region.
"
cs,A non-perturbative proof of Bertrand's theorem,"  We discuss an alternative non-perturbative proof of Bertrand's theorem that
leads in a concise way directly to the two allowed fields: the newtonian and
the isotropic harmonic oscillator central fields.
"
cs,"Gravity-induced electric polarization of matter and planetary magnetic
  fields","  This paper has been withdrawn due to copyright reasons.
"
cs,Differential Recursion and Differentially Algebraic Functions,"  Moore introduced a class of real-valued ""recursive"" functions by analogy with
Kleene's formulation of the standard recursive functions. While his concise
definition inspired a new line of research on analog computation, it contains
some technical inaccuracies. Focusing on his ""primitive recursive"" functions,
we pin down what is problematic and discuss possible attempts to remove the
ambiguity regarding the behavior of the differential recursion operator on
partial functions. It turns out that in any case the purported relation to
differentially algebraic functions, and hence to Shannon's model of analog
computation, fails.
"
cs,"Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo
  Codes","  It has been observed that particular rate-1/2 partially systematic parallel
concatenated convolutional codes (PCCCs) can achieve a lower error floor than
that of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can
only be identified by means of an exhaustive search, whilst convergence towards
low bit error probabilities can be problematic when the systematic output of a
rate-1/2 partially systematic PCCC is heavily punctured. In this paper, we
present and study a family of rate-1/2 partially systematic PCCCs, which we
call pseudo-randomly punctured codes. We evaluate their bit error rate
performance and we show that they always yield a lower error floor than that of
their rate-1/3 parent codes. Furthermore, we compare analytic results to
simulations and we demonstrate that their performance converges towards the
error floor region, owning to the moderate puncturing of their systematic
output. Consequently, we propose pseudo-random puncturing as a means of
improving the bandwidth efficiency of a PCCC and simultaneously lowering its
error floor.
"
cs,Lessons Learned from the deployment of a high-interaction honeypot,"  This paper presents an experimental study and the lessons learned from the
observation of the attackers when logged on a compromised machine. The results
are based on a six months period during which a controlled experiment has been
run with a high interaction honeypot. We correlate our findings with those
obtained with a worldwide distributed system of lowinteraction honeypots.
"
cs,"Availability assessment of SunOS/Solaris Unix Systems based on Syslogd
  and wtmpx logfiles : a case study","  This paper presents a measurement-based availability assessment study using
field data collected during a 4-year period from 373 SunOS/Solaris Unix
workstations and servers interconnected through a local area network. We focus
on the estimation of machine uptimes, downtimes and availability based on the
identification of failures that caused total service loss. Data corresponds to
syslogd event logs that contain a large amount of information about the normal
activity of the studied systems as well as their behavior in the presence of
failures. It is widely recognized that the information contained in such event
logs might be incomplete or imperfect. The solution investigated in this paper
to address this problem is based on the use of auxiliary sources of data
obtained from wtmpx files maintained by the SunOS/Solaris Unix operating
system. The results obtained suggest that the combined use of wtmpx and syslogd
log files provides more complete information on the state of the target systems
that is useful to provide availability estimations that better reflect reality.
"
cs,Statistical analysis of weighted networks,"  The purpose of this paper is to assess the statistical characterization of
weighted networks in terms of the generalization of the relevant parameters,
namely average path length, degree distribution and clustering coefficient.
Although the degree distribution and the average path length admit
straightforward generalizations, for the clustering coefficient several
different definitions have been proposed in the literature. We examined the
different definitions and identified the similarities and differences between
them. In order to elucidate the significance of different definitions of the
weighted clustering coefficient, we studied their dependence on the weights of
the connections. For this purpose, we introduce the relative perturbation norm
of the weights as an index to assess the weight distribution. This study
revealed new interesting statistical regularities in terms of the relative
perturbation norm useful for the statistical characterization of weighted
graphs.
"
cs,A High Robustness and Low Cost Model for Cascading Failures,"  We study numerically the cascading failure problem by using artificially
created scale-free networks and the real network structure of the power grid.
The capacity for a vertex is assigned as a monotonically increasing function of
the load (or the betweenness centrality). Through the use of a simple
functional form with two free parameters, revealed is that it is indeed
possible to make networks more robust while spending less cost. We suggest that
our method to prevent cascade by protecting less vertices is particularly
important for the design of more robust real-world networks to cascading
failures.
"
cs,"Birth, survival and death of languages by Monte Carlo simulation","  Simulations of physicists for the competition between adult languages since
2003 are reviewed. How many languages are spoken by how many people? How many
languages are contained in various language families? How do language
similarities decay with geographical distance, and what effects do natural
boundaries have? New simulations of bilinguality are given in an appendix.
"
cs,"On the Achievable Rate Regions for Interference Channels with Degraded
  Message Sets","  The interference channel with degraded message sets (IC-DMS) refers to a
communication model in which two senders attempt to communicate with their
respective receivers simultaneously through a common medium, and one of the
senders has complete and a priori (non-causal) knowledge about the message
being transmitted by the other. A coding scheme that collectively has
advantages of cooperative coding, collaborative coding, and dirty paper coding,
is developed for such a channel. With resorting to this coding scheme,
achievable rate regions of the IC-DMS in both discrete memoryless and Gaussian
cases are derived, which, in general, include several previously known rate
regions. Numerical examples for the Gaussian case demonstrate that in the
high-interference-gain regime, the derived achievable rate regions offer
considerable improvements over these existing results.
"
cs,Eigen Equation of the Nonlinear Spinor,"  How to effectively solve the eigen solutions of the nonlinear spinor field
equation coupling with some other interaction fields is important to understand
the behavior of the elementary particles. In this paper, we derive a simplified
form of the eigen equation of the nonlinear spinor, and then propose a scheme
to solve their numerical solutions. This simplified equation has elegant and
neat structure, which is more convenient for both theoretical analysis and
numerical computation.
"
cs,"Intelligent location of simultaneously active acoustic emission sources:
  Part I","  The intelligent acoustic emission locator is described in Part I, while Part
II discusses blind source separation, time delay estimation and location of two
simultaneously active continuous acoustic emission sources.
  The location of acoustic emission on complicated aircraft frame structures is
a difficult problem of non-destructive testing. This article describes an
intelligent acoustic emission source locator. The intelligent locator comprises
a sensor antenna and a general regression neural network, which solves the
location problem based on learning from examples. Locator performance was
tested on different test specimens. Tests have shown that the accuracy of
location depends on sound velocity and attenuation in the specimen, the
dimensions of the tested area, and the properties of stored data. The location
accuracy achieved by the intelligent locator is comparable to that obtained by
the conventional triangulation method, while the applicability of the
intelligent locator is more general since analysis of sonic ray paths is
avoided. This is a promising method for non-destructive testing of aircraft
frame structures by the acoustic emission method.
"
cs,Genetic Optimization of Photonic Bandgap Structures,"  We investigate the use of a Genetic Algorithm (GA) to design a set of
photonic crystals (PCs) in one and two dimensions. Our flexible design
methodology allows us to optimize PC structures which are optimized for
specific objectives. In this paper, we report the results of several such
GA-based PC optimizations. We show that the GA performs well even in very
complex design spaces, and therefore has great potential for use as a robust
design tool in present and future applications.
"
cs,"Robust manipulation of electron spin coherence in an ensemble of singly
  charged quantum dots","  Using the recently reported mode locking effect we demonstrate a highly
robust control of electron spin coherence in an ensemble of (In,Ga)As quantum
dots during the single spin coherence time. The spin precession in a transverse
magnetic field can be fully controlled up to 25 K by the parameters of the
exciting pulsed laser protocol such as the pulse train sequence, leading to
adjustable quantum beat bursts in Faraday rotation. Flipping of the electron
spin precession phase was demonstrated by inverting the polarization within a
pulse doublet sequence.
"
cs,"Coupling of whispering-gallery modes in size-mismatched microdisk
  photonic molecules","  Mechanisms of whispering-gallery (WG) modes coupling in microdisk photonic
molecules (PMs) with slight and significant size mismatch are numerically
investigated. The results reveal two different scenarios of modes interaction
depending on the degree of this mismatch and offer new insight into how PM
parameters can be tuned to control and modify WG-modes wavelengths and
Q-factors. From a practical point of view, these findings offer a way to
fabricate PM microlaser structures that exhibit low thresholds and directional
emission, and at the same time are more tolerant to fabrication errors than
previously explored coupled-cavity structures composed of identical
microresonators.
"
cs,"Capacity of a Multiple-Antenna Fading Channel with a Quantized Precoding
  Matrix","  Given a multiple-input multiple-output (MIMO) channel, feedback from the
receiver can be used to specify a transmit precoding matrix, which selectively
activates the strongest channel modes. Here we analyze the performance of
Random Vector Quantization (RVQ), in which the precoding matrix is selected
from a random codebook containing independent, isotropically distributed
entries. We assume that channel elements are i.i.d. and known to the receiver,
which relays the optimal (rate-maximizing) precoder codebook index to the
transmitter using B bits. We first derive the large system capacity of
beamforming (rank-one precoding matrix) as a function of B, where large system
refers to the limit as B and the number of transmit and receive antennas all go
to infinity with fixed ratios. With beamforming RVQ is asymptotically optimal,
i.e., no other quantization scheme can achieve a larger asymptotic rate. The
performance of RVQ is also compared with that of a simpler reduced-rank scalar
quantization scheme in which the beamformer is constrained to lie in a random
subspace. We subsequently consider a precoding matrix with arbitrary rank, and
approximate the asymptotic RVQ performance with optimal and linear receivers
(matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show
that these approximations accurately predict the performance of finite-size
systems of interest. Given a target spectral efficiency, numerical examples
show that the amount of feedback required by the linear MMSE receiver is only
slightly more than that required by the optimal receiver, whereas the matched
filter can require significantly more feedback.
"
cs,"A general approach to statistical modeling of physical laws:
  nonparametric regression","  Statistical modeling of experimental physical laws is based on the
probability density function of measured variables. It is expressed by
experimental data via a kernel estimator. The kernel is determined objectively
by the scattering of data during calibration of experimental setup. A physical
law, which relates measured variables, is optimally extracted from experimental
data by the conditional average estimator. It is derived directly from the
kernel estimator and corresponds to a general nonparametric regression. The
proposed method is demonstrated by the modeling of a return map of noisy
chaotic data. In this example, the nonparametric regression is used to predict
a future value of chaotic time series from the present one. The mean predictor
error is used in the definition of predictor quality, while the redundancy is
expressed by the mean square distance between data points. Both statistics are
used in a new definition of predictor cost function. From the minimum of the
predictor cost function, a proper number of data in the model is estimated.
"
cs,"Local-field effects in radiatively broadened magneto-dielectric media:
  negative refraction and absorption reduction","  We give a microscopic derivation of the Clausius-Mossotti relations for a
homogeneous and isotropic magneto-dielectric medium consisting of radiatively
broadened atomic oscillators. To this end the diagram series of electromagnetic
propagators is calculated exactly for an infinite bi-cubic lattice of
dielectric and magnetic dipoles for a lattice constant small compared to the
resonance wavelength $\lambda$. Modifications of transition frequencies and
linewidth of the elementary oscillators are taken into account in a
selfconsistent way by a proper incorporation of the singular self-interaction
terms. We show that in radiatively broadened media sufficiently close to the
free-space resonance the real part of the index of refraction approaches the
value -2 in the limit of $\rho \lambda^3 \gg 1$, where $\rho$ is the number
density of scatterers. Since at the same time the imaginary part vanishes as
$1/\rho$ local field effects can have important consequences for realizing
low-loss negative index materials.
"
cs,Architecture for Pseudo Acausal Evolvable Embedded Systems,"  Advances in semiconductor technology are contributing to the increasing
complexity in the design of embedded systems. Architectures with novel
techniques such as evolvable nature and autonomous behavior have engrossed lot
of attention. This paper demonstrates conceptually evolvable embedded systems
can be characterized basing on acausal nature. It is noted that in acausal
systems, future input needs to be known, here we make a mechanism such that the
system predicts the future inputs and exhibits pseudo acausal nature. An
embedded system that uses theoretical framework of acausality is proposed. Our
method aims at a novel architecture that features the hardware evolability and
autonomous behavior alongside pseudo acausality. Various aspects of this
architecture are discussed in detail along with the limitations.
"
cs,A POVM view of the ensemble approach to polarization optics,"  Statistical ensemble formalism of Kim, Mandel and Wolf (J. Opt. Soc. Am. A 4,
433 (1987)) offers a realistic model for characterizing the effect of
stochastic non-image forming optical media on the state of polarization of
transmittedlight. With suitable choice of the Jones ensemble, various Mueller
transformations - some of which have been unknown so far - are deduced. It is
observed that the ensemble approach is formally identical to the positive
operator valued measures (POVM) on the quantum density matrix. This
observation, in combination with the recent suggestion by Ahnert and Payne
(Phys. Rev. A 71, 012330, (2005)) - in the context of generalized quantum
measurement on single photon polarization states - that linear optics elements
can be employed in setting up all possible POVMs, enables us to propose a way
of realizing different types of Mueller devices.
"
cs,Reducing SAT to 2-SAT,"  Description of a polynomial time reduction of SAT to 2-SAT of polynomial
size.
"
cs,Optimal Synthesis of Multiple Algorithms,"  In this paper we give a definition of ""algorithm,"" ""finite algorithm,""
""equivalent algorithms,"" and what it means for a single algorithm to dominate a
set of algorithms. We define a derived algorithm which may have a smaller mean
execution time than any of its component algorithms. We give an explicit
expression for the mean execution time (when it exists) of the derived
algorithm. We give several illustrative examples of derived algorithms with two
component algorithms. We include mean execution time solutions for
two-algorithm processors whose joint density of execution times are of several
general forms. For the case in which the joint density for a two-algorithm
processor is a step function, we give a maximum-likelihood estimation scheme
with which to analyze empirical processing time data.
"
cs,Revisiting the Issues On Netflow Sample and Export Performance,"  The high volume of packets and packet rates of traffic on some router links
makes it exceedingly difficult for routers to examine every packet in order to
keep detailed statistics about the traffic which is traversing the router.
Sampling is commonly applied on routers in order to limit the load incurred by
the collection of information that the router has to undertake when evaluating
flow information for monitoring purposes. The sampling process in nearly all
cases is a deterministic process of choosing 1 in every N packets on a
per-interface basis, and then forming the flow statistics based on the
collected sampled statistics. Even though this sampling may not be significant
for some statistics, such as packet rate, others can be severely distorted.
However, it is important to consider the sampling techniques and their relative
accuracy when applied to different traffic patterns. The main disadvantage of
sampling is the loss of accuracy in the collected trace when compared to the
original traffic stream. To date there has not been a detailed analysis of the
impact of sampling at a router in various traffic profiles and flow criteria.
In this paper, we assess the performance of the sampling process as used in
NetFlow in detail, and we discuss some techniques for the compensation of loss
of monitoring detail.
"
cs,"Thermal decomposition of norbornane (bicyclo[2.2.1]heptane) dissolved in
  benzene. Experimental study and mechanism investigation","  The thermal decomposition of norbornane (dissolved in benzene) has been
studied in a jet stirred reactor at temperatures between 873 and 973 K, at
residence times ranging from 1 to 4 s and at atmospheric pressure, leading to
conversions from 0.04 to 22.6%. 25 reaction products were identified and
quantified by gas chromatography, amongst which the main ones are hydrogen,
ethylene and 1,3-cyclopentadiene. A mechanism investigation of the thermal
decomposition of the norbornane - benzene binary mixture has been performed.
Reactions involved in the mechanism have been reviewed: unimolecular
initiations 1 by C-C bond scission of norbornane, fate of the generated
diradicals, reactions of transfer and propagation of norbornyl radicals,
reactions of benzene and cross-coupling reactions.
"
cs,"Laser spectroscopy of hyperfine structure in highly-charged ions: a test
  of QED at high fields","  An overview is presented of laser spectroscopy experiments with cold,
trapped, highly-charged ions, which will be performed at the HITRAP facility at
GSI in Darmstadt (Germany). These high-resolution measurements of ground state
hyperfine splittings will be three orders of magnitude more precise than
previous measurements. Moreover, from a comparison of measurements of the
hyperfine splittings in hydrogen- and lithium-like ions of the same isotope,
QED effects at high electromagnetic fields can be determined within a few
percent. Several candidate ions suited for these laser spectroscopy studies are
presented.
"
cs,Collective behavior of stock price movements in an emerging market,"  To investigate the universality of the structure of interactions in different
markets, we analyze the cross-correlation matrix C of stock price fluctuations
in the National Stock Exchange (NSE) of India. We find that this emerging
market exhibits strong correlations in the movement of stock prices compared to
developed markets, such as the New York Stock Exchange (NYSE). This is shown to
be due to the dominant influence of a common market mode on the stock prices.
By comparison, interactions between related stocks, e.g., those belonging to
the same business sector, are much weaker. This lack of distinct sector
identity in emerging markets is explicitly shown by reconstructing the network
of mutually interacting stocks. Spectral analysis of C for NSE reveals that,
the few largest eigenvalues deviate from the bulk of the spectrum predicted by
random matrix theory, but they are far fewer in number compared to, e.g., NYSE.
We show this to be due to the relative weakness of intra-sector interactions
between stocks, compared to the market mode, by modeling stock price dynamics
with a two-factor model. Our results suggest that the emergence of an internal
structure comprising multiple groups of strongly coupled components is a
signature of market development.
"
cs,Electromagnetic wormholes via handlebody constructions,"  Cloaking devices are prescriptions of electrostatic, optical or
electromagnetic parameter fields (conductivity $\sigma(x)$, index of refraction
$n(x)$, or electric permittivity $\epsilon(x)$ and magnetic permeability
$\mu(x)$) which are piecewise smooth on $\mathbb R^3$ and singular on a
hypersurface $\Sigma$, and such that objects in the region enclosed by $\Sigma$
are not detectable to external observation by waves. Here, we give related
constructions of invisible tunnels, which allow electromagnetic waves to pass
between possibly distant points, but with only the ends of the tunnels visible
to electromagnetic imaging. Effectively, these change the topology of space
with respect to solutions of Maxwell's equations, corresponding to attaching a
handlebody to $\mathbb R^3$. The resulting devices thus function as
electromagnetic wormholes.
"
cs,"On thermal effects in solid state lasers: the case of ytterbium-doped
  materials","  A review of theoretical and experimental studies of thermal effects in
solid-state lasers is presented, with a special focus on diode-pumped
ytterbium-doped materials. A large part of this review provides however general
information applicable to any kind of solid-state laser. Our aim here is not to
make a list of the techniques that have been used to minimize thermal effects,
but instead to give an overview of the theoretical aspects underneath, and give
a state-of-the-art of the tools at the disposal of the laser scientist to
measure thermal effects. After a presentation of some general properties of
Yb-doped materials, we address the issue of evaluating the temperature map in
Yb-doped laser crystals, both theoretically and experimentally. This is the
first step before studying the complex problem of thermal lensing (part III).
We will focus on some newly discussed aspects, like the definition of the
thermo-optic coefficient: we will highlight some misleading interpretations of
thermal lensing experiments due to the use of the dn/dT parameter in a context
where it is not relevant. Part IV will be devoted to a state-of-the-art of
experimental techniques used to measure thermal lensing. Eventually, in part V,
we will give some concrete examples in Yb-doped materials, where their
peculiarities will be pointed out.
"
cs,"Protein and ionic surfactants - promoters and inhibitors of contact line
  pinning","  We report a new effect of surfactants in pinning a drop contact line,
specifically that lysozyme promotes while lauryl sulfate inhibits pinning. We
explain the pinning disparity assuming difference in wetting: the protein-laden
drop wets a ""clean"" surface and the surfactant-laden drop wets an
auto-precursored surface.
"
cs,Pairwise comparisons of typological profiles (of languages),"  No abstract given; compares pairs of languages from World Atlas of Language
Structures.
"
cs,"A Rigorous Time-Domain Analysis of Full--Wave Electromagnetic Cloaking
  (Invisibility)","  There is currently a great deal of interest in the theoretical and practical
possibility of cloaking objects from the observation by electromagnetic waves.
The basic idea of these invisibility devices \cite{glu1, glu2, le},\cite{pss1}
is to use anisotropic {\it transformation media} whose permittivity and
permeability $\var^{\lambda\nu}, \mu^{\lambda\nu}$, are obtained from the ones,
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$, of isotropic media, by singular
transformations of coordinates. In this paper we study electromagnetic cloaking
in the time-domain using the formalism of time-dependent scattering theory.
This formalism allows us to settle in an unambiguous way the mathematical
problems posed by the singularities of the inverse of the permittivity and the
permeability of the {\it transformation media} on the boundary of the cloaked
objects. We write Maxwell's equations in Schr\""odinger form with the
electromagnetic propagator playing the role of the Hamiltonian. We prove that
the electromagnetic propagator outside of the cloaked objects is essentially
self-adjoint. Moreover, the unique self-adjoint extension is unitarily
equivalent to the electromagnetic propagator in the medium
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$. Using this fact, and since the
coordinate transformation is the identity outside of a ball, we prove that the
scattering operator is the identity. Our results give a rigorous proof that the
construction of \cite{glu1, glu2, le}, \cite{pss1} perfectly cloaks passive and
active devices from observation by electromagnetic waves. Furthermore, we prove
cloaking for general anisotropic materials. In particular, our results prove
that it is possible to cloak objects inside general crystals.
"
cs,"Fluctuation-dissipation relation on a Melde string in a turbulent flow,
  considerations on a ""dynamical temperature""","  We report on measurements of the transverse fluctuations of a string in a
turbulent air jet flow. Harmonic modes are excited by the fluctuating drag
force, at different wave-numbers. This simple mechanical probe makes it
possible to measure excitations of the flow at specific scales, averaged over
space and time: it is a scale-resolved, global measurement. We also measure the
dissipation associated to the string motion, and we consider the ratio of the
fluctuations over dissipation (FDR). In an exploratory approach, we investigate
the concept of {\it effective temperature} defined through the FDR. We compare
our observations with other definitions of temperature in turbulence. From the
theory of Kolmogorov (1941), we derive the exponent -11/3 expected for the
spectrum of the fluctuations. This simple model and our experimental results
are in good agreement, over the range of wave-numbers, and Reynolds number
accessible ($74000 \leq Re \leq 170000$).
"
cs,"Compounding Fields and Their Quantum Equations in the Trigintaduonion
  Space","  The 32-dimensional compounding fields and their quantum interplays in the
trigintaduonion space can be presented by analogy with octonion and sedenion
electromagnetic, gravitational, strong and weak interactions. In the
trigintaduonion fields which are associated with the electromagnetic,
gravitational, strong and weak interactions, the study deduces some conclusions
of field source particles (quarks and leptons) and intermediate particles which
are consistent with current some sorts of interaction theories. In the
trigintaduonion fields which are associated with the hyper-strong and
strong-weak fields, the paper draws some predicts and conclusions of the field
source particles (sub-quarks) and intermediate particles. The research results
show that there may exist some new particles in the nature.
"
cs,"Effective conservation of energy and momentum algorithm using switching
  potentials suitable for molecular dynamics simulation of thermodynamical
  systems","  During a crossover via a switching mechanism from one 2-body potential to
another as might be applied in modeling (chemical) reactions in the vicinity of
bond formation, energy violations would occur due to finite step size which
determines the trajectory of the particles relative to the potential
interactions of the unbonded state by numerical (e.g. Verlet) integration. This
problem is overcome by an algorithm which preserves the coordinates of the
system for each move, but corrects for energy discrepancies by ensuring both
energy and momentum conservation in the dynamics. The algorithm is tested for a
hysteresis loop reaction model with an without the implementation of the
algorithm. The tests involve checking the rate of energy flow out of the MD
simulation box; in the equilibrium state, no net rate of flows within
experimental error should be observed. The temperature and pressure of the box
should also be invariant within the range of fluctuation of these quantities.
It is demonstrated that the algorithm satisfies these criteria.
"
cs,"New version announcement for TaylUR, an arbitrary-order diagonal
  automatic differentiation package for Fortran 95","  We present a new version of TaylUR, a Fortran 95 module to automatically
compute the numerical values of a complex-valued function's derivatives with
respect to several variables up to an arbitrary order in each variable, but
excluding mixed derivatives. The new version fixes a potentially serious bug in
the code for exponential-related functions that could corrupt the imaginary
parts of derivatives, as well as being compatible with a wider range of
compilers.
"
cs,"Nonlinear Dynamics of the Phonon Stimulated Emission in Microwave
  Solid-State Resonator of the Nonautonomous Phaser Generator","  The microwave phonon stimulated emission (SE) has been experimentally and
numerically investigated in a nonautonomous microwave acoustic quantum
generator, called also microwave phonon laser or phaser (see previous works
arXiv:cond-mat/0303188 ; arXiv:cond-mat/0402640 ; arXiv:nlin.CG/0703050)
Phenomena of branching and long-time refractority (absence of the reaction on
the external pulses) for deterministic chaotic and regular processes of SE were
observed in experiments with various levels of electromagnetic pumping. At the
pumping level growth, the clearly depined increasing of the number of
coexisting SE states has been observed both in real physical experiments and in
computer simulations. This confirms the analytical estimations of the branching
density in the phase space. The nature of the refractority of SE pulses is
closely connected with the pointed branching and reflects the crises of strange
attractors, i.e. their collisions with unstable periodic components of the
higher branches of SE states in the nonautonomous microwave phonon laser.
"
cs,Semi-spheroidal Quantum Harmonic Oscillator,"  A new single-particle shell model is derived by solving the Schr\""odinger
equation for a semi-spheroidal potential well. Only the negative parity states
of the $Z(z)$ component of the wave function are allowed, so that new magic
numbers are obtained for oblate semi-spheroids, semi-sphere and prolate
semi-spheroids. The semi-spherical magic numbers are identical with those
obtained at the oblate spheroidal superdeformed shape: 2, 6, 14, 26, 44, 68,
100, 140, ... The superdeformed prolate magic numbers of the semi-spheroidal
shape are identical with those obtained at the spherical shape of the
spheroidal harmonic oscillator: 2, 8, 20, 40, 70, 112, 168 ...
"
cs,P-adic arithmetic coding,"  A new incremental algorithm for data compression is presented. For a sequence
of input symbols algorithm incrementally constructs a p-adic integer number as
an output. Decoding process starts with less significant part of a p-adic
integer and incrementally reconstructs a sequence of input symbols. Algorithm
is based on certain features of p-adic numbers and p-adic norm. p-adic coding
algorithm may be considered as of generalization a popular compression
technique - arithmetic coding algorithms. It is shown that for p = 2 the
algorithm works as integer variant of arithmetic coding; for a special class of
models it gives exactly the same codes as Huffman's algorithm, for another
special model and a specific alphabet it gives Golomb-Rice codes.
"
cs,Long-range correlation and multifractality in Bach's Inventions pitches,"  We show that it can be considered some of Bach pitches series as a stochastic
process with scaling behavior. Using multifractal deterend fluctuation analysis
(MF-DFA) method, frequency series of Bach pitches have been analyzed. In this
view we find same second moment exponents (after double profiling) in ranges
(1.7-1.8) in his works. Comparing MF-DFA results of original series to those
for shuffled and surrogate series we can distinguish multifractality due to
long-range correlations and a broad probability density function. Finally we
determine the scaling exponents and singularity spectrum. We conclude fat tail
has more effect in its multifractality nature than long-range correlations.
"
cs,"Polymerization Force Driven Buckling of Microtubule Bundles Determines
  the Wavelength of Patterns Formed in Tubulin Solutions","  We present a model for the spontaneous formation of a striated pattern in
polymerizing microtubule solutions. It describes the buckling of a single
microtubule (MT) bundle within an elastic network formed by other similarly
aligned and buckling bundles and unaligned MTs. Phase contrast and polarization
microscopy studies of the temporal evolution of the pattern imply that the
polymerization of MTs within the bundles creates the driving compressional
force. Using the measured rate of buckling, the established MT force-velocity
curve and the pattern wavelength, we obtain reasonable estimates for the MT
bundle bending rigidity and the elastic constant of the network. The analysis
implies that the bundles buckle as solid rods.
"
cs,Sparsely-spread CDMA - a statistical mechanics based analysis,"  Sparse Code Division Multiple Access (CDMA), a variation on the standard CDMA
method in which the spreading (signature) matrix contains only a relatively
small number of non-zero elements, is presented and analysed using methods of
statistical physics. The analysis provides results on the performance of
maximum likelihood decoding for sparse spreading codes in the large system
limit. We present results for both cases of regular and irregular spreading
matrices for the binary additive white Gaussian noise channel (BIAWGN) with a
comparison to the canonical (dense) random spreading code.
"
cs,Refuting the Pseudo Attack on the REESSE1+ Cryptosystem,"  We illustrate through example 1 and 2 that the condition at theorem 1 in [8]
dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does
not hold, namely the condition Z/M - L/Ak < 1/(2 Ak^2) is not sufficient for
f(i) + f(j) = f(k). Illuminate through an analysis and ex.3 that there is a
logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4
to be invalid. Demonstrate through ex.4 and 5 that each or the combination of
qu+1 > qu * D at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) +
f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4
and alg.2 based on table 1 are disordered and wrong logically. Further,
manifest through a repeated experiment and ex.5 that the data at table 2 is
falsified, and the example in [8] is woven elaborately. We explain why Cx = Ax
* W^f(x) (% M) is changed to Cx = (Ax * W^f(x))^d (% M) in REESSE1+ v2.1. To
the signature fraud, we point out that [8] misunderstands the existence of T^-1
and Q^-1 % (M-1), and forging of Q can be easily avoided through moving H.
Therefore, the conclusion of [8] that REESSE1+ is not secure at all (which
connotes that [8] can extract a related private key from any public key in
REESSE1+) is fully incorrect, and as long as the parameter Omega is fitly
selected, REESSE1+ with Cx = Ax * W^f(x) (% M) is secure.
"
cs,"Convergence of the discrete dipole approximation. I. Theoretical
  analysis","  We performed a rigorous theoretical convergence analysis of the discrete
dipole approximation (DDA). We prove that errors in any measured quantity are
bounded by a sum of a linear and quadratic term in the size of a dipole d, when
the latter is in the range of DDA applicability. Moreover, the linear term is
significantly smaller for cubically than for non-cubically shaped scatterers.
Therefore, for small d errors for cubically shaped particles are much smaller
than for non-cubically shaped. The relative importance of the linear term
decreases with increasing size, hence convergence of DDA for large enough
scatterers is quadratic in the common range of d. Extensive numerical
simulations were carried out for a wide range of d. Finally we discuss a number
of new developments in DDA and their consequences for convergence.
"
cs,The Einstein-Varicak Correspondence on Relativistic Rigid Rotation,"  The historical significance of the problem of relativistic rigid rotation is
reviewed in light of recently published correspondence between Einstein and the
mathematician Vladimir Varicak from the years 1909 to 1913.
"
cs,"Alternative Approaches to the Equilibrium Properties of Hard-Sphere
  Liquids","  An overview of some analytical approaches to the computation of the
structural and thermodynamic properties of single component and multicomponent
hard-sphere fluids is provided. For the structural properties, they yield a
thermodynamically consistent formulation, thus improving and extending the
known analytical results of the Percus-Yevick theory. Approximate expressions
for the contact values of the radial distribution functions and the
corresponding analytical equations of state are also discussed. Extensions of
this methodology to related systems, such as sticky hard spheres and
square-well fluids, as well as its use in connection with the perturbation
theory of fluids are briefly addressed.
"
cs,Daemons and DAMA: Their Celestial-Mechanics Interrelations,"  The assumption of the capture by the Solar System of the electrically charged
Planckian DM objects (daemons) from the galactic disk is confirmed not only by
the St.Petersburg (SPb) experiments detecting particles with V<30 km/s. Here
the daemon approach is analyzed considering the positive model independent
result of the DAMA/NaI experiment. We explain the maximum in DAMA signals
observed in the May-June period to be associated with the formation behind the
Sun of a trail of daemons that the Sun captures into elongated orbits as it
moves to the apex. The range of significant 2-6-keV DAMA signals fits well the
iodine nuclei elastically knocked out of the NaI(Tl) scintillator by particles
falling on the Earth with V=30-50 km/s from strongly elongated heliocentric
orbits. The half-year periodicity of the slower daemons observed in SPb
originates from the transfer of particles that are deflected through ~90 deg
into near-Earth orbits each time the particles cross the outer reaches of the
Sun which had captured them. Their multi-loop (cross-like) trajectories
traverse many times the Earth's orbit in March and September, which increases
the probability for the particles to enter near-Earth orbits during this time.
Corroboration of celestial mechanics calculations with observations yields
~1e-19 cm2 for the cross section of daemon interaction with the solar matter.
"
cs,"Measurement of the Aerosol Phase Function at the Pierre Auger
  Observatory","  Air fluorescence detectors measure the energy of ultra-high energy cosmic
rays by collecting fluorescence light emitted from nitrogen molecules along the
extensive air shower cascade. To ensure a reliable energy determination, the
light signal needs to be corrected for atmospheric effects, which not only
attenuate the signal, but also produce a non-negligible background component
due to scattered Cherenkov light and multiple-scattered light. The correction
requires regular measurements of the aerosol attenuation length and the aerosol
phase function, defined as the probability of light scattered in a given
direction. At the Pierre Auger Observatory in Malargue, Argentina, the phase
function is measured on an hourly basis using two Aerosol Phase Function (APF)
light sources. These sources direct a UV light beam across the field of view of
the fluorescence detectors; the phase function can be extracted from the image
of the shots in the fluorescence detector cameras. This paper describes the
design, current status, standard operation procedure, and performance of the
APF system at the Pierre Auger Observatory.
"
cs,"Sensor Networks with Random Links: Topology Design for Distributed
  Consensus","  In a sensor network, in practice, the communication among sensors is subject
to:(1) errors or failures at random times; (3) costs; and(2) constraints since
sensors and networks operate under scarce resources, such as power, data rate,
or communication. The signal-to-noise ratio (SNR) is usually a main factor in
determining the probability of error (or of communication failure) in a link.
These probabilities are then a proxy for the SNR under which the links operate.
The paper studies the problem of designing the topology, i.e., assigning the
probabilities of reliable communication among sensors (or of link failures) to
maximize the rate of convergence of average consensus, when the link
communication costs are taken into account, and there is an overall
communication budget constraint. To consider this problem, we address a number
of preliminary issues: (1) model the network as a random topology; (2)
establish necessary and sufficient conditions for mean square sense (mss) and
almost sure (a.s.) convergence of average consensus when network links fail;
and, in particular, (3) show that a necessary and sufficient condition for both
mss and a.s. convergence is for the algebraic connectivity of the mean graph
describing the network topology to be strictly positive. With these results, we
formulate topology design, subject to random link failures and to a
communication cost constraint, as a constrained convex optimization problem to
which we apply semidefinite programming techniques. We show by an extensive
numerical study that the optimal design improves significantly the convergence
speed of the consensus algorithm and can achieve the asymptotic performance of
a non-random network at a fraction of the communication cost.
"
cs,Learning from compressed observations,"  The problem of statistical learning is to construct a predictor of a random
variable $Y$ as a function of a related random variable $X$ on the basis of an
i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable
predictors are drawn from some specified class, and the goal is to approach
asymptotically the performance (expected loss) of the best predictor in the
class. We consider the setting in which one has perfect observation of the
$X$-part of the sample, while the $Y$-part has to be communicated at some
finite bit rate. The encoding of the $Y$-values is allowed to depend on the
$X$-values. Under suitable regularity conditions on the admissible predictors,
the underlying family of probability distributions and the loss function, we
give an information-theoretic characterization of achievable predictor
performance in terms of conditional distortion-rate functions. The ideas are
illustrated on the example of nonparametric regression in Gaussian noise.
"
cs,"Astrophysical gyrokinetics: kinetic and fluid turbulent cascades in
  magnetized weakly collisional plasmas","  We present a theoretical framework for plasma turbulence in astrophysical
plasmas (solar wind, interstellar medium, galaxy clusters, accretion disks).
The key assumptions are that the turbulence is anisotropic with respect to the
mean magnetic field and frequencies are low compared to the ion cyclotron
frequency. The energy injected at the outer scale scale has to be converted
into heat, which ultimately cannot be done without collisions. A KINETIC
CASCADE develops that brings the energy to collisional scales both in space and
velocity. Its nature depends on the physics of plasma fluctuations. In each of
the physically distinct scale ranges, the kinetic problem is systematically
reduced to a more tractable set of equations. In the ""inertial range"" above the
ion gyroscale, the kinetic cascade splits into a cascade of Alfvenic
fluctuations, which are governed by the RMHD equations at both the collisional
and collisionless scales, and a passive cascade of compressive fluctuations,
which obey a linear kinetic equation along the moving field lines associated
with the Alfvenic component. In the ""dissipation range"" between the ion and
electron gyroscales, there are again two cascades: the kinetic-Alfven-wave
(KAW) cascade governed by two fluid-like Electron RMHD equations and a passive
phase-space cascade of ion entropy fluctuations. The latter cascade brings the
energy of the inertial-range fluctuations that was damped by collisionless
wave-particle interaction at the ion gyroscale to collisional scales in the
phase space and leads to ion heating. The KAW energy is similarly damped at the
electron gyroscale and converted into electron heat. Kolmogorov-style scaling
relations are derived for these cascades. Astrophysical and space-physical
applications are discussed in detail.
"
cs,"Analysis of the real estate market in Las Vegas: Bubble, seasonal
  patterns, and prediction of the CSW indexes","  We analyze 27 house price indexes of Las Vegas from Jun. 1983 to Mar. 2005,
corresponding to 27 different zip codes. These analyses confirm the existence
of a real-estate bubble, defined as a price acceleration faster than
exponential, which is found however to be confined to a rather limited time
interval in the recent past from approximately 2003 to mid-2004 and has
progressively transformed into a more normal growth rate comparable to
pre-bubble levels in 2005. There has been no bubble till 2002 except for a
medium-sized surge in 1990. In addition, we have identified a strong yearly
periodicity which provides a good potential for fine-tuned prediction from
month to month. A monthly monitoring using a model that we have developed could
confirm, by testing the intra-year structure, if indeed the market has returned
to ``normal'' or if more turbulence is expected ahead. We predict the evolution
of the indexes one year ahead, which is validated with new data up to Sep.
2006. The present analysis demonstrates the existence of very significant
variations at the local scale, in the sense that the bubble in Las Vegas seems
to have preceded the more global USA bubble and has ended approximately two
years earlier (mid 2004 for Las Vegas compared with mid-2006 for the whole of
the USA).
"
cs,Simulation of Robustness against Lesions of Cortical Networks,"  Structure entails function and thus a structural description of the brain
will help to understand its function and may provide insights into many
properties of brain systems, from their robustness and recovery from damage, to
their dynamics and even their evolution. Advances in the analysis of complex
networks provide useful new approaches to understanding structural and
functional properties of brain networks. Structural properties of networks
recently described allow their characterization as small-world, random
(exponential) and scale-free. They complement the set of other properties that
have been explored in the context of brain connectivity, such as topology,
hodology, clustering, and hierarchical organization. Here we apply new network
analysis methods to cortical inter-areal connectivity networks for the cat and
macaque brains. We compare these corticocortical fibre networks to benchmark
rewired, small-world, scale-free and random networks, using two analysis
strategies, in which we measure the effects of the removal of nodes and
connections on the structural properties of the cortical networks. The brain
networks' structural decay is in most respects similar to that of scale-free
networks. The results implicate highly connected hub-nodes and bottleneck
connections as structural basis for some of the conditional robustness of brain
systems. This informs the understanding of the development of brain networks'
connectivity.
"
cs,Sparsity-certifying Graph Decompositions,"  We describe a new algorithm, the $(k,\ell)$-pebble game with colors, and use
it obtain a characterization of the family of $(k,\ell)$-sparse graphs and
algorithmic solutions to a family of problems concerning tree decompositions of
graphs. Special instances of sparse graphs appear in rigidity theory and have
received increased attention in recent years. In particular, our colored
pebbles generalize and strengthen the previous results of Lee and Streinu and
give a new proof of the Tutte-Nash-Williams characterization of arboricity. We
also present a new decomposition that certifies sparsity based on the
$(k,\ell)$-pebble game with colors. Our work also exposes connections between
pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and
Westermann and Hendrickson.
"
cs,"The evolution of the Earth-Moon system based on the dark matter field
  fluid model","  The evolution of Earth-Moon system is described by the dark matter field
fluid model proposed in the Meeting of Division of Particle and Field 2004,
American Physical Society. The current behavior of the Earth-Moon system agrees
with this model very well and the general pattern of the evolution of the
Moon-Earth system described by this model agrees with geological and fossil
evidence. The closest distance of the Moon to Earth was about 259000 km at 4.5
billion years ago, which is far beyond the Roche's limit. The result suggests
that the tidal friction may not be the primary cause for the evolution of the
Earth-Moon system. The average dark matter field fluid constant derived from
Earth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts
that the Mars's rotation is also slowing with the angular acceleration rate
about -4.38 x 10^(-22) rad s^(-2).
"
cs,"Time and motion in physics: the Reciprocity Principle, relativistic
  invariance of the lengths of rulers and time dilatation","  Ponderable objects moving in free space according to Newton's First Law
constitute both rulers and clocks when one such object is viewed from the rest
frame of another. Together with the Reciprocity Principle this is used to
demonstrate, in both Galilean and special relativity, the invariance of the
measured length of a ruler in motion. The different times: `proper', `improper'
and `apparent' appearing in different formulations of the relativistic time
dilatation relation are discussed and exemplified by experimental applications.
A non-intuitive `length expansion' effect predicted by the Reciprocity
Principle as a necessary consequence of time dilatation is pointed out
"
q-bio,The World as Evolving Information,"  This paper discusses the benefits of describing the world as information,
especially in the study of the evolution of life and cognition. Traditional
studies encounter problems because it is difficult to describe life and
cognition in terms of matter and energy, since their laws are valid only at the
physical scale. However, if matter and energy, as well as life and cognition,
are described in terms of information, evolution can be described consistently
as information becoming more complex.
  The paper presents eight tentative laws of information, valid at multiple
scales, which are generalizations of Darwinian, cybernetic, thermodynamic,
psychological, philosophical, and complexity principles. These are further used
to discuss the notions of life, cognition and their evolution.
"
q-bio,Complexities of Human Promoter Sequences,"  By means of the diffusion entropy approach, we detect the scale-invariance
characteristics embedded in the 4737 human promoter sequences. The exponent for
the scale-invariance is in a wide range of $[ {0.3,0.9} ]$, which centered at
$\delta_c = 0.66$. The distribution of the exponent can be separated into left
and right branches with respect to the maximum. The left and right branches are
asymmetric and can be fitted exactly with Gaussian form with different widths,
respectively.
"
q-bio,"Evolutionary Neural Gas (ENG): A Model of Self Organizing Network from
  Input Categorization","  Despite their claimed biological plausibility, most self organizing networks
have strict topological constraints and consequently they cannot take into
account a wide range of external stimuli. Furthermore their evolution is
conditioned by deterministic laws which often are not correlated with the
structural parameters and the global status of the network, as it should happen
in a real biological system. In nature the environmental inputs are noise
affected and fuzzy. Which thing sets the problem to investigate the possibility
of emergent behaviour in a not strictly constrained net and subjected to
different inputs. It is here presented a new model of Evolutionary Neural Gas
(ENG) with any topological constraints, trained by probabilistic laws depending
on the local distortion errors and the network dimension. The network is
considered as a population of nodes that coexist in an ecosystem sharing local
and global resources. Those particular features allow the network to quickly
adapt to the environment, according to its dimensions. The ENG model analysis
shows that the net evolves as a scale-free graph, and justifies in a deeply
physical sense- the term gas here used.
"
q-bio,A remark on the number of steady states in a multiple futile cycle,"  The multisite phosphorylation-dephosphorylation cycle is a motif repeatedly
used in cell signaling. This motif itself can generate a variety of dynamic
behaviors like bistability and ultrasensitivity without direct positive
feedbacks. In this paper, we study the number of positive steady states of a
general multisite phosphorylation-dephosphorylation cycle, and how the number
of positive steady states varies by changing the biological parameters. We show
analytically that (1) for some parameter ranges, there are at least n+1 (if n
is even) or n (if n is odd) steady states; (2) there never are more than 2n-1
steady states (in particular, this implies that for n=2, including single
levels of MAPK cascades, there are at most three steady states); (3) for
parameters near the standard Michaelis-Menten quasi-steady state conditions,
there are at most n+1 steady states; and (4) for parameters far from the
standard Michaelis-Menten quasi-steady state conditions, there is at most one
steady state.
"
q-bio,Parsimony via concensus,"  The parsimony score of a character on a tree equals the number of state
changes required to fit that character onto the tree. We show that for
unordered, reversible characters this score equals the number of tree
rearrangements required to fit the tree onto the character. We discuss
implications of this connection for the debate over the use of consensus trees
or total evidence, and show how it provides a link between incongruence of
characters and recombination.
"
q-bio,"Emergence of spatiotemporal chaos driven by far-field breakup of spiral
  waves in the plankton ecological systems","  Alexander B. Medvinsky \emph{et al} [A. B. Medvinsky, I. A. Tikhonova, R. R.
Aliev, B.-L. Li, Z.-S. Lin, and H. Malchow, Phys. Rev. E \textbf{64}, 021915
(2001)] and Marcus R. Garvie \emph{et al} [M. R. Garvie and C. Trenchea, SIAM
J. Control. Optim. \textbf{46}, 775-791 (2007)] shown that the minimal
spatially extended reaction-diffusion model of phytoplankton-zooplankton can
exhibit both regular, chaotic behavior, and spatiotemporal patterns in a patchy
environment. Based on that, the spatial plankton model is furtherly
investigated by means of computer simulations and theoretical analysis in the
present paper when its parameters would be expected in the case of mixed
Turing-Hopf bifurcation region. Our results show that the spiral waves exist in
that region and the spatiotemporal chaos emerge, which arise from the far-field
breakup of the spiral waves over large ranges of diffusion coefficients of
phytoplankton and zooplankton. Moreover, the spatiotemporal chaos arising from
the far-field breakup of spiral waves does not gradually involve the whole
space within that region. Our results are confirmed by means of computation
spectra and nonlinear bifurcation of wave trains. Finally, we give some
explanations about the spatially structured patterns from the community level.
"
q-bio,"A Finite Element framework for computation of protein normal modes and
  mechanical response","  A coarse-grained computational procedure based on the Finite Element Method
is proposed to calculate the normal modes and mechanical response of proteins
and their supramolecular assemblies. Motivated by the elastic network model,
proteins are modeled as homogeneous isotropic elastic solids with volume
defined by their solvent-excluded surface. The discretized Finite Element
representation is obtained using a surface simplification algorithm that
facilitates the generation of models of arbitrary prescribed spatial
resolution. The procedure is applied to compute the normal modes of a mutant of
T4 phage lysozyme and of filamentous actin, as well as the critical Euler
buckling load of the latter when subject to axial compression. Results compare
favorably with all-atom normal mode analysis, the Rotation Translation Blocks
procedure, and experiment. The proposed methodology establishes a computational
framework for the calculation of protein mechanical response that facilitates
the incorporation of specific atomic-level interactions into the model,
including aqueous-electrolyte-mediated electrostatic effects. The procedure is
equally applicable to proteins with known atomic coordinates as it is to
electron density maps of proteins, protein complexes, and supramolecular
assemblies of unknown atomic structure.
"
q-bio,Intricate Knots in Proteins: Function and Evolution,"  A number of recently discovered protein structures incorporate a rather
unexpected structural feature: a knot in the polypeptide backbone. These knots
are extremely rare, but their occurrence is likely connected to protein
function in as yet unexplored fashion. Our analysis of the complete Protein
Data Bank reveals several new knots which, along with previously discovered
ones, can shed light on such connections. In particular, we identify the most
complex knot discovered to date in human ubiquitin hydrolase, and suggest that
its entangled topology protects it against unfolding and degradation by the
proteasome. Knots in proteins are typically preserved across species and
sometimes even across kingdoms. However, we also identify a knot which only
appears in some transcarbamylases while being absent in homologous proteins of
similar structure. The emergence of the knot is accompanied by a shift in the
enzymatic function of the protein. We suggest that the simple insertion of a
short DNA fragment into the gene may suffice to turn an unknotted into a
knotted structure in this protein.
"
q-bio,Origin of adaptive mutants: a quantum measurement?,"  This is a supplement to the paper arXiv:q-bio/0701050, containing the text of
correspondence sent to Nature in 1990.
"
q-bio,Simulation of Robustness against Lesions of Cortical Networks,"  Structure entails function and thus a structural description of the brain
will help to understand its function and may provide insights into many
properties of brain systems, from their robustness and recovery from damage, to
their dynamics and even their evolution. Advances in the analysis of complex
networks provide useful new approaches to understanding structural and
functional properties of brain networks. Structural properties of networks
recently described allow their characterization as small-world, random
(exponential) and scale-free. They complement the set of other properties that
have been explored in the context of brain connectivity, such as topology,
hodology, clustering, and hierarchical organization. Here we apply new network
analysis methods to cortical inter-areal connectivity networks for the cat and
macaque brains. We compare these corticocortical fibre networks to benchmark
rewired, small-world, scale-free and random networks, using two analysis
strategies, in which we measure the effects of the removal of nodes and
connections on the structural properties of the cortical networks. The brain
networks' structural decay is in most respects similar to that of scale-free
networks. The results implicate highly connected hub-nodes and bottleneck
connections as structural basis for some of the conditional robustness of brain
systems. This informs the understanding of the development of brain networks'
connectivity.
"
q-bio,"Molecular Synchronization Waves in Arrays of Allosterically Regulated
  Enzymes","  Spatiotemporal pattern formation in a product-activated enzymic reaction at
high enzyme concentrations is investigated. Stochastic simulations show that
catalytic turnover cycles of individual enzymes can become coherent and that
complex wave patterns of molecular synchronization can develop. The analysis
based on the mean-field approximation indicates that the observed patterns
result from the presence of Hopf and wave bifurcations in the considered
system.
"
q-bio,"Optimal stimulus and noise distributions for information transmission
  via suprathreshold stochastic resonance","  Suprathreshold stochastic resonance (SSR) is a form of noise enhanced signal
transmission that occurs in a parallel array of independently noisy identical
threshold nonlinearities, including model neurons. Unlike most forms of
stochastic resonance, the output response to suprathreshold random input
signals of arbitrary magnitude is improved by the presence of even small
amounts of noise. In this paper the information transmission performance of SSR
in the limit of a large array size is considered. Using a relationship between
Shannon's mutual information and Fisher information, a sufficient condition for
optimality, i.e. channel capacity, is derived. It is shown that capacity is
achieved when the signal distribution is Jeffrey's prior, as formed from the
noise distribution, or when the noise distribution depends on the signal
distribution via a cosine relationship. These results provide theoretical
verification and justification for previous work in both computational
neuroscience and electronics.
"
q-bio,Behavioral response to strong aversive stimuli: A neurodynamical model,"  In this paper a theoretical model of functioning of a neural circuit during a
behavioral response has been proposed. A neural circuit can be thought of as a
directed multigraph whose each vertex is a neuron and each edge is a synapse.
It has been assumed in this paper that the behavior of such circuits is
manifested through the collective behavior of neurons belonging to that
circuit. Behavioral information of each neuron is contained in the coefficients
of the fast Fourier transform (FFT) over the output spike train. Those
coefficients form a vector in a multidimensional vector space. Behavioral
dynamics of a neuronal network in response to strong aversive stimuli has been
studied in a vector space in which a suitable pseudometric has been defined.
The neurodynamical model of network behavior has been formulated in terms of
existing memory, synaptic plasticity and feelings. The model has an analogy in
classical electrostatics, by which the notion of force and potential energy has
been introduced. Since the model takes input from each neuron in a network and
produces a behavior as the output, it would be extremely difficult or may even
be impossible to implement. But with the help of the model a possible
explanation for an hitherto unexplained neurological observation in human brain
has been offered. The model is compatible with a recent model of sequential
behavioral dynamics. The model is based on electrophysiology, but its relevance
to hemodynamics has been outlined.
"
q-bio,Evolutionary games on minimally structured populations,"  Population structure induced by both spatial embedding and more general
networks of interaction, such as model social networks, have been shown to have
a fundamental effect on the dynamics and outcome of evolutionary games. These
effects have, however, proved to be sensitive to the details of the underlying
topology and dynamics. Here we introduce a minimal population structure that is
described by two distinct hierarchical levels of interaction. We believe this
model is able to identify effects of spatial structure that do not depend on
the details of the topology. We derive the dynamics governing the evolution of
a system starting from fundamental individual level stochastic processes
through two successive meanfield approximations. In our model of population
structure the topology of interactions is described by only two parameters: the
effective population size at the local scale and the relative strength of local
dynamics to global mixing. We demonstrate, for example, the existence of a
continuous transition leading to the dominance of cooperation in populations
with hierarchical levels of unstructured mixing as the benefit to cost ratio
becomes smaller then the local population size. Applying our model of spatial
structure to the repeated prisoner's dilemma we uncover a novel and
counterintuitive mechanism by which the constant influx of defectors sustains
cooperation. Further exploring the phase space of the repeated prisoner's
dilemma and also of the ""rock-paper-scissor"" game we find indications of rich
structure and are able to reproduce several effects observed in other models
with explicit spatial embedding, such as the maintenance of biodiversity and
the emergence of global oscillations.
"
q-bio,"Symmetries by base substitutions in the genetic code predict 2' or 3'
  aminoacylation of tRNAs","  This letter reports complete sets of two-fold symmetries between partitions
of the universal genetic code. By substituting bases at each position of the
codons according to a fixed rule, it happens that properties of the degeneracy
pattern or of tRNA aminoacylation specificity are exchanged.
"
q-bio,Annealed importance sampling of dileucine peptide,"  Annealed importance sampling is a means to assign equilibrium weights to a
nonequilibrium sample that was generated by a simulated annealing protocol. The
weights may then be used to calculate equilibrium averages, and also serve as
an ``adiabatic signature'' of the chosen cooling schedule. In this paper we
demonstrate the method on the 50-atom dileucine peptide, showing that
equilibrium distributions are attained for manageable cooling schedules. For
this system, as naively implemented here, the method is modestly more efficient
than constant temperature simulation. However, the method is worth considering
whenever any simulated heating or cooling is performed (as is often done at the
beginning of a simulation project, or during an NMR structure calculation), as
it is simple to implement and requires minimal additional CPU expense.
Furthermore, the naive implementation presented here can be improved.
"
q-bio,"Quantitative Resolution to some ""Absolute Discrepancies"" in Cancer
  Theories: a View from Phage lambda Genetic Switch","  Is it possible to understand cancer? Or more specifically, is it possible to
understand cancer from genetic side? There already many answers in literature.
The most optimistic one has claimed that it is mission-possible. Duesberg and
his colleagues reviewed the impressive amount of research results on cancer
accumulated over 100 years. It confirms the a general opinion that considering
all available experimental results and clinical observations there is no cancer
theory without major difficulties, including the prevailing gene-based cancer
theories. They have then listed 9 ""absolute discrepancies"" for such cancer
theory. In this letter the quantitative evidence against one of their major
reasons for dismissing mutation cancer theory, by both in vivo experiment and a
first principle computation, is explicitly pointed out.
"
q-bio,"An individual based model with global competition interaction:
  fluctuations effects in pattern formation","  We present some numerical results obtained from a simple individual based
model that describes clustering of organisms caused by competition. Our aim is
to show how, even when a deterministic description developed for continuum
models predicts no pattern formation, an individual based model displays well
defined patterns, as a consequence of fluctuations effects caused by the
discrete nature of the interacting agents.
"
q-bio,Velocity oscillations in actin-based motility,"  We present a simple and generic theoretical description of actin-based
motility, where polymerization of filaments maintains propulsion. The dynamics
is driven by polymerization kinetics at the filaments' free ends, crosslinking
of the actin network, attachment and detachment of filaments to the obstacle
interfaces and entropic forces. We show that spontaneous oscillations in the
velocity emerge in a broad range of parameter values, and compare our findings
with experiments.
"
q-bio,Optimal flexibility for conformational transitions in macromolecules,"  Conformational transitions in macromolecular complexes often involve the
reorientation of lever-like structures. Using a simple theoretical model, we
show that the rate of such transitions is drastically enhanced if the lever is
bendable, e.g. at a localized ""hinge''. Surprisingly, the transition is fastest
with an intermediate flexibility of the hinge. In this intermediate regime, the
transition rate is also least sensitive to the amount of ""cargo'' attached to
the lever arm, which could be exploited by molecular motors. To explain this
effect, we generalize the Kramers-Langer theory for multi-dimensional barrier
crossing to configuration dependent mobility matrices.
"
q-bio,"AFM Imaging of SWI/SNF action: mapping the nucleosome remodeling and
  sliding","  We propose a combined experimental (Atomic Force Microscopy) and theoretical
study of the structural and dynamical properties of nucleosomes. In contrast to
biochemical approaches, this method allows to determine simultaneously the DNA
complexed length distribution and nucleosome position in various contexts.
First, we show that differences in the nucleo-proteic structure observed
between conventional H2A and H2A.Bbd variant nucleosomes induce quantitative
changes in the in the length distribution of DNA complexed with histones. Then,
the sliding action of remodeling complex SWI/SNF is characterized through the
evolution of the nucleosome position and wrapped DNA length mapping. Using a
linear energetic model for the distribution of DNA complexed length, we extract
the net wrapping energy of DNA onto the histone octamer, and compare it to
previous studies.
"
q-bio,"A practical guide to stochastic simulations of reaction-diffusion
  processes","  A practical introduction to stochastic modelling of reaction-diffusion
processes is presented. No prior knowledge of stochastic simulations is
assumed. The methods are explained using illustrative examples. The article
starts with the classical Gillespie algorithm for the stochastic modelling of
chemical reactions. Then stochastic algorithms for modelling molecular
diffusion are given. Finally, basic stochastic reaction-diffusion methods are
presented. The connections between stochastic simulations and deterministic
models are explained and basic mathematical tools (e.g. chemical master
equation) are presented. The article concludes with an overview of more
advanced methods and problems.
"
q-bio,Fast recursive filters for simulating nonlinear dynamic systems,"  A fast and accurate computational scheme for simulating nonlinear dynamic
systems is presented. The scheme assumes that the system can be represented by
a combination of components of only two different types: first-order low-pass
filters and static nonlinearities. The parameters of these filters and
nonlinearities may depend on system variables, and the topology of the system
may be complex, including feedback. Several examples taken from neuroscience
are given: phototransduction, photopigment bleaching, and spike generation
according to the Hodgkin-Huxley equations. The scheme uses two slightly
different forms of autoregressive filters, with an implicit delay of zero for
feedforward control and an implicit delay of half a sample distance for
feedback control. On a fairly complex model of the macaque retinal horizontal
cell it computes, for a given level of accuracy, 1-2 orders of magnitude faster
than 4th-order Runge-Kutta. The computational scheme has minimal memory
requirements, and is also suited for computation on a stream processor, such as
a GPU (Graphical Processing Unit).
"
q-bio,Stochastic fluctuations in metabolic pathways,"  Fluctuations in the abundance of molecules in the living cell may affect its
growth and well being. For regulatory molecules (e.g., signaling proteins or
transcription factors), fluctuations in their expression can affect the levels
of downstream targets in a network. Here, we develop an analytic framework to
investigate the phenomenon of noise correlation in molecular networks.
Specifically, we focus on the metabolic network, which is highly inter-linked,
and noise properties may constrain its structure and function. Motivated by the
analogy between the dynamics of a linear metabolic pathway and that of the
exactly soluable linear queueing network or, alternatively, a mass transfer
system, we derive a plethora of results concerning fluctuations in the
abundance of intermediate metabolites in various common motifs of the metabolic
network. For all but one case examined, we find the steady-state fluctuation in
different nodes of the pathways to be effectively uncorrelated. Consequently,
fluctuations in enzyme levels only affect local properties and do not propagate
elsewhere into metabolic networks, and intermediate metabolites can be freely
shared by different reactions. Our approach may be applicable to study
metabolic networks with more complex topologies, or protein signaling networks
which are governed by similar biochemical reactions. Possible implications for
bioinformatic analysis of metabolimic data are discussed.
"
q-bio,Holographic bound and protein linguistics,"  The holographic bound in physics constrains the complexity of life. The
finite storage capability of information in the observable universe requires
the protein linguistics in the evolution of life. We find that the evolution of
genetic code determines the variance of amino acid frequencies and genomic GC
content among species. The elegant linguistic mechanism is confirmed by the
experimental observations based on all known entire proteomes.
"
q-bio,Unifying Evolutionary and Network Dynamics,"  Many important real-world networks manifest ""small-world"" properties such as
scale-free degree distributions, small diameters, and clustering. The most
common model of growth for these networks is ""preferential attachment"", where
nodes acquire new links with probability proportional to the number of links
they already have. We show that preferential attachment is a special case of
the process of molecular evolution. We present a new single-parameter model of
network growth that unifies varieties of preferential attachment with the
quasispecies equation (which models molecular evolution), and also with the
Erdos-Renyi random graph model. We suggest some properties of evolutionary
models that might be applied to the study of networks. We also derive the form
of the degree distribution resulting from our algorithm, and we show through
simulations that the process also models aspects of network growth. The
unification allows mathematical machinery developed for evolutionary dynamics
to be applied in the study of network dynamics, and vice versa.
"
q-bio,Bone Cancer Rates in Dinosaurs Compared with Modern Vertebrates,"  Data on the prevalence of bone cancer in dinosaurs is available from past
radiological examination of preserved bones. We statistically test this data
for consistency with rates extrapolated from information on bone cancer in
modern vertebrates, and find that there is no evidence of a different rate.
Thus, this test provides no support for a possible role of ionizing radiation
in the K-T extinction event.
"
q-bio,"Evolution favors protein mutational robustness in sufficiently large
  populations","  BACKGROUND: An important question is whether evolution favors properties such
as mutational robustness or evolvability that do not directly benefit any
individual, but can influence the course of future evolution. Functionally
similar proteins can differ substantially in their robustness to mutations and
capacity to evolve new functions, but it has remained unclear whether any of
these differences might be due to evolutionary selection for these properties.
  RESULTS: Here we use laboratory experiments to demonstrate that evolution
favors protein mutational robustness if the evolving population is sufficiently
large. We neutrally evolve cytochrome P450 proteins under identical selection
pressures and mutation rates in populations of different sizes, and show that
proteins from the larger and thus more polymorphic population tend towards
higher mutational robustness. Proteins from the larger population also evolve
greater stability, a biophysical property that is known to enhance both
mutational robustness and evolvability. The excess mutational robustness and
stability is well described by existing mathematical theories, and can be
quantitatively related to the way that the proteins occupy their neutral
network.
  CONCLUSIONS: Our work is the first experimental demonstration of the general
tendency of evolution to favor mutational robustness and protein stability in
highly polymorphic populations. We suggest that this phenomenon may contribute
to the mutational robustness and evolvability of viruses and bacteria that
exist in large populations.
"
q-bio,On restrictions of balanced 2-interval graphs,"  The class of 2-interval graphs has been introduced for modelling scheduling
and allocation problems, and more recently for specific bioinformatic problems.
Some of those applications imply restrictions on the 2-interval graphs, and
justify the introduction of a hierarchy of subclasses of 2-interval graphs that
generalize line graphs: balanced 2-interval graphs, unit 2-interval graphs, and
(x,x)-interval graphs. We provide instances that show that all the inclusions
are strict. We extend the NP-completeness proof of recognizing 2-interval
graphs to the recognition of balanced 2-interval graphs. Finally we give hints
on the complexity of unit 2-interval graphs recognition, by studying
relationships with other graph classes: proper circular-arc, quasi-line graphs,
K_{1,5}-free graphs, ...
"
q-bio,"Phylogenetic mixtures on a single tree can mimic a tree of another
  topology","  Phylogenetic mixtures model the inhomogeneous molecular evolution commonly
observed in data. The performance of phylogenetic reconstruction methods where
the underlying data is generated by a mixture model has stimulated considerable
recent debate. Much of the controversy stems from simulations of mixture model
data on a given tree topology for which reconstruction algorithms output a tree
of a different topology; these findings were held up to show the shortcomings
of particular tree reconstruction methods. In so doing, the underlying
assumption was that mixture model data on one topology can be distinguished
from data evolved on an unmixed tree of another topology given enough data and
the ``correct'' method. Here we show that this assumption can be false. For
biologists our results imply that, for example, the combined data from two
genes whose phylogenetic trees differ only in terms of branch lengths can
perfectly fit a tree of a different topology.
"
q-bio,Multi-Agent Approach to the Self-Organization of Networks,"  Is it possible to link a set of nodes without using preexisting positional
information or any kind of long-range attraction of the nodes? Can the process
of generating positional information, i.e. the detection of ``unknown'' nodes
and the estabishment of chemical gradients, \emph{and} the process of network
formation, i.e. the establishment of links between nodes, occur in parallel, on
a comparable time scale, as a process of co-evolution?
  The paper discusses a model where the generation of relevant information for
establishing the links between nodes results from the interaction of many
\emph{agents}, i.e. subunits of the system that are capable of performing some
activities. Their collective interaction is based on (indirect) communication,
which also includes memory effects and the dissemination of information in the
system. The relevant (``pragmatic'') information that leads to the
establishment of the links then emerges from an evolutionary interplay of
selection and reamplification.
"
q-bio,"Observation of Multiple folding Pathways of beta-hairpin Trpzip2 from
  Independent Continuous Folding Trajectories","  We report 10 successfully folding events of trpzip2 by molecular dynamics
simulation. It is found that the trizip2 can fold into its native state through
different zipper pathways, depending on the ways of forming hydrophobic core.
We also find a very fast non-zipper pathway. This indicates that there may be
no inconsistencies in the current pictures of beta-hairpin folding mechanisms.
These pathways occur with different probabilities. zip-out is the most probable
one. This may explain the recent experiment that the turn formation is the
rate-limiting step for beta-hairpin folding.
"
q-bio,Boolean network model predicts cell cycle sequence of fission yeast,"  A Boolean network model of the cell-cycle regulatory network of fission yeast
(Schizosaccharomyces Pombe) is constructed solely on the basis of the known
biochemical interaction topology. Simulating the model in the computer,
faithfully reproduces the known sequence of regulatory activity patterns along
the cell cycle of the living cell. Contrary to existing differential equation
models, no parameters enter the model except the structure of the regulatory
circuitry. The dynamical properties of the model indicate that the biological
dynamical sequence is robustly implemented in the regulatory network, with the
biological stationary state G1 corresponding to the dominant attractor in state
space, and with the biological regulatory sequence being a strongly attractive
trajectory. Comparing the fission yeast cell-cycle model to a similar model of
the corresponding network in S. cerevisiae, a remarkable difference in
circuitry, as well as dynamics is observed. While the latter operates in a
strongly damped mode, driven by external excitation, the S. pombe network
represents an auto-excited system with external damping.
"
q-bio,A tree without leaves,"  The puzzle presented by the famous stumps of Gilboa, New York, finds a
solution in the discovery of two fossil specimens that allow the entire
structure of these early trees to be reconstructed.
"
q-bio,"Multilevel Deconstruction of the In Vivo Behavior of Looped DNA-Protein
  Complexes","  Protein-DNA complexes with loops play a fundamental role in a wide variety of
cellular processes, ranging from the regulation of DNA transcription to
telomere maintenance. As ubiquitous as they are, their precise in vivo
properties and their integration into the cellular function still remain
largely unexplored. Here, we present a multilevel approach that efficiently
connects in both directions molecular properties with cell physiology and use
it to characterize the molecular properties of the looped DNA-lac repressor
complex while functioning in vivo. The properties we uncover include the
presence of two representative conformations of the complex, the stabilization
of one conformation by DNA architectural proteins, and precise values of the
underlying twisting elastic constants and bending free energies. Incorporation
of all this molecular information into gene-regulation models reveals an
unprecedented versatility of looped DNA-protein complexes at shaping the
properties of gene expression.
"
q-bio,Mismatch Repair Error Implies Chargaff's Second Parity Rule,"  Chargaff's second parity rule holds empirically for most types of DNA that
along single strands of DNA the base contents are equal for complimentary
bases, A = T, G = C. A Markov chain model is constructed to track the evolution
of any single base position along single strands of genomes whose organisms are
equipped with replication mismatch repair. Under the key assumptions that
mismatch error rates primarily depend the number of hydrogen bonds of
nucleotides and that the mismatch repairing process itself makes strand
recognition error, the model shows that the steady state probabilities for any
base position to take on one of the 4 nucleotide bases are equal for
complimentary bases. As a result, Chargaff's second parity rule is the
manifestation of the Law of Large Number acting on the steady state
probabilities. More importantly, because the model pinpoints mismatch repair as
a basis of the rule, it is suitable for experimental verification.
"
q-bio,"Considering the Case for Biodiversity Cycles: Reexamining the Evidence
  for Periodicity in the Fossil Record","  Medvedev and Melott (2007) have suggested that periodicity in fossil
biodiversity may be induced by cosmic rays which vary as the Solar System
oscillates normal to the galactic disk. We re-examine the evidence for a 62
million year (Myr) periodicity in biodiversity throughout the Phanerozoic
history of animal life reported by Rohde & Mueller (2005), as well as related
questions of periodicity in origination and extinction. We find that the signal
is robust against variations in methods of analysis, and is based on
fluctuations in the Paleozoic and a substantial part of the Mesozoic.
Examination of origination and extinction is somewhat ambiguous, with results
depending upon procedure. Origination and extinction intensity as defined by RM
may be affected by an artifact at 27 Myr in the duration of stratigraphic
intervals. Nevertheless, when a procedure free of this artifact is implemented,
the 27 Myr periodicity appears in origination, suggesting that the artifact may
ultimately be based on a signal in the data. A 62 Myr feature appears in
extinction, when this same procedure is used. We conclude that evidence for a
periodicity at 62 Myr is robust, and evidence for periodicity at approximately
27 Myr is also present, albeit more ambiguous.
"
q-bio,"A quantitative study on the growth variability of tumour cell clones in
  vitro","  Objectives: In this study, we quantify the growth variability of tumour cell
clones from a human leukemia cell line. Materials and methods: We have used
microplate spectrophotometry to measure the growth kinetics of hundreds of
individual cell clones from the Molt3 cell line. The growth rate of each clonal
population has been estimated by fitting experimental data with the logistic
equation. Results: The growth rates were observed to vary among different
clones. Up to six clones with a growth rate above or below the mean growth rate
of the parent population were further cloned and the growth rates of their
offsprings were measured. The distribution of the growth rates of the subclones
did not significantly differ from that of the parent population thus suggesting
that growth variability has an epigenetic origin. To explain the observed
distributions of clonal growth rates we have developed a probabilistic model
assuming that the fluctuations in the number of mitochondria through successive
cell cycles are the leading cause of growth variability. For fitting purposes,
we have estimated experimentally by flow cytometry the maximum average number
of mitochondria in Molt3 cells. The model fits nicely the observed
distributions of growth rates, however, cells in which the mitochondria were
rendered non functional (rho-0 cells) showed only a 30% reduction in the clonal
growth variability with respect to normal cells. Conclusions: A tumor cell
population is a dynamic ensemble of clones with highly variable growth rate. At
least part of this variability is due to fluctuations in the number of
mitochondria.
"
q-bio,Fourier Analysis of Biological Evolution: Concept of Selection Moment,"  Secondary structure elements of many protein families exhibit differential
conservation on their opposing faces. Amphipathic helices and beta-sheets by
definition possess this property, and play crucial functional roles. This type
of evolutionary trajectory of a protein family is usually critical to the
functions of the protein family, as well as in creating functions within
subfamilies. That is, differential conservation maintains properties of a
protein structure related to its orientation, and that are important in
packing, recognition, and catalysis. Here I define and formulate a new concept,
called the selection moment, that detects this evolutionary process in protein
sequences. A treatment of its various applications is detailed.
"
q-bio,Inferring dynamic genetic networks with low order independencies,"  In this paper, we propose a novel inference method for dynamic genetic
networks which makes it possible to face with a number of time measurements n
much smaller than the number of genes p. The approach is based on the concept
of low order conditional dependence graph that we extend here in the case of
Dynamic Bayesian Networks. Most of our results are based on the theory of
graphical models associated with the Directed Acyclic Graphs (DAGs). In this
way, we define a minimal DAG G which describes exactly the full order
conditional dependencies given the past of the process. Then, to face with the
large p and small n estimation case, we propose to approximate DAG G by
considering low order conditional independencies. We introduce partial qth
order conditional dependence DAGs G(q) and analyze their probabilistic
properties. In general, DAGs G(q) differ from DAG G but still reflect relevant
dependence facts for sparse networks such as genetic networks. By using this
approximation, we set out a non-bayesian inference method and demonstrate the
effectiveness of this approach on both simulated and real data analysis. The
inference procedure is implemented in the R package 'G1DBN' freely available
from the CRAN archive.
"
q-bio,"Inferring DNA sequences from mechanical unzipping data: the
  large-bandwidth case","  The complementary strands of DNA molecules can be separated when stretched
apart by a force; the unzipping signal is correlated to the base content of the
sequence but is affected by thermal and instrumental noise. We consider here
the ideal case where opening events are known to a very good time resolution
(very large bandwidth), and study how the sequence can be reconstructed from
the unzipping data. Our approach relies on the use of statistical Bayesian
inference and of Viterbi decoding algorithm. Performances are studied
numerically on Monte Carlo generated data, and analytically. We show how
multiple unzippings of the same molecule may be exploited to improve the
quality of the prediction, and calculate analytically the number of required
unzippings as a function of the bandwidth, the sequence content, the elasticity
parameters of the unzipped strands.
"
q-bio,Deterministic characterization of stochastic genetic circuits,"  For cellular biochemical reaction systems where the numbers of molecules is
small, significant noise is associated with chemical reaction events. This
molecular noise can give rise to behavior that is very different from the
predictions of deterministic rate equation models. Unfortunately, there are few
analytic methods for examining the qualitative behavior of stochastic systems.
Here we describe such a method that extends deterministic analysis to include
leading-order corrections due to the molecular noise. The method allows the
steady-state behavior of the stochastic model to be easily computed,
facilitates the mapping of stability phase diagrams that include stochastic
effects and reveals how model parameters affect noise susceptibility, in a
manner not accessible to numerical simulation. By way of illustration we
consider two genetic circuits: a bistable positive-feedback loop and a
negative-feedback oscillator. We find in the positive feedback circuit that
translational activation leads to a far more stable system than transcriptional
control. Conversely, in a negative-feedback loop triggered by a
positive-feedback switch, the stochasticity of transcriptional control is
harnessed to generate reproducible oscillations.
"
q-bio,"Hedging our bets: the expected contribution of species to future
  phylogenetic diversity","  If predictions for species extinctions hold, then the `tree of life' today
may be quite different to that in (say) 100 years. We describe a technique to
quantify how much each species is likely to contribute to future biodiversity,
as measured by its expected contribution to phylogenetic diversity. Our
approach considers all possible scenarios for the set of species that will be
extant at some future time, and weights them according to their likelihood
under an independent (but not identical) distribution on species extinctions.
Although the number of extinction scenarios can typically be very large, we
show that there is a simple algorithm that will quickly compute this index. The
method is implemented and applied to the prosimian primates as a test case, and
the associated species ranking is compared to a related measure (the `Shapley
index'). We describe indices for rooted and unrooted trees, and a modification
that also includes the focal taxon's probability of extinction, making it
directly comparable to some new conservation metrics.
"
q-bio,"Modeling transcription factor binding events to DNA using a random
  walker/jumper representation on a 1D/2D lattice with different affinity sites","  Surviving in a diverse environment requires corresponding organism responses.
At the cellular level, such adjustment relies on the transcription factors
(TFs) which must rapidly find their target sequences amidst a vast amount of
non-relevant sequences on DNA molecules. Whether these transcription factors
locate their target sites through a 1D or 3D pathway is still a matter of
speculation. It has been suggested that the optimum search time is when the
protein equally shares its search time between 1D and 3D diffusions. In this
paper, we study the above problem using a Monte Carlo simulation by considering
a very simple physical model. A 1D strip, representing a DNA, with a number of
low affinity sites, corresponding to non-target sites, and high affinity sites,
corresponding to target sites, is considered and later extended to a 2D strip.
We study the 1D and 3D exploration pathways, and combinations of the two modes
by considering three different types of molecules: a walker that randomly walks
along the strip with no dissociation; a jumper that represents dissociation and
then re-association of a TF with the strip at later time at a distant site; and
a hopper that is similar to the jumper but it dissociates and then
re-associates at a faster rate than the jumper. We analyze the final
probability distribution of molecules for each case and find that TFs can
locate their targets fast enough even if they spend 15% of their search time
diffusing freely in the solution. This indeed agrees with recent experimental
results obtained by Elf et al. 2007 and is in contrast with theoretical
expectation.
"
q-bio,On Gene Duplication Models for Evolving Regulatory Networks,"  Background: Duplication of genes is important for evolution of molecular
networks. Many authors have therefore considered gene duplication as a driving
force in shaping the topology of molecular networks. In particular it has been
noted that growth via duplication would act as an implicit way of preferential
attachment, and thereby provide the observed broad degree distributions of
molecular networks.
  Results: We extend current models of gene duplication and rewiring by
including directions and the fact that molecular networks are not a result of
unidirectional growth. We introduce upstream sites and downstream shapes to
quantify potential links during duplication and rewiring. We find that this in
itself generates the observed scaling of transcription factors for genome sites
in procaryotes. The dynamical model can generate a scale-free degree
distribution, p(k)&prop; 1/k^&gamma;, with exponent &gamma;=1 in the
non-growing case, and with &gamma;>1 when the network is growing.
  Conclusions: We find that duplication of genes followed by substantial
recombination of upstream regions could generate main features of genetic
regulatory networks. Our steady state degree distribution is however to broad
to be consistent with data, thereby suggesting that selective pruning acts as a
main additional constraint on duplicated genes. Our analysis shows that gene
duplication can only be a main cause for the observed broad degree
distributions, if there is also substantial recombinations between upstream
regions of genes.
"
q-bio,Fundamental Limits to Position Determination by Concentration Gradients,"  Position determination in biological systems is often achieved through
protein concentration gradients. Measuring the local concentration of such a
protein with a spatially-varying distribution allows the measurement of
position within the system. In order for these systems to work effectively,
position determination must be robust to noise. Here, we calculate fundamental
limits to the precision of position determination by concentration gradients
due to unavoidable biochemical noise perturbing the gradients. We focus on
gradient proteins with first order reaction kinetics. Systems of this type have
been experimentally characterised in both developmental and cell biology
settings. For a single gradient we show that, through time-averaging, great
precision can potentially be achieved even with very low protein copy numbers.
As a second example, we investigate the ability of a system with oppositely
directed gradients to find its centre. With this mechanism, positional
precision close to the centre improves more slowly with increasing averaging
time, and so longer averaging times or higher copy numbers are required for
high precision. For both single and double gradients, we demonstrate the
existence of optimal length scales for the gradients, where precision is
maximized, as well as analyzing how precision depends on the size of the
concentration measuring apparatus. Our results provide fundamental constraints
on the positional precision supplied by concentration gradients in various
contexts, including both in developmental biology and also within a single
cell.
"
q-bio,"Efficient model chemistries for peptides. I. Split-valence Gaussian
  basis sets and the heterolevel approximation in RHF and MP2","  We present an exhaustive study of more than 250 ab initio potential energy
surfaces (PESs) of the model dipeptide HCO-L-Ala-NH2. The model chemistries
(MCs) used are constructed as homo- and heterolevels involving possibly
different RHF and MP2 calculations for the geometry and the energy. The basis
sets used belong to a sample of 39 selected representants from Pople's
split-valence families, ranging from the small 3-21G to the large
6-311++G(2df,2pd). The reference PES to which the rest are compared is the
MP2/6-311++G(2df,2pd) homolevel, which, as far as we are aware, is the more
accurate PES of a dipeptide in the literature. The aim of the study presented
is twofold: On the one hand, the evaluation of the influence of polarization
and diffuse functions in the basis set, distinguishing between those placed at
1st-row atoms and those placed at hydrogens, as well as the effect of different
contraction and valence splitting schemes. On the other hand, the investigation
of the heterolevel assumption, which is defined here to be that which states
that heterolevel MCs are more efficient than homolevel MCs. The heterolevel
approximation is very commonly used in the literature, but it is seldom
checked. As far as we know, the only tests for peptides or related systems,
have been performed using a small number of conformers, and this is the first
time that this potentially very economical approximation is tested in full
PESs. In order to achieve these goals, all data sets have been compared and
analyzed in a way which captures the nearness concept in the space of MCs.
"
q-bio,"A Model of Late Long-Term Potentiation Simulates Aspects of Memory
  Maintenance","  Late long-term potentiation (L-LTP) appears essential for the formation of
long-term memory, with memories at least partly encoded by patterns of
strengthened synapses. How memories are preserved for months or years, despite
molecular turnover, is not well understood. Ongoing recurrent neuronal
activity, during memory recall or during sleep, has been hypothesized to
preferentially potentiate strong synapses, preserving memories. This hypothesis
has not been evaluated in the context of a mathematical model representing
biochemical pathways important for L-LTP. I incorporated ongoing activity into
two such models: a reduced model that represents some of the essential
biochemical processes, and a more detailed published model. The reduced model
represents synaptic tagging and gene induction intuitively, and the detailed
model adds activation of essential kinases by Ca. Ongoing activity was modeled
as continual brief elevations of [Ca]. In each model, two stable states of
synaptic weight resulted. Positive feedback between synaptic weight and the
amplitude of ongoing Ca transients underlies this bistability. A tetanic or
theta-burst stimulus switches a model synapse from a low weight to a high
weight stabilized by ongoing activity. Bistability was robust to parameter
variations. Simulations illustrated that prolonged decreased activity reset
synapses to low weights, suggesting a plausible forgetting mechanism. However,
episodic activity with shorter inactive intervals maintained strong synapses.
Both models support experimental predictions. Tests of these predictions are
expected to further understanding of how neuronal activity is coupled to
maintenance of synaptic strength.
"
q-bio,"Propagation of external regulation and asynchronous dynamics in random
  Boolean networks","  Boolean Networks and their dynamics are of great interest as abstract
modeling schemes in various disciplines, ranging from biology to computer
science. Whereas parallel update schemes have been studied extensively in past
years, the level of understanding of asynchronous updates schemes is still very
poor. In this paper we study the propagation of external information given by
regulatory input variables into a random Boolean network. We compute both
analytically and numerically the time evolution and the asymptotic behavior of
this propagation of external regulation (PER). In particular, this allows us to
identify variables which are completely determined by this external
information. All those variables in the network which are not directly fixed by
PER form a core which contains in particular all non-trivial feedback loops. We
design a message-passing approach allowing to characterize the statistical
properties of these cores in dependence of the Boolean network and the external
condition. At the end we establish a link between PER dynamics and the full
random asynchronous dynamics of a Boolean network.
"
q-bio,"Inverse Geometric Approach to the Simulation of the Circular Growth. The
  Case of Multicellular Tumor Spheroids","  We demonstrate the power of the genetic algorithms to construct the cellular
automata model simulating the growth of 2-dimensional close-to-circular
clusters revealing the desired properties, such as the growth rate and, at the
same time, the fractal behavior of their contours. The possible application of
the approach in the field of tumor modeling is outlined.
"
q-bio,Clustering Coefficients of Protein-Protein Interaction Networks,"  The properties of certain networks are determined by hidden variables that
are not explicitly measured. The conditional probability (propagator) that a
vertex with a given value of the hidden variable is connected to k of other
vertices determines all measurable properties. We study hidden variable models
and find an averaging approximation that enables us to obtain a general
analytical result for the propagator. Analytic results showing the validity of
the approximation are obtained. We apply hidden variable models to
protein-protein interaction networks (PINs) in which the hidden variable is the
association free-energy, determined by distributions that depend on
biochemistry and evolution. We compute degree distributions as well as
clustering coefficients of several PINs of different species; good agreement
with measured data is obtained. For the human interactome two different
parameter sets give the same degree distributions, but the computed clustering
coefficients differ by a factor of about two. This shows that degree
distributions are not sufficient to determine the properties of PINs.
"
q-bio,Efficiency and versatility of distal multisite transcription regulation,"  Transcription regulation typically involves the binding of proteins over long
distances on multiple DNA sites that are brought close to each other by the
formation of DNA loops. The inherent complexity of the assembly of regulatory
complexes on looped DNA challenges the understanding of even the simplest
genetic systems, including the prototypical lac operon. Here we implement a
scalable quantitative computational approach to analyze systems regulated
through multiple DNA sites with looping. Our approach applied to the lac operon
accurately predicts the transcription rate over five orders of magnitude for
wild type and seven mutants accounting for all the combinations of deletions of
the three operators. A quantitative analysis of the model reveals that the
presence of three operators provides a mechanism to combine robust repression
with sensitive induction, two seemingly mutually exclusive properties that are
required for optimal functioning of metabolic switches.
"
q-bio,"Network Growth with Preferential Attachment for High Indegree and Low
  Outdegree","  We study the growth of a directed transportation network, such as a food web,
in which links carry resources. We propose a growth process in which new nodes
(or species) preferentially attach to existing nodes with high indegree (in
food-web language, number of prey) and low outdegree (or number of predators).
This scheme, which we call inverse preferential attachment, is intended to
maximize the amount of resources available to each new node. We show that the
outdegree (predator) distribution decays at least exponentially fast for large
outdegree and is continuously tunable between an exponential distribution and a
delta function. The indegree (prey) distribution is poissonian in the
large-network limit.
"
q-bio,"A new chaotic attractor in a basic multi-strain epidemiological model
  with temporary cross-immunity","  An epidemic multi-strain model with temporary cross-immunity shows chaos,
even in a previously unexpected parameter region. Especially dengue fever
models with strong enhanced infectivity on secondary infection have previously
shown deterministic chaos motivated by experimental findings of
antibody-dependent-enhancement (ADE). Including temporary cross-immunity in
such models, which is common knowledge among field researchers in dengue, we
find a deterministically chaotic attractor in the more realistic parameter
region of reduced infectivity on secondary infection (''inverse ADE'' parameter
region). This is realistic for dengue fever since on second infection people
are more likely to be hospitalized, hence do not contribute to the force of
infection as much as people with first infection.
  Our finding has wider implications beyond dengue in any multi-strain
epidemiological systems with altered infectivity upon secondary infection,
since we can relax the condition of rather high infectivity on secondary
infection previously required for deterministic chaos. For dengue the finding
of wide ranges of chaotic attractors open new ways to analysis of existing data
sets.
"
q-bio,Converting genetic network oscillations into somite spatial pattern,"  In most vertebrate species, the body axis is generated by the formation of
repeated transient structures called somites. This spatial periodicity in
somitogenesis has been related to the temporally sustained oscillations in
certain mRNAs and their associated gene products in the cells forming the
presomatic mesoderm. The mechanism underlying these oscillations have been
identified as due to the delays involved in the synthesis of mRNA and
translation into protein molecules [J. Lewis, Current Biol. {\bf 13}, 1398
(2003)]. In addition, in the zebrafish embryo intercellular Notch signalling
couples these oscillators and a longitudinal positional information signal in
the form of an Fgf8 gradient exists that could be used to transform these
coupled temporal oscillations into the observed spatial periodicity of somites.
Here we consider a simple model based on this known biology and study its
consequences for somitogenesis. Comparison is made with the known properties of
somite formation in the zebrafish embryo . We also study the effects of
localized Fgf8 perturbations on somite patterning.
"
q-bio,Primordial Evolution in the Finitary Process Soup,"  A general and basic model of primordial evolution--a soup of reacting
finitary and discrete processes--is employed to identify and analyze
fundamental mechanisms that generate and maintain complex structures in
prebiotic systems. The processes--$\epsilon$-machines as defined in
computational mechanics--and their interaction networks both provide well
defined notions of structure. This enables us to quantitatively demonstrate
hierarchical self-organization in the soup in terms of complexity. We found
that replicating processes evolve the strategy of successively building higher
levels of organization by autocatalysis. Moreover, this is facilitated by local
components that have low structural complexity, but high generality. In effect,
the finitary process soup spontaneously evolves a selection pressure that
favors such components. In light of the finitary process soup's generality,
these results suggest a fundamental law of hierarchical systems: global
complexity requires local simplicity.
"
q-bio,Coupling of transverse and longitudinal response in stiff polymers,"  The time-dependent transverse response of stiff polymers, represented as
weakly-bending wormlike chains (WLCs), is well-understood on the linear level,
where transverse degrees of freedom evolve independently from the longitudinal
ones. We show that, beyond a characteristic time scale, the nonlinear coupling
of transverse and longitudinal motion in an inextensible WLC significantly
weakens the polymer response compared to the widely used linear response
predictions. The corresponding feedback mechanism is rationalized by scaling
arguments and quantified by a multiple scale approach that exploits an inherent
separation of transverse and longitudinal correlation length scales. Crossover
scaling laws and exact analytical and numerical solutions for characteristic
response quantities are derived for different experimentally relevant setups.
Our findings are applicable to cytoskeletal filaments as well as DNA under
tension.
"
q-bio,"Linked by Loops: Network Structure and Switch Integration in Complex
  Dynamical Systems","  Simple nonlinear dynamical systems with multiple stable stationary states are
often taken as models for switchlike biological systems. This paper considers
the interaction of multiple such simple multistable systems when they are
embedded together into a larger dynamical ""supersystem."" Attention is focused
on the network structure of the resulting set of coupled differential
equations, and the consequences of this structure on the propensity of the
embedded switches to act independently versus cooperatively. Specifically, it
is argued that both larger average and larger variance of the node degree
distribution lead to increased switch independence. Given the frequency of
empirical observations of high variance degree distributions (e.g., power-law)
in biological networks, it is suggested that the results presented here may aid
in identifying switch-integrating subnetworks as comparatively homogenous,
low-degree, substructures. Potential applications to ecological problems such
as the relationship of stability and complexity are also briefly discussed.
"
q-bio,"Pr\'evention des escarres chez les parapl\'egiques : une nouvelle
  approche par \'electrostimulation linguale","  Pressure ulcers are recognized as a major health issue in individuals with
spinal cord injuries and new approaches to prevent this pathology are
necessary. An innovative health strategy is being developed through the use of
computer and sensory substitution via the tongue in order to compensate for the
sensory loss in the buttock area for individuals with paraplegia. This sensory
compensation will enable individuals with spinal cord injuries to be aware of a
localized excess of pressure at the skin/seat interface and, consequently, will
enable them to prevent the formation of pressure ulcers by relieving the
cutaneous area of suffering. This work reports an initial evaluation of this
approach and the feasibility of creating an adapted behavior, with a change in
pressure as a response to electro-stimulated information on the tongue.
Obtained during a clinical study in 10 healthy seated subjects, the first
results are encouraging, with 92% success in 100 performed tests. These
results, which have to be completed and validated in the paraplegic population,
may lead to a new approach to education in health to prevent the formation of
pressure ulcers within this population. Keywords: Spinal Cord Injuries,
Pressure Ulcer, Sensory Substitution, Health Education, Biomedical Informatics.
"
q-bio,Delay estimation in a two-node acyclic network,"  Linear measures such as cross-correlation have been used successfully to
determine time delays from the given processes. Such an analysis often precedes
identifying possible causal relationships between the observed processes. The
present study investigates the impact of a positively correlated driver whose
correlation function decreases monotonically with lag on the delay estimation
in a two-node acyclic network with one and two-delays. It is shown that
cross-correlation analysis of the given processes can result in spurious
identification of multiple delays between the driver and the dependent
processes. Subsequently, delay estimation of increment process as opposed to
the original process under certain implicit constraints is explored.
Short-range and long-range correlated driver processes along with those of
their coarse-grained counterparts are considered.
"
q-bio,Non-coding DNA programs express adaptation and its universal law,"  Significant fraction (98.5% in humans) of most animal genomes is non- coding
dark matter. Its largely unknown function (1-5) is related to programming
(rather than to spontaneous mutations) of accurate adaptation to rapidly
changing environment. Programmed adaptation to the same universal law for
non-competing animals from anaerobic yeast to human is revealed in the study of
their extensively quantified mortality (6-21). Adaptation of animals with
removed non-coding DNA fractions may specify their contribution to genomic
programming. Emergence of new adaptation programs and their (non-Mendelian)
heredity may be studied in antibiotic mini-extinctions (22-24). On a large
evolutionary scale rapid universal adaptation was vital for survival, and
evolved, in otherwise lethal for diverse species major mass extinctions
(25-28). Evolutionary and experimental data corroborate these conclusions
(6-21, 29-32). Universal law implies certain biological universality of diverse
species, thus quantifies applicability of animal models to humans). Genomic
adaptation programming calls for unusual approach to its study and implies
unanticipated perspectives, in particular, directed biological changes.
"
q-bio,Extracting falsifiable predictions from sloppy models,"  Successful predictions are among the most compelling validations of any
model. Extracting falsifiable predictions from nonlinear multiparameter models
is complicated by the fact that such models are commonly sloppy, possessing
sensitivities to different parameter combinations that range over many decades.
Here we discuss how sloppiness affects the sorts of data that best constrain
model predictions, makes linear uncertainty approximations dangerous, and
introduces computational difficulties in Monte-Carlo uncertainty analysis. We
also present a useful test problem and suggest refinements to the standards by
which models are communicated.
"
q-bio,"Predicting the connectivity of primate cortical networks from
  topological and spatial node properties","  The organization of the connectivity between mammalian cortical areas has
become a major subject of study, because of its important role in scaffolding
the macroscopic aspects of animal behavior and intelligence. In this study we
present a computational reconstruction approach to the problem of network
organization, by considering the topological and spatial features of each area
in the primate cerebral cortex as subsidy for the reconstruction of the global
cortical network connectivity. Starting with all areas being disconnected,
pairs of areas with similar sets of features are linked together, in an attempt
to recover the original network structure. Inferring primate cortical
connectivity from the properties of the nodes, remarkably good reconstructions
of the global network organization could be obtained, with the topological
features allowing slightly superior accuracy to the spatial ones. Analogous
reconstruction attempts for the C. elegans neuronal network resulted in
substantially poorer recovery, indicating that cortical area interconnections
are relatively stronger related to the considered topological and spatial
properties than neuronal projections in the nematode. The close relationship
between area-based features and global connectivity may hint on developmental
rules and constraints for cortical networks. Particularly, differences between
the predictions from topological and spatial properties, together with the
poorer recovery resulting from spatial properties, indicate that the
organization of cortical networks is not entirely determined by spatial
constraints.
"
q-bio,A balanced memory network,"  A fundamental problem in neuroscience is understanding how working memory --
the ability to store information at intermediate timescales, like 10s of
seconds -- is implemented in realistic neuronal networks. The most likely
candidate mechanism is the attractor network, and a great deal of effort has
gone toward investigating it theoretically. Yet, despite almost a quarter
century of intense work, attractor networks are not fully understood. In
particular, there are still two unanswered questions. First, how is it that
attractor networks exhibit irregular firing, as is observed experimentally
during working memory tasks? And second, how many memories can be stored under
biologically realistic conditions? Here we answer both questions by studying an
attractor neural network in which inhibition and excitation balance each other.
Using mean field analysis, we derive a three-variable description of attractor
networks. From this description it follows that irregular firing can exist only
if the number of neurons involved in a memory is large. The same mean field
analysis also shows that the number of memories that can be stored in a network
scales with the number of excitatory connections, a result that has been
suggested for simple models but never shown for realistic ones. Both of these
predictions are verified using simulations with large networks of spiking
neurons.
"
q-bio,"An Adaptive Strategy for the Classification of G-Protein Coupled
  Receptors","  One of the major problems in computational biology is the inability of
existing classification models to incorporate expanding and new domain
knowledge. This problem of static classification models is addressed in this
paper by the introduction of incremental learning for problems in
bioinformatics. Many machine learning tools have been applied to this problem
using static machine learning structures such as neural networks or support
vector machines that are unable to accommodate new information into their
existing models. We utilize the fuzzy ARTMAP as an alternate machine learning
system that has the ability of incrementally learning new data as it becomes
available. The fuzzy ARTMAP is found to be comparable to many of the widespread
machine learning systems. The use of an evolutionary strategy in the selection
and combination of individual classifiers into an ensemble system, coupled with
the incremental learning ability of the fuzzy ARTMAP is proven to be suitable
as a pattern classifier. The algorithm presented is tested using data from the
G-Coupled Protein Receptors Database and shows good accuracy of 83%. The system
presented is also generally applicable, and can be used in problems in genomics
and proteomics.
"
q-bio,"Erwin Schroedinger, Francis Crick and epigenetic stability","  Schroedinger's book 'What is Life?' is widely credited for having played a
crucial role in development of molecular and cellular biology. My essay
revisits the issues raised by this book from the modern perspective of
epigenetics and systems biology. I contrast two classes of potential mechanisms
of epigenetic stability: 'epigenetic templating' and 'systems biology'
approaches, and consider them from the point of view expressed by Schroedinger.
I also discuss how quantum entanglement, a nonclassical feature of quantum
mechanics, can help to address the 'problem of small numbers' that lead
Schroedinger to promote the idea of molecular code-script for explanation of
stability of biological order.
"
q-bio,The Worm-Like Chain Theory And Bending Of Short DNA,"  The probability distributions for bending angles in double helical DNA
obtained in all-atom molecular dynamics simulations are compared with
theoretical predictions. The computed distributions remarkably agree with the
worm-like chain theory for double helices of one helical turn and longer, and
qualitatively differ from predictions of the semi-elastic chain model. The
computed data exhibit only small anomalies in the apparent flexibility of short
DNA and cannot account for the recently reported AFM data (Wiggins et al,
Nature nanotechnology 1, 137 (2006)). It is possible that the current atomistic
DNA models miss some essential mechanisms of DNA bending on intermediate length
scales. Analysis of bent DNA structures reveals, however, that the bending
motion is structurally heterogeneous and directionally anisotropic on the
intermediate length scales where the experimental anomalies were detected.
These effects are essential for interpretation of the experimental data and
they also can be responsible for the apparent discrepancy.
"
q-bio,"Effect of beta-Dystroglycan Processing on Utrophin / DP116 Anchorage in
  Normal and MDX Mouse Schwann Cell Membrane","  In the peripheral nervous system, utrophin and the short dystrophin isoform
(Dp116) are co-localized at the outermost layer of the myelin sheath of nerve
fibers; together with the dystroglycan complex. In peripheral nerve, matrix
metalloproteinase (MMP) creates a 30 kDa fragment of beta-dystroglycan, leading
to a disruption of the link between the extracellular matrix and the cell
membrane. Here we asked if the processing of the beta-dystroglycan could
influence the anchorage of Dp116 or/and utrophin in normal and mdx Schwann cell
membrane. We showed that MMP-9 was more activated in mdx nerve than in
wild-type one. This activation leads to an accumulation of the 30 kDa
beta-dystroglycan isoform and have an impact on the anchorage of Dp116 and
utrophin isoforms in mdx Schwann cells membrane. Our results showed that Dp116
had greater affinity to the full length form of beta-dystroglycan than the 30
kDa form. Moreover, we showed for the first time that the short isoform of
utrophin (Up71) was over-expressed in mdx Schwann cells compared to wild-type.
In addition, this utrophin isoform (Up71) seems to have greater affinity to the
30 kDa beta-dystroglycan which could explain a more stabilization of this 30
kDa at the membrane compartment. Our results highlight the potential
participation of the short utrophin isoform and the cleaved form of
beta-dystroglycan in mdx Schwann cell membrane architecture.
"
q-bio,Python Unleashed on Systems Biology,"  We have built an open-source software system for the modeling of biomolecular
reaction networks, SloppyCell, which is written in Python and makes substantial
use of third-party libraries for numerics, visualization, and parallel
programming. We highlight here some of the powerful features that Python
provides that enable SloppyCell to do dynamic code synthesis, symbolic
manipulation, and parallel exploration of complex parameter spaces.
"
q-bio,A generic mechanism for adaptive growth rate regulation,"  How can a microorganism adapt to a variety of environmental conditions
despite there exists a limited number of signal transduction machineries? We
show that for any growing cells whose gene expression is under stochastic
fluctuations, adaptive cellular state is inevitably selected by noise, even
without specific signal transduction network for it. In general, changes in
protein concentration in a cell are given by its synthesis minus dilution and
degradation, both of which are proportional to the rate of cell growth. In an
adaptive state with a higher growth speed, both terms are large and balanced.
Under the presence of noise in gene expression, the adaptive state is less
affected by stochasticity since both the synthesis and dilution terms are
large, while for a non-adaptive state both the terms are smaller so that cells
are easily kicked out of the original state by noise. Hence, escape time from a
cellular state and the cellular growth rate are negatively correlated. This
leads to a selection of adaptive states with higher growth rates, and model
simulations confirm this selection to take place in general. The results
suggest a general form of adaptation that has never been brought to light - a
process that requires no specific machineries for sensory adaptation. The
present scheme may help explain a wide range of cellular adaptive responses
including the metabolic flux optimization for maximal cell growth.
"
q-bio,"Artificial Tongue-Placed Tactile Biofeedback for perceptual
  supplementation: application to human disability and biomedical engineering","  The present paper aims at introducing the innovative technologies, based on
the concept of ""sensory substitution"" or ""perceptual supplementation"", we are
developing in the fields of human disability and biomedical engineering.
Precisely, our goal is to design, develop and validate practical assistive
biomedical and/technical devices and/or rehabilitating procedures for persons
with disabilities, using artificial tongue-placed tactile biofeedback systems.
Proposed applications are dealing with: (1) pressure sores prevention in case
of spinal cord injuries (persons with paraplegia, or tetraplegia); (2) ankle
proprioceptive acuity improvement for driving assistance in older and/or
disabled adults; and (3) balance control improvement to prevent fall in older
and/or disabled adults. This paper presents results of three feasibility
studies performed on young healthy adults.
"
q-bio,Multiple pattern matching: A Markov chain approach,"  RNA motifs typically consist of short, modular patterns that include base
pairs formed within and between modules. Estimating the abundance of these
patterns is of fundamental importance for assessing the statistical
significance of matches in genomewide searches, and for predicting whether a
given function has evolved many times in different species or arose from a
single common ancestor. In this manuscript, we review in an integrated and
self-contained manner some basic concepts of automata theory, generating
functions and transfer matrix methods that are relevant to pattern analysis in
biological sequences. We formalize, in a general framework, the concept of
Markov chain embedding to analyze patterns in random strings produced by a
memoryless source. This conceptualization, together with the capability of
automata to recognize complicated patterns, allows a systematic analysis of
problems related to the occurrence and frequency of patterns in random strings.
The applications we present focus on the concept of synchronization of
automata, as well as automata used to search for a finite number of keywords
(including sets of patterns generated according to base pairing rules) in a
general text.
"
q-bio,Network Structure of Protein Folding Pathways,"  The classical approach to protein folding inspired by statistical mechanics
avoids the high dimensional structure of the conformation space by using
effective coordinates. Here we introduce a network approach to capture the
statistical properties of the structure of conformation spaces. Conformations
are represented as nodes of the network, while links are transitions via
elementary rotations around a chemical bond. Self-avoidance of a polypeptide
chain introduces degree correlations in the conformation network, which in turn
lead to energy landscape correlations. Folding can be interpreted as a biased
random walk on the conformation network. We show that the folding pathways
along energy gradients organize themselves into scale free networks, thus
explaining previous observations made via molecular dynamics simulations. We
also show that these energy landscape correlations are essential for recovering
the observed connectivity exponent, which belongs to a different universality
class than that of random energy models. In addition, we predict that the
exponent and therefore the structure of the folding network fundamentally
changes at high temperatures, as verified by our simulations on the AK peptide.
"
q-bio,"Self assembly of a model multicellular organism resembling the
  Dictyostelium slime molds","  The evolution of multicellular organisms from monocellular ancestors
represents one of the greatest advances of the history of life. The assembly of
such multicellular organisms requires signalling and response between cells:
over millions of years these signalling processes have become extremely
sophisticated and refined by evolution, such that study of modern organisms may
not be able to shed much light on the original ancient processes . Here we are
interested in determining how simple a signalling method can be, while still
achieving self-assembly. In 2D a coupled cellular automaton/differential
equation approach models organisms and chemotaxic chemicals, producing
spiralling aggregation. In 3D Lennard-Jones-like particles are used to
represent single cells, and their evolution in response to signalling is
followed by molecular dynamics. It is found that if a single cell is able to
emit a signal which induces others to move towards it, then a colony of
single-cell organisms can assemble into shapes as complex as a tower, a ball
atop a stalk, or a fast-moving slug. The similarity with the behaviour of
modern Dictyostelium slime molds signalling with cyclic adenosine monophosphate
(cAMP) is striking.
"
q-bio,Neutral genetic drift can aid functional protein evolution,"  BACKGROUND: Many of the mutations accumulated by naturally evolving proteins
are neutral in the sense that they do not significantly alter a protein's
ability to perform its primary biological function. However, new protein
functions evolve when selection begins to favor other, ""promiscuous"" functions
that are incidental to a protein's biological role. If mutations that are
neutral with respect to a protein's primary biological function cause
substantial changes in promiscuous functions, these mutations could enable
future functional evolution.
  RESULTS: Here we investigate this possibility experimentally by examining how
cytochrome P450 enzymes that have evolved neutrally with respect to activity on
a single substrate have changed in their abilities to catalyze reactions on
five other substrates. We find that the enzymes have sometimes changed as much
as four-fold in the promiscuous activities. The changes in promiscuous
activities tend to increase with the number of mutations, and can be largely
rationalized in terms of the chemical structures of the substrates. The
activities on chemically similar substrates tend to change in a coordinated
fashion, potentially providing a route for systematically predicting the change
in one function based on the measurement of several others.
  CONCLUSIONS: Our work suggests that initially neutral genetic drift can lead
to substantial changes in protein functions that are not currently under
selection, in effect poising the proteins to more readily undergo functional
evolution should selection ""ask new questions"" in the future.
"
q-bio,Neural networks with transient state dynamics,"  We investigate dynamical systems characterized by a time series of distinct
semi-stable activity patterns, as they are observed in cortical neural activity
patterns. We propose and discuss a general mechanism allowing for an adiabatic
continuation between attractor networks and a specific adjoined transient-state
network, which is strictly dissipative. Dynamical systems with transient states
retain functionality when their working point is autoregulated; avoiding
prolonged periods of stasis or drifting into a regime of rapid fluctuations. We
show, within a continuous-time neural network model, that a single local
updating rule for online learning allows simultaneously (i) for information
storage via unsupervised Hebbian-type learning, (ii) for adaptive regulation of
the working point and (iii) for the suppression of runaway synaptic growth.
Simulation results are presented; the spontaneous breaking of time-reversal
symmetry and link symmetry are discussed.
"
q-bio,"Immunohistochemical pitfalls in the demonstration of insulin-degrading
  enzyme in normal and neoplastic human tissues","  Previously, we have identified the cytoplasmic zinc metalloprotease
insulin-degrading enzyme(IDE) in human tissues by an immunohistochemical method
involving no antigen retrieval (AR) by pressure cooking to avoid artifacts by
endogenous biotin exposure and a detection kit based on the labeled
streptavidin biotin (LSAB) method. Thereby, we also employed 3% hydrogen
peroxide(H2O2) for the inhibition of endogenous peroxidase activity and
incubated the tissue sections with the biotinylated secondary antibody at room
temperature (RT). We now add the immunohistochemical details that had led us to
this optimized procedure as they also bear a more general relevance when
demonstrating intracellular tissue antigens. Our most important result is that
endogenous peroxidase inhibition by 0.3% H2O2 coincided with an apparently
positive IDE staining in an investigated breast cancer specimen whereas
combining a block by 3% H2O2 with an incubation of the biotinylated secondary
antibody at RT, yet not at 37 degrees Celsius, revealed this specimen as almost
entirely IDE-negative. Our present data caution against three different
immunohistochemical pitfalls that might cause falsely positive results and
artifacts when using an LSAB- and peroxidase-based detection method: pressure
cooking for AR, insufficient quenching of endogenous peroxidases and heating of
tissue sections while incubating with biotinylated secondary antibodies.
"
q-bio,Information flow and optimization in transcriptional control,"  In the simplest view of transcriptional regulation, the expression of a gene
is turned on or off by changes in the concentration of a transcription factor
(TF). We use recent data on noise levels in gene expression to show that it
should be possible to transmit much more than just one regulatory bit.
Realizing this optimal information capacity would require that the dynamic
range of TF concentrations used by the cell, the input/output relation of the
regulatory module, and the noise levels of binding and transcription satisfy
certain matching relations. This parameter-free prediction is in good agreement
with recent experiments on the Bicoid/Hunchback system in the early Drosophila
embryo, and this system achieves ~90% of its theoretical maximum information
transmission.
"
q-bio,Validating module network learning algorithms using simulated data,"  In recent years, several authors have used probabilistic graphical models to
learn expression modules and their regulatory programs from gene expression
data. Here, we demonstrate the use of the synthetic data generator SynTReN for
the purpose of testing and comparing module network learning algorithms. We
introduce a software package for learning module networks, called LeMoNe, which
incorporates a novel strategy for learning regulatory programs. Novelties
include the use of a bottom-up Bayesian hierarchical clustering to construct
the regulatory programs, and the use of a conditional entropy measure to assign
regulators to the regulation program nodes. Using SynTReN data, we test the
performance of LeMoNe in a completely controlled situation and assess the
effect of the methodological changes we made with respect to an existing
software package, namely Genomica. Additionally, we assess the effect of
various parameters, such as the size of the data set and the amount of noise,
on the inference performance. Overall, application of Genomica and LeMoNe to
simulated data sets gave comparable results. However, LeMoNe offers some
advantages, one of them being that the learning process is considerably faster
for larger data sets. Additionally, we show that the location of the regulators
in the LeMoNe regulation programs and their conditional entropy may be used to
prioritize regulators for functional validation, and that the combination of
the bottom-up clustering strategy with the conditional entropy-based assignment
of regulators improves the handling of missing or hidden regulators.
"
q-bio,"Effects of the DNA state fluctuation on single-cell dynamics of
  self-regulating gene","  A dynamical mean-field theory is developed to analyze stochastic single-cell
dynamics of gene expression. By explicitly taking account of nonequilibrium and
nonadiabatic features of the DNA state fluctuation, two-time correlation
functions and response functions of single-cell dynamics are derived. The
method is applied to a self-regulating gene to predict a rich variety of
dynamical phenomena such as anomalous increase of relaxation time and
oscillatory decay of correlations. Effective ""temperature"" defined as the ratio
of the correlation to the response in the protein number is small when the DNA
state change is frequent, while it grows large when the DNA state change is
infrequent, indicating the strong enhancement of noise in the latter case.
"
q-bio,"Cell adhesion and cortex contractility determine cell patterning in the
  Drosophila retina","  Hayashi and Carthew (Nature 431 [2004], 647) have shown that the packing of
cone cells in the Drosophila retina resembles soap bubble packing, and that
changing E- and N-cadherin expression can change this packing, as well as cell
shape.
  The analogy with bubbles suggests that cell packing is driven by surface
minimization. We find that this assumption is insufficient to model the
experimentally observed shapes and packing of the cells based on their cadherin
expression. We then consider a model in which adhesion leads to a surface
increase, balanced by cell cortex contraction. Using the experimentally
observed distributions of E- and N-cadherin, we simulate the packing and cell
shapes in the wildtype eye. Furthermore, by changing only the corresponding
parameters, this model can describe the mutants with different numbers of
cells, or changes in cadherin expression.
"
q-bio,Dimensionality and dynamics in the behavior of C. elegans,"  A major challenge in analyzing animal behavior is to discover some underlying
simplicity in complex motor actions. Here we show that the space of shapes
adopted by the nematode C. elegans is surprisingly low dimensional, with just
four dimensions accounting for 95% of the shape variance, and we partially
reconstruct ""equations of motion"" for the dynamics in this space. These
dynamics have multiple attractors, and we find that the worm visits these in a
rapid and almost completely deterministic response to weak thermal stimuli.
Stimulus-dependent correlations among the different modes suggest that one can
generate more reliable behaviors by synchronizing stimuli to the state of the
worm in shape space. We confirm this prediction, effectively ""steering"" the
worm in real time.
"
q-bio,Anisotropic probabilistic cellular automaton for a predator-prey system,"  We consider a probabilistic cellular automaton to analyze the stochastic
dynamics of a predator-prey system. The local rules are Markovian and are based
in the Lotka-Volterra model. The individuals of each species reside on the
sites of a lattice and interact with an unsymmetrical neighborhood. We look for
the effect of the space anisotropy in the characterization of the oscillations
of the species population densities. Our study of the probabilistic cellular
automaton is based on simple and pair mean-field approximations and explicitly
takes into account spatial anisotropy.
"
q-bio,Microscale swimming: The molecular dynamics approach,"  The self-propelled motion of microscopic bodies immersed in a fluid medium is
studied using molecular dynamics simulation. The advantage of the atomistic
approach is that the detailed level of description allows complete freedom in
specifying the swimmer design and its coupling with the surrounding fluid. A
series of two-dimensional swimming bodies employing a variety of propulsion
mechanisms -- motivated by biological and microrobotic designs -- is
investigated, including the use of moving limbs, changing body shapes and fluid
jets. The swimming efficiency and the nature of the induced, time-dependent
flow fields are found to differ widely among body designs and propulsion
mechanisms.
"
q-bio,Asymptotic velocity of one dimensional diffusions with periodic drift,"  We consider the asymptotic behaviour of the solution of one dimensional
stochastic differential equations and Langevin equations in periodic
backgrounds with zero average. We prove that in several such models, there is
generically a non vanishing asymptotic velocity, despite of the fact that the
average of the background is zero.
"
q-bio,Chromatin Folding in Relation to Human Genome Function,"  Three-dimensional (3D) chromatin structure is closely related to genome
function, in particular transcription. However, the folding path of the
chromatin fiber in the interphase nucleus is unknown. Here, we systematically
measured the 3D physical distance between pairwise labeled genomic positions in
gene-dense, highly transcribed domains and gene-poor less active areas on
chromosomes 1 and 11 in G1 nuclei of human primary fibroblasts, using
fluorescence in situ hybridization. Interpretation of our results and those
published by others, based on polymer physics, shows that the folding of the
chromatin fiber can be described as a polymer in a globular state (GS),
maintained by intra-polymer attractive interactions that counteract
self-avoidance forces. The GS polymer model is able to describe chromatin
folding in as well the highly expressed domains as the lowly expressed ones,
indicating that they differ in Kuhn length and chromatin compaction. Each type
of genomic domain constitutes an ensemble of relatively compact globular
folding states, resulting in a considerable cellto- cell variation between
otherwise identical cells. We present evidence for different polymer folding
regimes of the chromatin fiber on the length scale of a few mega base pairs and
on that of complete chromosome arms (several tens of Mb). Our results present a
novel view on the folding of the chromatin fiber in interphase and open the
possibility to explore the nature of the intra-chromatin fiber interactions.
"
q-bio,Nontrivial quantum effects in biology: A skeptical physicists' view,"  Invited contribution to ""Quantum Aspects of Life"", D. Abbott Ed. (World
Scientific, Singapore, 2007).
"
q-bio,Detection of Aneuploidy with Digital PCR,"  The widespread use of genetic testing in high risk pregnancies has created
strong interest in rapid and accurate molecular diagnostics for common
chromosomal aneuploidies. We show here that digital polymerase chain reaction
(dPCR) can be used for accurate measurement of trisomy 21 (Down's Syndrome),
the most common human aneuploidy. dPCR is generally applicable to any
aneuploidy, does not depend on allelic distribution or gender, and is able to
detect signals in the presence of mosaics or contaminating maternal DNA.
"
q-bio,"A three-state prediction of single point mutations on protein stability
  changes","  A basic question of protein structural studies is to which extent mutations
affect the stability. This question may be addressed starting from sequence
and/or from structure. In proteomics and genomics studies prediction of protein
stability free energy change (DDG) upon single point mutation may also help the
annotation process. The experimental SSG values are affected by uncertainty as
measured by standard deviations. Most of the DDG values are nearly zero (about
32% of the DDG data set ranges from -0.5 to 0.5 Kcal/mol) and both the value
and sign of DDG may be either positive or negative for the same mutation
blurring the relationship among mutations and expected DDG value. In order to
overcome this problem we describe a new predictor that discriminates between 3
mutation classes: destabilizing mutations (DDG<-0.5 Kcal/mol), stabilizing
mutations (DDG>0.5 Kcal/mol) and neutral mutations (-0.5<=DDG<=0.5 Kcal/mol).
In this paper a support vector machine starting from the protein sequence or
structure discriminates between stabilizing, destabilizing and neutral
mutations. We rank all the possible substitutions according to a three state
classification system and show that the overall accuracy of our predictor is as
high as 52% when performed starting from sequence information and 58% when the
protein structure is available, with a mean value correlation coefficient of
0.30 and 0.39, respectively. These values are about 20 points per cent higher
than those of a random predictor.
"
q-bio,"Theoretical Analysis of Subthreshold Oscillatory Behaviors in Nonlinear
  Autonomous Systems","  We have developed a linearization method to investigate the subthreshold
oscillatory behaviors in nonlinear autonomous systems. By considering firstly
the neuronal system as an example, we show that this theoretical approach can
predict quantitatively the subthreshold oscillatory activities, including the
damping coefficients and the oscillatory frequencies which are in good
agreement with those observed in experiments. Then we generalize the
linearization method to an arbitrary autonomous nonlinear system. The detailed
extension of this theoretical approach is also presented and further discussed.
"
q-bio,Risk perception in epidemic modeling,"  We investigate the effects of risk perception in a simple model of epidemic
spreading. We assume that the perception of the risk of being infected depends
on the fraction of neighbors that are ill. The effect of this factor is to
decrease the infectivity, that therefore becomes a dynamical component of the
model. We study the problem in the mean-field approximation and by numerical
simulations for regular, random and scale-free networks.
  We show that for homogeneous and random networks, there is always a value of
perception that stops the epidemics. In the ``worst-case'' scenario of a
scale-free network with diverging input connectivity, a linear perception
cannot stop the epidemics; however we show that a non-linear increase of the
perception risk may lead to the extinction of the disease. This transition is
discontinuous, and is not predicted by the mean-field analysis.
"
q-bio,Introduction to protein folding for physicists,"  The prediction of the three-dimensional native structure of proteins from the
knowledge of their amino acid sequence, known as the protein folding problem,
is one of the most important yet unsolved issues of modern science. Since the
conformational behaviour of flexible molecules is nothing more than a complex
physical problem, increasingly more physicists are moving into the study of
protein systems, bringing with them powerful mathematical and computational
tools, as well as the sharp intuition and deep images inherent to the physics
discipline. This work attempts to facilitate the first steps of such a
transition. In order to achieve this goal, we provide an exhaustive account of
the reasons underlying the protein folding problem enormous relevance and
summarize the present-day status of the methods aimed to solving it. We also
provide an introduction to the particular structure of these biological
heteropolymers, and we physically define the problem stating the assumptions
behind this (commonly implicit) definition. Finally, we review the 'special
flavor' of statistical mechanics that is typically used to study the
astronomically large phase spaces of macromolecules. Throughout the whole work,
much material that is found scattered in the literature has been put together
here to improve comprehension and to serve as a handy reference.
"
q-bio,Evolving inductive generalization via genetic self-assembly,"  We propose that genetic encoding of self-assembling components greatly
enhances the evolution of complex systems and provides an efficient platform
for inductive generalization, i.e. the inductive derivation of a solution to a
problem with a potentially infinite number of instances from a limited set of
test examples. We exemplify this in simulations by evolving scalable circuitry
for several problems. One of them, digital multiplication, has been intensively
studied in recent years, where hitherto the evolutionary design of only
specific small multipliers was achieved. The fact that this and other problems
can be solved in full generality employing self-assembly sheds light on the
evolutionary role of self-assembly in biology and is of relevance for the
design of complex systems in nano- and bionanotechnology.
"
q-bio,"Multipolar Reactive DPD: A Novel Tool for Spatially Resolved Systems
  Biology","  This article reports about a novel extension of dissipative particle dynamics
(DPD) that allows the study of the collective dynamics of complex chemical and
structural systems in a spatially resolved manner with a combinatorially
complex variety of different system constituents. We show that introducing
multipolar interactions between particles leads to extended membrane structures
emerging in a self-organized manner and exhibiting both the necessary
mechanical stability for transport and fluidity so as to provide a
two-dimensional self-organizing dynamic reaction environment for kinetic
studies in the context of cell biology. We further show that the emergent
dynamics of extended membrane bound objects is in accordance with scaling laws
imposed by physics.
"
q-bio,"Visual Data Mining of Genomic Databases by Immersive Graph-Based
  Exploration","  Biologists are leading current research on genome characterization
(sequencing, alignment, transcription), providing a huge quantity of raw data
about many genome organisms. Extracting knowledge from this raw data is an
important process for biologists, using usually data mining approaches.
However, it is difficult to deals with these genomic information using actual
bioinformatics data mining tools, because data are heterogeneous, huge in
quantity and geographically distributed. In this paper, we present a new
approach between data mining and virtual reality visualization, called visual
data mining. Indeed Virtual Reality becomes ripe, with efficient display
devices and intuitive interaction in an immersive context. Moreover, biologists
use to work with 3D representation of their molecules, but in a desktop
context. We present a software solution, Genome3DExplorer, which addresses the
problem of genomic data visualization, of scene management and interaction.
This solution is based on a well-adapted graphical and interaction paradigm,
where local and global topological characteristics of data are easily visible,
on the contrary to traditional genomic database browsers, always focused on the
zoom and details level.
"
q-bio,Statistical mechanics and stability of a model eco-system,"  We study a model ecosystem by means of dynamical techniques from disordered
systems theory. The model describes a set of species subject to competitive
interactions through a background of resources, which they feed upon.
Additionally direct competitive or co-operative interaction between species may
occur through a random coupling matrix. We compute the order parameters of the
system in a fixed point regime, and identify the onset of instability and
compute the phase diagram. We focus on the effects of variability of resources,
direct interaction between species, co-operation pressure and dilution on the
stability and the diversity of the ecosystem. It is shown that resources can be
exploited optimally only in absence of co-operation pressure or direct
interaction between species.
"
q-bio,"Controlling posture using a plantar pressure-based, tongue-placed
  tactile biofeedback system","  The present paper introduces an original biofeedback system for improving
human balance control, whose underlying principle consists in providing
additional sensory information related to foot sole pressure distribution to
the user through a tongue-placed tactile output device. To assess the effect of
this biofeedback system on postural control during quiet standing, ten young
healthy adults were asked to stand as immobile as possible with their eyes
closed in two conditions of No-biofeedback and Biofeedback. Centre of foot
pressure (CoP) displacements were recorded using a force platform. Results
showed reduced CoP displacements in the Biofeedback relative to the
No-biofeedback condition. The present findings evidenced the ability of the
central nervous system to efficiently integrate an artificial plantar-based,
tongue-placed tactile biofeedback for controlling control posture during quiet
standing.
"
q-bio,"Selection Against Demographic Stochasticity in Age-Structured
  Populations","  It has been shown that differences in fecundity variance can influence the
probability of invasion of a genotype in a population, i.e. a genotype with
lower variance in offspring number can be favored in finite populations even if
it has a somewhat lower mean fitness than a competitor. In this paper,
Gillespie's results are extended to population genetic systems with explicit
age structure, where the demographic variance (variance in growth rate)
calculated in the work of Engen and colleagues is used as a generalization of
""variance in offspring number"" to predict the interaction between deterministic
and random forces driving change in allele frequency. By calculating the
variance from the life history parameters, it is shown that selection against
variance in the growth rate will favor a genotypes with lower stochasticity in
age specific survival and fertility rates. A diffusion approximation for
selection and drift in a population with two genotypes with different life
history matrices (and therefore, different growth rates and demographic
variances) is derived and shown to be consistent with individual based
simulations. It is also argued that for finite populations, perturbation
analyses of both the growth rate and demographic variances may be necessary to
determine the sensitivity of ""fitness"" (broadly defined) to changes in the life
history parameters.
"
q-bio,Shape instabilities in vesicles: a phase-field model,"  A phase field model for dealing with shape instabilities in fluid membrane
vesicles is presented. This model takes into account the Canham-Helfrich
bending energy with spontaneous curvature. A dynamic equation for the
phase-field is also derived. With this model it is possible to see the vesicle
shape deformation dynamically, when some external agent instabilizes the
membrane, for instance, inducing an inhomogeneous spontaneous curvature. The
numerical scheme used is detailed and some stationary shapes are shown together
with a shape diagram for vesicles of spherical topology and no spontaneous
curvature, in agreement with known results.
"
q-bio,SIR epidemics in dynamic contact networks,"  Contact patterns in populations fundamentally influence the spread of
infectious diseases. Current mathematical methods for epidemiological
forecasting on networks largely assume that contacts between individuals are
fixed, at least for the duration of an outbreak. In reality, contact patterns
may be quite fluid, with individuals frequently making and breaking social or
sexual relationships. Here we develop a mathematical approach to predicting
disease transmission on dynamic networks in which each individual has a
characteristic behavior (typical contact number), but the identities of their
contacts change in time. We show that dynamic contact patterns shape
epidemiological dynamics in ways that cannot be adequately captured in static
network models or mass-action models. Our new model interpolates smoothly
between static network models and mass-action models using a mixing parameter,
thereby providing a bridge between disparate classes of epidemiological models.
Using epidemiological and sexual contact data from an Atlanta high school, we
then demonstrate the utility of this method for forecasting and controlling
sexually transmitted disease outbreaks.
"
q-fin,Collective behavior of stock price movements in an emerging market,"  To investigate the universality of the structure of interactions in different
markets, we analyze the cross-correlation matrix C of stock price fluctuations
in the National Stock Exchange (NSE) of India. We find that this emerging
market exhibits strong correlations in the movement of stock prices compared to
developed markets, such as the New York Stock Exchange (NYSE). This is shown to
be due to the dominant influence of a common market mode on the stock prices.
By comparison, interactions between related stocks, e.g., those belonging to
the same business sector, are much weaker. This lack of distinct sector
identity in emerging markets is explicitly shown by reconstructing the network
of mutually interacting stocks. Spectral analysis of C for NSE reveals that,
the few largest eigenvalues deviate from the bulk of the spectrum predicted by
random matrix theory, but they are far fewer in number compared to, e.g., NYSE.
We show this to be due to the relative weakness of intra-sector interactions
between stocks, compared to the market mode, by modeling stock price dynamics
with a two-factor model. Our results suggest that the emergence of an internal
structure comprising multiple groups of strongly coupled components is a
signature of market development.
"
q-fin,"Yield Curve Shapes and the Asymptotic Short Rate Distribution in Affine
  One-Factor Models","  We consider a model for interest rates, where the short rate is given by a
time-homogenous, one-dimensional affine process in the sense of Duffie,
Filipovic and Schachermayer. We show that in such a model yield curves can only
be normal, inverse or humped (i.e. endowed with a single local maximum). Each
case can be characterized by simple conditions on the present short rate. We
give conditions under which the short rate process will converge to a limit
distribution and describe the limit distribution in terms of its cumulant
generating function. We apply our results to the Vasicek model, the CIR model,
a CIR model with added jumps and a model of Ornstein-Uhlenbeck type.
"
q-fin,Average optimality for risk-sensitive control with general state space,"  This paper deals with discrete-time Markov control processes on a general
state space. A long-run risk-sensitive average cost criterion is used as a
performance measure. The one-step cost function is nonnegative and possibly
unbounded. Using the vanishing discount factor approach, the optimality
inequality and an optimal stationary strategy for the decision maker are
established.
"
q-fin,"Approximation of the distribution of a stationary Markov process with
  application to option pricing","  We build a sequence of empirical measures on the space D(R_+,R^d) of
R^d-valued c\`adl\`ag functions on R_+ in order to approximate the law of a
stationary R^d-valued Markov and Feller process (X_t). We obtain some general
results of convergence of this sequence. Then, we apply them to Brownian
diffusions and solutions to L\'evy driven SDE's under some Lyapunov-type
stability assumptions. As a numerical application of this work, we show that
this procedure gives an efficient way of option pricing in stochastic
volatility models.
"
q-fin,Stock market return distributions: from past to present,"  We show that recent stock market fluctuations are characterized by the
cumulative distributions whose tails on short, minute time scales exhibit power
scaling with the scaling index alpha > 3 and this index tends to increase
quickly with decreasing sampling frequency. Our study is based on
high-frequency recordings of the S&P500, DAX and WIG20 indices over the
interval May 2004 - May 2006. Our findings suggest that dynamics of the
contemporary market may differ from the one observed in the past. This effect
indicates a constantly increasing efficiency of world markets.
"
q-fin,"Analysis of the real estate market in Las Vegas: Bubble, seasonal
  patterns, and prediction of the CSW indexes","  We analyze 27 house price indexes of Las Vegas from Jun. 1983 to Mar. 2005,
corresponding to 27 different zip codes. These analyses confirm the existence
of a real-estate bubble, defined as a price acceleration faster than
exponential, which is found however to be confined to a rather limited time
interval in the recent past from approximately 2003 to mid-2004 and has
progressively transformed into a more normal growth rate comparable to
pre-bubble levels in 2005. There has been no bubble till 2002 except for a
medium-sized surge in 1990. In addition, we have identified a strong yearly
periodicity which provides a good potential for fine-tuned prediction from
month to month. A monthly monitoring using a model that we have developed could
confirm, by testing the intra-year structure, if indeed the market has returned
to ``normal'' or if more turbulence is expected ahead. We predict the evolution
of the indexes one year ahead, which is validated with new data up to Sep.
2006. The present analysis demonstrates the existence of very significant
variations at the local scale, in the sense that the bubble in Las Vegas seems
to have preceded the more global USA bubble and has ended approximately two
years earlier (mid 2004 for Las Vegas compared with mid-2006 for the whole of
the USA).
"
q-fin,"Weak and Strong Taylor methods for numerical solutions of stochastic
  differential equations","  We apply results of Malliavin-Thalmaier-Watanabe for strong and weak Taylor
expansions of solutions of perturbed stochastic differential equations (SDEs).
In particular, we work out weight expressions for the Taylor coefficients of
the expansion. The results are applied to LIBOR market models in order to deal
with the typical stochastic drift and with stochastic volatility. In contrast
to other accurate methods like numerical schemes for the full SDE, we obtain
easily tractable expressions for accurate pricing. In particular, we present an
easily tractable alternative to ``freezing the drift'' in LIBOR market models,
which has an accuracy similar to the full numerical scheme. Numerical examples
underline the results.
"
q-fin,Information-Based Asset Pricing,"  A new framework for asset price dynamics is introduced in which the concept
of noisy information about future cash flows is used to derive the price
processes. In this framework an asset is defined by its cash-flow structure.
Each cash flow is modelled by a random variable that can be expressed as a
function of a collection of independent random variables called market factors.
With each such ""X-factor"" we associate a market information process, the values
of which are accessible to market agents. Each information process is a sum of
two terms; one contains true information about the value of the market factor;
the other represents ""noise"". The noise term is modelled by an independent
Brownian bridge. The market filtration is assumed to be that generated by the
aggregate of the independent information processes. The price of an asset is
given by the expectation of the discounted cash flows in the risk-neutral
measure, conditional on the information provided by the market filtration. When
the cash flows are the dividend payments associated with equities, an explicit
model is obtained for the share-price, and the prices of options on
dividend-paying assets are derived. Remarkably, the resulting formula for the
price of a European call option is of the Black-Scholes-Merton type. The
information-based framework also generates a natural explanation for the origin
of stochastic volatility.
"
q-fin,"True and Apparent Scaling: The Proximity of the Markov-Switching
  Multifractal Model to Long-Range Dependence","  In this paper, we consider daily financial data of a collection of different
stock market indices, exchange rates, and interest rates, and we analyze their
multi-scaling properties by estimating a simple specification of the
Markov-switching multifractal model (MSM). In order to see how well the
estimated models capture the temporal dependence of the data, we estimate and
compare the scaling exponents $H(q)$ (for $q = 1, 2$) for both empirical data
and simulated data of the estimated MSM models. In most cases the multifractal
model appears to generate `apparent' long memory in agreement with the
empirical scaling laws.
"
q-fin,Patterns of dominant flows in the world trade web,"  The large-scale organization of the world economies is exhibiting
increasingly levels of local heterogeneity and global interdependency.
Understanding the relation between local and global features calls for
analytical tools able to uncover the global emerging organization of the
international trade network. Here we analyze the world network of bilateral
trade imbalances and characterize its overall flux organization, unraveling
local and global high-flux pathways that define the backbone of the trade
system. We develop a general procedure capable to progressively filter out in a
consistent and quantitative way the dominant trade channels. This procedure is
completely general and can be applied to any weighted network to detect the
underlying structure of transport flows. The trade fluxes properties of the
world trade web determines a ranking of trade partnerships that highlights
global interdependencies, providing information not accessible by simple local
analysis. The present work provides new quantitative tools for a dynamical
approach to the propagation of economic crises.
"
q-fin,The Epps effect revisited,"  We analyse the dependence of stock return cross-correlations on the sampling
frequency of the data known as the Epps effect: For high resolution data the
cross-correlations are significantly smaller than their asymptotic value as
observed on daily data. The former description implies that changing trading
frequency should alter the characteristic time of the phenomenon. This is not
true for the empirical data: The Epps curves do not scale with market activity.
The latter result indicates that the time scale of the phenomenon is connected
to the reaction time of market participants (this we denote as human time
scale), independent of market activity. In this paper we give a new description
of the Epps effect through the decomposition of cross-correlations. After
testing our method on a model of generated random walk price changes we justify
our analytical results by fitting the Epps curves of real world data.
"
q-fin,"Exact retrospective Monte Carlo computation of arithmetic average Asian
  options","  Taking advantage of the recent litterature on exact simulation algorithms
(Beskos, Papaspiliopoulos and Roberts) and unbiased estimation of the
expectation of certain fonctional integrals (Wagner, Beskos et al. and
Fearnhead et al.), we apply an exact simulation based technique for pricing
continuous arithmetic average Asian options in the Black and Scholes framework.
Unlike existing Monte Carlo methods, we are no longer prone to the
discretization bias resulting from the approximation of continuous time
processes through discrete sampling. Numerical results of simulation studies
are presented and variance reduction problems are considered.
"
q-fin,Large portfolio losses: A dynamic contagion model,"  Using particle system methodologies we study the propagation of financial
distress in a network of firms facing credit risk. We investigate the
phenomenon of a credit crisis and quantify the losses that a bank may suffer in
a large credit portfolio. Applying a large deviation principle we compute the
limiting distributions of the system and determine the time evolution of the
credit quality indicators of the firms, deriving moreover the dynamics of a
global financial health indicator. We finally describe a suitable version of
the ""Central Limit Theorem"" useful to study large portfolio losses. Simulation
results are provided as well as applications to portfolio loss distribution
analysis.
"
q-fin,Financial time-series analysis: A brief overview,"  Prices of commodities or assets produce what is called time-series. Different
kinds of financial time-series have been recorded and studied for decades.
Nowadays, all transactions on a financial market are recorded, leading to a
huge amount of data available, either for free in the Internet or commercially.
Financial time-series analysis is of great interest to practitioners as well as
to theoreticians, for making inferences and predictions. Furthermore, the
stochastic uncertainties inherent in financial time-series and the theory
needed to deal with them make the subject especially interesting not only to
economists, but also to statisticians and physicists. While it would be a
formidable task to make an exhaustive review on the topic, with this review we
try to give a flavor of some of its aspects.
"
q-fin,Why only few are so successful ?,"  In many professons employees are rewarded according to their relative
performance. Corresponding economy can be modeled by taking $N$ independent
agents who gain from the market with a rate which depends on their current
gain. We argue that this simple realistic rate generates a scale free
distribution even though intrinsic ability of agents are marginally different
from each other. As an evidence we provide distribution of scores for two
different systems (a) the global stock game where players invest in real stock
market and (b) the international cricket.
"
q-fin,"Uncovering the Internal Structure of the Indian Financial Market:
  Cross-correlation behavior in the NSE","  The cross-correlations between price fluctuations of 201 frequently traded
stocks in the National Stock Exchange (NSE) of India are analyzed in this
paper. We use daily closing prices for the period 1996-2006, which coincides
with the period of rapid transformation of the market following liberalization.
The eigenvalue distribution of the cross-correlation matrix, $\mathbf{C}$, of
NSE is found to be similar to that of developed markets, such as the New York
Stock Exchange (NYSE): the majority of eigenvalues fall within the bounds
expected for a random matrix constructed from mutually uncorrelated time
series. Of the few largest eigenvalues that deviate from the bulk, the largest
is identified with market-wide movements. The intermediate eigenvalues that
occur between the largest and the bulk have been associated in NYSE with
specific business sectors with strong intra-group interactions. However, in the
Indian market, these deviating eigenvalues are comparatively very few and lie
much closer to the bulk. We propose that this is because of the relative lack
of distinct sector identity in the market, with the movement of stocks
dominantly influenced by the overall market trend. This is shown by explicit
construction of the interaction network in the market, first by generating the
minimum spanning tree from the unfiltered correlation matrix, and later, using
an improved method of generating the graph after filtering out the market mode
and random effects from the data. Both methods show, compared to developed
markets, the relative absence of clusters of co-moving stocks that belong to
the same business sector. This is consistent with the general belief that
emerging markets tend to be more correlated than developed markets.
"
q-fin,Classical and quantum randomness and the financial market,"  We analyze complexity of financial (and general economic) processes by
comparing classical and quantum-like models for randomness. Our analysis
implies that it might be that a quantum-like probabilistic description is more
natural for financial market than the classical one. A part of our analysis is
devoted to study the possibility of application of the quantum probabilistic
model to agents of financial market. We show that, although the direct quantum
(physical) reduction (based on using the scales of quantum mechanics) is
meaningless, one may apply so called quantum-like models. In our approach
quantum-like probabilistic behaviour is a consequence of contextualy of
statistical data in finances (and economics in general). However, our
hypothesis on ""quantumness"" of financial data should be tested experimentally
(as opposed to the conventional description based on the noncontextual
classical probabilistic approach). We present a new statistical test based on a
generalization of the well known in quantum physics Bell's inequality.
"
q-fin,"Scaling laws of strategic behaviour and size heterogeneity in agent
  dynamics","  The dynamics of many socioeconomic systems is determined by the decision
making process of agents. The decision process depends on agent's
characteristics, such as preferences, risk aversion, behavioral biases, etc..
In addition, in some systems the size of agents can be highly heterogeneous
leading to very different impacts of agents on the system dynamics. The large
size of some agents poses challenging problems to agents who want to control
their impact, either by forcing the system in a given direction or by hiding
their intentionality. Here we consider the financial market as a model system,
and we study empirically how agents strategically adjust the properties of
large orders in order to meet their preference and minimize their impact. We
quantify this strategic behavior by detecting scaling relations of allometric
nature between the variables characterizing the trading activity of different
institutions. We observe power law distributions in the investment time
horizon, in the number of transactions needed to execute a large order and in
the traded value exchanged by large institutions and we show that heterogeneity
of agents is a key ingredient for the emergence of some aggregate properties
characterizing this complex system.
"
q-fin,"Proving Regularity of the Minimal Probability of Ruin via a Game of
  Stopping and Control","  We reveal an interesting convex duality relationship between two problems:
(a) minimizing the probability of lifetime ruin when the rate of consumption is
stochastic and when the individual can invest in a Black-Scholes financial
market; (b) a controller-and-stopper problem, in which the controller controls
the drift and volatility of a process in order to maximize a running reward
based on that process, and the stopper chooses the time to stop the running
reward and rewards the controller a final amount at that time. Our primary goal
is to show that the minimal probability of ruin, whose stochastic
representation does not have a classical form as does the utility maximization
problem (i.e., the objective's dependence on the initial values of the state
variables is implicit), is the unique classical solution of its
Hamilton-Jacobi-Bellman (HJB) equation, which is a non-linear boundary-value
problem. We establish our goal by exploiting the convex duality relationship
between (a) and (b).
"
q-fin,Modeling the Epps effect of cross correlations in asset prices,"  We review the decomposition method of stock return cross-correlations,
presented previously for studying the dependence of the correlation coefficient
on the resolution of data (Epps effect). Through a toy model of random
walk/Brownian motion and memoryless renewal process (i.e. Poisson point
process) of observation times we show that in case of analytical treatability,
by decomposing the correlations we get the exact result for the frequency
dependence. We also demonstrate that our approach produces reasonable fitting
of the dependence of correlations on the data resolution in case of empirical
data. Our results indicate that the Epps phenomenon is a product of the finite
time decay of lagged correlations of high resolution data, which does not scale
with activity. The characteristic time is due to a human time scale, the time
needed to react to news.
"
q-fin,"Deterministic Factors of Stock Networks based on Cross-correlation in
  Financial Market","  The stock market has been known to form homogeneous stock groups with a
higher correlation among different stocks according to common economic factors
that influence individual stocks. We investigate the role of common economic
factors in the market in the formation of stock networks, using the arbitrage
pricing model reflecting essential properties of common economic factors. We
find that the degree of consistency between real and model stock networks
increases as additional common economic factors are incorporated into our
model. Furthermore, we find that individual stocks with a large number of links
to other stocks in a network are more highly correlated with common economic
factors than those with a small number of links. This suggests that common
economic factors in the stock market can be understood in terms of
deterministic factors.
"
q-fin,Mutual Fund Theorems when Minimizing the Probability of Lifetime Ruin,"  We show that the mutual fund theorems of Merton (1971) extend to the problem
of optimal investment to minimize the probability of lifetime ruin. We obtain
two such theorems by considering a financial market both with and without a
riskless asset for random consumption. The striking result is that we obtain
two-fund theorems despite the additional source of randomness from consumption.
"
q-fin,"Change point estimation for the telegraph process observed at discrete
  times","  The telegraph process models a random motion with finite velocity and it is
usually proposed as an alternative to diffusion models. The process describes
the position of a particle moving on the real line, alternatively with constant
velocity $+ v$ or $-v$. The changes of direction are governed by an homogeneous
Poisson process with rate $\lambda >0.$ In this paper, we consider a change
point estimation problem for the rate of the underlying Poisson process by
means of least squares method. The consistency and the rate of convergence for
the change point estimator are obtained and its asymptotic distribution is
derived. Applications to real data are also presented.
"
q-fin,EGT through Quantum Mechanics & from Statistical Physics to Economics,"  By analyzing the relationships between a socioeconomical system modeled
through evolutionary game theory and a physical system modeled through quantum
mechanics we show how although both systems are described through two theories
apparently different both are analogous and thus exactly equivalents. The
extensions of quantum mechanics to statistical physics and information theory
let us use some of their definitions for the best understanding of the behavior
of economics and biology. The quantum analogue of the replicator dynamics is
the von Neumann equation. A system in where all its members are in Nash
equilibrium is equivalent to a system in a maximum entropy state. Nature is a
game in where its players compete for a common welfare and the equilibrium of
the system that they are members. They act as a whole besides individuals like
they obey a rule in where they prefer to work for the welfare of the collective
besides the individual welfare.
"
q-fin,Quantitative relations between corruption and economic factors,"  We report quantitative relations between corruption level and economic
factors, such as country wealth and foreign investment per capita, which are
characterized by a power law spanning multiple scales of wealth and investments
per capita. These relations hold for diverse countries, and also remain stable
over different time periods. We also observe a negative correlation between
level of corruption and long-term economic growth. We find similar results for
two independent indices of corruption, suggesting that the relation between
corruption and wealth does not depend on the specific measure of corruption.
The functional relations we report have implications when assessing the
relative level of corruption for two countries with comparable wealth, and for
quantifying the impact of corruption on economic growth and foreign
investments.
"
q-fin,Correlated multi-asset portfolio optimisation with transaction cost,"  We employ perturbation analysis technique to study multi-asset portfolio
optimisation with transaction cost. We allow for correlations in risky assets
and obtain optimal trading methods for general utility functions. Our
analytical results are supported by numerical simulations in the context of the
Long Term Growth Model.
"
q-fin,"Financial Valuation of Mortality Risk via the Instantaneous Sharpe
  Ratio: Applications to Pricing Pure Endowments","  We develop a theory for pricing non-diversifiable mortality risk in an
incomplete market. We do this by assuming that the company issuing a
mortality-contingent claim requires compensation for this risk in the form of a
pre-specified instantaneous Sharpe ratio. We prove that our ensuing valuation
formula satisfies a number of desirable properties. For example, we show that
it is subadditive in the number of contracts sold. A key result is that if the
hazard rate is stochastic, then the risk-adjusted survival probability is
greater than the physical survival probability, even as the number of contracts
approaches infinity.
"
q-fin,"The log-normal distribution from Non-Gibrat's law in the middle scale
  region of profits","  Employing profits data of Japanese firms in 2003--2005, we kinematically
exhibit the static log-normal distribution in the middle scale region. In the
derivation, a Non-Gibrat's law under the detailed balance is adopted together
with following two approximations. Firstly, the probability density function of
profits growth rate is described as a tent-shaped exponential function.
Secondly, the value of the origin of the growth rate distribution divided into
bins is constant. The derivation is confirmed in the database consistently.
  This static procedure is applied to a quasi-static system. We dynamically
describe a quasi-static log-normal distribution in the middle scale region. In
the derivation, a Non-Gibrat's law under the detailed quasi-balance is adopted
together with two approximations confirmed in the static system. The resultant
distribution is power-law with varying Pareto index in the large scale region
and the quasi-static log-normal distribution in the middle scale region. In the
distribution, not only the change of Pareto index but also the change of the
variance of the log-normal distribution depends on the parameter of the
detailed quasi-balance. As a result, Pareto index and the variance of the
log-normal distribution are related to each other.
"
q-fin,"Pricing Life Insurance under Stochastic Mortality via the Instantaneous
  Sharpe Ratio: Theorems and Proofs","  We develop a pricing rule for life insurance under stochastic mortality in an
incomplete market by assuming that the insurance company requires compensation
for its risk in the form of a pre-specified instantaneous Sharpe ratio. Our
valuation formula satisfies a number of desirable properties, many of which it
shares with the standard deviation premium principle. The major result of the
paper is that the price per contract solves a linear partial differential
equation as the number of contracts approaches infinity. One can interpret the
limiting price as an expectation with respect to an equivalent martingale
measure. Another important result is that if the hazard rate is stochastic,
then the risk-adjusted premium is greater than the net premium, even as the
number of contracts approaches infinity. We present a numerical example to
illustrate our results, along with the corresponding algorithms.
"
q-fin,Optimal quantization for the pricing of swing options,"  In this paper, we investigate a numerical algorithm for the pricing of swing
options, relying on the so-called optimal quantization method. The numerical
procedure is described in details and numerous simulations are provided to
assert its efficiency. In particular, we carry out a comparison with the
Longstaff-Schwartz algorithm.
"
q-fin,Kolkata Restaurant Problem as a generalised El Farol Bar Problem,"  Generalisation of the El Farol bar problem to that of many bars here leads to
the Kolkata restaurant problem, where the decision to go to any restaurant or
not is much simpler (depending on the previous experience of course, as in the
El Farol bar problem). This generalised problem can be exactly analysed in some
limiting cases discussed here. The fluctuation in the restaurant service can be
shown to have precisely an inverse cubic behavior, as widely seen in the stock
market fluctuations.
"
q-fin,"Entropy Oriented Trading: A Trading Strategy Based on the Second Law of
  Thermodynamics","  The author proposes a finance trading strategy named Entropy Oriented Trading
and apply thermodynamics on the strategy. The state variables are chosen so
that the strategy satisfies the second law of thermodynamics. Using the law,
the author proves that the rate of investment (ROI) of the strategy is equal to
or more than the rate of price change.
"
q-fin,A simple algorithm based on fluctuations to play the market,"  In Biology, all motor enzymes operate on the same principle: they trap
favourable brownian fluctuations in order to generate directed forces and to
move. Whether it is possible or not to copy one such strategy to play the
market was the starting point of our investigations. We found the answer is
yes. In this paper we describe one such strategy and appraise its performance
with historical data from the European Monetary System (EMS), the US Dow Jones,
the german Dax and the french Cac40.
"
q-fin,Network Topology of an Experimental Futures Exchange,"  Many systems of different nature exhibit scale free behaviors. Economic
systems with power law distribution in the wealth is one of the examples. To
better understand the working behind the complexity, we undertook an empirical
study measuring the interactions between market participants. A Web server was
setup to administer the exchange of futures contracts whose liquidation prices
were coupled to event outcomes. After free registration, participants started
trading to compete for the money prizes upon maturity of the futures contracts
at the end of the experiment. The evolving `cash' flow network was
reconstructed from the transactions between players. We show that the network
topology is hierarchical, disassortative and scale-free with a power law
exponent of 1.02+-0.09 in the degree distribution. The small-world property
emerged early in the experiment while the number of participants was still
small. We also show power law distributions of the net incomes and
inter-transaction time intervals. Big winners and losers are associated with
high degree, high betweenness centrality, low clustering coefficient and low
degree-correlation. We identify communities in the network as groups of the
like-minded. The distribution of the community sizes is shown to be power-law
distributed with an exponent of 1.19+-0.16.
"
q-fin,Optimal cross hedging for insurance derivatives,"  We consider insurance derivatives depending on an external physical risk
process, for example a temperature in a low dimensional climate model. We
assume that this process is correlated with a tradable financial asset. We
derive optimal strategies for exponential utility from terminal wealth,
determine the indifference prices of the derivatives, and interpret them in
terms of diversification pressure. Moreover we check the optimal investment
strategies for standard admissibility criteria. Finally we compare the static
risk connected with an insurance derivative to the reduced risk due to a
dynamic investment into the correlated asset. We show that dynamic hedging
reduces the risk aversion in terms of entropic risk measures by a factor
related to the correlation.
"
q-fin,"The Macro Model of the Inequality Process and The Surging Relative
  Frequency of Large Wage Incomes","  This paper presents a model of the dynamics of the wage income distribution.
"
q-fin,On a generalised model for time-dependent variance with long-term memory,"  The ARCH process (R. F. Engle, 1982) constitutes a paradigmatic generator of
stochastic time series with time-dependent variance like it appears on a wide
broad of systems besides economics in which ARCH was born. Although the ARCH
process captures the so-called ""volatility clustering"" and the asymptotic
power-law probability density distribution of the random variable, it is not
capable to reproduce further statistical properties of many of these time
series such as: the strong persistence of the instantaneous variance
characterised by large values of the Hurst exponent (H > 0.8), and asymptotic
power-law decay of the absolute values self-correlation function. By means of
considering an effective return obtained from a correlation of past returns
that has a q-exponential form we are able to fix the limitations of the
original model. Moreover, this improvement can be obtained through the correct
choice of a sole additional parameter, $q_{m}$. The assessment of its validity
and usefulness is made by mimicking daily fluctuations of SP500 financial
index.
"
q-fin,Detecting anchoring in financial markets,"  Anchoring is a term used in psychology to describe the common human tendency
to rely too heavily (anchor) on one piece of information when making decisions.
A trading algorithm inspired by biological motors, introduced by L.
Gil\cite{Gil}, is suggested as a testing ground for anchoring in financial
markets. An exact solution of the algorithm is presented for arbitrary price
distributions. Furthermore the algorithm is extended to cover the case of a
market neutral portfolio, revealing additional evidence that anchoring is
involved in the decision making of market participants. The exposure of
arbitrage possibilities created by anchoring gives yet another illustration on
the difficulty proving market efficiency by only considering lower order
correlations in past price time series
"
q-fin,"Kullback-Leibler distance as a measure of the information filtered from
  multivariate data","  We show that the Kullback-Leibler distance is a good measure of the
statistical uncertainty of correlation matrices estimated by using a finite set
of data. For correlation matrices of multivariate Gaussian variables we
analytically determine the expected values of the Kullback-Leibler distance of
a sample correlation matrix from a reference model and we show that the
expected values are known also when the specific model is unknown. We propose
to make use of the Kullback-Leibler distance to estimate the information
extracted from a correlation matrix by correlation filtering procedures. We
also show how to use this distance to measure the stability of filtering
procedures with respect to statistical uncertainty. We explain the
effectiveness of our method by comparing four filtering procedures, two of them
being based on spectral analysis and the other two on hierarchical clustering.
We compare these techniques as applied both to simulations of factor models and
empirical data. We investigate the ability of these filtering procedures in
recovering the correlation matrix of models from simulations. We discuss such
an ability in terms of both the heterogeneity of model parameters and the
length of data series. We also show that the two spectral techniques are
typically more informative about the sample correlation matrix than techniques
based on hierarchical clustering, whereas the latter are more stable with
respect to statistical uncertainty.
"
q-fin,Microscopic Origin of Non-Gaussian Distributions of Financial Returns,"  In this paper we study the possible microscopic origin of heavy-tailed
probability density distributions for the price variation of financial
instruments. We extend the standard log-normal process to include another
random component in the so-called stochastic volatility models. We study these
models under an assumption, akin to the Born-Oppenheimer approximation, in
which the volatility has already relaxed to its equilibrium distribution and
acts as a background to the evolution of the price process. In this
approximation, we show that all models of stochastic volatility should exhibit
a scaling relation in the time lag of zero-drift modified log-returns. We
verify that the Dow-Jones Industrial Average index indeed follows this scaling.
We then focus on two popular stochastic volatility models, the Heston and
Hull-White models. In particular, we show that in the Hull-White model the
resulting probability distribution of log-returns in this approximation
corresponds to the Tsallis (t-Student) distribution. The Tsallis parameters are
given in terms of the microscopic stochastic volatility model. Finally, we show
that the log-returns for 30 years Dow Jones index data is well fitted by a
Tsallis distribution, obtaining the relevant parameters.
"
q-fin,Stochastic analysis of an agent-based model,"  We analyze the dynamics of a forecasting game which exhibits the phenomenon
of information cascades. Each agent aims at correctly predicting a binary
variable and he/she can either look for independent information or herd on the
choice of others. We show that dynamics can be analitically described in terms
of a Langevin equation and its collective behavior is described by the solution
of a Kramers' problem. This provides very accurate results in the region where
the vast majority of agents herd, which corresponds to the most interesting one
from a game theoretic point of view.
"
q-fin,The limit order book on different time scales,"  Financial markets can be described on several time scales. We use data from
the limit order book of the London Stock Exchange (LSE) to compare how the
fluctuation dominated microstructure crosses over to a more systematic global
behavior.
"
q-fin,"Utility Maximization with a Stochastic Clock and an Unbounded Random
  Endowment","  We introduce a linear space of finitely additive measures to treat the
problem of optimal expected utility from consumption under a stochastic clock
and an unbounded random endowment process. In this way we establish existence
and uniqueness for a large class of utility maximization problems including the
classical ones of terminal wealth or consumption, as well as the problems
depending on a random time-horizon or multiple consumption instances. As an
example we treat explicitly the problem of maximizing the logarithmic utility
of a consumption stream, where the local time of an Ornstein-Uhlenbeck process
acts as a stochastic clock.
"
q-fin,Scale-free avalanches in the multifractal random walk,"  Avalanches, or Avalanche-like, events are often observed in the dynamical
behaviour of many complex systems which span from solar flaring to the Earth's
crust dynamics and from traffic flows to financial markets. Self-organized
criticality (SOC) is one of the most popular theories able to explain this
intermittent charge/discharge behaviour. Despite a large amount of theoretical
work, empirical tests for SOC are still in their infancy. In the present paper
we address the common problem of revealing SOC from a simple time series
without having much information about the underlying system. As a working
example we use a modified version of the multifractal random walk originally
proposed as a model for the stock market dynamics. The study reveals, despite
the lack of the typical ingredients of SOC, an avalanche-like dynamics similar
to that of many physical systems. While, on one hand, the results confirm the
relevance of cascade models in representing turbulent-like phenomena, on the
other, they also raise the question about the current state of reliability of
SOC inference from time series analysis.
"
q-fin,"Optimal consumption from investment and random endowment in incomplete
  semimartingale markets","  We consider the problem of maximizing expected utility from consumption in a
constrained incomplete semimartingale market with a random endowment process,
and establish a general existence and uniqueness result using techniques from
convex duality. The notion of asymptotic elasticity of Kramkov and
Schachermayer is extended to the time-dependent case. By imposing no smoothness
requirements on the utility function in the temporal argument, we can treat
both pure consumption and combined consumption/terminal wealth problems, in a
common framework. To make the duality approach possible, we provide a detailed
characterization of the enlarged dual domain which is reminiscent of the
enlargement of $L^1$ to its topological bidual $(L^{\infty})^*$, a space of
finitely-additive measures. As an application, we treat the case of a
constrained It\^ o-process market-model.
"
q-fin,Stability of utility-maximization in incomplete markets,"  The effectiveness of utility-maximization techniques for portfolio management
relies on our ability to estimate correctly the parameters of the dynamics of
the underlying financial assets. In the setting of complete or incomplete
financial markets, we investigate whether small perturbations of the market
coefficient processes lead to small changes in the agent's optimal behavior
derived from the solution of the related utility-maximization problems.
Specifically, we identify the topologies on the parameter process space and the
solution space under which utility-maximization is a continuous operation, and
we provide a counterexample showing that our results are best possible, in a
certain sense. A novel result about the structure of the solution of the
utility-maximization problem where prices are modeled by continuous
semimartingales is established as an offshoot of the proof of our central
theorem.
"
q-fin,The Quantum Black-Scholes Equation,"  Motivated by the work of Segal and Segal on the Black-Scholes pricing formula
in the quantum context, we study a quantum extension of the Black-Scholes
equation within the context of Hudson-Parthasarathy quantum stochastic
calculus. Our model includes stock markets described by quantum Brownian motion
and Poisson process.
"
q-fin,Rent seeking games with tax evasion,"  We consider the static and dynamic models of Cournot duopoly with tax
evasion. In the dynamic model we introduce the time delay and we analyze the
local stability of the stationary state. There is a critical value of the delay
when the Hopf bifurcation occurs.
"
q-fin,Maximizing the Growth Rate under Risk Constraints,"  We investigate the ergodic problem of growth-rate maximization under a class
of risk constraints in the context of incomplete, It\^{o}-process models of
financial markets with random ergodic coefficients. Including {\em
value-at-risk} (VaR), {\em tail-value-at-risk} (TVaR), and {\em limited
expected loss} (LEL), these constraints can be both wealth-dependent(relative)
and wealth-independent (absolute). The optimal policy is shown to exist in an
appropriate admissibility class, and can be obtained explicitly by uniform,
state-dependent scaling down of the unconstrained (Merton) optimal portfolio.
This implies that the risk-constrained wealth-growth optimizer locally behaves
like a CRRA-investor, with the relative risk-aversion coefficient depending on
the current values of the market coefficients.
"
q-fin,"Financial equilibria in the semimartingale setting: complete markets and
  markets with withdrawal constraints","  Existence of stochastic financial equilibria giving rise to semimartingale
asset prices is established under a general class of assumptions. These
equilibria are expressed in real terms and span complete markets or markets
with withdrawal constraints.We deal with random endowment density streams which
admit jumps and general time-dependent utility functions on which only
regularity conditions are imposed. As an integral part of the proof of the main
result, we establish a novel characterization of semimartingale functions.
"
q-fin,"Optimal Investment with an Unbounded Random Endowment and Utility-Based
  Pricing","  This paper studies the problem of maximizing the expected utility of terminal
wealth for a financial agent with an unbounded random endowment, and with a
utility function which supports both positive and negative wealth. We prove the
existence of an optimal trading strategy within a class of permissible
strategies -- those strategies whose wealth process is a supermartingale under
all pricing measures with finite relative entropy. We give necessary and
sufficient conditions for the absence of utility-based arbitrage, and for the
existence of a solution to the primal problem.
  We consider two utility-based methods which can be used to price contingent
claims. Firstly we investigate marginal utility-based price processes
(MUBPP's). We show that such processes can be characterized as local
martingales under the normalized optimal dual measure for the utility
maximizing investor. Finally, we present some new results on utility
indifference prices, including continuity properties and volume asymptotics for
the case of a general utility function, unbounded endowment and unbounded
contingent claims.
"
q-fin,On the semimartingale property via bounded logarithmic utility,"  This paper provides a new version of the condition of Di Nunno et al. (2003),
Ankirchner and Imkeller (2005) and Biagini and \{O}ksendal (2005) ensuring the
semimartingale property for a large class of continuous stochastic processes.
Unlike our predecessors, we base our modeling framework on the concept of
portfolio proportions which yields a short self-contained proof of the main
theorem, as well as a counterexample, showing that analogues of our results do
not hold in the discontinuous setting.
"
q-fin,Hiking the hypercube: producers and consumers,"  We study the dynamics of co-evolution of producers and customers described by
bit-strings representing individual traits. Individual ''size-like'' properties
are controlled by binary encounters which outcome depends upon a recognition
process. Depending upon the parameter set-up, mutual selection of producers and
customers results in different types of attractors, either an exclusive niches
regime or a competition regime.
"
q-fin,Are all highly liquid securities within the same class?,"  In this manuscript we analyse the leading statistical properties of
fluctuations of (log) 3-month US Treasury bill quotation in the secondary
market, namely: probability density function, autocorrelation, absolute values
autocorrelation, and absolute values persistency. We verify that this financial
instrument, in spite of its high liquidity, shows very peculiar properties.
Particularly, we verify that log-fluctuations belong to the Levy class of
stochastic variables.
"
q-fin,"Stability of the utility maximization problem with random endowment in
  incomplete markets","  We perform a stability analysis for the utility maximization problem in a
general semimartingale model where both liquid and illiquid assets (random
endowments) are present. Small misspecifications of preferences (as modeled via
expected utility), as well as views of the world or the market model (as
modeled via subjective probabilities) are considered. Simple sufficient
conditions are given for the problem to be well-posed, in the sense the optimal
wealth and the marginal utility-based prices are continuous functionals of
preferences and probabilistic views.
"
q-fin,Long Memory in Nonlinear Processes,"  It is generally accepted that many time series of practical interest exhibit
strong dependence, i.e., long memory. For such series, the sample
autocorrelations decay slowly and log-log periodogram plots indicate a
straight-line relationship. This necessitates a class of models for describing
such behavior. A popular class of such models is the autoregressive
fractionally integrated moving average (ARFIMA) which is a linear process.
However, there is also a need for nonlinear long memory models. For example,
series of returns on financial assets typically tend to show zero correlation,
whereas their squares or absolute values exhibit long memory. Furthermore, the
search for a realistic mechanism for generating long memory has led to the
development of other nonlinear long memory models. In this chapter, we will
present several nonlinear long memory models, and discuss the properties of the
models, as well as associated parametric andsemiparametric estimators.
"
q-fin,Uncertainty in the Fluctuations of the Price of Stocks,"  We report on a study of the Tehran Price Index (TEPIX) from 2001 to 2006 as
an emerging market that has been affected by several political crises during
the recent years, and analyze the non-Gaussian probability density function
(PDF) of the log returns of the stocks' prices. We show that while the average
of the index did not fall very much over the time period of the study, its
day-to-day fluctuations strongly increased due to the crises. Using an approach
based on multiplicative processes with a detrending procedure, we study the
scale-dependence of the non-Gaussian PDFs, and show that the temporal
dependence of their tails indicates a gradual and systematic increase in the
probability of the appearance of large increments in the returns on approaching
distinct critical time scales over which the TEPIX has exhibited maximum
uncertainty.
"
q-fin,Multifractality in stock indexes: Fact or fiction?,"  Multifractal analysis and extensive statistical tests are performed upon
intraday minutely data within individual trading days for four stock market
indexes (including HSI, SZSC, S&P500, and NASDAQ) to check whether the indexes
(instead of the returns) possess multifractality. We find that the mass
exponent $\tau(q)$ is linear and the singularity $\alpha(q)$ is close to 1 for
all trading days and all indexes. Furthermore, we find strong evidence showing
that the scaling behaviors of the original data sets cannot be distinguished
from those of the shuffled time series. Hence, the so-called multifractality in
the intraday stock market indexes is merely an illusion.
"
q-fin,"Heterogeneity and Increasing Returns May Drive Socio-Economic
  Transitions","  There are clear benefits associated with a particular consumer choice for
many current markets. For example, as we consider here, some products might
carry environmental or `green' benefits. Some consumers might value these
benefits while others do not. However, as evidenced by myriad failed attempts
of environmental products to maintain even a niche market, such benefits do not
necessarily outweigh the extra purchasing cost. The question we pose is, how
can such an initially economically-disadvantaged green product evolve to hold
the greater share of the market? We present a simple mathematical model for the
dynamics of product competition in a heterogeneous consumer population. Our
model preassigns a hierarchy to the products, which designates the consumer
choice when prices are comparable, while prices are dynamically rescaled to
reflect increasing returns to scale. Our approach allows us to model many
scenarios of technology substitution and provides a method for generalizing
market forces. With this model, we begin to forecast irreversible trends
associated with consumer dynamics as well as policies that could be made to
influence transitions
"
q-fin,Nurturing Breakthroughs: Lessons from Complexity Theory,"  A general theory of innovation and progress in human society is outlined,
based on the combat between two opposite forces (conservatism/inertia and
speculative herding ""bubble"" behavior). We contend that human affairs are
characterized by ubiquitous ``bubbles'', which involve huge risks which would
not otherwise be taken using standard cost/benefit analysis. Bubbles result
from self-reinforcing positive feedbacks. This leads to explore uncharted
territories and niches whose rare successes lead to extraordinary discoveries
and provide the base for the observed accelerating development of technology
and of the economy. But the returns are very heterogeneous, very risky and may
not occur. In other words, bubbles, which are characteristic definitions of
human activity, allow huge risks to get huge returns over large scales. We
outline some underlying mathematical structure and a few results involving
positive feedbacks, emergence, heavy-tailed power laws, outliers/kings/black
swans, the problem of predictability and the illusion of control, as well as
some policy implications.
"
q-fin,"Effects of payoff functions and preference distributions in an adaptive
  population","  Adaptive populations such as those in financial markets and distributed
control can be modeled by the Minority Game. We consider how their dynamics
depends on the agents' initial preferences of strategies, when the agents use
linear or quadratic payoff functions to evaluate their strategies. We find that
the fluctuations of the population making certain decisions (the volatility)
depends on the diversity of the distribution of the initial preferences of
strategies. When the diversity decreases, more agents tend to adapt their
strategies together. In systems with linear payoffs, this results in dynamical
transitions from vanishing volatility to a non-vanishing one. For low signal
dimensions, the dynamical transitions for the different signals do not take
place at the same critical diversity. Rather, a cascade of dynamical
transitions takes place when the diversity is reduced. In contrast, no phase
transitions are found in systems with the quadratic payoffs. Instead, a basin
boundary of attraction separates two groups of samples in the space of the
agents' decisions. Initial states inside this boundary converge to small
volatility, while those outside diverge to a large one. Furthermore, when the
preference distribution becomes more polarized, the dynamics becomes more
erratic. All the above results are supported by good agreement between
simulations and theory.
"
q-fin,"A Model for Counterparty Risk with Geometric Attenuation Effect and the
  Valuation of CDS","  In this paper, a geometric function is introduced to reflect the attenuation
speed of impact of one firm's default to its partner. If two firms are
competitions (copartners), the default intensity of one firm will decrease
(increase) abruptly when the other firm defaults. As time goes on, the impact
will decrease gradually until extinct. In this model, the joint distribution
and marginal distributions of default times are derived by employing the change
of measure, so can we value the fair swap premium of a CDS.
"
q-fin,The fractional volatility model: An agent-based interpretation,"  Based on criteria of mathematical simplicity and consistency with empirical
market data, a model with volatility driven by fractional noise has been
constructed which provides a fairly accurate mathematical parametrization of
the data. Here, some features of the model are discussed and, using agent-based
models, one tries to find which agent strategies and (or) properties of the
financial institutions might be responsible for the features of the fractional
volatility model.
"
q-fin,The minority game: An economics perspective,"  This paper gives a critical account of the minority game literature. The
minority game is a simple congestion game: players need to choose between two
options, and those who have selected the option chosen by the minority win. The
learning model proposed in this literature seems to differ markedly from the
learning models commonly used in economics. We relate the learning model from
the minority game literature to standard game-theoretic learning models, and
show that in fact it shares many features with these models. However, the
predictions of the learning model differ considerably from the predictions of
most other learning models. We discuss the main predictions of the learning
model proposed in the minority game literature, and compare these to
experimental findings on congestion games.
"
q-fin,"Specialization of strategies and herding behavior of trading firms in a
  financial market","  The understanding of complex social or economic systems is an important
scientific challenge. Here we present a comprehensive study of the Spanish
Stock Exchange showing that most financial firms trading in that market are
characterized by a resulting strategy and can be classified in groups of firms
with different specialization. Few large firms overally act as trending firms
whereas many heterogeneous firm act as reversing firms. The herding properties
of these two groups are markedly different and consistently observed over a
four-year period of trading.
"
q-fin,Quantum Nash Equilibria and Quantum Computing,"  In this paper we review our earlier work on quantum computing and the Nash
Equilibrium, in particular, tracing the history of the discovery of new Nash
Equilibria and then reviewing the ways in which quantum computing may be
expected to generate new classes of Nash equilibria. We then extend this work
through a substantive analysis of examples provided by Meyer, Flitney, Iqbal
and Weigert and Cheon and Tsutsui with respect to quantized games, quantum game
strategies and the extension of Nash Equilibrium to solvable games in Hilbert
space. Finally, we review earlier work by Sato, Taiji and Ikegami on non-linear
computation and computational classes by way of reference to coherence,
decoherence and quantum computating systems.
"
q-fin,Adaptation and Coevolution on an Emergent Global Competitive Landscape,"  Notions of Darwinian selection have been implicit in economic theory for at
least sixty years. Richard Nelson and Sidney Winter have argued that while
evolutionary thinking was prevalent in prewar economics, the postwar
Neoclassical school became almost entirely preoccupied with equilibrium
conditions and their mathematical conditions. One of the problems with the
economic interpretation of firm selection through competition has been a weak
grasp on an incomplete scientific paradigm. As I.F. Price notes, ""The
biological metaphor has long lurked in the background of management theory
largely because the message of 'survival of the fittest' (usually wrongly
attributed to Charles Darwin rather than Herbert Spencer) provides a seemingly
natural model for market competition (e.g. Alchian 1950, Merrell 1984,
Henderson 1989, Moore 1993), without seriously challenging the underlying
paradigms of what an organisation is."" In this paper we examine the application
of dynamic fitness landscape models to economic theory, particularly the theory
of technology substitution, drawing on recent work by Kauffman, Arthur,
McKelvey, Nelson and Winter, and Windrum and Birchenhall. In particular we use
Professor Post's early work with John Holland on the genetic algorithm to
explain some of the key differences between static and dynamic approaches to
economic modeling.
"
q-fin,"Maximum Entropy, the Collective Welfare Principle and the Globalization
  Process","  Although both systems analyzed are described through two theories apparently
different (quantum mechanics and game theory) it is shown that both are
analogous and thus exactly equivalents. The quantum analogue of the replicator
dynamics is the von Neumann equation. Quantum mechanics could be used to
explain more correctly biological and economical processes. It could even
encloses theories like games and evolutionary dynamics. We can take some
concepts and definitions from quantum mechanics and physics for the best
understanding of the behavior of economics and biology. Also, we could maybe
understand nature like a game in where its players compete for a common welfare
and the equilibrium of the system that they are members. All the members of our
system will play a game in which its maximum payoff is the equilibrium of the
system. They act as a whole besides individuals like they obey a rule in where
they prefer to work for the welfare of the collective besides the individual
welfare. A system where its members are in Nash Equilibrium (or ESS) is exactly
equivalent to a system in a maximum entropy state. A system is stable only if
it maximizes the welfare of the collective above the welfare of the individual.
If it is maximized the welfare of the individual above the welfare of the
collective the system gets unstable an eventually collapses. The results of
this work shows that the ""globalization"" process has a behavior exactly
equivalent to a system that is tending to a maximum entropy state and predicts
the apparition of big common markets and strong common currencies that will
find its ""equilibrium"" by decreasing its number until they get a state
characterized by only one common currency and only one common market around the
world.
"
q-fin,A Cultural Market Model,"  Social interactions and personal tastes shape our consumption behavior of
cultural products. In this study, we present a computational model of a
cultural market and we aim to analyze the behavior of the consumer population
as an emergent phenomena. Our results suggest that the final market shares of
cultural products dramatically depend on consumer heterogeneity and social
interaction pressure. Furthermore, the relation between the resulting market
shares and social interaction is robust with respect to a wide range of
variation in the parameter values and the type of topology.
"
q-fin,"Nonlinear behavior of the Chinese SSEC index with a unit root: Evidence
  from threshold unit root tests","  We investigate the behavior of the Shanghai Stock Exchange Composite (SSEC)
index for the period from 1990:12 to 2007:06 using an unconstrained two-regime
threshold autoregressive (TAR) model with an unit root developed by Caner and
Hansen. The method allows us to simultaneously consider non-stationarity and
nonlinearity in financial time series. Our finding indicates that the Shanghai
stock market exhibits nonlinear behavior with two regimes and has unit roots in
both regimes. The important implications of the threshold effect in stock
markets are also discussed.
"
q-fin,Credit risk - A structural model with jumps and correlations,"  We set up a structural model to study credit risk for a portfolio containing
several or many credit contracts. The model is based on a jump--diffusion
process for the risk factors, i.e. for the company assets. We also include
correlations between the companies. We discuss that models of this type have
much in common with other problems in statistical physics and in the theory of
complex systems. We study a simplified version of our model analytically.
Furthermore, we perform extensive numerical simulations for the full model. The
observables are the loss distribution of the credit portfolio, its moments and
other quantities derived thereof. We compile detailed information about the
parameter dependence of these observables. In the course of setting up and
analyzing our model, we also give a review of credit risk modeling for a
physics audience.
"
q-fin,Multi-scale correlations in different futures markets,"  In the present work we investigate the multiscale nature of the correlations
for high frequency data (1 minute) in different futures markets over a period
of two years, starting on the 1st of January 2003 and ending on the 31st of
December 2004. In particular, by using the concept of ""local"" Hurst exponent,
we point out how the behaviour of this parameter, usually considered as a
benchmark for persistency/antipersistency recognition in time series, is
largely time-scale dependent in the market context. These findings are a direct
consequence of the intrinsic complexity of a system where trading strategies
are scale-adaptive. Moreover, our analysis points out different regimes in the
dynamical behaviour of the market indices under consideration.
"
q-fin,Economic Amplifier - A New Econophysics Model,"  Most of the econometric and econophysics models have been borrowed from the
statistical physics, and as a cosequence, a new interdisciplinary science
called econophysics has emerged. In this paper we planned to extend the analogy
between different economic processes or phenomena and processes and phenomena
from different fields of physics, other than statistical physics. On the basis
of the economic development process and amplification phenomenon analogy, a new
econophysics model, named economic amplifier, on the electronic amplification
principle from applied physics was proposed und largely analyzed.
"
q-fin,Growth-optimal portfolios under transaction costs,"  This paper studies a portfolio optimization problem in a discrete-time
Markovian model of a financial market, in which asset price dynamics depend on
an external process of economic factors. There are transaction costs with a
structure that covers, in particular, the case of fixed plus proportional
costs. We prove that there exists a self-financing trading strategy maximizing
the average growth rate of the portfolio wealth. We show that this strategy has
a Markovian form. Our result is obtained by large deviations estimates on
empirical measures of the price process and by a generalization of the
vanishing discount method to discontinuous transition operators.
"
q-fin,A Bayesian Framework for Combining Valuation Estimates,"  Obtaining more accurate equity value estimates is the starting point for
stock selection, value-based indexing in a noisy market, and beating benchmark
indices through tactical style rotation. Unfortunately, discounted cash flow,
method of comparables, and fundamental analysis typically yield discrepant
valuation estimates. Moreover, the valuation estimates typically disagree with
market price. Can one form a superior valuation estimate by averaging over the
individual estimates, including market price? This article suggests a Bayesian
framework for combining two or more estimates into a superior valuation
estimate. The framework justifies the common practice of averaging over several
estimates to arrive at a final point estimate.
"
q-fin,"Indication of multiscaling in the volatility return intervals of stock
  markets","  The distribution of the return intervals $\tau$ between volatilities above a
threshold $q$ for financial records has been approximated by a scaling
behavior. To explore how accurate is the scaling and therefore understand the
underlined non-linear mechanism, we investigate intraday datasets of 500 stocks
which consist of the Standard & Poor's 500 index. We show that the cumulative
distribution of return intervals has systematic deviations from scaling. We
support this finding by studying the m-th moment $\mu_m \equiv
<(\tau/<\tau>)^m>^{1/m}$, which show a certain trend with the mean interval
$<\tau>$. We generate surrogate records using the Schreiber method, and find
that their cumulative distributions almost collapse to a single curve and
moments are almost constant for most range of $<\tau>$. Those substantial
differences suggest that non-linear correlations in the original volatility
sequence account for the deviations from a single scaling law. We also find
that the original and surrogate records exhibit slight tendencies for short and
long $<\tau>$, due to the discreteness and finite size effects of the records
respectively. To avoid as possible those effects for testing the multiscaling
behavior, we investigate the moments in the range $10<<\tau>\leq100$, and find
the exponent $\alpha$ from the power law fitting $\mu_m\sim<\tau>^\alpha$ has a
narrow distribution around $\alpha\neq0$ which depend on m for the 500 stocks.
The distribution of $\alpha$ for the surrogate records are very narrow and
centered around $\alpha=0$. This suggests that the return interval distribution
exhibit multiscaling behavior due to the non-linear correlations in the
original volatility.
"
q-fin,"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher
  complexities and oracle inequalities in risk minimization'' by V.
  Koltchinskii","  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]
"
q-fin,"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher
  complexities and oracle inequalities in risk minimization'' by V.
  Koltchinskii","  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]
"
q-fin,Information flow between composite stock index and individual stocks,"  We investigate the strength and the direction of information transfer in the
U.S. stock market between the composite stock price index of stock market and
prices of individual stocks using the transfer entropy. Through the
directionality of the information transfer, we find that individual stocks are
influenced by the index of the market.
"
q-fin,The International Trade Network: weighted network analysis and modelling,"  Tools of the theory of critical phenomena, namely the scaling analysis and
universality, are argued to be applicable to large complex web-like network
structures. Using a detailed analysis of the real data of the International
Trade Network we argue that the scaled link weight distribution has an
approximate log-normal distribution which remains robust over a period of 53
years. Another universal feature is observed in the power-law growth of the
trade strength with gross domestic product, the exponent being similar for all
countries. Using the 'rich-club' coefficient measure of the weighted networks
it has been shown that the size of the rich-club controlling half of the
world's trade is actually shrinking. While the gravity law is known to describe
well the social interactions in the static networks of population migration,
international trade, etc, here for the first time we studied a non-conservative
dynamical model based on the gravity law which excellently reproduced many
empirical features of the ITN.
"
q-fin,Sparse and stable Markowitz portfolios,"  We consider the problem of portfolio selection within the classical Markowitz
mean-variance framework, reformulated as a constrained least-squares regression
problem. We propose to add to the objective function a penalty proportional to
the sum of the absolute values of the portfolio weights. This penalty
regularizes (stabilizes) the optimization problem, encourages sparse portfolios
(i.e. portfolios with only few active positions), and allows to account for
transaction costs. Our approach recovers as special cases the
no-short-positions portfolios, but does allow for short positions in limited
number. We implement this methodology on two benchmark data sets constructed by
Fama and French. Using only a modest amount of training data, we construct
portfolios whose out-of-sample performance, as measured by Sharpe ratio, is
consistently and significantly better than that of the naive evenly-weighted
portfolio which constitutes, as shown in recent literature, a very tough
benchmark.
"
q-fin,The International Trade Network,"  Bilateral trade relationships in the international level between pairs of
countries in the world give rise to the notion of the International Trade
Network (ITN). This network has attracted the attention of network researchers
as it serves as an excellent example of the weighted networks, the link weight
being defined as a measure of the volume of trade between two countries. In
this paper we analyzed the international trade data for 53 years and studied in
detail the variations of different network related quantities associated with
the ITN. Our observation is that the ITN has also a scale invariant structure
like many other real-world networks.
"
q-fin,"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher
  complexities and oracle inequalities in risk minimization'' by V.
  Koltchinskii","  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]
"
q-fin,"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher
  complexities and oracle inequalities in risk minimization'' by V.
  Koltchinskii","  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]
"
q-fin,"Discussion of ""2004 IMS Medallion Lecture: Local Rademacher complexities
  and oracle inequalities in risk minimization"" by V. Koltchinskii","  Discussion of ""2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization"" by V. Koltchinskii [arXiv:0708.0083]
"
q-fin,"Stochastic Knapsack Problem Revisited: Switch-Over Policies and Dynamic
  Pricing","  The stochastic knapsack has been used as a model in wide ranging applications
from dynamic resource allocation to admission control in telecommunication. In
recent years, a variation of the model has become a basic tool in studying
problems that arise in revenue management and dynamic/flexible pricing; and it
is in this context that our study is undertaken. Based on a dynamic programming
formulation and associated properties of the value function, we study in this
paper a class of control that we call switch-over policies -- start from
accepting only orders of the highest price, and switch to including lower
prices as time goes by, with the switch-over times optimally decided via convex
programming. We establish the asymptotic optimality of the switch-over policy,
and develop pricing models based on this policy to optimize the price
reductions over the decision horizon.
"
q-fin,"The Local Fractal Properties of the Financial Time Series on the Polish
  Stock Exchange Market","  We investigate the local fractal properties of the financial time series
based on the evolution of the Warsaw Stock Exchange Index (WIG) connected with
the largest developing financial market in Europe. Calculating the local Hurst
exponent for the WIG time series we find an interesting dependence between the
behavior of the local fractal properties of the WIG time series and the crashes
appearance on the financial market.
"
q-fin,"A new formulation of asset trading games in continuous time with
  essential forcing of variation exponent","  We introduce a new formulation of asset trading games in continuous time in
the framework of the game-theoretic probability established by Shafer and Vovk
(Probability and Finance: It's Only a Game! (2001) Wiley). In our formulation,
the market moves continuously, but an investor trades in discrete times, which
can depend on the past path of the market. We prove that an investor can
essentially force that the asset price path behaves with the variation exponent
exactly equal to two. Our proof is based on embedding high-frequency
discrete-time games into the continuous-time game and the use of the Bayesian
strategy of Kumon, Takemura and Takeuchi (Stoch. Anal. Appl. 26 (2008)
1161--1180) for discrete-time coin-tossing games. We also show that the main
growth part of the investor's capital processes is clearly described by the
information quantities, which are derived from the Kullback--Leibler
information with respect to the empirical fluctuation of the asset price.
"
q-fin,Fine-tune your smile: Correction to Hagan et al,"  In this small note we use results derived in Berestycki et al. to correct the
celebrated formulae of Hagan et al. We derive explicitly the correct zero order
term in the expansion of the implied volatility in time to maturity. The new
term is consistent as $\beta\to 1$. Furthermore, numerical simulations show
that it reduces or eliminates known pathologies of the earlier formula.
"
q-fin,Models of Financial Markets with Extensive Participation Incentives,"  We consider models of financial markets in which all parties involved find
incentives to participate. Strategies are evaluated directly by their virtual
wealths. By tuning the price sensitivity and market impact, a phase diagram
with several attractor behaviors resembling those of real markets emerge,
reflecting the roles played by the arbitrageurs and trendsetters, and including
a phase with irregular price trends and positive sums. The positive-sumness of
the players' wealths provides participation incentives for them. Evolution and
the bid-ask spread provide mechanisms for the gain in wealth of both the
players and market-makers. New players survive in the market if the
evolutionary rate is sufficiently slow. We test the applicability of the model
on real Hang Seng Index data over 20 years. Comparisons with other models show
that our model has a superior average performance when applied to real
financial data.
"
q-fin,Group dynamics of the Japanese market,"  We investigated the network structures of the Japanese stock market through
the minimum spanning tree. We defined grouping coefficient to test the validity
of conventional grouping by industrial categories, and found a decreasing in
trend for the coefficient. This phenomenon supports the increasing external
influences on the market due to the globalization. To reduce this influence, we
used S&P500 index as the international market and removed its correlation with
every stock. We found stronger grouping in this measurement, compared to the
original analysis, which agrees with our assumption that the international
market influences to the Japanese market.
"
q-fin,Perpetual American options within CTRW's,"  Continuous-time random walks are a well suited tool for the description of
market behaviour at the smallest scale: the tick-to-tick evolution. We will
apply this kind of market model to the valuation of perpetual American options:
derivatives with no maturity that can be exercised at any time. Our approach
leads to option prices that fulfil financial formulas when canonical
assumptions on the dynamics governing the process are made, but it is still
suitable for more exotic market conditions.
"
q-fin,Investment and Consumption without Commitment,"  In this paper, we investigate the Merton portfolio management problem in the
context of non-exponential discounting. This gives rise to time-inconsistency
of the decision-maker. If the decision-maker at time t=0 can commit his/her
successors, he/she can choose the policy that is optimal from his/her point of
view, and constrain the others to abide by it, although they do not see it as
optimal for them. If there is no commitment mechanism, one must seek a
subgame-perfect equilibrium strategy between the successive decision-makers. In
the line of the earlier work by Ekeland and Lazrak we give a precise definition
of equilibrium strategies in the context of the portfolio management problem,
with finite horizon, we characterize it by a system of partial differential
equations, and we show existence in the case when the utility is CRRA and the
terminal time T is small. We also investigate the infinite-horizon case and we
give two different explicit solutions in the case when the utility is CRRA (in
contrast with the case of exponential discount, where there is only one). Some
of our results are proved under the assumption that the discount function h(t)
is a linear combination of two exponentials, or is the product of an
exponential by a linear function.
"
q-fin,The Product Space Conditions the Development of Nations,"  Economies grow by upgrading the type of products they produce and export. The
technology, capital, institutions and skills needed to make such new products
are more easily adapted from some products than others. We study the network of
relatedness between products, or product space, finding that most upscale
products are located in a densely connected core while lower income products
occupy a less connected periphery. We show that countries tend to move to goods
close to those they are currently specialized in, allowing nations located in
more connected parts of the product space to upgrade their exports basket more
quickly. Most countries can reach the core only if they jump over empirically
infrequent distances in the product space. This may help explain why poor
countries have trouble developing more competitive exports, failing to converge
to the income levels of rich countries.
"
q-fin,"Optimal execution strategies in limit order books with general shape
  functions","  We consider optimal execution strategies for block market orders placed in a
limit order book (LOB). We build on the resilience model proposed by Obizhaeva
and Wang (2005) but allow for a general shape of the LOB defined via a given
density function. Thus, we can allow for empirically observed LOB shapes and
obtain a nonlinear price impact of market orders. We distinguish two
possibilities for modeling the resilience of the LOB after a large market
order: the exponential recovery of the number of limit orders, i.e., of the
volume of the LOB, or the exponential recovery of the bid-ask spread. We
consider both of these resilience modes and, in each case, derive explicit
optimal execution strategies in discrete time. Applying our results to a
block-shaped LOB, we obtain a new closed-form representation for the optimal
strategy, which explicitly solves the recursive scheme given in Obizhaeva and
Wang (2005). We also provide some evidence for the robustness of optimal
strategies with respect to the choice of the shape function and the
resilience-type.
"
q-fin,On the Structure of General Mean-Variance Hedging Strategies,"  We provide a new characterization of mean-variance hedging strategies in a
general semimartingale market. The key point is the introduction of a new
probability measure $P^{\star}$ which turns the dynamic asset allocation
problem into a myopic one. The minimal martingale measure relative to
$P^{\star}$ coincides with the variance-optimal martingale measure relative to
the original probability measure $P$.
"
q-fin,"Nonlinear option pricing models for illiquid markets: scaling properties
  and explicit solutions","  Several models for the pricing of derivative securities in illiquid markets
are discussed. A typical type of nonlinear partial differential equations
arising from these investigation is studied. The scaling properties of these
equations are discussed. Explicit solutions for one of the models are obtained
and studied.
"
q-fin,"Models with time-dependent parameters using transform methods:
  application to Heston's model","  This paper presents a methodology to introduce time-dependent parameters for
a wide family of models preserving their analytic tractability. This family
includes hybrid models with stochastic volatility, stochastic interest-rates,
jumps and their non-hybrid counterparts. The methodology is applied to Heston's
model. A bootstrapping algorithm is presented for calibration. A case study
works out the calibration of the time-dependent parameters to the volatility
surface of the Eurostoxx 50 index. The methodology is also applied to the
analytic valuation of forward start vanilla options driven by Heston's model.
This result is used to explore the forward skew of the case study.
"
q-fin,Point estimation with exponentially tilted empirical likelihood,"  Parameters defined via general estimating equations (GEE) can be estimated by
maximizing the empirical likelihood (EL). Newey and Smith [Econometrica 72
(2004) 219--255] have recently shown that this EL estimator exhibits desirable
higher-order asymptotic properties, namely, that its $O(n^{-1})$ bias is small
and that bias-corrected EL is higher-order efficient. Although EL possesses
these properties when the model is correctly specified, this paper shows that,
in the presence of model misspecification, EL may cease to be root n convergent
when the functions defining the moment conditions are unbounded (even when
their expectations are bounded). In contrast, the related exponential tilting
(ET) estimator avoids this problem. This paper shows that the ET and EL
estimators can be naturally combined to yield an estimator called exponentially
tilted empirical likelihood (ETEL) exhibiting the same $O(n^{-1})$ bias and the
same $O(n^{-2})$ variance as EL, while maintaining root n convergence under
model misspecification.
"
q-fin,Eduction and Economy -- An Analysis of Statistical Data,"  In this paper the correlation between education, research and macroeconomic
strength of countries at a global scale is analyzed on the basis of statistical
data published by the UNIDO and OECD. It uses sets of composite indicators
describing the economical performance and competitiveness as well as those
relevant for human development, education, knowledge and technology achievement
and correlates them. It turns out that for countries with a human development
index (HDI) below 0.7 the basic education and technology achievement indices
are the driving force for further development, whereas for the industrialized
countries the knowledge index as a composite education and communication index
has the strongest effect on the economic strength of a country as measured by
the gross domestic product.
"
stat,Domain wall switching: optimizing the energy landscape,"  It has recently been suggested that exchange spring media offer a way to
increase media density without causing thermal instability
(superparamagnetism), by using a hard and a soft layer coupled by exchange.
Victora has suggested a figure of merit xi = 2 E_b/mu_0 m_s H_sw, the ratio of
the energy barrier to that of a Stoner-Wohlfarth system with the same switching
field, which is 1 for a Stoner-Wohlfarth (coherently switching) particle and 2
for an optimal two-layer composite medium. A number of theoretical approaches
have been used for this problem (e.g., various numbers of coupled
Stoner-Wohlfarth layers and continuum micromagnetics). In this paper we show
that many of these approaches can be regarded as special cases or
approximations to a variational formulation of the problem, in which the energy
is minimized for fixed magnetization. The results can be easily visualized in
terms of a plot of the energy as a function of magnetic moment m_z, in which
both the switching field [the maximum slope of E(m_z)] and the stability
(determined by the energy barrier E_b) are geometrically visible. In this
formulation we can prove a rigorous limit on the figure of merit xi, which can
be no higher than 4. We also show that a quadratic anistropy suggested by Suess
et al comes very close to this limit.
"
stat,"Matter-Wave Bright Solitons with a Finite Background in Spinor
  Bose-Einstein Condensates","  We investigate dynamical properties of bright solitons with a finite
background in the F=1 spinor Bose-Einstein condensate (BEC), based on an
integrable spinor model which is equivalent to the matrix nonlinear
Schr\""{o}dinger equation with a self-focusing nonlineality. We apply the
inverse scattering method formulated for nonvanishing boundary conditions. The
resulting soliton solutions can be regarded as a generalization of those under
vanishing boundary conditions. One-soliton solutions are derived in an explicit
manner. According to the behaviors at the infinity, they are classified into
two kinds, domain-wall (DW) type and phase-shift (PS) type. The DW-type implies
the ferromagnetic state with nonzero total spin and the PS-type implies the
polar state, where the total spin amounts to zero. We also discuss two-soliton
collisions. In particular, the spin-mixing phenomenon is confirmed in a
collision involving the DW-type. The results are consistent with those of the
previous studies for bright solitons under vanishing boundary conditions and
dark solitons. As a result, we establish the robustness and the usefulness of
the multiple matter-wave solitons in the spinor BECs.
"
stat,"Computation of Power Loss in Likelihood Ratio Tests for Probability
  Densities Extended by Lehmann Alternatives","  We compute the loss of power in likelihood ratio tests when we test the
original parameter of a probability density extended by the first Lehmann
alternative.
"
stat,"The density of critical percolation clusters touching the boundaries of
  strips and squares","  We consider the density of two-dimensional critical percolation clusters,
constrained to touch one or both boundaries, in infinite strips, half-infinite
strips, and squares, as well as several related quantities for the infinite
strip. Our theoretical results follow from conformal field theory, and are
compared with high-precision numerical simulation. For example, we show that
the density of clusters touching both boundaries of an infinite strip of unit
width (i.e. crossing clusters) is proportional to $(\sin \pi
y)^{-5/48}\{[\cos(\pi y/2)]^{1/3} +[\sin (\pi y/2)]^{1/3}-1\}$.
  We also determine numerically contours for the density of clusters crossing
squares and long rectangles with open boundaries on the sides, and compare with
theory for the density along an edge.
"
stat,"A density tensor hierarchy for open system dynamics: retrieving the
  noise","  We introduce a density tensor hierarchy for open system dynamics, that
recovers information about fluctuations lost in passing to the reduced density
matrix. For the case of fluctuations arising from a classical probability
distribution, the hierarchy is formed from expectations of products of pure
state density matrix elements, and can be compactly summarized by a simple
generating function. For the case of quantum fluctuations arising when a
quantum system interacts with a quantum environment in an overall pure state,
the corresponding hierarchy is defined as the environmental trace of products
of system matrix elements of the full density matrix. Only the lowest member of
the quantum noise hierarchy is directly experimentally measurable. The unit
trace and idempotence properties of the pure state density matrix imply descent
relations for the tensor hierarchies, that relate the order $n$ tensor, under
contraction of appropriate pairs of tensor indices, to the order $n-1$ tensor.
As examples to illustrate the classical probability distribution formalism, we
consider a quantum system evolving by It\^o stochastic and by jump process
Schr\""odinger equations. As examples to illustrate the corresponding trace
formalism in the quantum fluctuation case, we consider collisional Brownian
motion of an infinite mass Brownian particle, and the weak coupling Born-Markov
master equation. In different specializations, the latter gives the hierarchies
generalizing the quantum optical master equation and the Caldeira--Leggett
master equation. As a further application of the density tensor, we contrast
stochastic Schr\""odinger equations that reduce and that do not reduce the state
vector, and discuss why a quantum system coupled to a quantum environment
behaves like the latter.
"
stat,Driven activation versus thermal activation,"  Activated dynamics in a glassy system undergoing steady shear deformation is
studied by numerical simulations. Our results show that the external driving
force has a strong influence on the barrier crossing rate, even though the
reaction coordinate is only weakly coupled to the nonequilibrium system. This
""driven activation"" can be quantified by introducing in the Arrhenius
expression an effective temperature, which is close to the one determined from
the fluctuation-dissipation relation. This conclusion is supported by
analytical results for a simplified model system.
"
stat,"Dependence of the Critical Adsorption Point on Surface and Sequence
  Disorders for Self-Avoiding Walks Interacting with a Planar Surface","  The critical adsorption point (CAP) of self-avoiding walks (SAW) interacting
with a planar surface with surface disorder or sequence disorder has been
studied. We present theoretical equations, based on ones previously developed
by Soteros and Whittington (J. Phys. A.: Math. Gen. 2004, 37, R279-R325), that
describe the dependence of CAP on the disorders along with Monte Carlo
simulation data that are in agreement with the equations. We also show
simulation results that deviate from the equations when the approximations used
in the theory break down. Such knowledge is the first step toward understanding
the correlation of surface disorder and sequence disorder during polymer
adsorption.
"
stat,Application of Ewald summations to long-range dispersion forces,"  We present results illustrating the effects of using explicit summation terms
for the $r^{-6}$ dispersion term on the interfacial properties of a
Lennard-Jones fluid and SPC/E water. For the Lennard-Jones fluid, we find that
the use of long-range summations, even with a short ``crossover radius,''
yields results that are consistent with simulations using large cutoff radii.
Simulations of SPC/E water demonstrate that the long-range dispersion forces
are of secondary importance to the Coulombic forces. In both cases, we find
that the ratio of box size $L_{\parallel}$ to crossover radius $r_{\rm
c}^{\mathbf k}$ plays an important role in determining the magnitude of the
long-range dispersion correction, although its effect is secondary when
Coulombic interactions are also present.
"
stat,"Stable oscillations of a predator-prey probabilistic cellular automaton:
  a mean-field approach","  We analyze a probabilistic cellular automaton describing the dynamics of
coexistence of a predator-prey system. The individuals of each species are
localized over the sites of a lattice and the local stochastic updating rules
are inspired on the processes of the Lotka-Volterra model. Two levels of
mean-field approximations are set up. The simple approximation is equivalent to
an extended patch model, a simple metapopulation model with patches colonized
by prey, patches colonized by predators and empty patches. This approximation
is capable of describing the limited available space for species occupancy. The
pair approximation is moreover able to describe two types of coexistence of
prey and predators: one where population densities are constant in time and
another displaying self-sustained time-oscillations of the population
densities. The oscillations are associated with limit cycles and arise through
a Hopf bifurcation. They are stable against changes in the initial conditions
and, in this sense, they differ from the Lotka-Volterra cycles which depend on
initial conditions. In this respect, the present model is biologically more
realistic than the Lotka-Volterra model.
"
stat,Spinor Dynamics in an Antiferromagnetic Spin-1 Condensate,"  We observe coherent spin oscillations in an antiferromagnetic spin-1
Bose-Einstein condensate of sodium. The variation of the spin oscillations with
magnetic field shows a clear signature of nonlinearity, in agreement with
theory, which also predicts anharmonic oscillations near a critical magnetic
field. Measurements of the magnetic phase diagram agree with predictions made
in the approximation of a single spatial mode. The oscillation period yields
the best measurement to date of the sodium spin-dependent interaction
coefficient, determining that the difference between the sodium spin-dependent
s-wave scattering lengths $a_{f=2}-a_{f=0}$ is $2.47\pm0.27$ Bohr radii.
"
stat,Stochastic action principle and maximum entropy,"  A stochastic action principle for stochastic dynamics is revisited. We
present first numerical diffusion experiments showing that the diffusion path
probability depend exponentially on average Lagrangian action. This result is
then used to derive an uncertainty measure defined in a way mimicking the heat
or entropy in the first law of thermodynamics. It is shown that the path
uncertainty (or path entropy) can be measured by the Shannon information and
that the maximum entropy principle and the least action principle of classical
mechanics can be unified into a concise form. It is argued that this action
principle, hence the maximum entropy principle, is simply a consequence of the
mechanical equilibrium condition extended to the case of stochastic dynamics.
"
stat,Real Options for Project Schedules (ROPS),"  Real Options for Project Schedules (ROPS) has three recursive
sampling/optimization shells. An outer Adaptive Simulated Annealing (ASA)
optimization shell optimizes parameters of strategic Plans containing multiple
Projects containing ordered Tasks. A middle shell samples probability
distributions of durations of Tasks. An inner shell samples probability
distributions of costs of Tasks. PATHTREE is used to develop options on
schedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to
develop a relative risk analysis among projects.
"
stat,Quantifying social group evolution,"  The rich set of interactions between individuals in the society results in
complex community structure, capturing highly connected circles of friends,
families, or professional cliques in a social network. Thanks to frequent
changes in the activity and communication patterns of individuals, the
associated social and communication network is subject to constant evolution.
Our knowledge of the mechanisms governing the underlying community dynamics is
limited, but is essential for a deeper understanding of the development and
self-optimisation of the society as a whole. We have developed a new algorithm
based on clique percolation, that allows, for the first time, to investigate
the time dependence of overlapping communities on a large scale and as such, to
uncover basic relationships characterising community evolution. Our focus is on
networks capturing the collaboration between scientists and the calls between
mobile phone users. We find that large groups persist longer if they are
capable of dynamically altering their membership, suggesting that an ability to
change the composition results in better adaptability. The behaviour of small
groups displays the opposite tendency, the condition for stability being that
their composition remains unchanged. We also show that the knowledge of the
time commitment of the members to a given community can be used for estimating
the community's lifetime. These findings offer a new view on the fundamental
differences between the dynamics of small groups and large institutions.
"
stat,Equation-free implementation of statistical moment closures,"  We present a general numerical scheme for the practical implementation of
statistical moment closures suitable for modeling complex, large-scale,
nonlinear systems. Building on recently developed equation-free methods, this
approach numerically integrates the closure dynamics, the equations of which
may not even be available in closed form. Although closure dynamics introduce
statistical assumptions of unknown validity, they can have significant
computational advantages as they typically have fewer degrees of freedom and
may be much less stiff than the original detailed model. The closure method can
in principle be applied to a wide class of nonlinear problems, including
strongly-coupled systems (either deterministic or stochastic) for which there
may be no scale separation. We demonstrate the equation-free approach for
implementing entropy-based Eyink-Levermore closures on a nonlinear stochastic
partial differential equation.
"
stat,Crossover behavior in fluids with Coulomb interactions,"  According to extensive experimental findings, the Ginzburg temperature
$t_{G}$ for ionic fluids differs substantially from that of nonionic fluids
[Schr\""oer W., Weig\""{a}rtner H. 2004 {\it Pure Appl. Chem.} {\bf 76} 19]. A
theoretical investigation of this outcome is proposed here by a mean field
analysis of the interplay of short and long range interactions on the value of
$t_{G}$. We consider a quite general continuous charge-asymmetric model made of
charged hard spheres with additional short-range interactions (without
electrostatic interactions the model belongs to the same universality class as
the 3D Ising model). The effective Landau-Ginzburg Hamiltonian of the full
system near its gas-liquid critical point is derived from which the Ginzburg
temperature is calculated as a function of the ionicity. The results obtained
in this way for $t_{G}$ are in good qualitative and sufficient quantitative
agreement with available experimental data.
"
stat,Probability distributions generated by fractional diffusion equations,"  Fractional calculus allows one to generalize the linear, one-dimensional,
diffusion equation by replacing either the first time derivative or the second
space derivative by a derivative of fractional order. The fundamental solutions
of these equations provide probability density functions, evolving on time or
variable in space, which are related to the class of stable distributions. This
property is a noteworthy generalization of what happens for the standard
diffusion equation and can be relevant in treating financial and economical
problems where the stable probability distributions play a key role.
"
stat,When the Cramer-Rao Inequality provides no information,"  We investigate a one-parameter family of probability densities (related to
the Pareto distribution, which describes many natural phenomena) where the
Cramer-Rao inequality provides no information.
"
stat,"Ising-like dynamics and frozen states in systems of ultrafine magnetic
  particles","  We use Monte-Carlo simulations to study aging phenomena and the occurence of
spinglass phases in systems of single-domain ferromagnetic nanoparticles under
the combined influence of dipolar interaction and anisotropy energy, for
different combinations of positional and orientational disorder. We find that
the magnetic moments oriente themselves preferably parallel to their anisotropy
axes and changes of the total magnetization are solely achieved by 180 degree
flips of the magnetic moments, as in Ising systems. Since the dipolar
interaction favorizes the formation of antiparallel chain-like structures,
antiparallel chain-like patterns are frozen in at low temperatures, leading to
aging phenomena characteristic for spin-glasses. Contrary to the intuition,
these aging effects are more pronounced in ordered than in disordered
structures.
"
stat,"The dissolution of the vacancy gas and grain boundary diffusion in
  crystalline solids","  Based on the formula for the number density of vacancies in a solid under the
stress or tension, the model of grain boundary diffusion in crystalline solids
is developed. We obtain the activation energy of grain boundary diffusion
(dependent on the surface tension or the energy of the grain boundary) and also
the distributions of vacancies and the diffusing species in the vicinity of the
grain boundary.
"
stat,"Monitoring spatially heterogeneous dynamics in a drying colloidal thin
  film","  We report on a new type of experiment that enables us to monitor spatially
and temporally heterogeneous dynamic properties in complex fluids. Our approach
is based on the analysis of near-field speckles produced by light diffusely
reflected from the superficial volume of a strongly scattering medium. By
periodic modulation of an incident speckle beam we obtain pixel-wise ensemble
averages of the structure function coefficient, a measure of the dynamic
activity. To illustrate the application of our approach we follow the different
stages in the drying process of a colloidal thin film. We show that we can
access ensemble averaged dynamic properties on length scales as small as ten
micrometers over the full field of view.
"
stat,Growing Perfect Decagonal Quasicrystals by Local Rules,"  A local growth algorithm for a decagonal quasicrystal is presented. We show
that a perfect Penrose tiling (PPT) layer can be grown on a decapod tiling
layer by a three dimensional (3D) local rule growth. Once a PPT layer begins to
form on the upper layer, successive 2D PPT layers can be added on top resulting
in a perfect decagonal quasicrystalline structure in bulk with a point defect
only on the bottom surface layer. Our growth rule shows that an ideal
quasicrystal structure can be constructed by a local growth algorithm in 3D,
contrary to the necessity of non-local information for a 2D PPT growth.
"
stat,"Metropolis algorithm and equienergy sampling for two mean field spin
  systems","  In this paper we study the Metropolis algorithm in connection with two
mean--field spin systems, the so called mean--field Ising model and the
Blume--Emery--Griffiths model. In both this examples the naive choice of
proposal chain gives rise, for some parameters, to a slowly mixing Metropolis
chain, that is a chain whose spectral gap decreases exponentially fast (in the
dimension $N$ of the problem). Here we show how a slight variant in the
proposal chain can avoid this problem, keeping the mean computational cost
similar to the cost of the usual Metropolis. More precisely we prove that, with
a suitable variant in the proposal, the Metropolis chain has a spectral gap
which decreases polynomially in 1/N. Using some symmetry structure of the
energy, the method rests on allowing appropriate jumps within the energy level
of the starting state.
"
stat,"Preferential interaction coefficient for nucleic acids and other
  cylindrical poly-ions","  The thermodynamics of nucleic acid processes is heavily affected by the
electric double-layer of micro-ions around the polyions. We focus here on the
Coulombic contribution to the salt-polyelectrolyte preferential interaction
(Donnan) coefficient and we report extremely accurate analytical expressions
valid in the range of low salt concentration (when polyion radius is smaller
than the Debye length). The analysis is performed at Poisson-Boltzmann level,
in cylindrical geometry, with emphasis on highly charged poly-ions (beyond
``counter-ion condensation''). The results hold for any electrolyte of the form
$z_-$:$z_+$. We also obtain a remarkably accurate expression for the electric
potential in the vicinity of the poly-ion.
"
stat,Model C critical dynamics of random anisotropy magnets,"  We study the relaxational critical dynamics of the three-dimensional random
anisotropy magnets with the non-conserved n-component order parameter coupled
to a conserved scalar density. In the random anisotropy magnets the structural
disorder is present in a form of local quenched anisotropy axes of random
orientation. When the anisotropy axes are randomly distributed along the edges
of the n-dimensional hypercube, asymptotical dynamical critical properties
coincide with those of the random-site Ising model. However structural disorder
gives rise to considerable effects for non-asymptotic critical dynamics. We
investigate this phenomenon by a field-theoretical renormalization group
analysis in the two-loop order. We study critical slowing down and obtain
quantitative estimates for the effective and asymptotic critical exponents of
the order parameter and scalar density. The results predict complex scenarios
for the effective critical exponent approaching an asymptotic regime.
"
stat,Recurrence analysis of the Portevin-Le Chatelier effect,"  Tensile tests were carried out by deforming polycrystalline samples of
Al-2.5%Mg alloy at room temperature in a wide range of strain rates where the
Portevin-Le Chatelier (PLC) effect was observed. The experimental stress-time
series data have been analyzed using the recurrence analysis technique based on
the Recurrence Plot (RP) and the Recurrence Quantification Analysis (RQA) to
study the change in the dynamical behavior of the PLC effect with the imposed
strain rate. Our study revealed that the RQA is able to detect the unique
crossover phenomenon in the PLC dynamics.
"
stat,Fluctuations in glassy systems,"  We summarize a theoretical framework based on global time-reparametrization
invariance that explains the origin of dynamic fluctuations in glassy systems.
We introduce the main ideas without getting into much technical details. We
describe a number of consequences arising from this scenario that can be tested
numerically and experimentally distinguishing those that can also be explained
by other mechanisms from the ones that we believe, are special to our proposal.
We support our claims by presenting some numerical checks performed on the 3d
Edwards-Anderson spin-glass. Finally, we discuss up to which extent these ideas
apply to super-cooled liquids that have been studied in much more detail up to
present.
"
stat,"Solutions of fractional reaction-diffusion equations in terms of the
  H-function","  This paper deals with the investigation of the solution of an unified
fractional reaction-diffusion equation associated with the Caputo derivative as
the time-derivative and Riesz-Feller fractional derivative as the
space-derivative. The solution is derived by the application of the Laplace and
Fourier transforms in closed form in terms of the H-function. The results
derived are of general nature and include the results investigated earlier by
many authors, notably by Mainardi et al. (2001, 2005) for the fundamental
solution of the space-time fractional diffusion equation, and Saxena et al.
(2006a, b) for fractional reaction- diffusion equations. The advantage of using
Riesz-Feller derivative lies in the fact that the solution of the fractional
reaction-diffusion equation containing this derivative includes the fundamental
solution for space-time fractional diffusion, which itself is a generalization
of neutral fractional diffusion, space-fractional diffusion, and
time-fractional diffusion. These specialized types of diffusion can be
interpreted as spatial probability density functions evolving in time and are
expressible in terms of the H-functions in compact form.
"
stat,"Using decomposed household food acquisitions as inputs of a Kinetic
  Dietary Exposure Model","  Foods naturally contain a number of contaminants that may have different and
long term toxic effects. This paper introduces a novel approach for the
assessment of such chronic food risk that integrates the pharmacokinetic
properties of a given contaminant. The estimation of such a Kinetic Dietary
Exposure Model (KDEM) should be based on long term consumption data which, for
the moment, can only be provided by Household Budget Surveys such as the
SECODIP panel in France. A semi parametric model is proposed to decompose a
series of household quantities into individual quantities which are then used
as inputs of the KDEM. As an illustration, the risk assessment related to the
presence of methyl mercury in seafood is revisited using this novel approach.
"
stat,Algebraic geometry of Gaussian Bayesian networks,"  Conditional independence models in the Gaussian case are algebraic varieties
in the cone of positive definite covariance matrices. We study these varieties
in the case of Bayesian networks, with a view towards generalizing the
recursive factorization theorem to situations with hidden variables. In the
case when the underlying graph is a tree, we show that the vanishing ideal of
the model is generated by the conditional independence statements implied by
graph. We also show that the ideal of any Bayesian network is homogeneous with
respect to a multigrading induced by a collection of upstream random variables.
This has a number of important consequences for hidden variable models.
Finally, we relate the ideals of Bayesian networks to a number of classical
constructions in algebraic geometry including toric degenerations of the
Grassmannian, matrix Schubert varieties, and secant varieties.
"
stat,"Strong Spherical Asymptotics for Rotor-Router Aggregation and the
  Divisible Sandpile","  The rotor-router model is a deterministic analogue of random walk. It can be
used to define a deterministic growth model analogous to internal DLA. We prove
that the asymptotic shape of this model is a Euclidean ball, in a sense which
is stronger than our earlier work. For the shape consisting of $n=\omega_d r^d$
sites, where $\omega_d$ is the volume of the unit ball in $\R^d$, we show that
the inradius of the set of occupied sites is at least $r-O(\log r)$, while the
outradius is at most $r+O(r^\alpha)$ for any $\alpha > 1-1/d$. For a related
model, the divisible sandpile, we show that the domain of occupied sites is a
Euclidean ball with error in the radius a constant independent of the total
mass. For the classical abelian sandpile model in two dimensions, with $n=\pi
r^2$ particles, we show that the inradius is at least $r/\sqrt{3}$, and the
outradius is at most $(r+o(r))/\sqrt{2}$. This improves on bounds of Le Borgne
and Rossin. Similar bounds apply in higher dimensions.
"
stat,A General Nonlinear Fokker-Planck Equation and its Associated Entropy,"  A recently introduced nonlinear Fokker-Planck equation, derived directly from
a master equation, comes out as a very general tool to describe
phenomenologically systems presenting complex behavior, like anomalous
diffusion, in the presence of external forces. Such an equation is
characterized by a nonlinear diffusion term that may present, in general, two
distinct powers of the probability distribution. Herein, we calculate the
stationary-state distributions of this equation in some special cases, and
introduce associated classes of generalized entropies in order to satisfy the
H-theorem. Within this approach, the parameters associated with the transition
rates of the original master-equation are related to such generalized
entropies, and are shown to obey some restrictions. Some particular cases are
discussed.
"
stat,Gibbs fragmentation trees,"  We study fragmentation trees of Gibbs type. In the binary case, we identify
the most general Gibbs-type fragmentation tree with Aldous' beta-splitting
model, which has an extended parameter range $\beta>-2$ with respect to the
${\rm beta}(\beta+1,\beta+1)$ probability distributions on which it is based.
In the multifurcating case, we show that Gibbs fragmentation trees are
associated with the two-parameter Poisson--Dirichlet models for exchangeable
random partitions of $\mathbb {N}$, with an extended parameter range
$0\le\alpha\le1$, $\theta\ge-2\alpha$ and $\alpha<0$, $\theta =-m\alpha$, $m\in
\mathbb {N}$.
"
stat,Neel order in the two-dimensional S=1/2 Heisenberg Model,"  The existence of Neel order in the S=1/2 Heisenberg model on the square
lattice at T=0 is shown using inequalities set up by Kennedy, Lieb and Shastry
in combination with high precision Quantum Monte Carlo data.
"
stat,"Integral representations for convolutions of non-central multivariate
  gamma distributions","  Three types of integral representations for the cumulative distribution
functions of convolutions of non-central p-variate gamma distributions are
given by integration of elementary complex functions over the p-cube Cp =
(-pi,pi]x...x(-pi,pi]. In particular, the joint distribution of the diagonal
elements of a generalized quadratic form XAX' with n independent normally
distributed column vectors in X is obtained. For a single p-variate gamma
distribution function (p-1)-variate integrals over Cp-1 are derived. The
integrals are numerically more favourable than integrals obtained from the
Fourier or laplace inversion formula.
"
stat,Some aspects of the nonperturbative renormalization of the phi^4 model,"  A nonperturbative renormalization of the phi^4 model is considered. First we
integrate out only a single pair of conjugated modes with wave vectors +/- q.
Then we are looking for the RG equation which would describe the transformation
of the Hamiltonian under the integration over a shell Lambda - d Lambda < k <
Lambda, where d Lambda -> 0. We show that the known Wegner--Houghton equation
is consistent with the assumption of a simple superposition of the integration
results for +/- q. The renormalized action can be expanded in powers of the
phi^4 coupling constant u in the high temperature phase at u -> 0. We compare
the expansion coefficients with those exactly calculated by the diagrammatic
perturbative method, and find some inconsistency. It causes a question in which
sense the Wegner-Houghton equation is really exact.
"
stat,Capturing knots in polymers,"  This paper visualizes a knot reduction algorithm
"
stat,"Optimal stimulus and noise distributions for information transmission
  via suprathreshold stochastic resonance","  Suprathreshold stochastic resonance (SSR) is a form of noise enhanced signal
transmission that occurs in a parallel array of independently noisy identical
threshold nonlinearities, including model neurons. Unlike most forms of
stochastic resonance, the output response to suprathreshold random input
signals of arbitrary magnitude is improved by the presence of even small
amounts of noise. In this paper the information transmission performance of SSR
in the limit of a large array size is considered. Using a relationship between
Shannon's mutual information and Fisher information, a sufficient condition for
optimality, i.e. channel capacity, is derived. It is shown that capacity is
achieved when the signal distribution is Jeffrey's prior, as formed from the
noise distribution, or when the noise distribution depends on the signal
distribution via a cosine relationship. These results provide theoretical
verification and justification for previous work in both computational
neuroscience and electronics.
"
stat,A new approach to mutual information,"  A new expression as a certain asymptotic limit via ""discrete micro-states"" of
permutations is provided to the mutual information of both continuous and
discrete random variables.
"
stat,Approaching equilibrium and the distribution of clusters,"  We investigate the approach to stable and metastable equilibrium in Ising
models using a cluster representation. The distribution of nucleation times is
determined using the Metropolis algorithm and the corresponding $\phi^{4}$
model using Langevin dynamics. We find that the nucleation rate is suppressed
at early times even after global variables such as the magnetization and energy
have apparently reached their time independent values. The mean number of
clusters whose size is comparable to the size of the nucleating droplet becomes
time independent at about the same time that the nucleation rate reaches its
constant value. We also find subtle structural differences between the
nucleating droplets formed before and after apparent metastable equilibrium has
been established.
"
stat,"Density matrix elements and entanglement entropy for the spin-1/2 XXZ
  chain at $\Delta$=1/2","  We have analytically obtained all the density matrix elements up to six
lattice sites for the spin-1/2 Heisenberg XXZ chain at $\Delta=1/2$. We use the
multiple integral formula of the correlation function for the massless XXZ
chain derived by Jimbo and Miwa. As for the spin-spin correlation functions, we
have newly obtained the fourth- and fifth-neighbour transverse correlation
functions. We have calculated all the eigenvalues of the density matrix and
analyze the eigenvalue-distribution. Using these results the exact values of
the entanglement entropy for the reduced density matrix up six lattice sites
have been obtained. We observe that our exact results agree quite well with the
asymptotic formula predicted by the conformal field theory.
"
stat,"Competitive nucleation and the Ostwald rule in a generalized Potts model
  with multiple metastable phases","  We introduce a simple nearest-neighbor spin model with multiple metastable
phases, the number and decay pathways of which are explicitly controlled by the
parameters of the system. With this model we can construct, for example, a
system which evolves through an arbitrarily long succession of metastable
phases. We also construct systems in which different phases may nucleate
competitively from a single initial phase. For such a system, we present a
general method to extract from numerical simulations the individual nucleation
rates of the nucleating phases. The results show that the Ostwald rule, which
predicts which phase will nucleate, must be modified probabilistically when the
new phases are almost equally stable. Finally, we show that the nucleation rate
of a phase depends, among other things, on the number of other phases
accessible from it.
"
stat,A Topological Glass,"  We propose and study a model with glassy behavior. The state space of the
model is given by all triangulations of a sphere with $n$ nodes, half of which
are red and half are blue. Red nodes want to have 5 neighbors while blue ones
want 7. Energies of nodes with different numbers of neighbors are supposed to
be positive. The dynamics is that of flipping the diagonal of two adjacent
triangles, with a temperature dependent probability. We show that this system
has an approach to a steady state which is exponentially slow, and show that
the stationary state is unordered. We also study the local energy landscape and
show that it has the hierarchical structure known from spin glasses. Finally,
we show that the evolution can be described as that of a rarefied gas with
spontaneous generation of particles and annihilating collisions.
"
stat,"Phase structure of a surface model on dynamically triangulated spheres
  with elastic skeletons","  We find three distinct phases; a tubular phase, a planar phase, and the
spherical phase, in a triangulated fluid surface model. It is also found that
these phases are separated by discontinuous transitions. The fluid surface
model is investigated within the framework of the conventional curvature model
by using the canonical Monte Carlo simulations with dynamical triangulations.
The mechanical strength of the surface is given only by skeletons, and no
two-dimensional bending energy is assumed in the Hamiltonian. The skeletons are
composed of elastic linear-chains and rigid junctions and form a
compartmentalized structure on the surface, and for this reason the vertices of
triangles can diffuse freely only inside the compartments. As a consequence, an
inhomogeneous structure is introduced in the model; the surface strength inside
the compartments is different from the surface strength on the compartments.
However, the rotational symmetry is not influenced by the elastic skeletons;
there is no specific direction on the surface. In addition to the three phases
mentioned above, a collapsed phase is expected to exist in the low bending
rigidity regime that was not studied here. The inhomogeneous structure and the
fluidity of vertices are considered to be the origin of such variety of phases.
"
stat,"Confinement into a state with persistent current by thermal quenching of
  loop of Josephson junctions","  We study a loop of Josephson junctions that is quenched through its critical
temperature. For three or more junctions, symmetry breaking states can be
achieved without thermal activation, in spite of the fact that the relaxation
time is practically constant when the critical temperature is approached from
above. The probability for these states decreases with quenching time, but the
dependence is not allometric. For large number of junctions, cooling does not
have to be fast. For this case, we evaluate the standard deviation of the
induced flux. Our results are consistent with the available experimental data.
"
stat,Domain Wall Dynamics near a Quantum Critical Point,"  We study the real-time domain-wall dynamics near a quantum critical point of
the one-dimensional anisotropic ferromagnetic spin 1/2 chain. By numerical
simulation, we find the domain wall is dynamically stable in the
Heisenberg-Ising model. Near the quantum critical point, the width of the
domain wall diverges as $(\Delta -1) ^{-1/2}$.
"
stat,"Correlation functions in the Non Perturbative Renormalization Group and
  field expansion","  The usual procedure of including a finite number of vertices in Non
Perturbative Renormalization Group equations in order to obtain $n$-point
correlation functions at finite momenta is analyzed. This is done by exploiting
a general method recently introduced which includes simultaneously all vertices
although approximating their momentum dependence. The study is performed using
the self-energy of the tridimensional scalar model at criticality. At least in
this example, low order truncations miss quantities as the critical exponent
$\eta$ by as much as 60%. However, if one goes to high order truncations the
procedure seems to converge rapidly.
"
stat,"Vortex-induced topological transition of the bilinear-biquadratic
  Heisenberg antiferromagnet on the triangular lattice","  The ordering of the classical Heisenberg antiferromagnet on the triangular
lattice with the the bilinear-biquadratic interaction is studied by Monte Carlo
simulations. It is shown that the model exhibits a topological phase transition
at a finite-temperature driven by topologically stable vortices, while the spin
correlation length remains finite even at and below the transition point. The
relevant vortices could be of three different types, depending on the value of
the biquadratic coupling. Implications to recent experiments on the triangular
antiferromagnet NiGa$_2$S$_4$ is discussed.
"
stat,Nonequilibrium entropy limiters in lattice Boltzmann methods,"  We construct a system of nonequilibrium entropy limiters for the lattice
Boltzmann methods (LBM). These limiters erase spurious oscillations without
blurring of shocks, and do not affect smooth solutions. In general, they do the
same work for LBM as flux limiters do for finite differences, finite volumes
and finite elements methods, but for LBM the main idea behind the construction
of nonequilibrium entropy limiter schemes is to transform a field of a scalar
quantity - nonequilibrium entropy. There are two families of limiters: (i)
based on restriction of nonequilibrium entropy (entropy ""trimming"") and (ii)
based on filtering of nonequilibrium entropy (entropy filtering). The physical
properties of LBM provide some additional benefits: the control of entropy
production and accurate estimate of introduced artificial dissipation are
possible. The constructed limiters are tested on classical numerical examples:
1D athermal shock tubes with an initial density ratio 1:2 and the 2D lid-driven
cavity for Reynolds numbers Re between 2000 and 7500 on a coarse 100*100 grid.
All limiter constructions are applicable for both entropic and non-entropic
quasiequilibria.
"
stat,The 3D +-J Ising model at the ferromagnetic transition line,"  We study the critical behavior of the three-dimensional $\pm J$ Ising model
[with a random-exchange probability $P(J_{xy}) = p \delta(J_{xy} - J) + (1-p)
\delta(J_{xy} + J)$] at the transition line between the paramagnetic and
ferromagnetic phase, which extends from $p=1$ to a multicritical (Nishimori)
point at $p=p_N\approx 0.767$. By a finite-size scaling analysis of Monte Carlo
simulations at various values of $p$ in the region $p_N<p<1$, we provide strong
numerical evidence that the critical behavior along the ferromagnetic
transition line belongs to the same universality class as the three-dimensional
randomly-dilute Ising model. We obtain the results $\nu=0.682(3)$ and
$\eta=0.036(2)$ for the critical exponents, which are consistent with the
estimates $\nu=0.683(2)$ and $\eta=0.036(1)$ at the transition of
randomly-dilute Ising models.
"
stat,"Spectroscopic Properties of Polarons in Strongly Correlated Systems by
  Exact Diagrammatic Monte Carlo Method","  We present recent advances in understanding of the ground and excited states
of the electron-phonon coupled systems obtained by novel methods of
Diagrammatic Monte Carlo and Stochastic Optimization, which enable the
approximation-free calculation of Matsubara Green function in imaginary times
and perform unbiased analytic continuation to real frequencies. We present
exact numeric results on the ground state properties, Lehmann spectral function
and optical conductivity of different strongly correlated systems: Frohlich
polaron, Rashba-Pekar exciton-polaron, pseudo Jahn-Teller polaron, exciton, and
interacting with phonons hole in the t-J model.
"
stat,The S-Matrix of AdS/CFT and Yangian Symmetry,"  We review the algebraic construction of the S-matrix of AdS/CFT. We also
present its symmetry algebra which turns out to be a Yangian of the centrally
extended su(2|2) superalgebra.
"
stat,"Alternative Approaches to the Equilibrium Properties of Hard-Sphere
  Liquids","  An overview of some analytical approaches to the computation of the
structural and thermodynamic properties of single component and multicomponent
hard-sphere fluids is provided. For the structural properties, they yield a
thermodynamically consistent formulation, thus improving and extending the
known analytical results of the Percus-Yevick theory. Approximate expressions
for the contact values of the radial distribution functions and the
corresponding analytical equations of state are also discussed. Extensions of
this methodology to related systems, such as sticky hard spheres and
square-well fluids, as well as its use in connection with the perturbation
theory of fluids are briefly addressed.
"
stat,"Fluctuation-dissipation relation on a Melde string in a turbulent flow,
  considerations on a ""dynamical temperature""","  We report on measurements of the transverse fluctuations of a string in a
turbulent air jet flow. Harmonic modes are excited by the fluctuating drag
force, at different wave-numbers. This simple mechanical probe makes it
possible to measure excitations of the flow at specific scales, averaged over
space and time: it is a scale-resolved, global measurement. We also measure the
dissipation associated to the string motion, and we consider the ratio of the
fluctuations over dissipation (FDR). In an exploratory approach, we investigate
the concept of {\it effective temperature} defined through the FDR. We compare
our observations with other definitions of temperature in turbulence. From the
theory of Kolmogorov (1941), we derive the exponent -11/3 expected for the
spectrum of the fluctuations. This simple model and our experimental results
are in good agreement, over the range of wave-numbers, and Reynolds number
accessible ($74000 \leq Re \leq 170000$).
"
stat,Spline Single-Index Prediction Model,"  For the past two decades, single-index model, a special case of projection
pursuit regression, has proven to be an efficient way of coping with the high
dimensional problem in nonparametric regression. In this paper, based on weakly
dependent sample, we investigate the single-index prediction (SIP) model which
is robust against deviation from the single-index model. The single-index is
identified by the best approximation to the multivariate prediction function of
the response variable, regardless of whether the prediction function is a
genuine single-index function. A polynomial spline estimator is proposed for
the single-index prediction coefficients, and is shown to be root-n consistent
and asymptotically normal. An iterative optimization routine is used which is
sufficiently fast for the user to analyze large data of high dimension within
seconds. Simulation experiments have provided strong evidence that corroborates
with the asymptotic theory. Application of the proposed procedure to the rive
flow data of Iceland has yielded superior out-of-sample rolling forecasts.
"
stat,"An individual based model with global competition interaction:
  fluctuations effects in pattern formation","  We present some numerical results obtained from a simple individual based
model that describes clustering of organisms caused by competition. Our aim is
to show how, even when a deterministic description developed for continuum
models predicts no pattern formation, an individual based model displays well
defined patterns, as a consequence of fluctuations effects caused by the
discrete nature of the interacting agents.
"
stat,On generalized entropy measures and pathways,"  Product probability property, known in the literature as statistical
independence, is examined first. Then generalized entropies are introduced, all
of which give generalizations to Shannon entropy. It is shown that the nature
of the recursivity postulate automatically determines the logarithmic
functional form for Shannon entropy. Due to the logarithmic nature, Shannon
entropy naturally gives rise to additivity, when applied to situations having
product probability property. It is argued that the natural process is
non-additivity, important, for example, in statistical mechanics, even in
product probability property situations and additivity can hold due to the
involvement of a recursivity postulate leading to a logarithmic function.
Generalizations, including Mathai's generalized entropy are introduced and some
of the properties are examined. Situations are examined where Mathai's entropy
leads to pathway models, exponential and power law behavior and related
differential equations. Connection of Mathai's entropy to Kerridge's measure of
""inaccuracy"" is also explored.
"
stat,Connected Operators for the Totally Asymmetric Exclusion Process,"  We fully elucidate the structure of the hierarchy of the connected operators
that commute with the Markov matrix of the Totally Asymmetric Exclusion Process
(TASEP). We prove for the connected operators a combinatorial formula that was
conjectured in a previous work. Our derivation is purely algebraic and relies
on the algebra generated by the local jump operators involved in the TASEP.
  Keywords: Non-Equilibrium Statistical Mechanics, ASEP, Exact Results,
Algebraic Bethe Ansatz.
"
stat,Failure of the work-Hamiltonian connection for free energy calculations,"  Extensions of statistical mechanics are routinely being used to infer free
energies from the work performed over single-molecule nonequilibrium
trajectories. A key element of this approach is the ubiquitous expression
dW/dt=\partial H(x,t)/ \partial t which connects the microscopic work W
performed by a time-dependent force on the coordinate x with the corresponding
Hamiltonian H(x,t) at time t. Here we show that this connection, as pivotal as
it is, cannot be used to estimate free energy changes. We discuss the
implications of this result for single-molecule experiments and atomistic
molecular simulations and point out possible avenues to overcome these
limitations.
"
stat,Multi-Higgs U(1) Lattice Gauge Theory in Three Dimensions,"  We study the three-dimensional compact U(1) lattice gauge theory with $N$
Higgs fields numerically. This model is relevant to multi-component
superconductors, antiferromagnetic spin systems in easy plane, inflational
cosmology, etc. For N=2, the system has a second-order phase transition line
$\tilde{c}_1(c_2)$ in the $c_2$(gauge coupling)$-c_1$(Higgs coupling) plane,
which separates the confinement phase and the Higgs phase. For N=3, the
critical line is separated into two parts; one for $c_2 \alt 2.25$ with
first-order transitions, and the other for $c_2 \agt 2.25$ with second-order
transitions.
"
stat,"Rounding of first-order phase transitions and optimal cooperation in
  scale-free networks","  We consider the ferromagnetic large-$q$ state Potts model in complex evolving
networks, which is equivalent to an optimal cooperation problem, in which the
agents try to optimize the total sum of pair cooperation benefits and the
supports of independent projects. The agents are found to be typically of two
kinds: a fraction of $m$ (being the magnetization of the Potts model) belongs
to a large cooperating cluster, whereas the others are isolated one man's
projects. It is shown rigorously that the homogeneous model has a strongly
first-order phase transition, which turns to second-order for random
interactions (benefits), the properties of which are studied numerically on the
Barab\'asi-Albert network. The distribution of finite-size transition points is
characterized by a shift exponent, $1/\tilde{\nu}'=.26(1)$, and by a different
width exponent, $1/\nu'=.18(1)$, whereas the magnetization at the transition
point scales with the size of the network, $N$, as: $m\sim N^{-x}$, with
$x=.66(1)$.
"
stat,Critical Scaling of Shear Viscosity at the Jamming Transition,"  We carry out numerical simulations to study transport behavior about the
jamming transition of a model granular material in two dimensions at zero
temperature. Shear viscosity \eta is computed as a function of particle volume
density \rho and applied shear stress \sigma, for diffusively moving particles
with a soft core interaction. We find an excellent scaling collapse of our data
as a function of the scaling variable \sigma/|\rho_c-\rho|^\Delta, where \rho_c
is the critical density at \sigma=0 (""point J""), and \Delta is the crossover
scaling critical exponent. Our results show that jamming is a true critical
phenomenon, extending to driven steady states along the non-equilibrium \sigma
axis of the \rho-\sigma phase diagram.
"
stat,"Exact distribution of the sample variance from a gamma parent
  distribution","  Several representations of the exact cdf of the sum of squares of n
independent gamma-distributed random variables Xi are given, in particular by a
series of gamma distribution functions. Using a characterization of the gamma
distribution by Laha, an expansion of the exact distribution of the sample
variance is derived by a Taylor series approach with the former distribution as
its leading term. In particular for integer orders alpha some further series
are provided, including a convex combination of gamma distributions for alpha =
1 and nearly of this type for alpha > 1. Furthermore, some representations of
the distribution of the angle Phi between (X1,...,Xn) and (1,...,1) are given
by orthogonal series. All these series are based on the same sequence of easily
computed moments of cos(Phi).
"
stat,Stochastic fluctuations in metabolic pathways,"  Fluctuations in the abundance of molecules in the living cell may affect its
growth and well being. For regulatory molecules (e.g., signaling proteins or
transcription factors), fluctuations in their expression can affect the levels
of downstream targets in a network. Here, we develop an analytic framework to
investigate the phenomenon of noise correlation in molecular networks.
Specifically, we focus on the metabolic network, which is highly inter-linked,
and noise properties may constrain its structure and function. Motivated by the
analogy between the dynamics of a linear metabolic pathway and that of the
exactly soluable linear queueing network or, alternatively, a mass transfer
system, we derive a plethora of results concerning fluctuations in the
abundance of intermediate metabolites in various common motifs of the metabolic
network. For all but one case examined, we find the steady-state fluctuation in
different nodes of the pathways to be effectively uncorrelated. Consequently,
fluctuations in enzyme levels only affect local properties and do not propagate
elsewhere into metabolic networks, and intermediate metabolites can be freely
shared by different reactions. Our approach may be applicable to study
metabolic networks with more complex topologies, or protein signaling networks
which are governed by similar biochemical reactions. Possible implications for
bioinformatic analysis of metabolimic data are discussed.
"
stat,"Recent Results on Thermal Casimir Force between Dielectrics and Related
  Problems","  We review recent results obtained in the physics of the thermal Casimir force
acting between two dielectrics, dielectric and metal, and between metal and
semiconductor. The detailed derivation for the low-temperature behavior of the
Casimir free energy, pressure and entropy in the configuration of two real
dielectric plates is presented. For dielectrics with finite static dielectric
permittivity it is shown that the Nernst heat theorem is satisfied. Hence, the
Lifshitz theory of the van der Waals and Casimir forces is demonstrated to be
consistent with thermodynamics. The nonzero dc conductivity of dielectric
plates is proved to lead to a violation of the Nernst heat theorem and, thus,
is not related to the phenomenon of dispersion forces. The low-temperature
asymptotics of the Casimir free energy, pressure and entropy are derived also
in the configuration of one metal and one dielectric plate. The results are
shown to be consistent with thermodynamics if the dielectric plate possesses a
finite static dielectric permittivity. If the dc conductivity of a dielectric
plate is taken into account this results in the violation of the Nernst heat
theorem. We discuss both the experimental and theoretical results related to
the Casimir interaction between metal and semiconductor with different charge
carrier density. Discussions in the literature on the possible influence of
spatial dispersion on the thermal Casimir force are analyzed. In conclusion,
the conventional Lifshitz theory taking into account only the frequency
dispersion remains the reliable foundation for the interpretation of all
present experiments.
"
stat,Residual entropy in a model for the unfolding of single polymer chains,"  We study the unfolding of a single polymer chain due to an external force. We
use a simplified model which allows to perform all calculations in closed form
without assuming a Boltzmann-Gibbs form for the equilibrium distribution.
Temperature is then defined by calculating the Legendre transform of the
entropy under certain constraints. The application of the model is limited to
flexible polymers. It exhibits a gradual transition from compact globule to
rod. The boundary line between these two phases shows reentrant behavior. This
behavior is explained by the presence of residual entropy.
"
stat,Information-Based Asset Pricing,"  A new framework for asset price dynamics is introduced in which the concept
of noisy information about future cash flows is used to derive the price
processes. In this framework an asset is defined by its cash-flow structure.
Each cash flow is modelled by a random variable that can be expressed as a
function of a collection of independent random variables called market factors.
With each such ""X-factor"" we associate a market information process, the values
of which are accessible to market agents. Each information process is a sum of
two terms; one contains true information about the value of the market factor;
the other represents ""noise"". The noise term is modelled by an independent
Brownian bridge. The market filtration is assumed to be that generated by the
aggregate of the independent information processes. The price of an asset is
given by the expectation of the discounted cash flows in the risk-neutral
measure, conditional on the information provided by the market filtration. When
the cash flows are the dividend payments associated with equities, an explicit
model is obtained for the share-price, and the prices of options on
dividend-paying assets are derived. Remarkably, the resulting formula for the
price of a European call option is of the Black-Scholes-Merton type. The
information-based framework also generates a natural explanation for the origin
of stochastic volatility.
"
stat,"An information-based traffic control in a public conveyance system:
  reduced clustering and enhanced efficiency","  A new public conveyance model applicable to buses and trains is proposed in
this paper by using stochastic cellular automaton. We have found the optimal
density of vehicles, at which the average velocity becomes maximum,
significantly depends on the number of stops and passengers behavior of getting
on a vehicle at stops. The efficiency of the hail-and-ride system is also
discussed by comparing the different behavior of passengers. Moreover, we have
found that a big cluster of vehicles is divided into small clusters, by
incorporating information of the number of vehicles between successive stops.
"
stat,Spin-spin Correlation in Some Excited States of Transverse Ising Model,"  We consider the transverse Ising model in one dimension with
nearest-neighbour interaction and calculate exactly the longitudinal spin-spin
correlation for a class of excited states. These states are known to play an
important role in the perturbative treatment of one-dimensional transverse
Ising model with frustrated second-neighbour interaction. To calculate the
correlation, we follow the earlier procedure of Wu, use Szego's theorem and
also use Fisher-Hartwig conjecture. The result is that the correlation decays
algebraically with distance ($n$) as $1/\surd n$ and is oscillatory or
non-oscillatory depending on the magnitude of the transverse field.
"
stat,Singular Energy Distributions in Granular Media,"  We study the kinetic theory of driven granular gases, taking into account
both translational and rotational degrees of freedom. We obtain the high-energy
tail of the stationary bivariate energy distribution, depending on the total
energy E and the ratio x=sqrt{E_w/E} of rotational energy E_w to total energy.
Extremely energetic particles have a unique and well-defined distribution f(x)
which has several remarkable features: x is not uniformly distributed as in
molecular gases; f(x) is not smooth but has multiple singularities. The latter
behavior is sensitive to material properties such as the collision parameters,
the moment of inertia and the collision rate. Interestingly, there are
preferred ratios of rotational-to-total energy. In general, f(x) is strongly
correlated with energy and the deviations from a uniform distribution grow with
energy. We also solve for the energy distribution of freely cooling Maxwell
Molecules and find qualitatively similar behavior.
"
stat,What is the order of 2D polymer escape transition?,"  An end-grafted flexible polymer chain in 3d space between two pistons
undergoes an abrupt transition from a confined coil to a flower-like
conformation when the number of monomers in the chain, N, reaches a critical
value. In 2d geometry, excluded volume interactions between monomers of a chain
confined inside a strip of finite length 2L transform the coil conformation
into a linear string of blobs. However, the blob picture raises questions on
the nature of this escape transition. To check the theoretical predictions
based on the blob picture we study 2d single polymer chains with excluded
volume interactions and with one end grafted in the middle of a strip of length
  2L and width H by simulating self-avoiding walks on a square lattice with the
pruned-enriched-Rosenbluth method (PERM). We estimate the free energy, the
end-to-end distance, the number of imprisoned monomers, the order parameter,
and its distribution. It is shown that in the thermodynamic limit of large N
and L but finite L/N, there is a small but finite jump in several average
characteristics, including the order parameter. We also present a theoretical
description based on the Landau free energy approach, which is in good
agreement with the simulation results. Both simulation results and the
analytical theory indicate that the 2d escape transition is a weak first-order
phase transition.
"
stat,Balance of forces in simulated bilayers,"  Two kinds of simulated bilayers are described and the results are reported
for lateral tension and for partial contributions of intermolecular forces to
it.Data for a widest possible range of areas per surfactant head, from tunnel
formation through tensionless state, transition to floppy bilayer,to its
disintegration, are reported and discussed. The significance of the tensionless
state, is discussed. Conclusions: (1) the tensionless state is a
coincidence;(2) the transition from extended to floppy bilayer occurs nearby
and has hallmarks of a phase transition (3) there is no theory of that
transition.(4)The lateral tension of the floppy bilayer scales with size; that
of the extended bilayer does not depend on size. (4) The drumhead model not
appropriate for interfaces as these fluctuate via diffusion.(5) The radius of
gyration also! shows a discontinuity.
"
stat,U-max-Statistics,"  In 1948, W. Hoeffding introduced a large class of unbiased estimators called
U-statistics, defined as the average value of a real-valued k-variate function
h calculated at all possible sets of k points from a random sample. In the
present paper we investigate the corresponding extreme value analogue, which we
shall call U-max-statistics. We are concerned with the behavior of the largest
value of such function h instead of its average. Examples of U-max-statistics
are the diameter or the largest scalar product within a random sample.
U-max-statistics of higher degrees are given by triameters and other metric
invariants.
"
stat,"Interplay of Anisotropy and Disorder in the Doping-Dependent Melting and
  Glass Transitions of Vortices in Bi$_2$Sr$_2$CaCu$_2$O$_{8+\delta}$","  We study the oxygen doping dependence of the equilibrium first-order melting
and second-order glass transitions of vortices in
Bi$_2$Sr$_2$CaCu$_2$O$_{8+\delta}$. Doping affects both anisotropy and
disorder. Anisotropy scaling is shown to collapse the melting lines only where
thermal fluctuations are dominant. Yet, in the region where disorder breaks
that scaling, the glass lines are still collapsed. A quantitative fit to
melting and replica symmetry breaking lines of a 2D Ginzburg-Landau model
further reveals that disorder amplitude weakens with doping, but to a lesser
degree than thermal fluctuations, enhancing the relative role of disorder.
"
stat,"Dynamical Equilibrium, trajectories study in an economical system. The
  case of the labor market","  The paper deals with the study of labor market dynamics, and aims to
characterize its equilibriums and possible trajectories. The theoretical
background is the theory of the segmented labor market. The main idea is that
this theory is well adapted to interpret the observed trajectories, due to the
heterogeneity of the work situations.
"
stat,"Excitation Spectrum Gap and Spin-Wave Stiffness of XXZ Heisenberg
  Chains: Global Renormalization-Group Calculation","  The anisotropic XXZ spin-1/2 Heisenberg chain is studied using
renormalization-group theory. The specific heats and nearest-neighbor spin-spin
correlations are calculated thoughout the entire temperature and anisotropy
ranges in both ferromagnetic and antiferromagnetic regions, obtaining a global
description and quantitative results. We obtain, for all anisotropies, the
antiferromagnetic spin-liquid spin-wave velocity and the Isinglike
ferromagnetic excitation spectrum gap, exhibiting the spin-wave to spinon
crossover. A number of characteristics of purely quantum nature are found: The
in-plane interaction s_i^x s_j^x + s_i^y s_j^y induces an antiferromagnetic
correlation in the out-of-plane s_i^z component, at higher temperatures in the
antiferromagnetic XXZ chain, dominantly at low temperatures in the
ferromagnetic XXZ chain, and, in-between, at all temperatures in the XY chain.
We find that the converse effect also occurs in the antiferromagnetic XXZ
chain: an antiferromagnetic s_i^z s_j^z interaction induces a correlation in
the s_i^xy component. As another purely quantum effect, (i) in the
antiferromagnet, the value of the specific heat peak is insensitive to
anisotropy and the temperature of the specific heat peak decreases from the
isotropic (Heisenberg) with introduction of either type (Ising or XY)
anisotropy; (ii) in complete contrast, in the ferromagnet, the value and
temperature of the specific heat peak increase with either type of anisotropy.
"
stat,"A New Monte Carlo Method and Its Implications for Generalized Cluster
  Algorithms","  We describe a novel switching algorithm based on a ``reverse'' Monte Carlo
method, in which the potential is stochastically modified before the system
configuration is moved. This new algorithm facilitates a generalized
formulation of cluster-type Monte Carlo methods, and the generalization makes
it possible to derive cluster algorithms for systems with both discrete and
continuous degrees of freedom. The roughening transition in the sine-Gordon
model has been studied with this method, and high-accuracy simulations for
system sizes up to $1024^2$ were carried out to examine the logarithmic
divergence of the surface roughness above the transition temperature, revealing
clear evidence for universal scaling of the Kosterlitz-Thouless type.
"
stat,"Connecting microscopic simulations with kinetically constrained models
  of glasses","  Kinetically constrained spin models are known to exhibit dynamical behavior
mimicking that of glass forming systems. They are often understood as
coarse-grained models of glass formers, in terms of some ""mobility"" field. The
identity of this ""mobility"" field has remained elusive due to the lack of
coarse-graining procedures to obtain these models from a more microscopic point
of view. Here we exhibit a scheme to map the dynamics of a two-dimensional soft
disc glass former onto a kinetically constrained spin model, providing an
attempt at bridging these two approaches.
"
stat,High-dimensional variable selection,"  This paper explores the following question: what kind of statistical
guarantees can be given when doing variable selection in high-dimensional
models? In particular, we look at the error rates and power of some multi-stage
regression methods. In the first stage we fit a set of candidate models. In the
second stage we select one model by cross-validation. In the third stage we use
hypothesis testing to eliminate some variables. We refer to the first two
stages as ""screening"" and the last stage as ""cleaning."" We consider three
screening methods: the lasso, marginal regression, and forward stepwise
regression. Our method gives consistent variable selection under certain
conditions.
"
stat,The power of choice in network growth,"  The ""power of choice"" has been shown to radically alter the behavior of a
number of randomized algorithms. Here we explore the effects of choice on
models of tree and network growth. In our models each new node has k randomly
chosen contacts, where k > 1 is a constant. It then attaches to whichever one
of these contacts is most desirable in some sense, such as its distance from
the root or its degree. Even when the new node has just two choices, i.e., when
k=2, the resulting network can be very different from a random graph or tree.
For instance, if the new node attaches to the contact which is closest to the
root of the tree, the distribution of depths changes from Poisson to a
traveling wave solution. If the new node attaches to the contact with the
smallest degree, the degree distribution is closer to uniform than in a random
graph, so that with high probability there are no nodes in the network with
degree greater than O(log log N). Finally, if the new node attaches to the
contact with the largest degree, we find that the degree distribution is a
power law with exponent -1 up to degrees roughly equal to k, with an
exponential cutoff beyond that; thus, in this case, we need k >> 1 to see a
power law over a wide range of degrees.
"
stat,"Finite-size scaling of pseudo-critical point distributions in the random
  transverse-field Ising chain","  We study the distribution of finite size pseudo-critical points in a
one-dimensional random quantum magnet with a quantum phase transition described
by an infinite randomness fixed point. Pseudo-critical points are defined in
three different ways: the position of the maximum of the average entanglement
entropy, the scaling behavior of the surface magnetization, and the energy of a
soft mode. All three lead to a log-normal distribution of the pseudo-critical
transverse fields, where the width scales as $L^{-1/\nu}$ with $\nu=2$ and the
shift of the average value scales as $L^{-1/\nu_{typ}}$ with $\nu_{typ}=1$,
which we related to the scaling of average and typical quantities in the
critical region.
"
stat,Emergence of U(1) symmetry in the 3D XY model with Zq anisotropy,"  We study the three-dimensional XY model with a Z_q anisotropic term. At
temperatures T < Tc this dangerously irrelevant perturbation is relevant only
above a length scale Lambda, which diverges as a power of the correlation
length; Lambda ~ xi^a_q. Below Lambda the order parameter is U(1) symmetric. We
derive the full scaling function controlling the emergence of U(1) symmetry and
use Monte Carlo results to extract the exponent a_q for q=4,...,8. We find that
a_q = a_4 (q/4)^2, with a_4 only marginally larger than 1. We discuss these
results in the context of U(1) symmetry at ""deconfined"" quantum critical points
separating antiferromagnetic and valence-bond-solid states in quantum spin
systems.
"
stat,"Ramsey fringes formation during excitation of topological modes in a
  Bose-Einstein condensate","  The Ramsey fringes formation during the excitation of topological coherent
modes of a Bose-Einstein condensate by an external modulating field is
considered. The Ramsey fringes appear when a series of pulses of the excitation
field is applied. In both Rabi and Ramsey interrogations, there is a shift of
the population maximum transfer due to the strong non-linearity present in the
system. It is found that the Ramsey pattern itself retains information about
the accumulated relative phase between both ground and excited coherent modes.
"
stat,"Highly synchronized noise-driven oscillatory behavior of a
  FitzHugh-Nagumo ring with phase-repulsive coupling","  We investigate a ring of $N$ FitzHugh--Nagumo elements coupled in
\emph{phase-repulsive} fashion and submitted to a (subthreshold) common
oscillatory signal and independent Gaussian white noises. This system can be
regarded as a reduced version of the one studied in [Phys. Rev. E \textbf{64},
041912 (2001)], although externally forced and submitted to noise. The
noise-sustained synchronization of the system with the external signal is
characterized.
"
stat,Quantum Quenches in Extended Systems,"  We study in general the time-evolution of correlation functions in a extended
quantum system after the quench of a parameter in the hamiltonian. We show that
correlation functions in d dimensions can be extracted using methods of
boundary critical phenomena in d+1 dimensions. For d=1 this allows to use the
powerful tools of conformal field theory in the case of critical evolution.
Several results are obtained in generic dimension in the gaussian (mean-field)
approximation. These predictions are checked against the real-time evolution of
some solvable models that allows also to understand which features are valid
beyond the critical evolution.
  All our findings may be explained in terms of a picture generally valid,
whereby quasiparticles, entangled over regions of the order of the correlation
length in the initial state, then propagate with a finite speed through the
system. Furthermore we show that the long-time results can be interpreted in
terms of a generalized Gibbs ensemble. We discuss some open questions and
possible future developments.
"
stat,Biased Structural Fluctuations due to Electron Wind Force,"  Direct correlation between temporal structural fluctuations and electron wind
force is demonstrated, for the first time, by STM imaging and analysis of
atomically-resolved motion on a thin film surface under large applied current
(10e5 Amp/sqare cm). The magnitude of the momentum transfer between current
carriers and atoms in the fluctuating structure is at least five to fifteen
times (plus or minus one sigma range) larger than for freely diffusing adatoms.
The corresponding changes in surface resistivity will contribute significant
fluctuation signature to nanoscale electronic properties.
"
stat,Traitement Des Donnees Manquantes Au Moyen De L'Algorithme De Kohonen,"  Nous montrons comment il est possible d'utiliser l'algorithme d'auto
organisation de Kohonen pour traiter des donn\'ees avec valeurs manquantes et
estimer ces derni\`eres. Apr\`es un rappel m\'ethodologique, nous illustrons
notre propos \`a partir de trois applications \`a des donn\'ees r\'eelles.
  -----
  We show how it is possible to use the Kohonen self-organizing algorithm to
deal with data which contain missing values and to estimate them. After a
methodological recall, we illustrate our purpose from three real databases
applications.
"
stat,"Monte Carlo Simulations of Quantum Spin Systems in the Valence Bond
  Basis","  We discuss a projector Monte Carlo method for quantum spin models formulated
in the valence bond basis, using the S=1/2 Heisenberg antiferromagnet as an
example. Its singlet ground state can be projected out of an arbitrary basis
state as the trial state, but a more rapid convergence can be obtained using a
good variational state. As an alternative to first carrying out a time
consuming variational Monte Carlo calculation, we show that a very good trial
state can be generated in an iterative fashion in the course of the simulation
itself. We also show how the properties of the valence bond basis enable
calculations of quantities that are difficult to obtain with the standard basis
of Sz eigenstates. In particular, we discuss quantities involving
finite-momentum states in the triplet sector, such as the dispersion relation
and the spectral weight of the lowest triplet.
"
stat,"Subdiffusion and weak ergodicity breaking in the presence of a reactive
  boundary","  We derive the boundary condition for a subdiffusive particle interacting with
a reactive boundary with finite reaction rate. Molecular crowding conditions,
that are found to cause subdiffusion of larger molecules in biological cells,
are shown to effect long-tailed distributions with identical exponent for both
the unbinding times from the boundary to the bulk and the rebinding times from
the bulk. This causes a weak ergodicity breaking: typically, an individual
particle either stays bound or remains in the bulk for very long times. We
discuss why this may be beneficial for in vivo gene regulation by DNA-binding
proteins, whose typical concentrations are nanomolar
"
stat,"Sparse Estimators and the Oracle Property, or the Return of Hodges'
  Estimator","  We point out some pitfalls related to the concept of an oracle property as
used in Fan and Li (2001, 2002, 2004) which are reminiscent of the well-known
pitfalls related to Hodges' estimator. The oracle property is often a
consequence of sparsity of an estimator. We show that any estimator satisfying
a sparsity property has maximal risk that converges to the supremum of the loss
function; in particular, the maximal risk diverges to infinity whenever the
loss function is unbounded. For ease of presentation the result is set in the
framework of a linear regression model, but generalizes far beyond that
setting. In a Monte Carlo study we also assess the extent of the problem in
finite samples for the smoothly clipped absolute deviation (SCAD) estimator
introduced in Fan and Li (2001). We find that this estimator can perform rather
poorly in finite samples and that its worst-case performance relative to
maximum likelihood deteriorates with increasing sample size when the estimator
is tuned to sparsity.
"
stat,"Corner Transfer Matrix Renormalization Group Method Applied to the Ising
  Model on the Hyperbolic Plane","  Critical behavior of the Ising model is investigated at the center of large
scale finite size systems, where the lattice is represented as the tiling of
pentagons. The system is on the hyperbolic plane, and the recursive structure
of the lattice makes it possible to apply the corner transfer matrix
renormalization group method. From the calculated nearest neighbor spin
correlation function and the spontaneous magnetization, it is concluded that
the phase transition of this model is mean-field like. One parameter
deformation of the corner Hamiltonian on the hyperbolic plane is discussed.
"
stat,"Aspects of stochastic resonance in reaction-diffusion systems: The
  nonequilibrium-potential approach","  We analyze several aspects of the phenomenon of stochastic resonance in
reaction-diffusion systems, exploiting the nonequilibrium potential's
framework. The generalization of this formalism (sketched in the appendix) to
extended systems is first carried out in the context of a simplified scalar
model, for which stationary patterns can be found analytically. We first show
how system-size stochastic resonance arises naturally in this framework, and
then how the phenomenon of array-enhanced stochastic resonance can be further
enhanced by letting the diffusion coefficient depend on the field. A yet less
trivial generalization is exemplified by a stylized version of the
FitzHugh-Nagumo system, a paradigm of the activator-inhibitor class. After
discussing for this system the second aspect enumerated above, we derive from
it -through an adiabatic-like elimination of the inhibitor field- an effective
scalar model that includes a nonlocal contribution. Studying the role played by
the range of the nonlocal kernel and its effect on stochastic resonance, we
find an optimal range that maximizes the system's response.
"
stat,"Fluctuations of the partial filling factors in competitive RSA from
  binary mixtures","  Competitive random sequential adsorption on a line from a binary mix of
incident particles is studied using both an analytic recursive approach and
Monte Carlo simulations. We find a strong correlation between the small and the
large particle distributions so that while both partial contributions to the
fill factor fluctuate widely, the variance of the total fill factor remains
relatively small. The variances of partial contributions themselves are quite
different between the smaller and the larger particles, with the larger
particle distribution being more correlated. The disparity in fluctuations of
partial fill factors increases with the particle size ratio. The additional
variance in the partial contribution of smaller particle originates from the
fluctuations in the size of gaps between larger particles. We discuss the
implications of our results to semiconductor high-energy gamma detectors where
the detector energy resolution is controlled by correlations in the cascade
energy branching process.
"
stat,"Capillary ordering and layering transitions in two-dimensional hard-rod
  fluids","  In this article we calculate the surface phase diagram of a two-dimensional
hard-rod fluid confined between two hard lines. In a first stage we study the
semi-infinite system consisting of an isotropic fluid in contact with a single
hard line. We have found complete wetting by the columnar phase at the
wall-isotropic fluid interface. When the fluid is confined between two hard
walls, capillary columnar ordering occurs via a first-order phase transition.
For higher chemical potentials the system exhibits layering transitions even
for very narrow slits (near the one-dimensional limit). The theoretical model
used was a density-functional theory based on the Fundamental-Measure
Functional applied to a fluid of hard rectangles in the restricted-orientation
approximation (Zwanzig model). The results presented here can be checked
experimentally in two-dimensional granular media made of rods, where vertical
motions induced by an external source and excluded volume interactions between
the grains allow the system to explore those stationary states which
entropically maximize packing configurations. We claim that some of the surface
phenomena found here can be present in two-dimensional granular-media fluids.
"
stat,"Heisenberg antiferromagnet with anisotropic exchange on the Kagome
  lattice: Description of the magnetic properties of volborthite","  We study the properties of the Heisenberg antiferromagnet with spatially
anisotropic nearest-neighbour exchange couplings on the kagome net, i.e. with
coupling J in one lattice direction and couplings J' along the other two
directions. For J/J' > 1, this model is believed to describe the magnetic
properties of the mineral volborthite. In the classical limit, it exhibits two
kinds of ground states: a ferrimagnetic state for J/J' < 1/2 and a large
manifold of canted spin states for J/J' > 1/2. To include quantum effects
self-consistently, we investigate the Sp(N) symmetric generalisation of the
original SU(2) symmetric model in the large-N limit. In addition to the
dependence on the anisotropy, the Sp(N) symmetric model depends on a parameter
kappa that measures the importance of quantum effects. Our numerical
calculations reveal that in the kappa-J/J' plane, the system shows a rich phase
diagram containing a ferrimagnetic phase, an incommensurate phase, and a
decoupled chain phase, the latter two with short- and long-range order. We
corroborate these results by showing that the boundaries between the various
phases and several other features of the Sp(N) phase diagram can be determined
by analytical calculations. Finally, the application of a block-spin
perturbation expansion to the trimerised version of the original spin-1/2 model
leads us to suggest that in the limit of strong anisotropy, J/J' >> 1, the
ground state of the original model is a collinearly ordered antiferromagnet,
which is separated from the incommensurate state by a quantum phase transition.
"
stat,Optimal fluctuation approach to a directed polymer in a random medium,"  A modification of the optimal fluctuation approach is applied to study the
tails of the free energy distribution function P(F) for an elastic string in
quenched disorder both in the regions of the universal behavior of P(F) and in
the regions of large fluctuations, where the behavior of P(F) is non-universal.
The difference between the two regimes is shown to consist in whether it is
necessary or not to take into account the renormalization of parameters by the
fluctuations of disorder in the vicinity of the optimal fluctuation.
"
stat,Phase Transitions in the Coloring of Random Graphs,"  We consider the problem of coloring the vertices of a large sparse random
graph with a given number of colors so that no adjacent vertices have the same
color. Using the cavity method, we present a detailed and systematic analytical
study of the space of proper colorings (solutions).
  We show that for a fixed number of colors and as the average vertex degree
(number of constraints) increases, the set of solutions undergoes several phase
transitions similar to those observed in the mean field theory of glasses.
First, at the clustering transition, the entropically dominant part of the
phase space decomposes into an exponential number of pure states so that beyond
this transition a uniform sampling of solutions becomes hard. Afterward, the
space of solutions condenses over a finite number of the largest states and
consequently the total entropy of solutions becomes smaller than the annealed
one. Another transition takes place when in all the entropically dominant
states a finite fraction of nodes freezes so that each of these nodes is
allowed a single color in all the solutions inside the state. Eventually, above
the coloring threshold, no more solutions are available. We compute all the
critical connectivities for Erdos-Renyi and regular random graphs and determine
their asymptotic values for large number of colors.
  Finally, we discuss the algorithmic consequences of our findings. We argue
that the onset of computational hardness is not associated with the clustering
transition and we suggest instead that the freezing transition might be the
relevant phenomenon. We also discuss the performance of a simple local Walk-COL
algorithm and of the belief propagation algorithm in the light of our results.
"
stat,"Markov basis and Groebner basis of Segre-Veronese configuration for
  testing independence in group-wise selections","  We consider testing independence in group-wise selections with some
restrictions on combinations of choices. We present models for frequency data
of selections for which it is easy to perform conditional tests by Markov chain
Monte Carlo (MCMC) methods. When the restrictions on the combinations can be
described in terms of a Segre-Veronese configuration, an explicit form of a
Gr\""obner basis consisting of moves of degree two is readily available for
performing a Markov chain. We illustrate our setting with the National Center
Test for university entrance examinations in Japan. We also apply our method to
testing independence hypotheses involving genotypes at more than one locus or
haplotypes of alleles on the same chromosome.
"
stat,"Noise-induced phase transitions: Effects of the noises' statistics and
  spectrum","  The local, uncorrelated multiplicative noises driving a second-order, purely
noise-induced, ordering phase transition (NIPT) were assumed to be Gaussian and
white in the model of [Phys. Rev. Lett. \textbf{73}, 3395 (1994)]. The
potential scientific and technological interest of this phenomenon calls for a
study of the effects of the noises' statistics and spectrum. This task is
facilitated if these noises are dynamically generated by means of stochastic
differential equations (SDE) driven by white noises. One such case is that of
Ornstein--Uhlenbeck noises which are stationary, with Gaussian pdf and a
variance reduced by the self-correlation time (\tau), and whose effect on the
NIPT phase diagram has been studied some time ago. Another such case is when
the stationary pdf is a (colored) Tsallis' (q)--\emph{Gaussian} which, being a
\emph{fat-tail} distribution for (q>1) and a \emph{compact-support} one for
(q<1), allows for a controlled exploration of the effects of the departure from
Gaussian statistics. As done before with stochastic resonance and other
phenomena, we now exploit this tool to study--within a simple mean-field
approximation and with an emphasis on the \emph{order parameter} and the
``\emph{susceptibility}''--the combined effect on NIPT of the noises'
statistics and spectrum. Even for relatively small (\tau), it is shown that
whereas fat-tail noise distributions ((q>1)) counteract the effect of
self-correlation, compact-support ones ((q<1)) enhance it. Also, an interesting
effect on the susceptibility is seen in the last case.
"
stat,Critical Behavior of a Trapped Interacting Bose Gas,"  The phase transition of Bose-Einstein condensation is studied in the critical
regime, when fluctuations extend far beyond the length scale of thermal de
Broglie waves. Using matter-wave interference we measure the correlation length
of these critical fluctuations as a function of temperature. The diverging
behavior of the correlation length above the critical temperature is observed,
from which we determine the critical exponent of the correlation length for a
trapped, weakly interacting Bose gas to be $\nu=0.67\pm 0.13$. This measurement
has direct implications for the understanding of second order phase
transitions.
"
stat,"The property of kappa-deformed statistics for a relativistic gas in an
  electromagnetic field: kappa parameter and kappa-distribution","  We investigate the physical property of the kappa parameter and the
kappa-distribution in the kappa-deformed statistics, based on Kaniadakis
entropy, for a relativistic gas in an electromagnetic field. We derive two
relations for the relativistic gas in the framework of kappa-deformed
statistics, which describe the physical situation represented by the
relativistic kappa-distribution function, provide a reasonable connection
between the parameter kappa, the temperature four-gradient and the four-vector
potential gradient, and thus present for the case kappa different from zero a
clearly physical meaning. It is shown that such a physical situation is a
meta-equilibrium state of the system, but has a new physical characteristic.
"
eess,Optimization of Synthesis Oversampled Complex Filter Banks,"  An important issue with oversampled FIR analysis filter banks (FBs) is to
determine inverse synthesis FBs, when they exist. Given any complex oversampled
FIR analysis FB, we first provide an algorithm to determine whether there
exists an inverse FIR synthesis system. We also provide a method to ensure the
Hermitian symmetry property on the synthesis side, which is serviceable to
processing real-valued signals. As an invertible analysis scheme corresponds to
a redundant decomposition, there is no unique inverse FB. Given a particular
solution, we parameterize the whole family of inverses through a null space
projection. The resulting reduced parameter set simplifies design procedures,
since the perfect reconstruction constrained optimization problem is recast as
an unconstrained optimization problem. The design of optimized synthesis FBs
based on time or frequency localization criteria is then investigated, using a
simple yet efficient gradient algorithm.
"
eess,Dichotic harmony for the musical practice,"  The dichotic method of hearing sound adapts in the region of musical harmony.
The algorithm of the separation of the being dissonant voices into several
separate groups is proposed. For an increase in the pleasantness of chords the
different groups of voices are heard out through the different channels of
headphones. Is created two demonstration program for PC. Keywords: music,
harmony, chord, dichotic listening, dissonance, consonance, headphones,
pleasantness, midi.
"
eess,"Multi-Sensor Fuzzy Data Fusion Using Sensors with Different
  Characteristics","  This paper proposes a new approach to multi-sensor data fusion. It suggests
that aggregation of data from multiple sensors can be done more efficiently
when we consider information about sensors' different characteristics. Similar
to most research on effective sensors' characteristics, especially in control
systems, our focus is on sensors' accuracy and frequency response. A rule-based
fuzzy system is presented for fusion of raw data obtained from the sensors that
have complement characteristics in accuracy and bandwidth. Furthermore, a fuzzy
predictor system is suggested aiming for extreme accuracy which is a common
need in highly sensitive applications. Advantages of our proposed sensor fusion
system are shown by simulation of a control system utilizing the fusion system
for output estimation.
"
eess,Warping Peirce Quincuncial Panoramas,"  The Peirce quincuncial projection is a mapping of the surface of a sphere to
the interior of a square. It is a conformal map except for four points on the
equator. These points of non-conformality cause significant artifacts in
photographic applications. In this paper, we propose an algorithm and
user-interface to mitigate these artifacts. Moreover, in order to facilitate an
interactive user-interface, we present a fast algorithm for calculating the
Peirce quincuncial projection of spherical imagery. We then promote the Peirce
quincuncial projection as a viable alternative to the more popular
stereographic projection in some scenarios.
"
eess,"Rotation, Scaling and Translation Analysis of Biometric Signature
  Templates","  Biometric authentication systems that make use of signature verification
methods often render optimum performance only under limited and restricted
conditions. Such methods utilize several training samples so as to achieve high
accuracy. Moreover, several constraints are imposed on the end-user so that the
system may work optimally, and as expected. For example, the user is made to
sign within a small box, in order to limit their signature to a predefined set
of dimensions, thus eliminating scaling. Moreover, the angular rotation with
respect to the referenced signature that will be inadvertently introduced as
human error, hampers performance of biometric signature verification systems.
To eliminate this, traditionally, a user is asked to sign exactly on top of a
reference line. In this paper, we propose a robust system that optimizes the
signature obtained from the user for a large range of variation in
Rotation-Scaling-Translation (RST) and resolves these error parameters in the
user signature according to the reference signature stored in the database.
"
eess,Audio Watermarking with Error Correction,"  In recent times, communication through the internet has tremendously
facilitated the distribution of multimedia data. Although this is indubitably a
boon, one of its repercussions is that it has also given impetus to the
notorious issue of online music piracy. Unethical attempts can also be made to
deliberately alter such copyrighted data and thus, misuse it. Copyright
violation by means of unauthorized distribution, as well as unauthorized
tampering of copyrighted audio data is an important technological and research
issue. Audio watermarking has been proposed as a solution to tackle this issue.
The main purpose of audio watermarking is to protect against possible threats
to the audio data and in case of copyright violation or unauthorized tampering,
authenticity of such data can be disputed by virtue of audio watermarking.
"
eess,"Alternatives with stronger convergence than coordinate-descent iterative
  LMI algorithms","  In this note we aim at putting more emphasis on the fact that trying to solve
non-convex optimization problems with coordinate-descent iterative linear
matrix inequality algorithms leads to suboptimal solutions, and put forward
other optimization methods better equipped to deal with such problems (having
theoretical convergence guarantees and/or being more efficient in practice).
This fact, already outlined at several places in the literature, still appears
to be disregarded by a sizable part of the systems and control community. Thus,
main elements on this issue and better optimization alternatives are presented
and illustrated by means of an example.
"
eess,"Text-Independent Speaker Recognition for Low SNR Environments with
  Encryption","  Recognition systems are commonly designed to authenticate users at the access
control levels of a system. A number of voice recognition methods have been
developed using a pitch estimation process which are very vulnerable in low
Signal to Noise Ratio (SNR) environments thus, these programs fail to provide
the desired level of accuracy and robustness. Also, most text independent
speaker recognition programs are incapable of coping with unauthorized attempts
to gain access by tampering with the samples or reference database. The
proposed text-independent voice recognition system makes use of multilevel
cryptography to preserve data integrity while in transit or storage. Encryption
and decryption follow a transform based approach layered with pseudorandom
noise addition whereas for pitch detection, a modified version of the
autocorrelation pitch extraction algorithm is used. The experimental results
show that the proposed algorithm can decrypt the signal under test with
exponentially reducing Mean Square Error over an increasing range of SNR.
Further, it outperforms the conventional algorithms in actual identification
tasks even in noisy environments. The recognition rate thus obtained using the
proposed method is compared with other conventional methods used for speaker
identification.
"
eess,"A robust, low-cost approach to Face Detection and Face Recognition","  In the domain of Biometrics, recognition systems based on iris, fingerprint
or palm print scans etc. are often considered more dependable due to extremely
low variance in the properties of these entities with respect to time. However,
over the last decade data processing capability of computers has increased
manifold, which has made real-time video content analysis possible. This shows
that the need of the hour is a robust and highly automated Face Detection and
Recognition algorithm with credible accuracy rate. The proposed Face Detection
and Recognition system using Discrete Wavelet Transform (DWT) accepts face
frames as input from a database containing images from low cost devices such as
VGA cameras, webcams or even CCTV's, where image quality is inferior. Face
region is then detected using properties of L*a*b* color space and only Frontal
Face is extracted such that all additional background is eliminated. Further,
this extracted image is converted to grayscale and its dimensions are resized
to 128 x 128 pixels. DWT is then applied to entire image to obtain the
coefficients. Recognition is carried out by comparison of the DWT coefficients
belonging to the test image with those of the registered reference image. On
comparison, Euclidean distance classifier is deployed to validate the test
image from the database. Accuracy for various levels of DWT Decomposition is
obtained and hence, compared.
"
eess,Linear Consensus Algorithms Based on Balanced Asymmetric Chains,"  Multi agent consensus algorithms with update steps based on so-called
balanced asymmetric chains, are analyzed. For such algorithms it is shown that
(i) the set of accumulation points of states is finite, (ii) the asymptotic
unconditional occurrence of single consensus or multiple consensuses is
directly related to the property of absolute infinite flow for the underlying
update chain. The results are applied to well known consensus models.
"
eess,"Theorems about Ergodicity and Class-Ergodicity of Chains with
  Applications in Known Consensus Models","  In a multi-agent system, unconditional (multiple) consensus is the property
of reaching to (multiple) consensus irrespective of the instant and values at
which states are initialized. For linear algorithms, occurrence of
unconditional (multiple) consensus turns out to be equivalent to (class-)
ergodicity of the transition chain (A_n). For a wide class of chains, chains
with so-called balanced asymmetry property, necessary and sufficient conditions
for ergodicity and class-ergodicity are derived. The results are employed to
analyze the limiting behavior of agents' states in the JLM model, the Krause
model, and the Cucker-Smale model. In particular, unconditional single or
multiple consensus occurs in all three models. Moreover, a necessary and
sufficient condition for unconditional consensus in the JLM model and a
sufficient condition for consensus in the Cucker-Smale model are obtained.
"
eess,Tracking Tetrahymena Pyriformis Cells using Decision Trees,"  Matching cells over time has long been the most difficult step in cell
tracking. In this paper, we approach this problem by recasting it as a
classification problem. We construct a feature set for each cell, and compute a
feature difference vector between a cell in the current frame and a cell in a
previous frame. Then we determine whether the two cells represent the same cell
over time by training decision trees as our binary classifiers. With the output
of decision trees, we are able to formulate an assignment problem for our cell
association task and solve it using a modified version of the Hungarian
algorithm.
"
eess,Signal processing with Levy information,"  Levy processes, which have stationary independent increments, are ideal for
modelling the various types of noise that can arise in communication channels.
If a Levy process admits exponential moments, then there exists a parametric
family of measure changes called Esscher transformations. If the parameter is
replaced with an independent random variable, the true value of which
represents a ""message"", then under the transformed measure the original Levy
process takes on the character of an ""information process"". In this paper we
develop a theory of such Levy information processes. The underlying Levy
process, which we call the fiducial process, represents the ""noise type"". Each
such noise type is capable of carrying a message of a certain specification. A
number of examples are worked out in detail, including information processes of
the Brownian, Poisson, gamma, variance gamma, negative binomial, inverse
Gaussian, and normal inverse Gaussian type. Although in general there is no
additive decomposition of information into signal and noise, one is led
nevertheless for each noise type to a well-defined scheme for signal detection
and enhancement relevant to a variety of practical situations.
"
eess,"Analysis of a Modern Voice Morphing Approach using Gaussian Mixture
  Models for Laryngectomees","  This paper proposes a voice morphing system for people suffering from
Laryngectomy, which is the surgical removal of all or part of the larynx or the
voice box, particularly performed in cases of laryngeal cancer. A primitive
method of achieving voice morphing is by extracting the source's vocal
coefficients and then converting them into the target speaker's vocal
parameters. In this paper, we deploy Gaussian Mixture Models (GMM) for mapping
the coefficients from source to destination. However, the use of the
traditional/conventional GMM-based mapping approach results in the problem of
over-smoothening of the converted voice. Thus, we hereby propose a unique
method to perform efficient voice morphing and conversion based on GMM,which
overcomes the traditional-method effects of over-smoothening. It uses a
technique of glottal waveform separation and prediction of excitations and
hence the result shows that not only over-smoothening is eliminated but also
the transformed vocal tract parameters match with the target. Moreover, the
synthesized speech thus obtained is found to be of a sufficiently high quality.
Thus, voice morphing based on a unique GMM approach has been proposed and also
critically evaluated based on various subjective and objective evaluation
parameters. Further, an application of voice morphing for Laryngectomees which
deploys this unique approach has been recommended by this paper.
"
eess,"Modeling and performance evaluation of computer systems security
  operation","  A model of computer system security operation is developed based on the
fork-join queueing network formalism. We introduce a security operation
performance measure, and show how it may be used to performance evaluation of
actual systems.
"
eess,"Feature Learning in Deep Neural Networks - Studies on Speech Recognition
  Tasks","  Recent studies have shown that deep neural networks (DNNs) perform
significantly better than shallow networks and Gaussian mixture models (GMMs)
on large vocabulary speech recognition tasks. In this paper, we argue that the
improved accuracy achieved by the DNNs is the result of their ability to
extract discriminative internal representations that are robust to the many
sources of variability in speech signals. We show that these representations
become increasingly insensitive to small perturbations in the input with
increasing network depth, which leads to better speech recognition performance
with deeper networks. We also show that DNNs cannot extrapolate to test samples
that are substantially different from the training examples. If the training
data are sufficiently representative, however, internal features learned by the
DNN are relatively stable with respect to speaker differences, bandwidth
differences, and environment distortion. This enables DNN-based recognizers to
perform as well or better than state-of-the-art systems based on GMMs or
shallow networks without the need for explicit model adaptation or feature
normalization.
"
eess,Consensus Algorithms and the Decomposition-Separation Theorem,"  Convergence properties of time inhomogeneous Markov chain based discrete and
continuous time linear consensus algorithms are analyzed. Provided that a
so-called infinite jet flow property is satisfied by the underlying chains,
necessary conditions for both consensus and multiple consensus are established.
A recenet extension by Sonin of the classical Kolmogorov-Doeblin
decomposition-separation for homogeneous Markov chains to the inhomogeneous
case is then employed to show that the obtained necessary conditions are also
sufficient when the chain is of Class P*, as defined by Touri and Nedic. It is
also shown that Sonin's theorem leads to a rediscovery and generalization of
most of the existing related consensus results in the literature.
"
eess,"Comparing Edge Detection Methods based on Stochastic Entropies and
  Distances for PolSAR Imagery","  Polarimetric synthetic aperture radar (PolSAR) has achieved a prominent
position as a remote imaging method. However, PolSAR images are contaminated by
speckle noise due to the coherent illumination employed during the data
acquisition. This noise provides a granular aspect to the image, making its
processing and analysis (such as in edge detection) hard tasks. This paper
discusses seven methods for edge detection in multilook PolSAR images. In all
methods, the basic idea consists in detecting transition points in the finest
possible strip of data which spans two regions. The edge is contoured using the
transitions points and a B-spline curve. Four stochastic distances, two
differences of entropies, and the maximum likelihood criterion were used under
the scaled complex Wishart distribution; the first six stem from the h-phi
class of measures. The performance of the discussed detection methods was
quantified and analyzed by the computational time and probability of correct
edge detection, with respect to the number of looks, the backscatter matrix as
a whole, the SPAN, the covariance an the spatial resolution. The detection
procedures were applied to three real PolSAR images. Results provide evidence
that the methods based on the Bhattacharyya distance and the difference of
Shannon entropies outperform the other techniques.
"
eess,Sparse Representation-based Image Quality Assessment,"  A successful approach to image quality assessment involves comparing the
structural information between a distorted and its reference image. However,
extracting structural information that is perceptually important to our visual
system is a challenging task. This paper addresses this issue by employing a
sparse representation-based approach and proposes a new metric called the
\emph{sparse representation-based quality} (SPARQ) \emph{index}. The proposed
method learns the inherent structures of the reference image as a set of basis
vectors, such that any structure in the image can be represented by a linear
combination of only a few of those basis vectors. This sparse strategy is
employed because it is known to generate basis vectors that are qualitatively
similar to the receptive field of the simple cells present in the mammalian
primary visual cortex. The visual quality of the distorted image is estimated
by comparing the structures of the reference and the distorted images in terms
of the learnt basis vectors resembling cortical cells. Our approach is
evaluated on six publicly available subject-rated image quality assessment
datasets. The proposed SPARQ index consistently exhibits high correlation with
the subjective ratings on all datasets and performs better or at par with the
state-of-the-art.
"
eess,Cognitive Random Stepped Frequency Radar with Sparse Recovery,"  Random stepped frequency (RSF) radar, which transmits random-frequency
pulses, can suppress the range ambiguity, improve convert detection, and
possess excellent electronic counter-countermeasures (ECCM) ability [1]. In
this paper, we apply a sparse recovery method to estimate the range and Doppler
of targets. We also propose a cognitive mechanism for RSF radar to further
enhance the performance of the sparse recovery method. The carrier frequencies
of transmitted pulses are adaptively designed in response to the observed
circumstance. We investigate the criterion to design carrier frequencies, and
efficient methods are then devised. Simulation results demonstrate that the
adaptive frequency-design mechanism significantly improves the performance of
target reconstruction in comparison with the non-adaptive mechanism.
"
eess,Adaptive matching pursuit for off-grid compressed sensing,"  Compressive sensing (CS) can effectively recover a signal when it is sparse
in some discrete atoms. However, in some applications, signals are sparse in a
continuous parameter space, e.g., frequency space, rather than discrete atoms.
Usually, we divide the continuous parameter into finite discrete grid points
and build a dictionary from these grid points. However, the actual targets may
not exactly lie on the grid points no matter how densely the parameter is
grided, which introduces mismatch between the predefined dictionary and the
actual one. In this article, a novel method, namely adaptive matching pursuit
with constrained total least squares (AMP-CTLS), is proposed to find actual
atoms even if they are not included in the initial dictionary. In AMP-CTLS, the
grid and the dictionary are adaptively updated to better agree with
measurements. The convergence of the algorithm is discussed, and numerical
experiments demonstrate the advantages of AMP-CTLS.
"
eess,Sparsity-Promoting Sensor Selection for Non-linear Measurement Models,"  Sensor selection is an important design problem in large-scale sensor
networks. Sensor selection can be interpreted as the problem of selecting the
best subset of sensors that guarantees a certain estimation performance. We
focus on observations that are related to a general non-linear model. The
proposed framework is valid as long as the observations are independent, and
its likelihood satisfies the regularity conditions. We use several functions of
the Cram\'er-Rao bound (CRB) as a performance measure. We formulate the sensor
selection problem as the design of a selection vector, which in its original
form is a nonconvex l0-(quasi) norm optimization problem. We present relaxed
sensor selection solvers that can be efficiently solved in polynomial time. We
also propose a projected subgradient algorithm that is attractive for
large-scale problems and also show how the algorithm can be easily distributed.
The proposed framework is illustrated with a number of examples related to
sensor placement design for localization.
"
eess,Eminence Grise Coalitions: On the Shaping of Public Opinion,"  We consider a network of evolving opinions. It includes multiple individuals
with first-order opinion dynamics defined in continuous time and evolving based
on a general exogenously defined time-varying underlying graph. In such a
network, for an arbitrary fixed initial time, a subset of individuals forms an
eminence grise coalition, abbreviated as EGC, if the individuals in that subset
are capable of leading the entire network to agreeing on any desired opinion,
through a cooperative choice of their own initial opinions. In this endeavor,
the coalition members are assumed to have access to full profile of the
underlying graph of the network as well as the initial opinions of all other
individuals. While the complete coalition of individuals always qualifies as an
EGC, we establish the existence of a minimum size EGC for an arbitrary
time-varying network; also, we develop a non-trivial set of upper and lower
bounds on that size. As a result, we show that, even when the underlying graph
does not guarantee convergence to a global or multiple consensus, a generally
restricted coalition of agents can steer public opinion towards a desired
global consensus without affecting any of the predefined graph interactions,
provided they can cooperatively adjust their own initial opinions. Geometric
insights into the structure of EGC's are given. The results are also extended
to the discrete time case where the relation with Decomposition-Separation
Theorem is also made explicit.
"
eess,"Cramer-Rao Lower Bounds of Joint Delay-Doppler Estimation for an
  Extended Target","  The problem on the Cramer-Rao Lower Bounds (CRLBs) for the joint time delay
and Doppler stretch estimation of an extended target is considered in this
paper. The integral representations of the CRLBs for both the time delay and
the Doppler stretch are derived. To facilitate computation and analysis, series
representations and approximations of the CRLBs are introduced. According to
these series representations, the impact of several waveform parameters on the
estimation accuracy is investigated, which reveals that the CRLB of the Doppler
stretch is inversely proportional to the effective time-bandwidth product of
the waveform. This conclusion generalizes a previous result in the narrowband
case. The popular wideband ambiguity function (WBAF) based delay-Doppler
estimator is evaluated and compared with the CRLBs through numerical
experiments. Our results indicate that the WBAF estimator, originally derived
from a single scatterer model, is not suitable for the parameter estimation of
an extended target.
"
eess,"Dynamic Optimal Power Flow in Microgrids using the Alternating Direction
  Method of Multipliers","  Smart devices, storage and other distributed technologies have the potential
to greatly improve the utilisation of network infrastructure and renewable
generation. Decentralised control of these technologies overcomes many
scalability and privacy concerns, but in general still requires the underlying
problem to be convex in order to guarantee convergence to a global optimum.
Considering that AC power flows are non-convex in nature, and the operation of
household devices often requires discrete decisions, there has been uncertainty
surrounding the use of distributed methods in a realistic setting. This paper
extends prior work on the alternating direction method of multipliers (ADMM)
for solving the dynamic optimal power flow (D-OPF) problem. We utilise more
realistic line and load models, and introduce a two-stage approach to managing
discrete decisions and uncertainty. Our experiments on a suburb-sized microgrid
show that this approach provides near optimal results, in a time that is fast
enough for receding horizon control. This work brings distributed control of
smart-grid technologies closer to reality.
"
eess,Automatic Photo Adjustment Using Deep Neural Networks,"  Photo retouching enables photographers to invoke dramatic visual impressions
by artistically enhancing their photos through stylistic color and tone
adjustments. However, it is also a time-consuming and challenging task that
requires advanced skills beyond the abilities of casual photographers. Using an
automated algorithm is an appealing alternative to manual work but such an
algorithm faces many hurdles. Many photographic styles rely on subtle
adjustments that depend on the image content and even its semantics. Further,
these adjustments are often spatially varying. Because of these
characteristics, existing automatic algorithms are still limited and cover only
a subset of these challenges. Recently, deep machine learning has shown unique
abilities to address hard problems that resisted machine algorithms for long.
This motivated us to explore the use of deep learning in the context of photo
editing. In this paper, we explain how to formulate the automatic photo
adjustment problem in a way suitable for this approach. We also introduce an
image descriptor that accounts for the local semantics of an image. Our
experiments demonstrate that our deep learning formulation applied using these
descriptors successfully capture sophisticated photographic styles. In
particular and unlike previous techniques, it can model local adjustments that
depend on the image semantics. We show on several examples that this yields
results that are qualitatively and quantitatively better than previous work.
"
eess,"Implementation of an Automatic Syllabic Division Algorithm from Speech
  Files in Portuguese Language","  A new algorithm for voice automatic syllabic splitting in the Portuguese
language is proposed, which is based on the envelope of the speech signal of
the input audio file. A computational implementation in MatlabTM is presented
and made available at the URL
http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its
straightforwardness, the proposed method is very attractive for embedded
systems (e.g. i-phones). It can also be used as a screen to assist more
sophisticated methods. Voice excerpts containing more than one syllable and
identified by the same envelope are named as super-syllables and they are
subsequently separated. The results indicate which samples corresponds to the
beginning and end of each detected syllable. Preliminary tests were performed
to fifty words at an identification rate circa 70% (further improvements may be
incorporated to treat particular phonemes). This algorithm is also useful in
voice command systems, as a tool in the teaching of Portuguese language or even
for patients with speech pathology.
"
eess,"A Matrix Laurent Series-based Fast Fourier Transform for Blocklengths
  N=4 (mod 8)","  General guidelines for a new fast computation of blocklength 8m+4 DFTs are
presented, which is based on a Laurent series involving matrices. Results of
non-trivial real multiplicative complexity are presented for blocklengths N=64,
achieving lower multiplication counts than previously published FFTs. A
detailed description for the cases m=1 and m=2 is presented.
"
eess,The Z Transform over Finite Fields,"  Finite field transforms have many applications and, in many cases, can be
implemented with a low computational complexity. In this paper, the Z Transform
over a finite field is introduced and some of its properties are presented.
"
eess,"A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation
  Recording","  This paper presents a new approach for a vocoder design based on full
frequency masking by octaves in addition to a technique for spectral filling
via beta probability distribution. Some psycho-acoustic characteristics of
human hearing - inaudibility masking in frequency and phase - are used as a
basis for the proposed algorithm. The results confirm that this technique may
be useful to save bandwidth in applications requiring intelligibility. It is
recommended for the legal eavesdropping of long voice conversations.
"
eess,Real-Time Stochastic Optimal Control for Multi-agent Quadrotor Systems,"  This paper presents a novel method for controlling teams of unmanned aerial
vehicles using Stochastic Optimal Control (SOC) theory. The approach consists
of a centralized high-level planner that computes optimal state trajectories as
velocity sequences, and a platform-specific low-level controller which ensures
that these velocity sequences are met. The planning task is expressed as a
centralized path-integral control problem, for which optimal control
computation corresponds to a probabilistic inference problem that can be solved
by efficient sampling methods. Through simulation we show that our SOC approach
(a) has significant benefits compared to deterministic control and other SOC
methods in multimodal problems with noise-dependent optimal solutions, (b) is
capable of controlling a large number of platforms in real-time, and (c) yields
collective emergent behaviour in the form of flight formations. Finally, we
show that our approach works for real platforms, by controlling a team of three
quadrotors in outdoor conditions.
"
eess,"A Flexible Implementation of a Matrix Laurent Series-Based 16-Point Fast
  Fourier and Hartley Transforms","  This paper describes a flexible architecture for implementing a new fast
computation of the discrete Fourier and Hartley transforms, which is based on a
matrix Laurent series. The device calculates the transforms based on a single
bit selection operator. The hardware structure and synthesis are presented,
which handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E
device.
"
eess,"Cluster Synchronization of Coupled Systems with Nonidentical Linear
  Dynamics","  This paper considers the cluster synchronization problem of generic linear
dynamical systems whose system models are distinct in different clusters. These
nonidentical linear models render control design and coupling conditions highly
correlated if static couplings are used for all individual systems. In this
paper, a dynamic coupling structure, which incorporates a global weighting
factor and a vanishing auxiliary control variable, is proposed for each agent
and is shown to be a feasible solution. Lower bounds on the global and local
weighting factors are derived under the condition that every interaction
subgraph associated with each cluster admits a directed spanning tree. The
spanning tree requirement is further shown to be a necessary condition when the
clusters connect acyclically with each other. Simulations for two applications,
cluster heading alignment of nonidentical ships and cluster phase
synchronization of nonidentical harmonic oscillators, illustrate essential
parts of the derived theoretical results.
"
eess,"New Algorithms for Computing a Single Component of the Discrete Fourier
  Transform","  This paper introduces the theory and hardware implementation of two new
algorithms for computing a single component of the discrete Fourier transform.
In terms of multiplicative complexity, both algorithms are more efficient, in
general, than the well known Goertzel Algorithm.
"
eess,The Discrete Cosine Transform over Prime Finite Fields,"  This paper examines finite field trigonometry as a tool to construct
trigonometric digital transforms. In particular, by using properties of the
k-cosine function over GF(p), the Finite Field Discrete Cosine Transform
(FFDCT) is introduced. The FFDCT pair in GF(p) is defined, having blocklengths
that are divisors of (p+1)/2. A special case is the Mersenne FFDCT, defined
when p is a Mersenne prime. In this instance blocklengths that are powers of
two are possible and radix-2 fast algorithms can be used to compute the
transform.
"
eess,Video Inpainting of Complex Scenes,"  We propose an automatic video inpainting algorithm which relies on the
optimisation of a global, patch-based functional. Our algorithm is able to deal
with a variety of challenging situations which naturally arise in video
inpainting, such as the correct reconstruction of dynamic textures, multiple
moving objects and moving background. Furthermore, we achieve this in an order
of magnitude less execution time with respect to the state-of-the-art. We are
also able to achieve good quality results on high definition videos. Finally,
we provide specific algorithmic details to make implementation of our algorithm
as easy as possible. The resulting algorithm requires no segmentation or manual
input other than the definition of the inpainting mask, and can deal with a
wider variety of situations than is handled by previous work. 1. Introduction.
Advanced image and video editing techniques are increasingly common in the
image processing and computer vision world, and are also starting to be used in
media entertainment. One common and difficult task closely linked to the world
of video editing is image and video "" inpainting "". Generally speaking, this is
the task of replacing the content of an image or video with some other content
which is visually pleasing. This subject has been extensively studied in the
case of images, to such an extent that commercial image inpainting products
destined for the general public are available, such as Photoshop's "" Content
Aware fill "" [1]. However, while some impressive results have been obtained in
the case of videos, the subject has been studied far less extensively than
image inpainting. This relative lack of research can largely be attributed to
high time complexity due to the added temporal dimension. Indeed, it has only
very recently become possible to produce good quality inpainting results on
high definition videos, and this only in a semi-automatic manner. Nevertheless,
high-quality video inpainting has many important and useful applications such
as film restoration, professional post-production in cinema and video editing
for personal use. For this reason, we believe that an automatic, generic video
inpainting algorithm would be extremely useful for both academic and
professional communities.
"
eess,"Global stabilization of multiple integrators by a bounded feedback with
  constraints on its successive derivatives","  In this paper, we address the global stabilization of chains of integrators
by means of a bounded static feedback law whose p first time derivatives are
bounded. Our construction is based on the technique of nested saturations
introduced by Teel. We show that the control amplitude and the maximum value of
its p first derivatives can be imposed below any prescribed values. Our results
are illustrated by the stabilization of the third order integrator on the
feedback and its first two derivatives.
"
eess,"Energy-efficient hybrid spintronic-straintronic reconfigurable bit
  comparator","  We propose a reconfigurable bit comparator implemented with a nanowire spin
valve whose two contacts are magnetostrictive with bistable magnetization.
Reference and input bits are ""written"" into the magnetization states of the two
contacts with electrically generated strain and the spin-valve's resistance is
lowered if they match. Multiple comparators can be interfaced in parallel with
a magneto-tunneling junction to determine if an N-bit input stream matches an
N-bit reference stream bit by bit. The system is robust against thermal noise
at room temperature and a 16-bit comparator can operate at roughly 416 MHz
while dissipating at most 420 aJ per cycle.
"
eess,"Multiscale edge detection and parametric shape modeling for boundary
  delineation in optoacoustic images","  In this article, we present a novel scheme for segmenting the image boundary
(with the background) in optoacoustic small animal in vivo imaging systems. The
method utilizes a multiscale edge detection algorithm to generate a binary edge
map. A scale dependent morphological operation is employed to clean spurious
edges. Thereafter, an ellipse is fitted to the edge map through constrained
parametric transformations and iterative goodness of fit calculations. The
method delimits the tissue edges through the curve fitting model, which has
shown high levels of accuracy. Thus, this method enables segmentation of
optoacoutic images with minimal human intervention, by eliminating need of
scale selection for multiscale processing and seed point determination for
contour mapping.
"
eess,"Low PMEPR OFDM radar waveform design using the iterative least squares
  algorithm","  This letter considers waveform design of orthogonal frequency division
multiplexing (OFDM) signal for radar applications, and aims at mitigating the
envelope fluctuation in OFDM. A novel method is proposed to reduce the
peak-to-mean envelope power ratio (PMEPR), which is commonly used to evaluate
the fluctuation. The proposed method is based on the tone reservation approach,
in which some bits or subcarriers of OFDM are allocated for decreasing PMEPR.
We introduce the coefficient of variation of envelopes (CVE) as the cost
function for waveform optimization, and develop an iterative least squares
algorithm. Minimizing CVE leads to distinct PMEPR reduction, and it is
guaranteed that the cost function monotonically decreases by applying the
iterative algorithm. Simulations demonstrate that the envelope is significantly
smoothed by the proposed method.
"
eess,"An Iterative Receiver for OFDM With Sparsity-Based Parametric Channel
  Estimation","  In this work we design a receiver that iteratively passes soft information
between the channel estimation and data decoding stages. The receiver
incorporates sparsity-based parametric channel estimation. State-of-the-art
sparsity-based iterative receivers simplify the channel estimation problem by
restricting the multipath delays to a grid. Our receiver does not impose such a
restriction. As a result it does not suffer from the leakage effect, which
destroys sparsity. Communication at near capacity rates in high SNR requires a
large modulation order. Due to the close proximity of modulation symbols in
such systems, the grid-based approximation is of insufficient accuracy. We show
numerically that a state-of-the-art iterative receiver with grid-based sparse
channel estimation exhibits a bit-error-rate floor in the high SNR regime. On
the contrary, our receiver performs very close to the perfect channel state
information bound for all SNR values. We also demonstrate both theoretically
and numerically that parametric channel estimation works well in dense
channels, i.e., when the number of multipath components is large and each
individual component cannot be resolved.
"
eess,A Pessimistic Approximation for the Fisher Information Measure,"  The problem of determining the intrinsic quality of a signal processing
system with respect to the inference of an unknown deterministic parameter
$\theta$ is considered. While the Fisher information measure $F(\theta)$ forms
a classical tool for such a problem, direct computation of the information
measure can become difficult in various situations. For the estimation
theoretic performance analysis of nonlinear measurement systems, the form of
the likelihood function can make the calculation of the information measure
$F(\theta)$ challenging. In situations where no closed-form expression of the
statistical system model is available, the analytical derivation of $F(\theta)$
is not possible at all. Based on the Cauchy-Schwarz inequality, we derive an
alternative information measure $S(\theta)$. It provides a lower bound on the
Fisher information $F(\theta)$ and has the property of being evaluated with the
mean, the variance, the skewness and the kurtosis of the system model at hand.
These entities usually exhibit good mathematical tractability or can be
determined at low-complexity by real-world measurements in a calibrated setup.
With various examples, we show that $S(\theta)$ provides a good conservative
approximation for $F(\theta)$ and outline different estimation theoretic
problems where the presented information bound turns out to be useful.
"
eess,Dual-Layer Video Encryption using RSA Algorithm,"  This paper proposes a video encryption algorithm using RSA and Pseudo Noise
(PN) sequence, aimed at applications requiring sensitive video information
transfers. The system is primarily designed to work with files encoded using
the Audio Video Interleaved (AVI) codec, although it can be easily ported for
use with Moving Picture Experts Group (MPEG) encoded files. The audio and video
components of the source separately undergo two layers of encryption to ensure
a reasonable level of security. Encryption of the video component involves
applying the RSA algorithm followed by the PN-based encryption. Similarly, the
audio component is first encrypted using PN and further subjected to encryption
using the Discrete Cosine Transform. Combining these techniques, an efficient
system, invulnerable to security breaches and attacks with favorable values of
parameters such as encryption/decryption speed, encryption/decryption ratio and
visual degradation; has been put forth. For applications requiring encryption
of sensitive data wherein stringent security requirements are of prime concern,
the system is found to yield negligible similarities in visual perception
between the original and the encrypted video sequence. For applications wherein
visual similarity is not of major concern, we limit the encryption task to a
single level of encryption which is accomplished by using RSA, thereby
quickening the encryption process. Although some similarity between the
original and encrypted video is observed in this case, it is not enough to
comprehend the happenings in the video.
"
eess,"Visual Quality Enhancement in Optoacoustic Tomography using Active
  Contour Segmentation Priors","  Segmentation of biomedical images is essential for studying and
characterizing anatomical structures, detection and evaluation of pathological
tissues. Segmentation has been further shown to enhance the reconstruction
performance in many tomographic imaging modalities by accounting for
heterogeneities of the excitation field and tissue properties in the imaged
region. This is particularly relevant in optoacoustic tomography, where
discontinuities in the optical and acoustic tissue properties, if not properly
accounted for, may result in deterioration of the imaging performance.
Efficient segmentation of optoacoustic images is often hampered by the
relatively low intrinsic contrast of large anatomical structures, which is
further impaired by the limited angular coverage of some commonly employed
tomographic imaging configurations. Herein, we analyze the performance of
active contour models for boundary segmentation in cross-sectional optoacoustic
tomography. The segmented mask is employed to construct a two compartment model
for the acoustic and optical parameters of the imaged tissues, which is
subsequently used to improve accuracy of the image reconstruction routines. The
performance of the suggested segmentation and modeling approach are showcased
in tissue-mimicking phantoms and small animal imaging experiments.
"
eess,"Prediction-Adaptation-Correction Recurrent Neural Networks for
  Low-Resource Language Speech Recognition","  In this paper, we investigate the use of prediction-adaptation-correction
recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A
PAC-RNN is comprised of a pair of neural networks in which a {\it correction}
network uses auxiliary information given by a {\it prediction} network to help
estimate the state probability. The information from the correction network is
also used by the prediction network in a recurrent loop. Our model outperforms
other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.
Moreover, transfer learning from a language that is similar to the target
language can help improve performance further.
"
eess,Highway Long Short-Term Memory RNNs for Distant Speech Recognition,"  In this paper, we extend the deep long short-term memory (DLSTM) recurrent
neural networks by introducing gated direct connections between memory cells in
adjacent layers. These direct links, called highway connections, enable
unimpeded information flow across different layers and thus alleviate the
gradient vanishing problem when building deeper LSTMs. We further introduce the
latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole
history while keeping the latency under control. Efficient algorithms are
proposed to train these novel networks using both frame and sequence
discriminative criteria. Experiments on the AMI distant speech recognition
(DSR) task indicate that we can train deeper LSTMs and achieve better
improvement from sequence training with highway LSTMs (HLSTMs). Our novel model
obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all
previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and
$5.3\%$ relative improvement respectively.
"
eess,"On Computational Complexity Reduction Methods for Kalman Filter
  Extensions","  The Kalman filter and its extensions are used in a vast number of aerospace
and navigation applications for nonlinear state estimation of time series. In
the literature, different approaches have been proposed to exploit the
structure of the state and measurement models to reduce the computational
demand of the algorithms. In this tutorial, we survey existing code
optimization methods and present them using unified notation that allows them
to be used with various Kalman filter extensions. We develop the optimization
methods to cover a wider range of models, show how different structural
optimizations can be combined, and present new applications for the existing
optimizations. Furthermore, we present an example that shows that the
exploitation of the structure of the problem can lead to improved estimation
accuracy while reducing the computational load. This tutorial is intended for
persons who are familiar with Kalman filtering and want to get insights for
reducing the computational demand of different Kalman filter extensions.
"
eess,"Sensitivity Analysis for Binary Sampling Systems via Quantitative Fisher
  Information Lower Bounds","  The problem of determining the achievable sensitivity with digitization
exhibiting minimal complexity is addressed. In this case, measurements are
exclusively available in hard-limited form. Assessing the achievable
sensitivity via the Cram\'{e}r-Rao lower bound requires characterization of the
likelihood function, which is intractable for multivariate binary
distributions. In this context, the Fisher matrix of the exponential family and
a lower bound for arbitrary probabilistic models are discussed. The
conservative approximation for Fisher's information matrix rests on a surrogate
exponential family distribution connected to the actual data-generating system
by two compact equivalences. Without characterizing the likelihood and its
support, this probabilistic notion enables designing estimators that
consistently achieve the sensitivity as defined by the inverse of the
conservative information matrix. For parameter estimation with multivariate
binary samples, a quadratic exponential surrogate distribution tames
statistical complexity such that a quantitative assessment of an achievable
sensitivity level becomes tractable. This fact is exploited for the performance
analysis concerning parameter estimation with an array of low-complexity binary
sensors in comparison to an ideal system featuring infinite amplitude
resolution. Additionally, data-driven assessment by estimating a conservative
approximation for the Fisher matrix under recursive binary sampling as
implemented in $\Sigma\Delta$-modulating analog-to-digital converters is
demonstrated.
"
eess,"Plug-and-Play Priors for Bright Field Electron Tomography and Sparse
  Interpolation","  Many material and biological samples in scientific imaging are characterized
by non-local repeating structures. These are studied using scanning electron
microscopy and electron tomography. Sparse sampling of individual pixels in a
2D image acquisition geometry, or sparse sampling of projection images with
large tilt increments in a tomography experiment, can enable high speed data
acquisition and minimize sample damage caused by the electron beam.
  In this paper, we present an algorithm for electron tomographic
reconstruction and sparse image interpolation that exploits the non-local
redundancy in images. We adapt a framework, termed plug-and-play (P&P) priors,
to solve these imaging problems in a regularized inversion setting. The power
of the P&P approach is that it allows a wide array of modern denoising
algorithms to be used as a ""prior model"" for tomography and image
interpolation. We also present sufficient mathematical conditions that ensure
convergence of the P&P approach, and we use these insights to design a new
non-local means denoising algorithm. Finally, we demonstrate that the algorithm
produces higher quality reconstructions on both simulated and real electron
microscope data, along with improved convergence properties compared to other
methods.
"
eess,"Privacy, Secrecy, and Storage with Multiple Noisy Measurements of
  Identifiers","  The key-leakage-storage region is derived for a generalization of a classic
two-terminal key agreement model. The additions to the model are that the
encoder observes a hidden, or noisy, version of the identifier, and that the
encoder and decoder can perform multiple measurements. To illustrate the
behavior of the region, the theory is applied to binary identifiers and noise
modeled via binary symmetric channels. In particular, the key-leakage-storage
region is simplified by applying Mrs. Gerber's lemma twice in different
directions to a Markov chain. The growth in the region as the number of
measurements increases is quantified. The amount by which the privacy-leakage
rate reduces for a hidden identifier as compared to a noise-free (visible)
identifier at the encoder is also given. If the encoder incorrectly models the
source as visible, it is shown that substantial secrecy leakage may occur and
the reliability of the reconstructed key might decrease.
"
eess,Analysis of Random Pulse Repetition Interval Radar,"  Random pulse repetition interval (PRI) waveform arouses great interests in
the field of modern radars due to its ability to alleviate range and Doppler
ambiguities as well as enhance electronic counter-countermeasures (ECCM)
capabilities. Theoretical results pertaining to the statistical characteristics
of ambiguity function (AF) are derived in this work, indicating that the range
and Doppler ambiguities can be effectively suppressed by increasing the number
of pulses and the range of PRI jitters. This provides an important guidance in
terms of waveform design. As is well known, the significantly lifted sidelobe
pedestal induced by PRI randomization will degrade the performance of weak
target detection. Proceeding from that, we propose to employ orthogonal
matching pursuit (OMP) to overcome this issue. Simulation results demonstrate
that the OMP method can effectively lower the sidelobe pedestal of strong
target and improve the performance of weak target estimation.
"
eess,"Stochastic Battery Model for Aggregation of Thermostatically Controlled
  Loads","  The potential of demand side as a frequency reserve proposes interesting
opportunity in handling imbalances due to intermittent renewable energy
sources. This paper proposes a novel approach for computing the parameters of a
stochastic battery model representing the aggregation of Thermostatically
Controlled Loads (TCLs). A hysteresis based non-disruptive control is used
using priority stack algorithm to track the reference regulation signal. The
parameters of admissible ramp-rate and the charge limits of the battery are
dynamically calculated using the information from TCLs that is the status
(on/off), availability and relative temperature distance till the switching
boundary. The approach builds on and improves on the existing research work by
providing a straight-forward mechanism for calculation of stochastic parameters
of equivalent battery model. The effectiveness of proposed approach is
demonstrated by a test case having a large number of residential TCLs tracking
a scaled down real frequency regulation signal.
"
eess,Voltage stress minimization by optimal reactive power control,"  A standard operational requirement in power systems is that the voltage
magnitudes lie within prespecified bounds. Conventional engineering wisdom
suggests that such a tightly-regulated profile, imposed for system design
purposes and good operation of the network, should also guarantee a secure
system, operating far from static bifurcation instabilities such as voltage
collapse. In general however, these two objectives are distinct and must be
separately enforced. We formulate an optimization problem which maximizes the
distance to voltage collapse through injections of reactive power, subject to
power flow and operational voltage constraints. By exploiting a linear
approximation of the power flow equations we arrive at a convex reformulation
which can be efficiently solved for the optimal injections. We also address the
planning problem of allocating the resources by recasting our problem in a
sparsity-promoting framework that allows us to choose a desired trade-off
between optimality of injections and the number of required actuators. Finally,
we present a distributed algorithm to solve the optimization problem, showing
that it can be implemented on-line as a feedback controller. We illustrate the
performance of our results with the IEEE30 bus network.
"
eess,"Grading of Mammalian Cumulus Oocyte Complexes using Machine Learning for
  in Vitro Embryo Culture","  Visual observation of Cumulus Oocyte Complexes provides only limited
information about its functional competence, whereas the molecular evaluations
methods are cumbersome or costly. Image analysis of mammalian oocytes can
provide attractive alternative to address this challenge. However, it is
complex, given the huge number of oocytes under inspection and the subjective
nature of the features inspected for identification. Supervised machine
learning methods like random forest with annotations from expert biologists can
make the analysis task standardized and reduces inter-subject variability. We
present a semi-automatic framework for predicting the class an oocyte belongs
to, based on multi-object parametric segmentation on the acquired microscopic
image followed by a feature based classification using random forests.
"
eess,Subsampling for Graph Power Spectrum Estimation,"  In this paper we focus on subsampling stationary random processes that reside
on the vertices of undirected graphs. Second-order stationary graph signals are
obtained by filtering white noise and they admit a well-defined power spectrum.
Estimating the graph power spectrum forms a central component of stationary
graph signal processing and related inference tasks. We show that by sampling a
significantly smaller subset of vertices and using simple least squares, we can
reconstruct the power spectrum of the graph signal from the subsampled
observations, without any spectral priors. In addition, a near-optimal greedy
algorithm is developed to design the subsampling scheme.
"
eess,"A pairwise approach to simultaneous onset/offset detection for singing
  voice using correntropy","  In this paper, we propose a novelmethod to search for precise locations of
paired note onset and offset in a singing voice signal. In comparison with the
existing onset detection algorithms,our approach differs in two key respects.
First, we employ Correntropy, a generalized correlation function inspired from
Reyni's entropy, as a detection function to capture the instantaneous flux
while preserving insensitiveness to outliers. Next, a novel peak picking
algorithm is specially designed for this detection function. By calculating the
fitness of a pre-defined inverse hyperbolic kernel to a detection function, it
is possible to find an onset and its corresponding offset simultaneously.
Experimental results show that the proposed method achieves performance
significantly better than or comparable to other state-of-the-art techniques
for onset detection in singing voice.
"
eess,"Extended Object Tracking: Introduction, Overview and Applications","  This article provides an elaborate overview of current research in extended
object tracking. We provide a clear definition of the extended object tracking
problem and discuss its delimitation to other types of object tracking. Next,
different aspects of extended object modelling are extensively discussed.
Subsequently, we give a tutorial introduction to two basic and well used
extended object tracking approaches - the random matrix approach and the Kalman
filter-based approach for star-convex shapes. The next part treats the tracking
of multiple extended objects and elaborates how the large number of feasible
association hypotheses can be tackled using both Random Finite Set (RFS) and
Non-RFS multi-object trackers. The article concludes with a summary of current
applications, where four example applications involving camera, X-band radar,
light detection and ranging (lidar), red-green-blue-depth (RGB-D) sensors are
highlighted.
"
eess,"Brain Emotional Learning-Based Prediction Model (For Long-Term Chaotic
  Prediction Applications)","  This study suggests a new prediction model for chaotic time series inspired
by the brain emotional learning of mammals. We describe the structure and
function of this model, which is referred to as BELPM (Brain Emotional
Learning-Based Prediction Model). Structurally, the model mimics the connection
between the regions of the limbic system, and functionally it uses weighted k
nearest neighbors to imitate the roles of those regions. The learning algorithm
of BELPM is defined using steepest descent (SD) and the least square estimator
(LSE). Two benchmark chaotic time series, Lorenz and Henon, have been used to
evaluate the performance of BELPM. The obtained results have been compared with
those of other prediction methods. The results show that BELPM has the
capability to achieve a reasonable accuracy for long-term prediction of chaotic
time series, using a limited amount of training data and a reasonably low
computational time.
"
eess,On the Performance of Mismatched Data Detection in Large MIMO Systems,"  We investigate the performance of mismatched data detection in large
multiple-input multiple-output (MIMO) systems, where the prior distribution of
the transmit signal used in the data detector differs from the true prior. To
minimize the performance loss caused by this prior mismatch, we include a
tuning stage into our recently-proposed large MIMO approximate message passing
(LAMA) algorithm, which allows us to develop mismatched LAMA algorithms with
optimal as well as sub-optimal tuning. We show that carefully-selected priors
often enable simpler and computationally more efficient algorithms compared to
LAMA with the true prior while achieving near-optimal performance. A
performance analysis of our algorithms for a Gaussian prior and a uniform prior
within a hypercube covering the QAM constellation recovers classical and recent
results on linear and non-linear MIMO data detection, respectively.
"
eess,An Efficient and Flexible Spike Train Model via Empirical Bayes,"  Accurate statistical models of neural spike responses can characterize the
information carried by neural populations. But the limited samples of spike
counts during recording usually result in model overfitting. Besides, current
models assume spike counts to be Poisson-distributed, which ignores the fact
that many neurons demonstrate over-dispersed spiking behaviour. Although the
Negative Binomial Generalized Linear Model (NB-GLM) provides a powerful tool
for modeling over-dispersed spike counts, the maximum likelihood-based standard
NB-GLM leads to highly variable and inaccurate parameter estimates. Thus, we
propose a hierarchical parametric empirical Bayes method to estimate the neural
spike responses among neuronal population. Our method integrates both
Generalized Linear Models (GLMs) and empirical Bayes theory, which aims to (1)
improve the accuracy and reliability of parameter estimation, compared to the
maximum likelihood-based method for NB-GLM and Poisson-GLM; (2) effectively
capture the over-dispersion nature of spike counts from both simulated data and
experimental data; and (3) provide insight into both neural interactions and
spiking behaviours of the neuronal populations. We apply our approach to study
both simulated data and experimental neural data. The estimation of simulation
data indicates that the new framework can accurately predict mean spike counts
simulated from different models and recover the connectivity weights among
neural populations. The estimation based on retinal neurons demonstrate the
proposed method outperforms both NB-GLM and Poisson-GLM in terms of the
predictive log-likelihood of held-out data. Codes are available in
https://doi.org/10.5281/zenodo.4704423
"
eess,Optimal Number of Choices in Rating Contexts,"  In many settings people must give numerical scores to entities from a small
discrete set. For instance, rating physical attractiveness from 1--5 on dating
sites, or papers from 1--10 for conference reviewing. We study the problem of
understanding when using a different number of options is optimal. We consider
the case when scores are uniform random and Gaussian. We study computationally
when using 2, 3, 4, 5, and 10 options out of a total of 100 is optimal in these
models (though our theoretical analysis is for a more general setting with $k$
choices from $n$ total options as well as a continuous underlying space). One
may expect that using more options would always improve performance in this
model, but we show that this is not necessarily the case, and that using fewer
choices---even just two---can surprisingly be optimal in certain situations.
While in theory for this setting it would be optimal to use all 100 options, in
practice this is prohibitive, and it is preferable to utilize a smaller number
of options due to humans' limited computational resources. Our results could
have many potential applications, as settings requiring entities to be ranked
by humans are ubiquitous. There could also be applications to other fields such
as signal or image processing where input values from a large set must be
mapped to output values in a smaller set.
"
eess,"Using instantaneous frequency and aperiodicity detection to estimate F0
  for high-quality speech synthesis","  This paper introduces a general and flexible framework for F0 and
aperiodicity (additive non periodic component) analysis, specifically intended
for high-quality speech synthesis and modification applications. The proposed
framework consists of three subsystems: instantaneous frequency estimator and
initial aperiodicity detector, F0 trajectory tracker, and F0 refinement and
aperiodicity extractor. A preliminary implementation of the proposed framework
substantially outperformed (by a factor of 10 in terms of RMS F0 estimation
error) existing F0 extractors in tracking ability of temporally varying F0
trajectories. The front end aperiodicity detector consists of a complex-valued
wavelet analysis filter with a highly selective temporal and spectral envelope.
This front end aperiodicity detector uses a new measure that quantifies the
deviation from periodicity. The measure is less sensitive to slow FM and AM and
closely correlates with the signal to noise ratio.
"
eess,Multiple target tracking based on sets of trajectories,"  We propose a solution of the multiple target tracking (MTT) problem based on
sets of trajectories and the random finite set framework. A full Bayesian
approach to MTT should characterise the distribution of the trajectories given
the measurements, as it contains all information about the trajectories. We
attain this by considering multi-object density functions in which objects are
trajectories. For the standard tracking models, we also describe a conjugate
family of multitrajectory density functions.
"
eess,"Permutation Invariant Training of Deep Models for Speaker-Independent
  Multi-talker Speech Separation","  We propose a novel deep learning model, which supports permutation invariant
training (PIT), for speaker independent multi-talker speech separation,
commonly known as the cocktail-party problem. Different from most of the prior
arts that treat speech separation as a multi-class regression problem and the
deep clustering technique that considers it a segmentation (or clustering)
problem, our model optimizes for the separation regression error, ignoring the
order of mixing sources. This strategy cleverly solves the long-lasting label
permutation problem that has prevented progress on deep learning based
techniques for speech separation. Experiments on the equal-energy mixing setup
of a Danish corpus confirms the effectiveness of PIT. We believe improvements
built upon PIT can eventually solve the cocktail-party problem and enable
real-world adoption of, e.g., automatic meeting transcription and multi-party
human-computer interaction, where overlapping speech is common.
"
eess,How Much Do Downlink Pilots Improve Cell-Free Massive MIMO?,"  In this paper, we analyze the benefits of including downlink pilots in a
cell-free massive MIMO system. We derive an approximate per-user achievable
downlink rate for conjugate beamforming processing, which takes into account
both uplink and downlink channel estimation errors, and power control. A
performance comparison is carried out, in terms of per-user net throughput,
considering cell-free massive MIMO operation with and without downlink
training, for different network densities. We take also into account the
performance improvement provided by max-min fairness power control in the
downlink. Numerical results show that, exploiting downlink pilots, the
performance can be considerably improved in low density networks over the
conventional scheme where the users rely on statistical channel knowledge only.
In high density networks, performance improvements are moderate.
"
eess,"On the Performance of Cell-Free Massive MIMO with Short-Term Power
  Constraints","  In this paper we consider a time-division duplex cell-free massive
multiple-input multiple-output (MIMO) system where many distributed access
points (APs) simultaneously serve many users. A normalized conjugate
beamforming scheme, which satisfies short-term average power constraints at the
APs, is proposed and analyzed taking into account the effect of imperfect
channel information. We derive an approximate closed-form expression for the
per-user achievable downlink rate of this scheme. We also provide, analytically
and numerically, a performance comparison between the normalized conjugate
beamforming and the conventional conjugate beamforming scheme in [1] (which
satisfies long-term average power constraints). Normalized conjugate
beamforming scheme reduces the beamforming uncertainty gain, which comes from
the users' lack of the channel state information knowledge, and hence, it
improves the achievable downlink rate compared to the conventional conjugate
beamforming scheme.
"
eess,Learning Sparse Graphs Under Smoothness Prior,"  In this paper, we are interested in learning the underlying graph structure
behind training data. Solving this basic problem is essential to carry out any
graph signal processing or machine learning task. To realize this, we assume
that the data is smooth with respect to the graph topology, and we parameterize
the graph topology using an edge sampling function. That is, the graph
Laplacian is expressed in terms of a sparse edge selection vector, which
provides an explicit handle to control the sparsity level of the graph. We
solve the sparse graph learning problem given some training data in both the
noiseless and noisy settings. Given the true smooth data, the posed sparse
graph learning problem can be solved optimally and is based on simple rank
ordering. Given the noisy data, we show that the joint sparse graph learning
and denoising problem can be simplified to designing only the sparse edge
selection vector, which can be solved using convex optimization.
"
eess,The Microsoft 2016 Conversational Speech Recognition System,"  We describe Microsoft's conversational speech recognition system, in which we
combine recent developments in neural-network-based acoustic and language
modeling to advance the state of the art on the Switchboard recognition task.
Inspired by machine learning ensemble techniques, the system uses a range of
convolutional and recurrent neural networks. I-vector modeling and lattice-free
MMI training provide significant gains for all acoustic model architectures.
Language model rescoring with multiple forward and backward running RNNLMs, and
word posterior-based system combination provide a 20% boost. The best single
system uses a ResNet architecture acoustic model with RNNLM rescoring, and
achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The
combined system has an error rate of 6.2%, representing an improvement over
previously reported results on this benchmark task.
"
eess,Complex Laplacian based Distributed Control for Multi-Agent Network,"  The work done in this paper, proposes a complex Laplacian-based distributed
control scheme for convergence in the multi-agent network. The proposed scheme
has been designated as cascade formulation. The proposed technique exploits the
traditional method of organizing large scattered networks into smaller
interconnected clusters to optimize information flow within the network. The
complex Laplacian-based approach results in a hierarchical structure, with
formation of a meta-cluster leading other clusters in the network. The proposed
formulation enables flexibility to constrain the eigen spectra of the overall
closed-loop dynamics, ensuring desired convergence rate and control input
intensity. The sufficient conditions ensuring globally stable formation for
proposed formulation are also asserted. Robustness of the proposed formulation
to uncertainties like loss in communication links and actuator failure has also
been discussed. The effectiveness of the proposed approach is illustrated by
simulating a finitely large network of thirty vehicles.
"
eess,"Macroscopic Modeling, Calibration, and Simulation of Managed
  Lane-Freeway Networks, Part I: Topological and Phenomenological Modeling","  To help mitigate road congestion caused by the unrelenting growth of traffic
demand, many transit authorities have implemented managed lane policies.
Managed lanes typically run parallel to a freeway's standard, general-purpose
(GP) lanes, but are restricted to certain types of vehicles. It was originally
thought that managed lanes would improve the use of existing infrastructure
through incentivization of demand-management behaviors like carpooling, but
implementations have often been characterized by unpredicted phenomena that is
often to detrimental system performance.
  This paper presents several macroscopic traffic modeling tools we have used
for study of freeways equipped with managed lanes, or ""managed lane-freeway
networks."" The proposed framework is based on the widely-used first-order
kinematic wave theory. In this model, the GP and the managed lanes are modeled
as parallel links connected by nodes, where certain type of traffic may switch
between GP and managed lane links. Two types of managed lane topologies are
considered: full-access, where vehicles can switch between the GP and the
managed lanes anywhere; and separated, where such switching is allowed only at
certain locations called gates.
  We also describe methods to incorporate in three phenomena into our model
that are particular to managed lane-freeway networks. The inertia effect
reflects drivers' inclination to stay in their lane as long as possible and
switch only if this would obviously improve their travel condition. The
friction effect reflects the empirically-observed driver fear of moving fast in
a managed lane while traffic in the adjacent GP lanes moves slowly due to
congestion. The smoothing effect describes how managed lanes can increase
throughput at bottlenecks by reducing lane changes. We present simple models
for each of these phenomena that fit within the general macroscopic theory.
"
eess,"Approximate Gram-Matrix Interpolation for Wideband Massive MU-MIMO
  Systems","  Numerous linear and non-linear data-detection and precoding algorithms for
wideband massive multi-user (MU) multiple-input multiple-output (MIMO) wireless
systems that rely on orthogonal frequency-division multiplexing (OFDM) or
single-carrier frequency-division multiple access (SC-FDMA) require the
computation of the Gram matrix for each active subcarrier. Computing the Gram
matrix for each active subcarrier, however, results in excessively high
computational complexity. In this paper, we propose novel, approximate
algorithms that significantly reduce the complexity of Gram-matrix computation
by simultaneously exploiting correlation across subcarriers and channel
hardening. We show analytically that a small fraction of Gram-matrix
computations in combination with approximate interpolation schemes are
sufficient to achieve near-optimal error-rate performance at low computational
complexity in massive MU-MIMO systems. We also demonstrate that the proposed
methods exhibit improved robustness against channel-estimation errors compared
to exact Gram-matrix interpolation algorithms that typically require high
computational complexity.
"
eess,"Covert Single-hop Communication in a Wireless Network with Distributed
  Artificial Noise Generation","  Covert communication, also known as low probability of detection (LPD)
communication, prevents the adversary from knowing that a communication is
taking place. Recent work has demonstrated that, in a three-party scenario with
a transmitter (Alice), intended recipient (Bob), and adversary (Warden Willie),
the maximum number of bits that can be transmitted reliably from Alice to Bob
without detection by Willie, when additive white Gaussian noise (AWGN) channels
exist between all parties, is on the order of the square root of the number of
channel uses. In this paper, we begin consideration of network scenarios by
studying the case where there are additional ""friendly"" nodes present in the
environment that can produce artificial noise to aid in hiding the
communication. We establish achievability results by considering constructions
where the system node closest to the warden produces artificial noise and
demonstrate a significant improvement in the throughput achieved covertly,
without requiring close coordination between Alice and the noise-generating
node. Conversely, under mild restrictions on the communication strategy, we
demonstrate no higher covert throughput is possible. Extensions to the
consideration of the achievable covert throughput when multiple wardens
randomly located in the environment collaborate to attempt detection of the
transmitter are also considered.
"
eess,A System Level Approach to Controller Synthesis,"  Biological and advanced cyberphysical control systems often have limited,
sparse, uncertain, and distributed communication and computing in addition to
sensing and actuation. Fortunately, the corresponding plants and performance
requirements are also sparse and structured, and this must be exploited to make
constrained controller design feasible and tractable. We introduce a new
""system level"" (SL) approach involving three complementary SL elements. System
Level Parameterizations (SLPs) generalize state space and Youla
parameterizations of all stabilizing controllers and the responses they
achieve, and combine with System Level Constraints (SLCs) to parameterize the
largest known class of constrained stabilizing controllers that admit a convex
characterization, generalizing quadratic invariance (QI). SLPs also lead to a
generalization of detectability and stabilizability, suggesting the existence
of a rich separation structure, that when combined with SLCs, is naturally
applicable to structurally constrained controllers and systems. We further
provide a catalog of useful SLCs, most importantly including sparsity, delay,
and locality constraints on both communication and computing internal to the
controller, and external system performance. The resulting System Level
Synthesis (SLS) problems that arise define the broadest known class of
constrained optimal control problems that can be solved using convex
programming. An example illustrates how this system level approach can
systematically explore tradeoffs in controller performance, robustness, and
synthesis/implementation complexity.
"
eess,Achieving Human Parity in Conversational Speech Recognition,"  Conversational speech recognition has served as a flagship speech recognition
task since the release of the Switchboard corpus in the 1990s. In this paper,
we measure the human error rate on the widely used NIST 2000 test set, and find
that our latest automated system has reached human parity. The error rate of
professional transcribers is 5.9% for the Switchboard portion of the data, in
which newly acquainted pairs of people discuss an assigned topic, and 11.3% for
the CallHome portion where friends and family members have open-ended
conversations. In both cases, our automated system establishes a new state of
the art, and edges past the human benchmark, achieving error rates of 5.8% and
11.0%, respectively. The key to our system's performance is the use of various
convolutional and LSTM acoustic model architectures, combined with a novel
spatial smoothing method and lattice-free MMI acoustic training, multiple
recurrent neural network language modeling approaches, and a systematic use of
system combination.
"
eess,Inverse Power Flow Problem,"  This paper formulates an inverse power flow problem which is to infer a nodal
admittance matrix (hence the network structure of a power system) from voltage
and current phasors measured at a number of buses. We show that the admittance
matrix can be uniquely identified from a sequence of measurements corresponding
to different steady states when every node in the system is equipped with a
measurement device, and a Kron-reduced admittance matrix can be determined even
if some nodes in the system are not monitored (hidden nodes). Furthermore, we
propose effective algorithms based on graph theory to uncover the actual
admittance matrix of radial systems with hidden nodes. We provide theoretical
guarantees for the recovered admittance matrix and demonstrate that the actual
admittance matrix can be fully recovered even from the Kron-reduced admittance
matrix under some mild assumptions. Simulations on standard test systems
confirm that these algorithms are capable of providing accurate estimates of
the admittance matrix from noisy sensor data.
"
eess,"Approximate eigenvalue distribution of a cylindrically isotropic noise
  sample covariance matrix","  The statistical behavior of the eigenvalues of the sample covariance matrix
(SCM) plays a key role in determining the performance of adaptive beamformers
(ABF) in presence of noise. This paper presents a method to compute the
approximate eigenvalue density function (EDF) for the SCM of a \cin{} field
when only a finite number of shapshots are available. The EDF of the ensemble
covariance matrix (ECM) is modeled as an atomic density with many fewer atoms
than the SCM size. The model results in substantial computational savings over
more direct methods of computing the EDF. The approximate EDF obtained from
this method agrees closely with histograms of eigenvalues obtained from
simulation.
"
eess,Unit circle MVDR beamformer,"  The array polynomial is the z-transform of the array weights for a narrowband
planewave beamformer using a uniform linear array (ULA). Evaluating the array
polynomial on the unit circle in the complex plane yields the beampattern. The
locations of the polynomial zeros on the unit circle indicate the nulls of the
beampattern. For planewave signals measured with a ULA, the locations of the
ensemble MVDR polynomial zeros are constrained on the unit circle. However,
sample matrix inversion (SMI) MVDR polynomial zeros generally do not fall on
the unit circle. The proposed unit circle MVDR (UC MVDR) projects the zeros of
the SMI MVDR polynomial radially on the unit circle. This satisfies the
constraint on the zeros of ensemble MVDR polynomial. Numerical simulations show
that the UC MVDR beamformer suppresses interferers better than the SMI MVDR and
the diagonal loaded MVDR beamformer and also improves the white noise gain
(WNG).
"
eess,"Computationally Efficient Target Classification in Multispectral Image
  Data with Deep Neural Networks","  Detecting and classifying targets in video streams from surveillance cameras
is a cumbersome, error-prone and expensive task. Often, the incurred costs are
prohibitive for real-time monitoring. This leads to data being stored locally
or transmitted to a central storage site for post-incident examination. The
required communication links and archiving of the video data are still
expensive and this setup excludes preemptive actions to respond to imminent
threats. An effective way to overcome these limitations is to build a smart
camera that transmits alerts when relevant video sequences are detected. Deep
neural networks (DNNs) have come to outperform humans in visual classifications
tasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be
extended to make use of higher-dimensional input data such as multispectral
data. We explore this opportunity in terms of achievable accuracy and required
computational effort. To analyze the precision of DNNs for scene labeling in an
urban surveillance scenario we have created a dataset with 8 classes obtained
in a field experiment. We combine an RGB camera with a 25-channel VIS-NIR
snapshot sensor to assess the potential of multispectral image data for target
classification. We evaluate several new DNNs, showing that the spectral
information fused together with the RGB frames can be used to improve the
accuracy of the system or to achieve similar accuracy with a 3x smaller
computation effort. We achieve a very high per-pixel accuracy of 99.1%. Even
for scarcely occurring, but particularly interesting classes, such as cars, 75%
of the pixels are labeled correctly with errors occurring only around the
border of the objects. This high accuracy was obtained with a training set of
only 30 labeled images, paving the way for fast adaptation to various
application scenarios.
"
eess,Stall Pattern Avoidance in Polynomial Product Codes,"  Product codes are a concatenated error-correction scheme that has been often
considered for applications requiring very low bit-error rates, which demand
that the error floor be decreased as much as possible. In this work, we
consider product codes constructed from polynomial algebraic codes, and propose
a novel low-complexity post-processing technique that is able to improve the
error-correction performance by orders of magnitude. We provide lower bounds
for the error rate achievable under post processing, and present simulation
results indicating that these bounds are tight.
"
eess,"Image Processing with Dipole-Coupled Nanomagnets: Noise Suppression and
  Edge Enhancement Detection","  Hardware based image processing offers speed and convenience not found in
software-centric approaches. Here, we show theoretically that a two-dimensional
periodic array of dipole-coupled elliptical nanomagnets, delineated on a
piezoelectric substrate, can act as a dynamical system for specific image
processing functions. Each nanomagnet has two stable magnetization states that
encode pixel color (black or white). An image containing black and white pixels
is first converted to voltage states and then mapped into the magnetization
states of a nanomagnet array with magneto-tunneling junctions (MTJs). The same
MTJs are employed to read out the processed pixel colors later. Dipole
interaction between the nanomagnets implements specific image processing tasks
such as noise reduction and edge enhancement detection. These functions are
triggered by applying a global strain to the nanomagnets with a voltage dropped
across the piezoelectric substrate. An image containing an arbitrary number of
black and white pixels can be processed in few nanoseconds with very low energy
cost.
"
eess,"Microseismic events enhancement and detection in sensor arrays using
  autocorrelation based filtering","  Passive microseismic data are commonly buried in noise, which presents a
significant challenge for signal detection and recovery. For recordings from a
surface sensor array where each trace contains a time-delayed arrival from the
event, we propose an autocorrelation-based stacking method that designs a
denoising filter from all the traces, as well as a multi-channel detection
scheme. This approach circumvents the issue of time aligning the traces prior
to stacking because every trace's autocorrelation is centered at zero in the
lag domain. The effect of white noise is concentrated near zero lag, so the
filter design requires a predictable adjustment of the zero-lag value.
Truncation of the autocorrelation is employed to smooth the impulse response of
the denoising filter. In order to extend the applicability of the algorithm, we
also propose a noise prewhitening scheme that addresses cases with colored
noise. The simplicity and robustness of this method are validated with
synthetic and real seismic traces.
"
eess,"Safety Verification and Control for Collision Avoidance at Road
  Intersections","  This paper presents the design of a supervisory algorithm that monitors
safety at road intersections and overrides drivers with a safe input when
necessary. The design of the supervisor consists of two parts: safety
verification and control design. Safety verification is the problem to
determine if vehicles will be able to cross the intersection without colliding
with current drivers' inputs. We translate this safety verification problem
into a jobshop scheduling problem, which minimizes the maximum lateness and
evaluates if the optimal cost is zero. The zero optimal cost corresponds to the
case in which all vehicles can cross each conflict area without collisions.
Computing the optimal cost requires solving a Mixed Integer Nonlinear
Programming (MINLP) problem due to the nonlinear second-order dynamics of the
vehicles. We therefore estimate this optimal cost by formulating two related
Mixed Integer Linear Programming (MILP) problems that assume simpler vehicle
dynamics. We prove that these two MILP problems yield lower and upper bounds of
the optimal cost. We also quantify the worst case approximation errors of these
MILP problems. We design the supervisor to override the vehicles with a safe
control input if the MILP problem that computes the upper bound yields a
positive optimal cost. We theoretically demonstrate that the supervisor keeps
the intersection safe and is non-blocking. Computer simulations further
validate that the algorithms can run in real time for problems of realistic
size.
"
eess,Kernel-based Reconstruction of Space-time Functions on Dynamic Graphs,"  Graph-based methods pervade the inference toolkits of numerous disciplines
including sociology, biology, neuroscience, physics, chemistry, and
engineering. A challenging problem encountered in this context pertains to
determining the attributes of a set of vertices given those of another subset
at possibly different time instants. Leveraging spatiotemporal dynamics can
drastically reduce the number of observed vertices, and hence the cost of
sampling. Alleviating the limited flexibility of existing approaches, the
present paper broadens the existing kernel-based graph function reconstruction
framework to accommodate time-evolving functions over possibly time-evolving
topologies. This approach inherits the versatility and generality of
kernel-based methods, for which no knowledge on distributions or second-order
statistics is required. Systematic guidelines are provided to construct two
families of space-time kernels with complementary strengths. The first
facilitates judicious control of regularization on a space-time frequency
plane, whereas the second can afford time-varying topologies. Batch and online
estimators are also put forth, and a novel kernel Kalman filter is developed to
obtain these estimates at affordable computational cost. Numerical tests with
real data sets corroborate the merits of the proposed methods relative to
competing alternatives.
"
eess,"Local Sparse Approximation for Image Restoration with Adaptive Block
  Size Selection","  In this paper the problem of image restoration (denoising and inpainting) is
approached using sparse approximation of local image blocks. The local image
blocks are extracted by sliding square windows over the image. An adaptive
block size selection procedure for local sparse approximation is proposed,
which affects the global recovery of underlying image. Ideally the adaptive
local block selection yields the minimum mean square error (MMSE) in recovered
image. This framework gives us a clustered image based on the selected block
size, then each cluster is restored separately using sparse approximation. The
results obtained using the proposed framework are very much comparable with the
recently proposed image restoration techniques.
"
eess,Image biomarker standardisation initiative,"  The image biomarker standardisation initiative (IBSI) is an independent
international collaboration which works towards standardising the extraction of
image biomarkers from acquired imaging for the purpose of high-throughput
quantitative image analysis (radiomics). Lack of reproducibility and validation
of high-throughput quantitative image analysis studies is considered to be a
major challenge for the field. Part of this challenge lies in the scantiness of
consensus-based guidelines and definitions for the process of translating
acquired imaging into high-throughput image biomarkers. The IBSI therefore
seeks to provide image biomarker nomenclature and definitions, benchmark data
sets, and benchmark values to verify image processing and image biomarker
calculations, as well as reporting guidelines, for high-throughput image
analysis.
"
eess,"Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive
  Patterns in Vowel Acoustics","  Most of the previous approaches to lyrics-to-audio alignment used a
pre-developed automatic speech recognition (ASR) system that innately suffered
from several difficulties to adapt the speech model to individual singers. A
significant aspect missing in previous works is the self-learnability of
repetitive vowel patterns in the singing voice, where the vowel part used is
more consistent than the consonant part. Based on this, our system first learns
a discriminative subspace of vowel sequences, based on weighted symmetric
non-negative matrix factorization (WS-NMF), by taking the self-similarity of a
standard acoustic feature as an input. Then, we make use of canonical time
warping (CTW), derived from a recent computer vision technique, to find an
optimal spatiotemporal transformation between the text and the acoustic
sequences. Experiments with Korean and English data sets showed that deploying
this method after a pre-developed, unsupervised, singing source separation
achieved more promising results than other state-of-the-art unsupervised
approaches and an existing ASR-based system.
"
eess,"PI(D) tuning for Flight Control Systems via Incremental Nonlinear
  Dynamic Inversion","  Previous results reported in the robotics literature show the relationship
between time-delay control (TDC) and proportional-integral-derivative control
(PID). In this paper, we show that incremental nonlinear dynamic inversion
(INDI) - more familiar in the aerospace community - are in fact equivalent to
TDC. This leads to a meaningful and systematic method for PI(D)-control tuning
of robust nonlinear flight control systems via INDI. We considered a
reformulation of the plant dynamics inversion which removes effector blending
models from the resulting control law, resulting in robust model-free control
laws like PI(D)-control.
"
eess,"A New View of Multi-User Hybrid Massive MIMO: Non-Orthogonal Angle
  Division Multiple Access","  This paper presents a new view of multi-user (MU) hybrid massive
multiple-input and multiple-output (MIMO) systems from array signal processing
perspective. We first show that the instantaneous channel vectors corresponding
to different users are asymptotically orthogonal if the angles of arrival
(AOAs) of users are different. We then decompose the channel matrix into an
angle domain basis matrix and a gain matrix. The former can be formulated by
steering vectors and the latter has the same size as the number of RF chains,
which perfectly matches the structure of hybrid precoding. A novel hybrid
channel estimation is proposed by separately estimating the angle information
and the gain matrix, which could significantly save the training overhead and
substantially improve the channel estimation accuracy compared to the
conventional beamspace approach. Moreover, with the aid of the angle domain
matrix, the MU massive MIMO system can be viewed as a type of non-orthogonal
angle division multiple access (ADMA) to simultaneously serve multiple users at
the same frequency band. Finally, the performance of the proposed scheme is
validated by computer simulation results.
"
eess,"A new cosine series antialiasing function and its application to
  aliasing-free glottal source models for speech and singing synthesis","  We formulated and implemented a procedure to generate aliasing-free
excitation source signals. It uses a new antialiasing filter in the continuous
time domain followed by an IIR digital filter for response equalization. We
introduced a cosine-series-based general design procedure for the new
antialiasing function. We applied this new procedure to implement the
antialiased Fujisaki-Ljungqvist model. We also applied it to revise our
previous implementation of the antialiased Fant-Liljencrants model. A
combination of these signals and a lattice implementation of the time varying
vocal tract model provides a reliable and flexible basis to test fo extractors
and source aperiodicity analysis methods. MATLAB implementations of these
antialiased excitation source models are available as part of our open source
tools for speech science.
"
eess,"Modification of Social Dominance in Social Networks by Selective
  Adjustment of Interpersonal Weights","  According to the DeGroot-Friedkin model of a social network, an individual's
social power evolves as the network discusses individual opinions over a
sequence of issues. Under mild assumptions on the connectivity of the network,
the social power of every individual converges to a constant strictly positive
value as the number of issues discussed increases. If the network has a special
topology, termed ""star topology"", then all social power accumulates with the
individual at the centre of the star. This paper studies the strategic
introduction of new individuals and/or interpersonal relationships into a
social network with star topology to reduce the social power of the centre
individual. In fact, several strategies are proposed. For each strategy, we
derive necessary and sufficient conditions on the strength of the new
interpersonal relationships, based on local information, which ensures that the
centre individual no longer has the greatest social power within the social
network. Interpretations of these conditions show that the strategies are
remarkably intuitive and that certain strategies are favourable compared to
others, all of which is sociologically expected.
"
eess,"Numerical Integration and Dynamic Discretization in Heuristic Search
  Planning over Hybrid Domains","  In this paper we look into the problem of planning over hybrid domains, where
change can be both discrete and instantaneous, or continuous over time. In
addition, it is required that each state on the trajectory induced by the
execution of plans complies with a given set of global constraints. We approach
the computation of plans for such domains as the problem of searching over a
deterministic state model. In this model, some of the successor states are
obtained by solving numerically the so-called initial value problem over a set
of ordinary differential equations (ODE) given by the current plan prefix.
These equations hold over time intervals whose duration is determined
dynamically, according to whether zero crossing events take place for a set of
invariant conditions. The resulting planner, FS+, incorporates these features
together with effective heuristic guidance. FS+ does not impose any of the
syntactic restrictions on process effects often found on the existing
literature on Hybrid Planning. A key concept of our approach is that a clear
separation is struck between planning and simulation time steps. The former is
the time allowed to observe the evolution of a given dynamical system before
committing to a future course of action, whilst the later is part of the model
of the environment. FS+ is shown to be a robust planner over a diverse set of
hybrid domains, taken from the existing literature on hybrid planning and
systems.
"
eess,"Robust Power System Dynamic State Estimator with Non-Gaussian
  Measurement Noise: Part I--Theory","  This paper develops the theoretical framework and the equations of a new
robust Generalized Maximum-likelihood-type Unscented Kalman Filter (GM-UKF)
that is able to suppress observation and innovation outliers while filtering
out non-Gaussian measurement noise. Because the errors of the real and reactive
power measurements calculated using Phasor Measurement Units (PMUs) follow
long-tailed probability distributions, the conventional UKF provides strongly
biased state estimates since it relies on the weighted least squares estimator.
By contrast, the state estimates and residuals of our GM-UKF are proved to be
roughly Gaussian, allowing the sigma points to reliably approximate the mean
and the covariance matrices of the predicted and corrected state vectors. To
develop our GM-UKF, we first derive a batch-mode regression form by processing
the predictions and observations simultaneously, where the statistical
linearization approach is used. We show that the set of equations so derived
are equivalent to those of the unscented transformation. Then, a robust
GM-estimator that minimizes a convex Huber cost function while using weights
calculated via Projection Statistics (PS's) is proposed. The PS's are applied
to a two-dimensional matrix that consists of serially correlated predicted
state and innovation vectors to detect observation and innovation outliers.
These outliers are suppressed by the GM-estimator using the iteratively
reweighted least squares algorithm. Finally, the asymptotic error covariance
matrix of the GM-UKF state estimates is derived from the total influence
function. In the companion paper, extensive simulation results will be shown to
verify the effectiveness and robustness of the proposed method.
"
eess,"On the Analysis of the DeGroot-Friedkin Model with Dynamic Relative
  Interaction Matrices","  This paper analyses the DeGroot-Friedkin model for evolution of the
individuals' social powers in a social network when the network topology varies
dynamically (described by dynamic relative interaction matrices). The
DeGroot-Friedkin model describes how individual social power (self-appraisal,
self-weight) evolves as a network of individuals discuss a sequence of issues.
We seek to study dynamically changing relative interactions because
interactions may change depending on the issue being discussed. In order to
explore the problem in detail, two different cases of issue-dependent network
topologies are studied. First, if the topology varies between issues in a
periodic manner, it is shown that the individuals' self-appraisals admit a
periodic solution. Second, if the topology changes arbitrarily, under the
assumption that each relative interaction matrix is doubly stochastic and
irreducible, the individuals' self-appraisals asymptotically converge to a
unique non-trivial equilibrium.
"
eess,"Robust Power System Dynamic State Estimator with Non-Gaussian
  Measurement Noise: Part II--Implementation and Results","  This paper is the second of a two-part series that discusses the
implementation issues and test results of a robust Unscented Kalman Filter
(UKF) for power system dynamic state estimation with non-Gaussian synchrophasor
measurement noise. The tuning of the parameters of our Generalized
Maximum-Likelihood-type robust UKF (GM-UKF) is presented and discussed in a
systematic way. Using simulations carried out on the IEEE 39-bus system, its
performance is evaluated under different scenarios, including i) the occurrence
of two different types of noises following thick-tailed distributions, namely
the Laplace or Cauchy probability distributions for real and reactive power
measurements; ii) the occurrence of observation and innovation outliers; iii)
the occurrence of PMU measurement losses due to communication failures; iv)
cyber attacks; and v) strong system nonlinearities. It is also compared to the
UKF and the Generalized Maximum-Likelihood-type robust iterated EKF (GM-IEKF).
Simulation results reveal that the GM-UKF outperforms the GM-IEKF and the UKF
in all scenarios considered. In particular, when the system is operating under
stressed conditions, inducing system nonlinearities, the GM-IEKF and the UKF
diverge while our GM-UKF does converge. In addition, when the power measurement
noises obey a Cauchy distribution, our GM-UKF converges to a state estimate
vector that exhibits a much higher statistical efficiency than that of the
GM-IEKF; by contrast, the UKF fails to converge. Finally, potential
applications and future work of the proposed GM-UKF are discussed in concluding
remarks section.
"
eess,"Empirical Evaluation of Parallel Training Algorithms on Acoustic
  Modeling","  Deep learning models (DLMs) are state-of-the-art techniques in speech
recognition. However, training good DLMs can be time consuming especially for
production-size models and corpora. Although several parallel training
algorithms have been proposed to improve training efficiency, there is no clear
guidance on which one to choose for the task in hand due to lack of systematic
and fair comparison among them. In this paper we aim at filling this gap by
comparing four popular parallel training algorithms in speech recognition,
namely asynchronous stochastic gradient descent (ASGD), blockwise model-update
filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging
stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using
feed-forward deep neural networks (DNNs) and convolutional, long short-term
memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the
top choice to train acoustic models since it is most stable, scales well with
number of GPUs, can achieve reproducible results, and in many cases even
outperforms single-GPU SGD. ASGD can be used as a substitute in some cases.
"
eess,"Multi-talker Speech Separation with Utterance-level Permutation
  Invariant Training of Deep Recurrent Neural Networks","  In this paper we propose the utterance-level Permutation Invariant Training
(uPIT) technique. uPIT is a practically applicable, end-to-end, deep learning
based solution for speaker independent multi-talker speech separation.
Specifically, uPIT extends the recently proposed Permutation Invariant Training
(PIT) technique with an utterance-level cost function, hence eliminating the
need for solving an additional permutation problem during inference, which is
otherwise required by frame-level PIT. We achieve this using Recurrent Neural
Networks (RNNs) that, during training, minimize the utterance-level separation
error, hence forcing separated frames belonging to the same speaker to be
aligned to the same output stream. In practice, this allows RNNs, trained with
uPIT, to separate multi-talker mixed speech without any prior knowledge of
signal duration, number of speakers, speaker identity or gender. We evaluated
uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks
and found that uPIT outperforms techniques based on Non-negative Matrix
Factorization (NMF) and Computational Auditory Scene Analysis (CASA), and
compares favorably with Deep Clustering (DPCL) and the Deep Attractor Network
(DANet). Furthermore, we found that models trained with uPIT generalize well to
unseen speakers and languages. Finally, we found that a single model, trained
with uPIT, can handle both two-speaker, and three-speaker speech mixtures.
"
eess,"Flatness-based control of a two-degree-of-freedom platform with
  pneumatic artificial muscles","  Pneumatic artificial muscles are a quite interesting type of actuators which
have a very high power-to-weight and power-to-volume ratio. However, their
efficient use requires very accurate control methods which can take into
account their complex dynamic, which is highly nonlinear. This paper consider a
model of two-degree-of-freedom platform whose attitude is determined by three
pneumatic muscles controlled by servovalves, which mimics a simplified version
of a Stewart platform. For this testbed, a model-based control approach is
proposed, based on accurate first principle modeling of the muscles and the
platform and on a static model for the servovalve. The employed control method
is the so-called flatness-based control introduced by Fliess. The paper first
recalls the basics of this control technique and then it shows how it can be
applied to the proposed experimental platform; being flatness-based control an
open-loop kind of control, a proportional-integral controller is added on top
of it in order to add robustness with respect to modelling errors and external
perturbations. At the end of the paper, the effectiveness of the proposed
approach is shown by means of experimental results. A clear improvement of the
tracking performance is visible compared to a simple proportional-integral
controller.
"
eess,"Cooperative Localisation of a GPS-Denied UAV in 3-Dimensional Space
  Using Direction of Arrival Measurements","  This paper presents a novel approach for localising a GPS (Global Positioning
System)-denied Unmanned Aerial Vehicle (UAV) with the aid of a GPS-equipped UAV
in three-dimensional space. The GPS-equipped UAV makes discrete-time broadcasts
of its global coordinates. The GPS-denied UAV simultaneously receives the
broadcast and takes direction of arrival (DOA) measurements towards the origin
of the broadcast in its local coordinate frame (obtained via an inertial
navigation system (INS)). The aim is to determine the difference between the
local and global frames, described by a rotation and a translation. In the
noiseless case, global coordinates were recovered exactly by solving a system
of linear equations. When DOA measurements are contaminated with noise, rank
relaxed semidefinite programming (SDP) and the Orthogonal Procrustes algorithm
are employed. Simulations are provided and factors affecting accuracy, such as
noise levels and number of measurements, are explored.
"
eess,"Online Simultaneous State and Parameter Estimation for Second-order
  Nonlinear Systems","  In this paper, a concurrent learning based adaptive observer is developed for
a class of second-order nonlinear time-invariant systems with uncertain
dynamics. The developed technique results in simultaneous online state and
parameter estimation. A Lyapunov-based analysis is used to show that the state
and parameter estimation errors are uniformly ultimately bounded. As opposed to
persistent excitation which is required for parameter estimation in traditional
adaptive control methods, the developed technique only requires excitation over
a finite time interval.
"
eess,Optimal Precoders for Tracking the AoD and AoA of a mm-Wave Path,"  In millimeter-wave channels, most of the received energy is carried by a few
paths. Traditional precoders sweep the angle-of-departure (AoD) and
angle-of-arrival (AoA) space with directional precoders to identify directions
with largest power. Such precoders are heuristic and lead to sub-optimal
AoD/AoA estimation. We derive optimal precoders, minimizing the Cram\'{e}r-Rao
bound (CRB) of the AoD/AoA, assuming a fully digital architecture at the
transmitter and spatial filtering of a single path. The precoders are found by
solving a suitable convex optimization problem. We demonstrate that the
accuracy can be improved by at least a factor of two over traditional
precoders, and show that there is an optimal number of distinct precoders
beyond which the CRB does not improve.
"
econ,Quantile and Probability Curves Without Crossing,"  This paper proposes a method to address the longstanding problem of lack of
monotonicity in estimation of conditional and structural quantile functions,
also known as the quantile crossing problem. The method consists in sorting or
monotone rearranging the original estimated non-monotone curve into a monotone
rearranged curve. We show that the rearranged curve is closer to the true
quantile curve in finite samples than the original curve, establish a
functional delta method for rearrangement-related operators, and derive
functional limit theory for the entire rearranged curve and its functionals. We
also establish validity of the bootstrap for estimating the limit law of the
the entire rearranged curve and its functionals. Our limit results are generic
in that they apply to every estimator of a monotone econometric function,
provided that the estimator satisfies a functional central limit theorem and
the function satisfies some smoothness conditions. Consequently, our results
apply to estimation of other econometric functions with monotonicity
restrictions, such as demand, production, distribution, and structural
distribution functions. We illustrate the results with an application to
estimation of structural quantile functions using data on Vietnam veteran
status and earnings.
"
econ,Improving Estimates of Monotone Functions by Rearrangement,"  Suppose that a target function is monotonic, namely, weakly increasing, and
an original estimate of the target function is available, which is not weakly
increasing. Many common estimation methods used in statistics produce such
estimates. We show that these estimates can always be improved with no harm
using rearrangement techniques: The rearrangement methods, univariate and
multivariate, transform the original estimate to a monotonic estimate, and the
resulting estimate is closer to the true curve in common metrics than the
original estimate. We illustrate the results with a computational example and
an empirical example dealing with age-height growth charts.
"
econ,Rearranging Edgeworth-Cornish-Fisher Expansions,"  This paper applies a regularization procedure called increasing rearrangement
to monotonize Edgeworth and Cornish-Fisher expansions and any other related
approximations of distribution and quantile functions of sample statistics.
Besides satisfying the logical monotonicity, required of distribution and
quantile functions, the procedure often delivers strikingly better
approximations to the distribution and quantile functions of the sample mean
than the original Edgeworth-Cornish-Fisher expansions.
"
econ,"Evolutionarily stable strategies of random games, and the vertices of
  random polygons","  An evolutionarily stable strategy (ESS) is an equilibrium strategy that is
immune to invasions by rare alternative (``mutant'') strategies. Unlike Nash
equilibria, ESS do not always exist in finite games. In this paper we address
the question of what happens when the size of the game increases: does an ESS
exist for ``almost every large'' game? Letting the entries in the $n\times n$
game matrix be independently randomly chosen according to a distribution $F$,
we study the number of ESS with support of size $2.$ In particular, we show
that, as $n\to \infty$, the probability of having such an ESS: (i) converges to
1 for distributions $F$ with ``exponential and faster decreasing tails'' (e.g.,
uniform, normal, exponential); and (ii) converges to $1-1/\sqrt{e}$ for
distributions $F$ with ``slower than exponential decreasing tails'' (e.g.,
lognormal, Pareto, Cauchy). Our results also imply that the expected number of
vertices of the convex hull of $n$ random points in the plane converges to
infinity for the distributions in (i), and to 4 for the distributions in (ii).
"
econ,Projective Expected Utility,"  Motivated by several classic decision-theoretic paradoxes, and by analogies
with the paradoxes which in physics motivated the development of quantum
mechanics, we introduce a projective generalization of expected utility along
the lines of the quantum-mechanical generalization of probability theory. The
resulting decision theory accommodates the dominant paradoxes, while retaining
significant simplicity and tractability. In particular, every finite game
within this larger class of preferences still has an equilibrium.
"
econ,"Improving Point and Interval Estimates of Monotone Functions by
  Rearrangement","  Suppose that a target function is monotonic, namely, weakly increasing, and
an available original estimate of this target function is not weakly
increasing. Rearrangements, univariate and multivariate, transform the original
estimate to a monotonic estimate that always lies closer in common metrics to
the target function. Furthermore, suppose an original simultaneous confidence
interval, which covers the target function with probability at least
$1-\alpha$, is defined by an upper and lower end-point functions that are not
weakly increasing. Then the rearranged confidence interval, defined by the
rearranged upper and lower end-point functions, is shorter in length in common
norms than the original interval and also covers the target function with
probability at least $1-\alpha$. We demonstrate the utility of the improved
point and interval estimates with an age-height growth chart example.
"
econ,Inference on Counterfactual Distributions,"  Counterfactual distributions are important ingredients for policy analysis
and decomposition analysis in empirical economics. In this article we develop
modeling and inference tools for counterfactual distributions based on
regression methods. The counterfactual scenarios that we consider consist of
ceteris paribus changes in either the distribution of covariates related to the
outcome of interest or the conditional distribution of the outcome given
covariates. For either of these scenarios we derive joint functional central
limit theorems and bootstrap validity results for regression-based estimators
of the status quo and counterfactual outcome distributions. These results allow
us to construct simultaneous confidence sets for function-valued effects of the
counterfactual changes, including the effects on the entire distribution and
quantile functions of the outcome as well as on related functionals. These
confidence sets can be used to test functional hypotheses such as no-effect,
positive effect, or stochastic dominance. Our theory applies to general
counterfactual changes and covers the main regression methods including
classical, quantile, duration, and distribution regressions. We illustrate the
results with an empirical application to wage decompositions using data for the
United States.
  As a part of developing the main results, we introduce distribution
regression as a comprehensive and flexible tool for modeling and estimating the
\textit{entire} conditional distribution. We show that distribution regression
encompasses the Cox duration regression and represents a useful alternative to
quantile regression. We establish functional central limit theorems and
bootstrap validity results for the empirical distribution regression process
and various related functionals.
"
econ,Average and Quantile Effects in Nonseparable Panel Models,"  Nonseparable panel models are important in a variety of economic settings,
including discrete choice. This paper gives identification and estimation
results for nonseparable models under time homogeneity conditions that are like
""time is randomly assigned"" or ""time is an instrument."" Partial identification
results for average and quantile effects are given for discrete regressors,
under static or dynamic conditions, in fully nonparametric and in
semiparametric models, with time effects. It is shown that the usual, linear,
fixed-effects estimator is not a consistent estimator of the identified average
effect, and a consistent estimator is given. A simple estimator of identified
quantile treatment effects is given, providing a solution to the important
problem of estimating quantile treatment effects from panel data. Bounds for
overall effects in static and dynamic models are given. The dynamic bounds
provide a partial identification solution to the important problem of
estimating the effect of state dependence in the presence of unobserved
heterogeneity. The impact of $T$, the number of time periods, is shown by
deriving shrinkage rates for the identified set as $T$ grows. We also consider
semiparametric, discrete-choice models and find that semiparametric panel
bounds can be much tighter than nonparametric bounds.
Computationally-convenient methods for semiparametric models are presented. We
propose a novel inference method that applies in panel data and other settings
and show that it produces uniformly valid confidence regions in large samples.
We give empirical illustrations.
"
econ,L1-Penalized Quantile Regression in High-Dimensional Sparse Models,"  We consider median regression and, more generally, a possibly infinite
collection of quantile regressions in high-dimensional sparse models. In these
models the overall number of regressors $p$ is very large, possibly larger than
the sample size $n$, but only $s$ of these regressors have non-zero impact on
the conditional quantile of the response variable, where $s$ grows slower than
$n$. We consider quantile regression penalized by the $\ell_1$-norm of
coefficients ($\ell_1$-QR). First, we show that $\ell_1$-QR is consistent at
the rate $\sqrt{s/n} \sqrt{\log p}$. The overall number of regressors $p$
affects the rate only through the $\log p$ factor, thus allowing nearly
exponential growth in the number of zero-impact regressors. The rate result
holds under relatively weak conditions, requiring that $s/n$ converges to zero
at a super-logarithmic speed and that regularization parameter satisfies
certain theoretical constraints. Second, we propose a pivotal, data-driven
choice of the regularization parameter and show that it satisfies these
theoretical constraints. Third, we show that $\ell_1$-QR correctly selects the
true minimal model as a valid submodel, when the non-zero coefficients of the
true model are well separated from zero. We also show that the number of
non-zero coefficients in $\ell_1$-QR is of same stochastic order as $s$.
Fourth, we analyze the rate of convergence of a two-step estimator that applies
ordinary quantile regression to the selected model. Fifth, we evaluate the
performance of $\ell_1$-QR in a Monte-Carlo experiment, and illustrate its use
on an international economic growth application.
"
econ,"Posterior Inference in Curved Exponential Families under Increasing
  Dimensions","  This work studies the large sample properties of the posterior-based
inference in the curved exponential family under increasing dimension. The
curved structure arises from the imposition of various restrictions on the
model, such as moment restrictions, and plays a fundamental role in
econometrics and others branches of data analysis. We establish conditions
under which the posterior distribution is approximately normal, which in turn
implies various good properties of estimation and inference procedures based on
the posterior. In the process we also revisit and improve upon previous results
for the exponential family under increasing dimension by making use of
concentration of measure. We also discuss a variety of applications to
high-dimensional versions of the classical econometric models including the
multinomial model with moment restrictions, seemingly unrelated regression
equations, and single structural equation models. In our analysis, both the
parameter dimension and the number of moments are increasing with the sample
size.
"
econ,Constructive Decision Theory,"  In most contemporary approaches to decision making, a decision problem is
described by a sets of states and set of outcomes, and a rich set of acts,
which are functions from states to outcomes over which the decision maker (DM)
has preferences. Most interesting decision problems, however, do not come with
a state space and an outcome space. Indeed, in complex problems it is often far
from clear what the state and outcome spaces would be. We present an
alternative foundation for decision making, in which the primitive objects of
choice are syntactic programs. A representation theorem is proved in the spirit
of standard representation theorems, showing that if the DM's preference
relation on objects of choice satisfies appropriate axioms, then there exist a
set S of states, a set O of outcomes, a way of interpreting the objects of
choice as functions from S to O, a probability on S, and a utility function on
O, such that the DM prefers choice a to choice b if and only if the expected
utility of a is higher than that of b. Thus, the state space and outcome space
are subjective, just like the probability and utility; they are not part of the
description of the problem. In principle, a modeler can test for SEU behavior
without having access to states or outcomes. We illustrate the power of our
approach by showing that it can capture decision makers who are subject to
framing effects.
"
econ,"Complete Characterization of Functions Satisfying the Conditions of
  Arrow's Theorem","  Arrow's theorem implies that a social choice function satisfying
Transitivity, the Pareto Principle (Unanimity) and Independence of Irrelevant
Alternatives (IIA) must be dictatorial. When non-strict preferences are
allowed, a dictatorial social choice function is defined as a function for
which there exists a single voter whose strict preferences are followed. This
definition allows for many different dictatorial functions. In particular, we
construct examples of dictatorial functions which do not satisfy Transitivity
and IIA. Thus Arrow's theorem, in the case of non-strict preferences, does not
provide a complete characterization of all social choice functions satisfying
Transitivity, the Pareto Principle, and IIA.
  The main results of this article provide such a characterization for Arrow's
theorem, as well as for follow up results by Wilson. In particular, we
strengthen Arrow's and Wilson's result by giving an exact if and only if
condition for a function to satisfy Transitivity and IIA (and the Pareto
Principle). Additionally, we derive formulas for the number of functions
satisfying these conditions.
"
econ,"Inference for Extremal Conditional Quantile Models, with an Application
  to Market and Birthweight Risks","  Quantile regression is an increasingly important empirical tool in economics
and other sciences for analyzing the impact of a set of regressors on the
conditional distribution of an outcome. Extremal quantile regression, or
quantile regression applied to the tails, is of interest in many economic and
financial applications, such as conditional value-at-risk, production
efficiency, and adjustment bands in (S,s) models. In this paper we provide
feasible inference tools for extremal conditional quantile models that rely
upon extreme value approximations to the distribution of self-normalized
quantile regression statistics. The methods are simple to implement and can be
of independent interest even in the non-regression case. We illustrate the
results with two empirical examples analyzing extreme fluctuations of a stock
return and extremely low percentiles of live infants' birthweights in the range
between 250 and 1500 grams.
"
econ,Toy Model for Large Non-Symmetric Random Matrices,"  Non-symmetric rectangular correlation matrices occur in many problems in
economics. We test the method of extracting statistically meaningful
correlations between input and output variables of large dimensionality and
build a toy model for artificially included correlations in large random time
series.The results are then applied to analysis of polish macroeconomic data
and can be used as an alternative to classical cointegration approach.
"
econ,"Sparse Models and Methods for Optimal Instruments with an Application to
  Eminent Domain","  We develop results for the use of Lasso and Post-Lasso methods to form
first-stage predictions and estimate optimal instruments in linear instrumental
variables (IV) models with many instruments, $p$. Our results apply even when
$p$ is much larger than the sample size, $n$. We show that the IV estimator
based on using Lasso or Post-Lasso in the first stage is root-n consistent and
asymptotically normal when the first-stage is approximately sparse; i.e. when
the conditional expectation of the endogenous variables given the instruments
can be well-approximated by a relatively small set of variables whose
identities may be unknown. We also show the estimator is semi-parametrically
efficient when the structural error is homoscedastic. Notably our results allow
for imperfect model selection, and do not rely upon the unrealistic ""beta-min""
conditions that are widely used to establish validity of inference following
model selection. In simulation experiments, the Lasso-based IV estimator with a
data-driven penalty performs well compared to recently advocated
many-instrument-robust procedures. In an empirical example dealing with the
effect of judicial eminent domain decisions on economic outcomes, the
Lasso-based IV estimator outperforms an intuitive benchmark.
  In developing the IV results, we establish a series of new results for Lasso
and Post-Lasso estimators of nonparametric conditional expectation functions
which are of independent theoretical and practical interest. We construct a
modification of Lasso designed to deal with non-Gaussian, heteroscedastic
disturbances which uses a data-weighted $\ell_1$-penalty function. Using
moderate deviation theory for self-normalized sums, we provide convergence
rates for the resulting Lasso and Post-Lasso estimators that are as sharp as
the corresponding rates in the homoscedastic Gaussian case under the condition
that $\log p = o(n^{1/3})$.
"
econ,LASSO Methods for Gaussian Instrumental Variables Models,"  In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO,
sqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate
optimal instruments in linear instrumental variables (IV) models with many
instruments in the canonical Gaussian case. The methods apply even when the
number of instruments is much larger than the sample size. We derive asymptotic
distributions for the resulting IV estimators and provide conditions under
which these sparsity-based IV estimators are asymptotically oracle-efficient.
In simulation experiments, a sparsity-based IV estimator with a data-driven
penalty performs well compared to recently advocated many-instrument-robust
procedures. We illustrate the procedure in an empirical example using the
Angrist and Krueger (1991) schooling data.
"
econ,Quantile Regression with Censoring and Endogeneity,"  In this paper, we develop a new censored quantile instrumental variable
(CQIV) estimator and describe its properties and computation. The CQIV
estimator combines Powell (1986) censored quantile regression (CQR) to deal
with censoring, with a control variable approach to incorporate endogenous
regressors. The CQIV estimator is obtained in two stages that are non-additive
in the unobservables. The first stage estimates a non-additive model with
infinite dimensional parameters for the control variable, such as a quantile or
distribution regression model. The second stage estimates a non-additive
censored quantile regression model for the response variable of interest,
including the estimated control variable to deal with endogeneity. For
computation, we extend the algorithm for CQR developed by Chernozhukov and Hong
(2002) to incorporate the estimation of the control variable. We give generic
regularity conditions for asymptotic normality of the CQIV estimator and for
the validity of resampling methods to approximate its asymptotic distribution.
We verify these conditions for quantile and distribution regression estimation
of the control variable. Our analysis covers two-stage (uncensored) quantile
regression with non-additive first stage as an important special case. We
illustrate the computation and applicability of the CQIV estimator with a
Monte-Carlo numerical example and an empirical application on estimation of
Engel curves for alcohol.
"
econ,Conditional Quantile Processes based on Series or Many Regressors,"  Quantile regression (QR) is a principal regression method for analyzing the
impact of covariates on outcomes. The impact is described by the conditional
quantile function and its functionals. In this paper we develop the
nonparametric QR-series framework, covering many regressors as a special case,
for performing inference on the entire conditional quantile function and its
linear functionals. In this framework, we approximate the entire conditional
quantile function by a linear combination of series terms with
quantile-specific coefficients and estimate the function-valued coefficients
from the data. We develop large sample theory for the QR-series coefficient
process, namely we obtain uniform strong approximations to the QR-series
coefficient process by conditionally pivotal and Gaussian processes. Based on
these strong approximations, or couplings, we develop four resampling methods
(pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be
used for inference on the entire QR-series coefficient function.
  We apply these results to obtain estimation and inference methods for linear
functionals of the conditional quantile function, such as the conditional
quantile function itself, its partial derivatives, average partial derivatives,
and conditional average partial derivatives. Specifically, we obtain uniform
rates of convergence and show how to use the four resampling methods mentioned
above for inference on the functionals. All of the above results are for
function-valued parameters, holding uniformly in both the quantile index and
the covariate value, and covering the pointwise case as a by-product. We
demonstrate the practical utility of these results with an example, where we
estimate the price elasticity function and test the Slutsky condition of the
individual demand for gasoline, as indexed by the individual unobserved
propensity for gasoline consumption.
"
econ,High Dimensional Sparse Econometric Models: An Introduction,"  In this chapter we discuss conceptually high dimensional sparse econometric
models as well as estimation of these models using L1-penalization and
post-L1-penalization methods. Focusing on linear and nonparametric regression
frameworks, we discuss various econometric examples, present basic theoretical
results, and illustrate the concepts and methods with Monte Carlo simulations
and an empirical application. In the application, we examine and confirm the
empirical validity of the Solow-Swan model for international economic growth.
"
econ,Team Decision Problems with Classical and Quantum Signals,"  We study team decision problems where communication is not possible, but
coordination among team members can be realized via signals in a shared
environment. We consider a variety of decision problems that differ in what
team members know about one another's actions and knowledge. For each type of
decision problem, we investigate how different assumptions on the available
signals affect team performance. Specifically, we consider the cases of
perfectly correlated, i.i.d., and exchangeable classical signals, as well as
the case of quantum signals. We find that, whereas in perfect-recall trees
(Kuhn [1950], [1953]) no type of signal improves performance, in
imperfect-recall trees quantum signals may bring an improvement. Isbell [1957]
proved that in non-Kuhn trees, classical i.i.d. signals may improve
performance. We show that further improvement may be possible by use of
classical exchangeable or quantum signals. We include an example of the effect
of quantum signals in the context of high-frequency trading.
"
econ,Inference for High-Dimensional Sparse Econometric Models,"  This article is about estimation and inference methods for high dimensional
sparse (HDS) regression models in econometrics. High dimensional sparse models
arise in situations where many regressors (or series terms) are available and
the regression function is well-approximated by a parsimonious, yet unknown set
of regressors. The latter condition makes it possible to estimate the entire
regression function effectively by searching for approximately the right set of
regressors. We discuss methods for identifying this set of regressors and
estimating their coefficients based on $\ell_1$-penalization and describe key
theoretical results. In order to capture realistic practical situations, we
expressly allow for imperfect selection of regressors and study the impact of
this imperfect selection on estimation and inference results. We focus the main
part of the article on the use of HDS models and methods in the instrumental
variables model and the partially linear model. We present a set of novel
inference results for these models and illustrate their use with applications
to returns to schooling and growth regression.
"
econ,"Inference on Treatment Effects After Selection Amongst High-Dimensional
  Controls","  We propose robust methods for inference on the effect of a treatment variable
on a scalar outcome in the presence of very many controls. Our setting is a
partially linear model with possibly non-Gaussian and heteroscedastic
disturbances. Our analysis allows the number of controls to be much larger than
the sample size. To make informative inference feasible, we require the model
to be approximately sparse; that is, we require that the effect of confounding
factors can be controlled for up to a small approximation error by conditioning
on a relatively small number of controls whose identities are unknown. The
latter condition makes it possible to estimate the treatment effect by
selecting approximately the right set of controls. We develop a novel
estimation and uniformly valid inference method for the treatment effect in
this setting, called the ""post-double-selection"" method. Our results apply to
Lasso-type methods used for covariate selection as well as to any other model
selection method that is able to find a sparse model with good approximation
properties.
  The main attractive feature of our method is that it allows for imperfect
selection of the controls and provides confidence intervals that are valid
uniformly across a large class of models. In contrast, standard post-model
selection estimators fail to provide uniform inference even in simple cases
with a small, fixed number of controls. Thus our method resolves the problem of
uniform inference after model selection for a large, interesting class of
models. We illustrate the use of the developed methods with numerical
simulations and an application to the effect of abortion on crime rates.
"
econ,Approximate Revenue Maximization with Multiple Items,"  Maximizing the revenue from selling _more than one_ good (or item) to a
single buyer is a notoriously difficult problem, in stark contrast to the
one-good case. For two goods, we show that simple ""one-dimensional"" mechanisms,
such as selling the goods separately, _guarantee_ at least 73% of the optimal
revenue when the valuations of the two goods are independent and identically
distributed, and at least $50\%$ when they are independent. For the case of
$k>2$ independent goods, we show that selling them separately guarantees at
least a $c/\log^2 k$ fraction of the optimal revenue; and, for independent and
identically distributed goods, we show that selling them as one bundle
guarantees at least a $c/\log k$ fraction of the optimal revenue. Additional
results compare the revenues from the two simple mechanisms of selling the
goods separately and bundled, identify situations where bundling is optimal,
and extend the analysis to multiple buyers.
"
econ,"Panel Data Models with Nonadditive Unobserved Heterogeneity: Estimation
  and Inference","  This paper considers fixed effects estimation and inference in linear and
nonlinear panel data models with random coefficients and endogenous regressors.
The quantities of interest -- means, variances, and other moments of the random
coefficients -- are estimated by cross sectional sample moments of GMM
estimators applied separately to the time series of each individual. To deal
with the incidental parameter problem introduced by the noise of the
within-individual estimators in short panels, we develop bias corrections.
These corrections are based on higher-order asymptotic expansions of the GMM
estimators and produce improved point and interval estimates in moderately long
panels. Under asymptotic sequences where the cross sectional and time series
dimensions of the panel pass to infinity at the same rate, the uncorrected
estimator has an asymptotic bias of the same order as the asymptotic variance.
The bias corrections remove the bias without increasing variance. An empirical
example on cigarette demand based on Becker, Grossman and Murphy (1994) shows
significant heterogeneity in the price effect across U.S. states.
"
econ,Social learning equilibria,"  We consider a large class of social learning models in which a group of
agents face uncertainty regarding a state of the world, share the same utility
function, observe private signals, and interact in a general dynamic setting.
We introduce Social Learning Equilibria, a static equilibrium concept that
abstracts away from the details of the given extensive form, but nevertheless
captures the corresponding asymptotic equilibrium behavior. We establish
general conditions for agreement, herding, and information aggregation in
equilibrium, highlighting a connection between agreement and information
aggregation.
"
econ,Strategic Learning and the Topology of Social Networks,"  We consider a group of strategic agents who must each repeatedly take one of
two possible actions. They learn which of the two actions is preferable from
initial private signals, and by observing the actions of their neighbors in a
social network.
  We show that the question of whether or not the agents learn efficiently
depends on the topology of the social network. In particular, we identify a
geometric ""egalitarianism"" condition on the social network that guarantees
learning in infinite networks, or learning with high probability in large
finite networks, in any equilibrium. We also give examples of non-egalitarian
networks with equilibria in which learning fails.
"
econ,Dual Regression,"  We propose dual regression as an alternative to the quantile regression
process for the global estimation of conditional distribution functions under
minimal assumptions. Dual regression provides all the interpretational power of
the quantile regression process while avoiding the need for repairing the
intersecting conditional quantile surfaces that quantile regression often
produces in practice. Our approach introduces a mathematical programming
characterization of conditional distribution functions which, in its simplest
form, is the dual program of a simultaneous estimator for linear location-scale
models. We apply our general characterization to the specification and
estimation of a flexible class of conditional distribution functions, and
present asymptotic theory for the corresponding empirical dual regression
process.
"
econ,"Some New Asymptotic Theory for Least Squares Series: Pointwise and
  Uniform Results","  In applications it is common that the exact form of a conditional expectation
is unknown and having flexible functional forms can lead to improvements.
Series method offers that by approximating the unknown function based on $k$
basis functions, where $k$ is allowed to grow with the sample size $n$. We
consider series estimators for the conditional mean in light of: (i) sharp LLNs
for matrices derived from the noncommutative Khinchin inequalities, (ii) bounds
on the Lebesgue factor that controls the ratio between the $L^\infty$ and
$L_2$-norms of approximation errors, (iii) maximal inequalities for processes
whose entropy integrals diverge, and (iv) strong approximations to series-type
processes.
  These technical tools allow us to contribute to the series literature,
specifically the seminal work of Newey (1997), as follows. First, we weaken the
condition on the number $k$ of approximating functions used in series
estimation from the typical $k^2/n \to 0$ to $k/n \to 0$, up to log factors,
which was available only for spline series before. Second, we derive $L_2$
rates and pointwise central limit theorems results when the approximation error
vanishes. Under an incorrectly specified model, i.e. when the approximation
error does not vanish, analogous results are also shown. Third, under stronger
conditions we derive uniform rates and functional central limit theorems that
hold if the approximation error vanishes or not. That is, we derive the strong
approximation for the entire estimate of the nonparametric function.
  We derive uniform rates, Gaussian approximations, and uniform confidence
bands for a wide collection of linear functionals of the conditional
expectation function.
"
econ,"Semi-parametric Bayesian Partially Identified Models based on Support
  Function","  We provide a comprehensive semi-parametric study of Bayesian partially
identified econometric models. While the existing literature on Bayesian
partial identification has mostly focused on the structural parameter, our
primary focus is on Bayesian credible sets (BCS's) of the unknown identified
set and the posterior distribution of its support function. We construct a
(two-sided) BCS based on the support function of the identified set. We prove
the Bernstein-von Mises theorem for the posterior distribution of the support
function. This powerful result in turn infers that, while the BCS and the
frequentist confidence set for the partially identified parameter are
asymptotically different, our constructed BCS for the identified set has an
asymptotically correct frequentist coverage probability. Importantly, we
illustrate that the constructed BCS for the identified set does not require a
prior on the structural parameter. It can be computed efficiently for subset
inference, especially when the target of interest is a sub-vector of the
partially identified parameter, where projecting to a low-dimensional subset is
often required. Hence, the proposed methods are useful in many applications.
  The Bayesian partial identification literature has been assuming a known
parametric likelihood function. However, econometric models usually only
identify a set of moment inequalities, and therefore using an incorrect
likelihood function may result in misleading inferences. In contrast, with a
nonparametric prior on the unknown likelihood function, our proposed Bayesian
procedure only requires a set of moment conditions, and can efficiently make
inference about both the partially identified parameter and its identified set.
This makes it widely applicable in general moment inequality models. Finally,
the proposed method is illustrated in a financial asset pricing problem.
"
econ,"Gaussian approximations and multiplier bootstrap for maxima of sums of
  high-dimensional random vectors","  We derive a Gaussian approximation result for the maximum of a sum of
high-dimensional random vectors. Specifically, we establish conditions under
which the distribution of the maximum is approximated by that of the maximum of
a sum of the Gaussian random vectors with the same covariance matrices as the
original vectors. This result applies when the dimension of random vectors
($p$) is large compared to the sample size ($n$); in fact, $p$ can be much
larger than $n$, without restricting correlations of the coordinates of these
vectors. We also show that the distribution of the maximum of a sum of the
random vectors with unknown covariance matrices can be consistently estimated
by the distribution of the maximum of a sum of the conditional Gaussian random
vectors obtained by multiplying the original vectors with i.i.d. Gaussian
multipliers. This is the Gaussian multiplier (or wild) bootstrap procedure.
Here too, $p$ can be large or even much larger than $n$. These distributional
approximations, either Gaussian or conditional Gaussian, yield a high-quality
approximation to the distribution of the original maximum, often with
approximation error decreasing polynomially in the sample size, and hence are
of interest in many applications. We demonstrate how our Gaussian
approximations and the multiplier bootstrap can be used for modern
high-dimensional estimation, multiple hypothesis testing, and adaptive
specification testing. All these results contain nonasymptotic bounds on
approximation errors.
"
econ,Quantile Models with Endogeneity,"  In this article, we review quantile models with endogeneity. We focus on
models that achieve identification through the use of instrumental variables
and discuss conditions under which partial and point identification are
obtained. We discuss key conditions, which include monotonicity and
full-rank-type conditions, in detail. In providing this review, we update the
identification results of Chernozhukov and Hansen (2005, Econometrica). We
illustrate the modeling assumptions through economically motivated examples. We
also briefly review the literature on estimation and inference.
  Key Words: identification, treatment effects, structural models, instrumental
variables
"
econ,"Uniform Post Selection Inference for LAD Regression and Other
  Z-estimation problems","  We develop uniformly valid confidence regions for regression coefficients in
a high-dimensional sparse median regression model with homoscedastic errors.
Our methods are based on a moment equation that is immunized against
non-regular estimation of the nuisance part of the median regression function
by using Neyman's orthogonalization. We establish that the resulting
instrumental median regression estimator of a target regression coefficient is
asymptotically normally distributed uniformly with respect to the underlying
sparse model and is semi-parametrically efficient. We also generalize our
method to a general non-smooth Z-estimation framework with the number of target
parameters $p_1$ being possibly much larger than the sample size $n$. We extend
Huber's results on asymptotic normality to this setting, demonstrating uniform
asymptotic normality of the proposed estimators over $p_1$-dimensional
rectangles, constructing simultaneous confidence bands on all of the $p_1$
target parameters, and establishing asymptotic validity of the bands uniformly
over underlying approximately sparse models.
  Keywords: Instrument; Post-selection inference; Sparsity; Neyman's Orthogonal
Score test; Uniformly valid inference; Z-estimation.
"
econ,"Post-Selection Inference for Generalized Linear Models with Many
  Controls","  This paper considers generalized linear models in the presence of many
controls. We lay out a general methodology to estimate an effect of interest
based on the construction of an instrument that immunize against model
selection mistakes and apply it to the case of logistic binary choice model.
More specifically we propose new methods for estimating and constructing
confidence regions for a regression parameter of primary interest $\alpha_0$, a
parameter in front of the regressor of interest, such as the treatment variable
or a policy variable. These methods allow to estimate $\alpha_0$ at the
root-$n$ rate when the total number $p$ of other regressors, called controls,
potentially exceed the sample size $n$ using sparsity assumptions. The sparsity
assumption means that there is a subset of $s<n$ controls which suffices to
accurately approximate the nuisance part of the regression function.
Importantly, the estimators and these resulting confidence regions are valid
uniformly over $s$-sparse models satisfying $s^2\log^2 p = o(n)$ and other
technical conditions. These procedures do not rely on traditional consistent
model selection arguments for their validity. In fact, they are robust with
respect to moderate model selection mistakes in variable selection. Under
suitable conditions, the estimators are semi-parametrically efficient in the
sense of attaining the semi-parametric efficiency bounds for the class of
models in this paper.
"
econ,"Selling Multiple Correlated Goods: Revenue Maximization and Menu-Size
  Complexity (old title: ""The Menu-Size Complexity of Auctions"")","  We consider the well known, and notoriously difficult, problem of a single
revenue-maximizing seller selling two or more heterogeneous goods to a single
buyer whose private values for the goods are drawn from a (possibly correlated)
known distribution, and whose valuation is additive over the goods. We show
that when there are two (or more) goods, _simple mechanisms_ -- such as selling
the goods separately or as a bundle -- _may yield only a negligible fraction of
the optimal revenue_. This resolves the open problem of Briest, Chawla,
Kleinberg, and Weinberg (JET 2015) who prove the result for at least three
goods in the related setup of a unit-demand buyer. We also introduce the menu
size as a simple measure of the complexity of mechanisms, and show that the
revenue may increase polynomially with _menu size_ and that no bounded menu
size can ensure any positive fraction of the optimal revenue. The menu size
also turns out to ""pin down"" the revenue properties of deterministic
mechanisms.
"
econ,The Query Complexity of Correlated Equilibria,"  We consider the complexity of finding a correlated equilibrium of an
$n$-player game in a model that allows the algorithm to make queries on
players' payoffs at pure strategy profiles. Randomized regret-based dynamics
are known to yield an approximate correlated equilibrium efficiently, namely,
in time that is polynomial in the number of players $n$. Here we show that both
randomization and approximation are necessary: no efficient deterministic
algorithm can reach even an approximate correlated equilibrium, and no
efficient randomized algorithm can reach an exact correlated equilibrium. The
results are obtained by bounding from below the number of payoff queries that
are needed.
"
econ,"Supplementary Appendix for ""Inference on Treatment Effects After
  Selection Amongst High-Dimensional Controls""","  In this supplementary appendix we provide additional results, omitted proofs
and extensive simulations that complement the analysis of the main text
(arXiv:1201.0224).
"
econ,"Periodic Strategies: A New Solution Concept and an Algorithm for
  NonTrivial Strategic Form Games","  We introduce a new solution concept, called periodicity, for selecting
optimal strategies in strategic form games. This periodicity solution concept
yields new insight into non-trivial games. In mixed strategy strategic form
games, periodic solutions yield values for the utility function of each player
that are equal to the Nash equilibrium ones. In contrast to the Nash
strategies, here the payoffs of each player are robust against what the
opponent plays. Sometimes, periodicity strategies yield higher utilities, and
sometimes the Nash strategies do, but often the utilities of these two
strategies coincide. We formally define and study periodic strategies in two
player perfect information strategic form games with pure strategies and we
prove that every non-trivial finite game has at least one periodic strategy,
with non-trivial meaning non-degenerate payoffs. In some classes of games where
mixed strategies are used, we identify quantitative features. Particularly
interesting are the implications for collective action games, since there the
collective action strategy can be incorporated in a purely non-cooperative
context. Moreover, we address the periodicity issue when the players have a
continuum set of strategies available.
"
econ,"Robust Inference on Average Treatment Effects with Possibly More
  Covariates than Observations","  This paper concerns robust inference on average treatment effects following
model selection. In the selection on observables framework, we show how to
construct confidence intervals based on a doubly-robust estimator that are
robust to model selection errors and prove that they are valid uniformly over a
large class of treatment effect models. The class allows for multivalued
treatments with heterogeneous effects (in observables), general
heteroskedasticity, and selection amongst (possibly) more covariates than
observations. Our estimator attains the semiparametric efficiency bound under
appropriate conditions. Precise conditions are given for any model selector to
yield these results, and we show how to combine data-driven selection with
economic theory. For implementation, we give a specific proposal for selection
based on the group lasso, which is particularly well-suited to treatment
effects data, and derive new results for high-dimensional, sparse multinomial
logistic regression. A simulation study shows our estimator performs very well
in finite samples over a wide range of models. Revisiting the National
Supported Work demonstration data, our method yields accurate estimates and
tight confidence intervals.
"
econ,"Optimal Uniform Convergence Rates for Sieve Nonparametric Instrumental
  Variables Regression","  We study the problem of nonparametric regression when the regressor is
endogenous, which is an important nonparametric instrumental variables (NPIV)
regression in econometrics and a difficult ill-posed inverse problem with
unknown operator in statistics. We first establish a general upper bound on the
sup-norm (uniform) convergence rate of a sieve estimator, allowing for
endogenous regressors and weakly dependent data. This result leads to the
optimal sup-norm convergence rates for spline and wavelet least squares
regression estimators under weakly dependent data and heavy-tailed error terms.
This upper bound also yields the sup-norm convergence rates for sieve NPIV
estimators under i.i.d. data: the rates coincide with the known optimal
$L^2$-norm rates for severely ill-posed problems, and are power of $\log(n)$
slower than the optimal $L^2$-norm rates for mildly ill-posed problems. We then
establish the minimax risk lower bound in sup-norm loss, which coincides with
our upper bounds on sup-norm rates for the spline and wavelet sieve NPIV
estimators. This sup-norm rate optimality provides another justification for
the wide application of sieve NPIV estimators. Useful results on
weakly-dependent random matrices are also provided.
"
econ,Program Evaluation and Causal Inference with High-Dimensional Data,"  In this paper, we provide efficient estimators and honest confidence bands
for a variety of treatment effects including local average (LATE) and local
quantile treatment effects (LQTE) in data-rich environments. We can handle very
many control variables, endogenous receipt of treatment, heterogeneous
treatment effects, and function-valued outcomes. Our framework covers the
special case of exogenous receipt of treatment, either conditional on controls
or unconditionally as in randomized control trials. In the latter case, our
approach produces efficient estimators and honest bands for (functional)
average treatment effects (ATE) and quantile treatment effects (QTE). To make
informative inference possible, we assume that key reduced form predictive
relationships are approximately sparse. This assumption allows the use of
regularization and selection methods to estimate those relations, and we
provide methods for post-regularization and post-selection inference that are
uniformly valid (honest) across a wide-range of models. We show that a key
ingredient enabling honest inference is the use of orthogonal or doubly robust
moment conditions in estimating certain reduced form functional parameters. We
illustrate the use of the proposed methods with an application to estimating
the effect of 401(k) eligibility and participation on accumulated assets.
"
econ,"Individual and Time Effects in Nonlinear Panel Models with Large N, T","  We derive fixed effects estimators of parameters and average partial effects
in (possibly dynamic) nonlinear panel data models with individual and time
effects. They cover logit, probit, ordered probit, Poisson and Tobit models
that are important for many empirical applications in micro and macroeconomics.
Our estimators use analytical and jackknife bias corrections to deal with the
incidental parameter problem, and are asymptotically unbiased under asymptotic
sequences where $N/T$ converges to a constant. We develop inference methods and
show that they perform well in numerical examples.
"
econ,Nonparametric Identification in Panels using Quantiles,"  This paper considers identification and estimation of ceteris paribus effects
of continuous regressors in nonseparable panel models with time homogeneity.
The effects of interest are derivatives of the average and quantile structural
functions of the model. We find that these derivatives are identified with two
time periods for ""stayers"", i.e. for individuals with the same regressor values
in two time periods. We show that the identification results carry over to
models that allow location and scale time effects. We propose nonparametric
series methods and a weighted bootstrap scheme to estimate and make inference
on the identified effects. The bootstrap proposed allows uniform inference for
function-valued parameters such as quantile effects uniformly over a region of
quantile indices and/or regressor values. An empirical application to Engel
curve estimation with panel data illustrates the results.
"
econ,"Valid Post-Selection Inference in High-Dimensional Approximately Sparse
  Quantile Regression Models","  This work proposes new inference methods for a regression coefficient of
interest in a (heterogeneous) quantile regression model. We consider a
high-dimensional model where the number of regressors potentially exceeds the
sample size but a subset of them suffice to construct a reasonable
approximation to the conditional quantile function. The proposed methods are
(explicitly or implicitly) based on orthogonal score functions that protect
against moderate model selection mistakes, which are often inevitable in the
approximately sparse model considered in the present paper. We establish the
uniform validity of the proposed confidence regions for the quantile regression
coefficient. Importantly, these methods directly apply to more than one
variable and a continuum of quantile indices. In addition, the performance of
the proposed methods is illustrated through Monte-Carlo experiments and an
empirical example, dealing with risk factors in childhood malnutrition.
"
econ,"Inference on causal and structural parameters using many moment
  inequalities","  This paper considers the problem of testing many moment inequalities where
the number of moment inequalities, denoted by $p$, is possibly much larger than
the sample size $n$. There is a variety of economic applications where solving
this problem allows to carry out inference on causal and structural parameters,
a notable example is the market structure model of Ciliberto and Tamer (2009)
where $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter
the market. We consider the test statistic given by the maximum of $p$
Studentized (or $t$-type) inequality-specific statistics, and analyze various
ways to compute critical values for the test statistic. Specifically, we
consider critical values based upon (i) the union bound combined with a
moderate deviation inequality for self-normalized sums, (ii) the multiplier and
empirical bootstraps, and (iii) two-step and three-step variants of (i) and
(ii) by incorporating the selection of uninformative inequalities that are far
from being binding and a novel selection of weakly informative inequalities
that are potentially binding but do not provide first order information. We
prove validity of these methods, showing that under mild conditions, they lead
to tests with the error in size decreasing polynomially in $n$ while allowing
for $p$ being much larger than $n$, indeed $p$ can be of order $\exp (n^{c})$
for some $c > 0$. Importantly, all these results hold without any restriction
on the correlation structure between $p$ Studentized statistics, and also hold
uniformly with respect to suitably large classes of underlying distributions.
Moreover, in the online supplement, we show validity of a test based on the
block multiplier bootstrap in the case of dependent data under some general
mixing conditions.
"
econ,"What does the financial market pricing do? A simulation analysis with a
  view to systemic volatility, exuberance and vagary","  Biondi et al. (2012) develop an analytical model to examine the emergent
dynamic properties of share market price formation over time, capable to
capture important stylized facts. These latter properties prove to be sensitive
to regulatory regimes for fundamental information provision, as well as to
market confidence conditions among actual and potential investors. Regimes
based upon mark-to-market (fair value) measurement of traded security, while
generating higher linear correlation between market prices and fundamental
signals, also involve higher market instability and volatility. These regimes
also incur more relevant episodes of market exuberance and vagary in some
regions of the market confidence space, where lower market liquidity further
occurs.
"
econ,Efficient Modeling and Forecasting of the Electricity Spot Price,"  The increasing importance of renewable energy, especially solar and wind
power, has led to new forces in the formation of electricity prices. Hence,
this paper introduces an econometric model for the hourly time series of
electricity prices of the European Power Exchange (EPEX) which incorporates
specific features like renewable energy. The model consists of several
sophisticated and established approaches and can be regarded as a periodic
VAR-TARCH with wind power, solar power, and load as influences on the time
series. It is able to map the distinct and well-known features of electricity
prices in Germany. An efficient iteratively reweighted lasso approach is used
for the estimation. Moreover, it is shown that several existing models are
outperformed by the procedure developed in this paper.
"
econ,Graphical potential games,"  We study the class of potential games that are also graphical games with
respect to a given graph $G$ of connections between the players. We show that,
up to strategic equivalence, this class of games can be identified with the set
of Markov random fields on $G$.
  From this characterization, and from the Hammersley-Clifford theorem, it
follows that the potentials of such games can be decomposed to local
potentials. We use this decomposition to strongly bound the number of strategy
changes of a single player along a better response path. This result extends to
generalized graphical potential games, which are played on infinite graphs.
"
econ,Zero-determinant strategies in iterated multi-strategy games,"  Self-serving, rational agents sometimes cooperate to their mutual benefit.
The two-player iterated prisoner's dilemma game is a model for including the
emergence of cooperation. It is generally believed that there is no simple
ultimatum strategy which a player can control the return of the other
participants. The recent discovery of the powerful class of zero-determinant
strategies in the iterated prisoner's dilemma dramatically expands our
understanding of the classic game by uncovering strategies that provide a
unilateral advantage to sentient players pitted against unwitting opponents.
However, strategies in the prisoner's dilemma game are only two strategies. Are
there these results for general multi-strategy games? To address this question,
the paper develops a theory for zero-determinant strategies for multi-strategy
games, with any number of strategies. The analytical results exhibit a similar
yet different scenario to the case of two-strategy games. Zero-determinant
strategies in iterated prisoner's dilemma can be seen as degenerate case of our
results. The results are also applied to the snowdrift game, the hawk-dove game
and the chicken game.
"
econ,Fiscal stimulus as an optimal control problem,"  During the Great Recession, Democrats in the United States argued that
government spending could be utilized to ""grease the wheels"" of the economy in
order to create wealth and to increase employment; Republicans, on the other
hand, contended that government spending is wasteful and discouraged
investment, thereby increasing unemployment. Today, in 2020, we find ourselves
in the midst of another crisis where government spending and fiscal stimulus is
again being considered as a solution. In the present paper, we address this
question by formulating an optimal control problem generalizing the model of
Radner & Shepp (1996). The model allows for the company to borrow continuously
from the government. We prove that there exists an optimal strategy; rigorous
verification proofs for its optimality are provided. We proceed to prove that
government loans increase the expected net value of a company. We also examine
the consequences of different profit-taking behaviors among firms who receive
fiscal stimulus.
"
econ,"Sieve Wald and QLR Inferences on Semi/nonparametric Conditional Moment
  Models","  This paper considers inference on functionals of semi/nonparametric
conditional moment restrictions with possibly nonsmooth generalized residuals,
which include all of the (nonlinear) nonparametric instrumental variables (IV)
as special cases. These models are often ill-posed and hence it is difficult to
verify whether a (possibly nonlinear) functional is root-$n$ estimable or not.
We provide computationally simple, unified inference procedures that are
asymptotically valid regardless of whether a functional is root-$n$ estimable
or not. We establish the following new useful results: (1) the asymptotic
normality of a plug-in penalized sieve minimum distance (PSMD) estimator of a
(possibly nonlinear) functional; (2) the consistency of simple sieve variance
estimators for the plug-in PSMD estimator, and hence the asymptotic chi-square
distribution of the sieve Wald statistic; (3) the asymptotic chi-square
distribution of an optimally weighted sieve quasi likelihood ratio (QLR) test
under the null hypothesis; (4) the asymptotic tight distribution of a
non-optimally weighted sieve QLR statistic under the null; (5) the consistency
of generalized residual bootstrap sieve Wald and QLR tests; (6) local power
properties of sieve Wald and QLR tests and of their bootstrap versions; (7)
asymptotic properties of sieve Wald and SQLR for functionals of increasing
dimension. Simulation studies and an empirical illustration of a nonparametric
quantile IV regression are presented.
"
econ,"Bootstrap Consistency for Quadratic Forms of Sample Averages with
  Increasing Dimension","  This paper establishes consistency of the weighted bootstrap for quadratic
forms $\left( n^{-1/2} \sum_{i=1}^{n} Z_{i,n} \right)^{T}\left( n^{-1/2}
\sum_{i=1}^{n} Z_{i,n} \right)$ where $(Z_{i,n})_{i=1}^{n}$ are mean zero,
independent $\mathbb{R}^{d}$-valued random variables and $d=d(n)$ is allowed to
grow with the sample size $n$, slower than $n^{1/4}$. The proof relies on an
adaptation of Lindeberg interpolation technique whereby we simplify the
original problem to a Gaussian approximation problem. We apply our bootstrap
results to model-specification testing problems when the number of moments is
allowed to grow with the sample size.
"
econ,Identification and Estimation of Multidimensional Screening,"  We study the identification and estimation of a multidimensional screening
model, where a monopolist sells a multi-attribute product to consumers with
private information about their multidimensional preferences. Under optimal
screening, the seller designs product and payment rules that exclude ""low-type""
consumers, bunches the ""medium types"" at ""medium-quality"" products, and
perfectly screens the ""high types."" Under the assumption that the cost function
is quadratic and additively separable in products, we determine sufficient
conditions to identify the joint distribution of preferences and the marginal
costs from data on optimal individual choices and payments. Then, we propose
estimators for these objects, establish their asymptotic properties, and assess
their small-sample performance using Monte Carlo experiments.
"
econ,"Inference in High Dimensional Panel Models with an Application to Gun
  Control","  We consider estimation and inference in panel data models with additive
unobserved individual specific heterogeneity in a high dimensional setting. The
setting allows the number of time varying regressors to be larger than the
sample size. To make informative estimation and inference feasible, we require
that the overall contribution of the time varying variables after eliminating
the individual specific heterogeneity can be captured by a relatively small
number of the available variables whose identities are unknown. This
restriction allows the problem of estimation to proceed as a variable selection
problem. Importantly, we treat the individual specific heterogeneity as fixed
effects which allows this heterogeneity to be related to the observed time
varying variables in an unspecified way and allows that this heterogeneity may
be non-zero for all individuals. Within this framework, we provide procedures
that give uniformly valid inference over a fixed subset of parameters in the
canonical linear fixed effects model and over coefficients on a fixed vector of
endogenous variables in panel data instrumental variables models with fixed
effects and many instruments. An input to developing the properties of our
proposed procedures is the use of a variant of the Lasso estimator that allows
for a grouped data structure where data across groups are independent and
dependence within groups is unrestricted. We provide formal conditions within
this structure under which the proposed Lasso variant selects a sparse model
with good approximation properties. We present simulation results in support of
the theoretical developments and illustrate the use of the methods in an
application aimed at estimating the effect of gun prevalence on crime rates.
"
econ,"Comprehensive Time-Series Regression Models Using GRETL -- U.S. GDP and
  Government Consumption Expenditures & Gross Investment from 1980 to 2013","  Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman
filter, transfer-function and intervention models, unit root tests,
cointegration test, volatility models (ARCH, GARCH, ARCH-M, GARCH-M,
Taylor-Schwert GARCH, GJR, TARCH, NARCH, APARCH, EGARCH) to analyze quarterly
time series of GDP and Government Consumption Expenditures & Gross Investment
(GCEGI) from 1980 to 2013. The article is organized as: (I) Definition; (II)
Regression Models; (III) Discussion. Additionally, I discovered a unique
interaction between GDP and GCEGI in both the short-run and the long-run and
provided policy makers with some suggestions. For example in the short run, GDP
responded positively and very significantly (0.00248) to GCEGI, while GCEGI
reacted positively but not too significantly (0.08051) to GDP. In the long run,
current GDP responded negatively and permanently (0.09229) to a shock in past
GCEGI, while current GCEGI reacted negatively yet temporarily (0.29821) to a
shock in past GDP. Therefore, policy makers should not adjust current GCEGI
based merely on the condition of current and past GDP. Although increasing
GCEGI does help GDP in the short-term, significantly abrupt increase in GCEGI
might not be good to the long-term health of GDP. Instead, a balanced,
sustainable, and economically viable solution is recommended, so that the
short-term benefits to the current economy from increasing GCEGI often largely
secured by the long-term loan outweigh or at least equal to the negative effect
to the future economy from the long-term debt incurred by the loan. Finally, I
found that non-normally distributed volatility models generally perform better
than normally distributed ones. More specifically, TARCH-GED performs the best
in the group of non-normally distributed, while GARCH-M does the best in the
group of normally distributed.
"
econ,Nonlinear Factor Models for Network and Panel Data,"  Factor structures or interactive effects are convenient devices to
incorporate latent variables in panel data models. We consider fixed effect
estimation of nonlinear panel single-index models with factor structures in the
unobservables, which include logit, probit, ordered probit and Poisson
specifications. We establish that fixed effect estimators of model parameters
and average partial effects have normal distributions when the two dimensions
of the panel grow large, but might suffer of incidental parameter bias. We show
how models with factor structures can also be applied to capture important
features of network data such as reciprocity, degree heterogeneity, homophily
in latent variables and clustering. We illustrate this applicability with an
empirical example to the estimation of a gravity equation of international
trade between countries using a Poisson model with multiple factors.
"
econ,Rational Groupthink,"  We study how long-lived rational agents learn from repeatedly observing a
private signal and each others' actions. With normal signals, a group of any
size learns more slowly than just four agents who directly observe each others'
private signals in each period. Similar results apply to general signal
structures. We identify rational groupthink---in which agents ignore their
private signals and choose the same action for long periods of time---as the
cause of this failure of information aggregation.
"
econ,"Monge-Kantorovich Depth, Quantiles, Ranks, and Signs","  We propose new concepts of statistical depth, multivariate quantiles, ranks
and signs, based on canonical transportation maps between a distribution of
interest on $R^d$ and a reference distribution on the $d$-dimensional unit
ball. The new depth concept, called Monge-Kantorovich depth, specializes to
halfspace depth in the case of spherical distributions, but, for more general
distributions, differs from the latter in the ability for its contours to
account for non convex features of the distribution of interest. We propose
empirical counterparts to the population versions of those Monge-Kantorovich
depth contours, quantiles, ranks and signs, and show their consistency by
establishing a uniform convergence property for empirical transport maps, which
is of independent interest.
"
econ,"Forecasting day ahead electricity spot prices: The impact of the EXAA to
  other European electricity markets","  In our paper we analyze the relationship between the day-ahead electricity
price of the Energy Exchange Austria (EXAA) and other day-ahead electricity
prices in Europe. We focus on markets, which settle their prices after the
EXAA, which enables traders to include the EXAA price into their calculations.
For each market we employ econometric models to incorporate the EXAA price and
compare them with their counterparts without the price of the Austrian
exchange. By employing a forecasting study, we find that electricity price
models can be improved when EXAA prices are considered.
"
econ,The ABC of Simulation Estimation with Auxiliary Statistics,"  The frequentist method of simulated minimum distance (SMD) is widely used in
economics to estimate complex models with an intractable likelihood. In other
disciplines, a Bayesian approach known as Approximate Bayesian Computation
(ABC) is far more popular. This paper connects these two seemingly related
approaches to likelihood-free estimation by means of a Reverse Sampler that
uses both optimization and importance weighting to target the posterior
distribution. Its hybrid features enable us to analyze an ABC estimate from the
perspective of SMD. We show that an ideal ABC estimate can be obtained as a
weighted average of a sequence of SMD modes, each being the minimizer of the
deviations between the data and the model. This contrasts with the SMD, which
is the mode of the average deviations. Using stochastic expansions, we provide
a general characterization of frequentist estimators and those based on
Bayesian computations including Laplace-type estimators. Their differences are
illustrated using analytical examples and a simulation study of the dynamic
panel model.
"
econ,"Post-Selection and Post-Regularization Inference in Linear Models with
  Many Controls and Instruments","  In this note, we offer an approach to estimating causal/structural parameters
in the presence of many instruments and controls based on methods for
estimating sparse high-dimensional models. We use these high-dimensional
methods to select both which instruments and which control variables to use.
The approach we take extends BCCH2012, which covers selection of instruments
for IV models with a small number of controls, and extends BCH2014, which
covers selection of controls in models where the variable of interest is
exogenous conditional on observables, to accommodate both a large number of
controls and a large number of instruments. We illustrate the approach with a
simulation and an empirical example. Technical supporting material is available
in a supplementary online appendix.
"
econ,"Valid Post-Selection and Post-Regularization Inference: An Elementary,
  General Approach","  Here we present an expository, general analysis of valid post-selection or
post-regularization inference about a low-dimensional target parameter,
$\alpha$, in the presence of a very high-dimensional nuisance parameter,
$\eta$, which is estimated using modern selection or regularization methods.
Our analysis relies on high-level, easy-to-interpret conditions that allow one
to clearly see the structures needed for achieving valid post-regularization
inference. Simple, readily verifiable sufficient conditions are provided for a
class of affine-quadratic models. We focus our discussion on estimation and
inference procedures based on using the empirical analog of theoretical
equations $$M(\alpha, \eta)=0$$ which identify $\alpha$. Within this structure,
we show that setting up such equations in a manner such that the
orthogonality/immunization condition $$\partial_\eta M(\alpha, \eta) = 0$$ at
the true parameter values is satisfied, coupled with plausible conditions on
the smoothness of $M$ and the quality of the estimator $\hat \eta$, guarantees
that inference on for the main parameter $\alpha$ based on testing or point
estimation methods discussed below will be regular despite selection or
regularization biases occurring in estimation of $\eta$. In particular, the
estimator of $\alpha$ will often be uniformly consistent at the root-$n$ rate
and uniformly asymptotically normal even though estimators $\hat \eta$ will
generally not be asymptotically linear and regular. The uniformity holds over
large classes of models that do not impose highly implausible ""beta-min""
conditions. We also show that inference can be carried out by inverting tests
formed from Neyman's $C(\alpha)$ (orthogonal score) statistics.
"
econ,A lava attack on the recovery of sums of dense and sparse signals,"  Common high-dimensional methods for prediction rely on having either a sparse
signal model, a model in which most parameters are zero and there are a small
number of non-zero parameters that are large in magnitude, or a dense signal
model, a model with no large parameters and very many small non-zero
parameters. We consider a generalization of these two basic models, termed here
a ""sparse+dense"" model, in which the signal is given by the sum of a sparse
signal and a dense signal. Such a structure poses problems for traditional
sparse estimators, such as the lasso, and for traditional dense estimation
methods, such as ridge estimation. We propose a new penalization-based method,
called lava, which is computationally efficient. With suitable choices of
penalty parameters, the proposed method strictly dominates both lasso and
ridge. We derive analytic expressions for the finite-sample risk function of
the lava estimator in the Gaussian sequence model. We also provide an deviation
bound for the prediction risk in the Gaussian regression model with fixed
design. In both cases, we provide Stein's unbiased estimator for lava's
prediction risk. A simulation example compares the performance of lava to
lasso, ridge, and elastic net in a regression example using feasible,
data-dependent penalty parameters and illustrates lava's improved performance
relative to these benchmarks.
"
econ,Equilibrium in Misspecified Markov Decision Processes,"  We study Markov decision problems where the agent does not know the
transition probability function mapping current states and actions to future
states. The agent has a prior belief over a set of possible transition
functions and updates beliefs using Bayes' rule. We allow her to be
misspecified in the sense that the true transition probability function is not
in the support of her prior. This problem is relevant in many economic settings
but is usually not amenable to analysis by the researcher. We make the problem
tractable by studying asymptotic behavior. We propose an equilibrium notion and
provide conditions under which it characterizes steady state behavior. In the
special case where the problem is static, equilibrium coincides with the
single-agent version of Berk-Nash equilibrium (Esponda and Pouzo (2016)). We
also discuss subtle issues that arise exclusively in dynamic settings due to
the possibility of a negative value of experimentation.
"
econ,Recursive Partitioning for Heterogeneous Causal Effects,"  In this paper we study the problems of estimating heterogeneity in causal
effects in experimental or observational studies and conducting inference about
the magnitude of the differences in treatment effects across subsets of the
population. In applications, our method provides a data-driven approach to
determine which subpopulations have large or small treatment effects and to
test hypotheses about the differences in these effects. For experiments, our
method allows researchers to identify heterogeneity in treatment effects that
was not specified in a pre-analysis plan, without concern about invalidating
inference due to multiple testing. In most of the literature on supervised
machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal
is to build a model of the relationship between a unit's attributes and an
observed outcome. A prominent role in these methods is played by
cross-validation which compares predictions to actual outcomes in test samples,
in order to select the level of complexity of the model that provides the best
predictive power. Our method is closely related, but it differs in that it is
tailored for predicting causal effects of a treatment rather than a unit's
outcome. The challenge is that the ""ground truth"" for a causal effect is not
observed for any individual unit: we observe the unit with the treatment, or
without the treatment, but not both at the same time. Thus, it is not obvious
how to use cross-validation to determine whether a causal effect has been
accurately predicted. We propose several novel cross-validation criteria for
this problem and demonstrate through simulations the conditions under which
they perform better than standard methods for the problem of causal effects. We
then apply the method to a large-scale field experiment re-ranking results on a
search engine.
"
econ,"Alternative Asymptotics and the Partially Linear Model with Many
  Regressors","  Non-standard distributional approximations have received considerable
attention in recent years. They often provide more accurate approximations in
small samples, and theoretical improvements in some cases. This paper shows
that the seemingly unrelated ""many instruments asymptotics"" and ""small
bandwidth asymptotics"" share a common structure, where the object determining
the limiting distribution is a V-statistic with a remainder that is an
asymptotically normal degenerate U-statistic. We illustrate how this general
structure can be used to derive new results by obtaining a new asymptotic
distribution of a series estimator of the partially linear model when the
number of terms in the series approximation possibly grows as fast as the
sample size, which we call ""many terms asymptotics"".
"
econ,"Enhanced Gravity Model of trade: reconciling macroeconomic and network
  models","  The structure of the International Trade Network (ITN), whose nodes and links
represent world countries and their trade relations respectively, affects key
economic processes worldwide, including globalization, economic integration,
industrial production, and the propagation of shocks and instabilities.
Characterizing the ITN via a simple yet accurate model is an open problem. The
traditional Gravity Model (GM) successfully reproduces the volume of trade
between connected countries, using macroeconomic properties such as GDP,
geographic distance, and possibly other factors. However, it predicts a network
with complete or homogeneous topology, thus failing to reproduce the highly
heterogeneous structure of the ITN. On the other hand, recent maximum-entropy
network models successfully reproduce the complex topology of the ITN, but
provide no information about trade volumes. Here we integrate these two
currently incompatible approaches via the introduction of an Enhanced Gravity
Model (EGM) of trade. The EGM is the simplest model combining the GM with the
network approach within a maximum-entropy framework. Via a unified and
principled mechanism that is transparent enough to be generalized to any
economic network, the EGM provides a new econometric framework wherein trade
probabilities and trade volumes can be separately controlled by any combination
of dyadic and country-specific macroeconomic variables. The model successfully
reproduces both the global topology and the local link weights of the ITN,
parsimoniously reconciling the conflicting approaches. It also indicates that
the probability that any two countries trade a certain volume should follow a
geometric or exponential distribution with an additional point mass at zero
volume.
"
econ,"On Game-Theoretic Risk Management (Part One) -- Towards a Theory of
  Games with Payoffs that are Probability-Distributions","  Optimal behavior in (competitive) situation is traditionally determined with
the help of utility functions that measure the payoff of different actions.
Given an ordering on the space of revenues (payoffs), the classical axiomatic
approach of von Neumann and Morgenstern establishes the existence of suitable
utility functions, and yields to game-theory as the most prominent
materialization of a theory to determine optimal behavior. Although this
appears to be a most natural approach to risk management too, applications in
critical infrastructures often violate the implicit assumption of actions
leading to deterministic consequences. In that sense, the gameplay in a
critical infrastructure risk control competition is intrinsically random in the
sense of actions having uncertain consequences. Mathematically, this takes us
to utility functions that are probability-distribution-valued, in which case we
loose the canonic (in fact every possible) ordering on the space of payoffs,
and the original techniques of von Neumann and Morgenstern no longer apply.
  This work introduces a new kind of game in which uncertainty applies to the
payoff functions rather than the player's actions (a setting that has been
widely studied in the literature, yielding to celebrated notions like the
trembling hands equilibrium or the purification theorem). In detail, we show
how to fix the non-existence of a (canonic) ordering on the space of
probability distributions by only mildly restricting the full set to a subset
that can be totally ordered. Our vehicle to define the ordering and establish
basic game-theory is non-standard analysis and hyperreal numbers.
"
econ,"Inference in Linear Regression Models with Many Covariates and
  Heteroskedasticity","  The linear regression model is widely used in empirical work in Economics,
Statistics, and many other disciplines. Researchers often include many
covariates in their linear model specification in an attempt to control for
confounders. We give inference methods that allow for many covariates and
heteroskedasticity. Our results are obtained using high-dimensional
approximations, where the number of included covariates are allowed to grow as
fast as the sample size. We find that all of the usual versions of Eicker-White
heteroskedasticity consistent standard error estimators for linear models are
inconsistent under this asymptotics. We then propose a new heteroskedasticity
consistent standard error formula that is fully automatic and robust to both
(conditional)\ heteroskedasticity of unknown form and the inclusion of possibly
many covariates. We apply our findings to three settings: parametric linear
models with many covariates, linear panel models with many fixed effects, and
semiparametric semi-linear models with many technical regressors. Simulation
evidence consistent with our theoretical results is also provided. The proposed
methods are also illustrated with an empirical application.
"
econ,Nonparametric instrumental variable estimation under monotonicity,"  The ill-posedness of the inverse problem of recovering a regression function
in a nonparametric instrumental variable model leads to estimators that may
suffer from a very slow, logarithmic rate of convergence. In this paper, we
show that restricting the problem to models with monotone regression functions
and monotone instruments significantly weakens the ill-posedness of the
problem. In stark contrast to the existing literature, the presence of a
monotone instrument implies boundedness of our measure of ill-posedness when
restricted to the space of monotone functions. Based on this result we derive a
novel non-asymptotic error bound for the constrained estimator that imposes
monotonicity of the regression function. For a given sample size, the bound is
independent of the degree of ill-posedness as long as the regression function
is not too steep. As an implication, the bound allows us to show that the
constrained estimator converges at a fast, polynomial rate, independently of
the degree of ill-posedness, in a large, but slowly shrinking neighborhood of
constant functions. Our simulation study demonstrates significant finite-sample
performance gains from imposing monotonicity even when the regression function
is rather far from being a constant. We apply the constrained estimator to the
problem of estimating gasoline demand functions from U.S. data.
"
econ,"On the Effect of Bias Estimation on Coverage Accuracy in Nonparametric
  Inference","  Nonparametric methods play a central role in modern empirical work. While
they provide inference procedures that are more robust to parametric
misspecification bias, they may be quite sensitive to tuning parameter choices.
We study the effects of bias correction on confidence interval coverage in the
context of kernel density and local polynomial regression estimation, and prove
that bias correction can be preferred to undersmoothing for minimizing coverage
error and increasing robustness to tuning parameter choice. This is achieved
using a novel, yet simple, Studentization, which leads to a new way of
constructing kernel-based bias-corrected confidence intervals. In addition, for
practical cases, we derive coverage error optimal bandwidths and discuss
easy-to-implement bandwidth selectors. For interior points, we show that the
MSE-optimal bandwidth for the original point estimator (before bias correction)
delivers the fastest coverage error decay rate after bias correction when
second-order (equivalent) kernels are employed, but is otherwise suboptimal
because it is too ""large"". Finally, for odd-degree local polynomial regression,
we show that, as with point estimation, coverage error adapts to boundary
points automatically when appropriate Studentization is used; however, the
MSE-optimal bandwidth for the original point estimator is suboptimal. All the
results are established using valid Edgeworth expansions and illustrated with
simulated data. Our findings have important consequences for empirical work as
they indicate that bias-corrected confidence intervals, coupled with
appropriate standard errors, have smaller coverage error and are less sensitive
to tuning parameter choices in practically relevant cases where additional
smoothness is available.
"
econ,"Optimal Sup-norm Rates and Uniform Inference on Nonlinear Functionals of
  Nonparametric IV Regression","  This paper makes several important contributions to the literature about
nonparametric instrumental variables (NPIV) estimation and inference on a
structural function $h_0$ and its functionals. First, we derive sup-norm
convergence rates for computationally simple sieve NPIV (series 2SLS)
estimators of $h_0$ and its derivatives. Second, we derive a lower bound that
describes the best possible (minimax) sup-norm rates of estimating $h_0$ and
its derivatives, and show that the sieve NPIV estimator can attain the minimax
rates when $h_0$ is approximated via a spline or wavelet sieve. Our optimal
sup-norm rates surprisingly coincide with the optimal root-mean-squared rates
for severely ill-posed problems, and are only a logarithmic factor slower than
the optimal root-mean-squared rates for mildly ill-posed problems. Third, we
use our sup-norm rates to establish the uniform Gaussian process strong
approximations and the score bootstrap uniform confidence bands (UCBs) for
collections of nonlinear functionals of $h_0$ under primitive conditions,
allowing for mildly and severely ill-posed problems. Fourth, as applications,
we obtain the first asymptotic pointwise and uniform inference results for
plug-in sieve t-statistics of exact consumer surplus (CS) and deadweight loss
(DL) welfare functionals under low-level conditions when demand is estimated
via sieve NPIV. Empiricists could read our real data application of UCBs for
exact CS and DL functionals of gasoline demand that reveals interesting
patterns and is applicable to other markets.
"
econ,Trading Networks with Bilateral Contracts,"  We consider a model of matching in trading networks in which firms can enter
into bilateral contracts. In trading networks, stable outcomes, which are
immune to deviations of arbitrary sets of firms, may not exist. We define a new
solution concept called trail stability. Trail-stable outcomes are immune to
consecutive, pairwise deviations between linked firms. We show that any trading
network with bilateral contracts has a trail-stable outcome whenever firms'
choice functions satisfy the full substitutability condition. For trail-stable
outcomes, we prove results on the lattice structure, the rural hospitals
theorem, strategy-proofness, and comparative statics of firm entry and exit. We
also introduce weak trail stability which is implied by trail stability under
full substitutability. We describe relationships between the solution concepts.
"
econ,Deriving Priorities From Inconsistent PCM using the Network Algorithms,"  In several multiobjective decision problems Pairwise Comparison Matrices
(PCM) are applied to evaluate the decision variants. The problem that arises
very often is the inconsistency of a given PCM. In such a situation it is
important to approximate the PCM with a consistent one. The most common way is
to minimize the Euclidean distance between the matrices. In the paper we
consider the problem of minimizing the maximum distance. After applying the
logarithmic transformation we are able to formulate the obtained subproblem as
a Shortest Path Problem and solve it more efficiently. We analyze and
completely characterize the form of the set of optimal solutions and provide an
algorithm that results in a unique, Pareto-efficient solution.
"
econ,"My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em
  Competition","  The first ever human vs. computer no-limit Texas hold 'em competition took
place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this
article I present my thoughts on the competition design, agent architecture,
and lessons learned.
"
econ,"Estimating the effect of treatments allocated by randomized waiting
  lists","  Oversubscribed treatments are often allocated using randomized waiting lists.
Applicants are ranked randomly, and treatment offers are made following that
ranking until all seats are filled. To estimate causal effects, researchers
often compare applicants getting and not getting an offer. We show that those
two groups are not statistically comparable. Therefore, the estimator arising
from that comparison is inconsistent. We propose a new estimator, and show that
it is consistent. Finally, we revisit an application, and we show that using
our estimator can lead to sizably different results from those obtained using
the commonly used estimator.
"
econ,Adaptive estimation for some nonparametric instrumental variable models,"  The problem of endogeneity in statistics and econometrics is often handled by
introducing instrumental variables (IV) which fulfill the mean independence
assumption, i.e. the unobservable is mean independent of the instruments. When
full independence of IV's and the unobservable is assumed, nonparametric IV
regression models and nonparametric demand models lead to nonlinear integral
equations with unknown integral kernels. We prove convergence rates for the
mean integrated square error of the iteratively regularized Newton method
applied to these problems. Compared to related results we derive stronger
convergence results that rely on weaker nonlinearity restrictions. We
demonstrate in numerical simulations for a nonparametric IV regression that the
method produces better results than the standard model.
"
econ,"On Game-Theoretic Risk Management (Part Two) -- Algorithms to Compute
  Nash-Equilibria in Games with Distributions as Payoffs","  The game-theoretic risk management framework put forth in the precursor work
""Towards a Theory of Games with Payoffs that are Probability-Distributions""
(arXiv:1506.07368 [q-fin.EC]) is herein extended by algorithmic details on how
to compute equilibria in games where the payoffs are probability distributions.
Our approach is ""data driven"" in the sense that we assume empirical data
(measurements, simulation, etc.) to be available that can be compiled into
distribution models, which are suitable for efficient decisions about
preferences, and setting up and solving games using these as payoffs. While
preferences among distributions turn out to be quite simple if nonparametric
methods (kernel density estimates) are used, computing Nash-equilibria in games
using such models is discovered as inefficient (if not impossible). In fact, we
give a counterexample in which fictitious play fails to converge for the
(specifically unfortunate) choice of payoff distributions in the game, and
introduce a suitable tail approximation of the payoff densities to tackle the
issue. The overall procedure is essentially a modified version of fictitious
play, and is herein described for standard and multicriteria games, to
iteratively deliver an (approximate) Nash-equilibrium. An exact method using
linear programming is also given.
"
econ,Money as Minimal Complexity,"  We consider mechanisms that provide traders the opportunity to exchange
commodity $i$ for commodity $j$, for certain ordered pairs $ij$. Given any
connected graph $G$ of opportunities, we show that there is a unique mechanism
$M_{G}$ that satisfies some natural conditions of ""fairness"" and ""convenience"".
Let $\mathfrak{M}(m)$ denote the class of mechanisms $M_{G}$ obtained by
varying $G$ on the commodity set $\left\{1,\ldots,m\right\} $. We define the
complexity of a mechanism $M$ in $\mathfrak{M(m)}$ to be a certain pair of
integers $\tau(M),\pi(M)$ which represent the time required to exchange $i$ for
$j$ and the information needed to determine the exchange ratio (each in the
worst case scenario, across all $i\neq j$). This induces a quasiorder $\preceq$
on $\mathfrak{M}(m)$ by the rule \[ M\preceq
M^{\prime}\text{if}\tau(M)\leq\tau(M^{\prime})\text{and}\pi(M)\leq\pi(M^{\prime}).
\] We show that, for $m>3$, there are precisely three $\preceq$-minimal
mechanisms $M_{G}$ in $\mathfrak{M}(m)$, where $G$ corresponds to the star,
cycle and complete graphs. The star mechanism has a distinguished commodity --
the money -- that serves as the sole medium of exchange and mediates trade
between decentralized markets for the other commodities.
  Our main result is that, for any weights $\lambda,\mu>0,$ the star mechanism
is the unique minimizer of $\lambda\tau(M)+\mu\pi(M)$ on $\mathfrak{M}(m)$ for
large enough $m$.
"
econ,Graphical Exchange Mechanisms,"  Consider an exchange mechanism which accepts diversified offers of various
commodities and redistributes everything it receives. We impose certain
conditions of fairness and convenience on such a mechanism and show that it
admits unique prices, which equalize the value of offers and returns for each
individual.
  We next define the complexity of a mechanism in terms of certain integers
$\tau_{ij},\pi_{ij}$ and $k_{i}$ that represent the time required to exchange
$i$ for $j$, the difficulty in determining the exchange ratio, and the
dimension of the message space. We show that there are a finite number of
minimally complex mechanisms, in each of which all trade is conducted through
markets for commodity pairs.
  Finally we consider minimal mechanisms with smallest worst-case complexities
$\tau=\max\tau_{ij}$ and $\pi=\max\pi_{ij}$. For $m>3$ commodities, there are
precisely three such mechanisms, one of which has a distinguished commodity --
the money -- that serves as the sole medium of exchange. As $m\rightarrow
\infty$ the money mechanism is the only one with bounded $\left( \pi
,\tau\right) $.
"
econ,"The Sorted Effects Method: Discovering Heterogeneous Effects Beyond
  Their Averages","  The partial (ceteris paribus) effects of interest in nonlinear and
interactive linear models are heterogeneous as they can vary dramatically with
the underlying observed or unobserved covariates. Despite the apparent
importance of heterogeneity, a common practice in modern empirical work is to
largely ignore it by reporting average partial effects (or, at best, average
effects for some groups). While average effects provide very convenient scalar
summaries of typical effects, by definition they fail to reflect the entire
variety of the heterogeneous effects. In order to discover these effects much
more fully, we propose to estimate and report sorted effects -- a collection of
estimated partial effects sorted in increasing order and indexed by
percentiles. By construction the sorted effect curves completely represent and
help visualize the range of the heterogeneous effects in one plot. They are as
convenient and easy to report in practice as the conventional average partial
effects. They also serve as a basis for classification analysis, where we
divide the observational units into most or least affected groups and summarize
their characteristics. We provide a quantification of uncertainty (standard
errors and confidence bands) for the estimated sorted effects and related
classification analysis, and provide confidence sets for the most and least
affected groups. The derived statistical results rely on establishing key, new
mathematical results on Hadamard differentiability of a multivariate sorting
operator and a related classification operator, which are of independent
interest. We apply the sorted effects method and classification analysis to
demonstrate several striking patterns in the gender wage gap.
"
econ,On the Non-Asymptotic Properties of Regularized M-estimators,"  We propose a general framework for regularization in M-estimation problems
under time dependent (absolutely regular-mixing) data which encompasses many of
the existing estimators. We derive non-asymptotic concentration bounds for the
regularized M-estimator. Our results exhibit a variance-bias trade-off, with
the variance term being governed by a novel measure of the complexity of the
parameter set. We also show that the mixing structure affect the variance term
by scaling the number of observations; depending on the decay rate of the
mixing coefficients, this scaling can even affect the asymptotic behavior.
Finally, we propose a data-driven method for choosing the tuning parameters of
the regularized estimator which yield the same (up to constants) concentration
bound as one that optimally balances the (squared) bias and variance terms. We
illustrate the results with several canonical examples.
"
econ,Confidence Intervals for Projections of Partially Identified Parameters,"  We propose a bootstrap-based calibrated projection procedure to build
confidence intervals for single components and for smooth functions of a
partially identified parameter vector in moment (in)equality models. The method
controls asymptotic coverage uniformly over a large class of data generating
processes. The extreme points of the calibrated projection confidence interval
are obtained by extremizing the value of the function of interest subject to a
proper relaxation of studentized sample analogs of the moment (in)equality
conditions. The degree of relaxation, or critical level, is calibrated so that
the function of theta, not theta itself, is uniformly asymptotically covered
with prespecified probability. This calibration is based on repeatedly checking
feasibility of linear programming problems, rendering it computationally
attractive.
  Nonetheless, the program defining an extreme point of the confidence interval
is generally nonlinear and potentially intricate. We provide an algorithm,
based on the response surface method for global optimization, that approximates
the solution rapidly and accurately, and we establish its rate of convergence.
The algorithm is of independent interest for optimization problems with simple
objectives and complicated constraints. An empirical application estimating an
entry game illustrates the usefulness of the method. Monte Carlo simulations
confirm the accuracy of the solution algorithm, the good statistical as well as
computational performance of calibrated projection (including in comparison to
other methods), and the algorithm's potential to greatly accelerate computation
of other confidence intervals.
"
econ,"Doubly Robust Uniform Confidence Band for the Conditional Average
  Treatment Effect Function","  In this paper, we propose a doubly robust method to present the heterogeneity
of the average treatment effect with respect to observed covariates of
interest. We consider a situation where a large number of covariates are needed
for identifying the average treatment effect but the covariates of interest for
analyzing heterogeneity are of much lower dimension. Our proposed estimator is
doubly robust and avoids the curse of dimensionality. We propose a uniform
confidence band that is easy to compute, and we illustrate its usefulness via
Monte Carlo experiments and an application to the effects of smoking on birth
weights.
"
econ,"Efficient Bayesian Inference for Multivariate Factor Stochastic
  Volatility Models","  We discuss efficient Bayesian estimation of dynamic covariance matrices in
multivariate time series through a factor stochastic volatility model. In
particular, we propose two interweaving strategies (Yu and Meng, Journal of
Computational and Graphical Statistics, 20(3), 531-570, 2011) to substantially
accelerate convergence and mixing of standard MCMC approaches. Similar to
marginal data augmentation techniques, the proposed acceleration procedures
exploit non-identifiability issues which frequently arise in factor models. Our
new interweaving strategies are easy to implement and come at almost no extra
computational cost; nevertheless, they can boost estimation efficiency by
several orders of magnitude as is shown in extensive simulation studies. To
conclude, the application of our algorithm to a 26-dimensional exchange rate
data set illustrates the superior performance of the new approach for
real-world data.
"
econ,High-Dimensional $L_2$Boosting: Rate of Convergence,"  Boosting is one of the most significant developments in machine learning.
This paper studies the rate of convergence of $L_2$Boosting, which is tailored
for regression, in a high-dimensional setting. Moreover, we introduce so-called
\textquotedblleft post-Boosting\textquotedblright. This is a post-selection
estimator which applies ordinary least squares to the variables selected in the
first stage by $L_2$Boosting. Another variant is \textquotedblleft Orthogonal
Boosting\textquotedblright\ where after each step an orthogonal projection is
conducted. We show that both post-$L_2$Boosting and the orthogonal boosting
achieve the same rate of convergence as LASSO in a sparse, high-dimensional
setting. We show that the rate of convergence of the classical $L_2$Boosting
depends on the design matrix described by a sparse eigenvalue constant. To show
the latter results, we derive new approximation results for the pure greedy
algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also
introduce feasible rules for early stopping, which can be easily implemented
and used in applied work. Our results also allow a direct comparison between
LASSO and boosting which has been missing from the literature. Finally, we
present simulation studies and applications to illustrate the relevance of our
theoretical results and to provide insights into the practical aspects of
boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms
LASSO.
"
econ,"Oracle Estimation of a Change Point in High Dimensional Quantile
  Regression","  In this paper, we consider a high-dimensional quantile regression model where
the sparsity structure may differ between two sub-populations. We develop
$\ell_1$-penalized estimators of both regression coefficients and the threshold
parameter. Our penalized estimators not only select covariates but also
discriminate between a model with homogeneous sparsity and a model with a
change point. As a result, it is not necessary to know or pretest whether the
change point is present, or where it occurs. Our estimator of the change point
achieves an oracle property in the sense that its asymptotic distribution is
the same as if the unknown active sets of regression coefficients were known.
Importantly, we establish this oracle property without a perfect covariate
selection, thereby avoiding the need for the minimum level condition on the
signals of active covariates. Dealing with high-dimensional quantile regression
with an unknown change point calls for a new proof technique since the quantile
loss function is non-smooth and furthermore the corresponding objective
function is non-convex with respect to the change point. The technique
developed in this paper is applicable to a general M-estimation framework with
a change point, which may be of independent interest. The proposed methods are
then illustrated via Monte Carlo experiments and an application to tipping in
the dynamics of racial segregation.
"
econ,"Coordination Event Detection and Initiator Identification in Time Series
  Data","  Behavior initiation is a form of leadership and is an important aspect of
social organization that affects the processes of group formation, dynamics,
and decision-making in human societies and other social animal species. In this
work, we formalize the ""Coordination Initiator Inference Problem"" and propose a
simple yet powerful framework for extracting periods of coordinated activity
and determining individuals who initiated this coordination, based solely on
the activity of individuals within a group during those periods. The proposed
approach, given arbitrary individual time series, automatically (1) identifies
times of coordinated group activity, (2) determines the identities of
initiators of those activities, and (3) classifies the likely mechanism by
which the group coordination occurred, all of which are novel computational
tasks. We demonstrate our framework on both simulated and real-world data:
trajectories tracking of animals as well as stock market data. Our method is
competitive with existing global leadership inference methods but provides the
first approaches for local leadership and coordination mechanism
classification. Our results are consistent with ground-truthed biological data
and the framework finds many known events in financial data which are not
otherwise reflected in the aggregate NASDAQ index. Our method is easily
generalizable to any coordinated time-series data from interacting entities.
"
econ,High-Dimensional Metrics in R,"  The package High-dimensional Metrics (\Rpackage{hdm}) is an evolving
collection of statistical methods for estimation and quantification of
uncertainty in high-dimensional approximately sparse models. It focuses on
providing confidence intervals and significance testing for (possibly many)
low-dimensional subcomponents of the high-dimensional parameter vector.
Efficient estimators and uniformly valid confidence intervals for regression
coefficients on target variables (e.g., treatment or policy variable) in a
high-dimensional approximately sparse regression model, for average treatment
effect (ATE) and average treatment effect for the treated (ATET), as well for
extensions of these parameters to the endogenous setting are provided. Theory
grounded, data-driven methods for selecting the penalization parameter in Lasso
regressions under heteroscedastic and non-Gaussian errors are implemented.
Moreover, joint/ simultaneous confidence intervals for regression coefficients
of a high-dimensional sparse regression are implemented, including a joint
significance test for Lasso regression. Data sets which have been used in the
literature and might be useful for classroom demonstration and for testing new
estimators are included. \R and the package \Rpackage{hdm} are open-source
software projects and can be freely downloaded from CRAN:
\texttt{http://cran.r-project.org}.
"
econ,Optimal Data Collection for Randomized Control Trials,"  In a randomized control trial, the precision of an average treatment effect
estimator can be improved either by collecting data on additional individuals,
or by collecting additional covariates that predict the outcome variable. We
propose the use of pre-experimental data such as a census, or a household
survey, to inform the choice of both the sample size and the covariates to be
collected. Our procedure seeks to minimize the resulting average treatment
effect estimator's mean squared error, subject to the researcher's budget
constraint. We rely on a modification of an orthogonal greedy algorithm that is
conceptually simple and easy to implement in the presence of a large number of
potential covariates, and does not require any tuning parameters. In two
empirical applications, we show that our procedure can lead to substantial
gains of up to 58%, measured either in terms of reductions in data collection
costs or in terms of improvements in the precision of the treatment effect
estimator.
"
econ,"Augmented Factor Models with Applications to Validating Market Risk
  Factors and Forecasting Bond Risk Premia","  We study factor models augmented by observed covariates that have explanatory
powers on the unknown factors. In financial factor models, the unknown factors
can be reasonably well explained by a few observable proxies, such as the
Fama-French factors. In diffusion index forecasts, identified factors are
strongly related to several directly measurable economic variables such as
consumption-wealth variable, financial ratios, and term spread. With those
covariates, both the factors and loadings are identifiable up to a rotation
matrix even only with a finite dimension. To incorporate the explanatory power
of these covariates, we propose a smoothed principal component analysis (PCA):
(i) regress the data onto the observed covariates, and (ii) take the principal
components of the fitted data to estimate the loadings and factors. This allows
us to accurately estimate the percentage of both explained and unexplained
components in factors and thus to assess the explanatory power of covariates.
We show that both the estimated factors and loadings can be estimated with
improved rates of convergence compared to the benchmark method. The degree of
improvement depends on the strength of the signals, representing the
explanatory power of the covariates on the factors. The proposed estimator is
robust to possibly heavy-tailed distributions. We apply the model to forecast
US bond risk premia, and find that the observed macroeconomic characteristics
contain strong explanatory powers of the factors. The gain of forecast is more
substantial when the characteristics are incorporated to estimate the common
factors than directly used for forecasts.
"
econ,"Estimating Treatment Effects using Multiple Surrogates: The Role of the
  Surrogate Score and the Surrogate Index","  Estimating the long-term effects of treatments is of interest in many fields.
A common challenge in estimating such treatment effects is that long-term
outcomes are unobserved in the time frame needed to make policy decisions. One
approach to overcome this missing data problem is to analyze treatments effects
on an intermediate outcome, often called a statistical surrogate, if it
satisfies the condition that treatment and outcome are independent conditional
on the statistical surrogate. The validity of the surrogacy condition is often
controversial. Here we exploit that fact that in modern datasets, researchers
often observe a large number, possibly hundreds or thousands, of intermediate
outcomes, thought to lie on or close to the causal chain between the treatment
and the long-term outcome of interest. Even if none of the individual proxies
satisfies the statistical surrogacy criterion by itself, using multiple proxies
can be useful in causal inference. We focus primarily on a setting with two
samples, an experimental sample containing data about the treatment indicator
and the surrogates and an observational sample containing information about the
surrogates and the primary outcome. We state assumptions under which the
average treatment effect be identified and estimated with a high-dimensional
vector of proxies that collectively satisfy the surrogacy assumption, and
derive the bias from violations of the surrogacy assumption, and show that even
if the primary outcome is also observed in the experimental sample, there is
still information to be gained from using surrogates.
"
econ,The Mittag-Leffler Fitting of the Phillips Curve,"  In this paper, a mathematical model based on the one-parameter Mittag-Leffler
function is proposed to be used for the first time to describe the relation
between unemployment rate and inflation rate, also known as the Phillips curve.
The Phillips curve is in the literature often represented by an
exponential-like shape. On the other hand, Phillips in his fundamental paper
used a power function in the model definition. Considering that the ordinary as
well as generalised Mittag-Leffler function behaves between a purely
exponential function and a power function it is natural to implement it in the
definition of the model used to describe the relation between the data
representing the Phillips curve. For the modelling purposes the data of two
different European economies, France and Switzerland, were used and an
""out-of-sample"" forecast was done to compare the performance of the
Mittag-Leffler model to the performance of the power-type and exponential-type
model. The results demonstrate that the ability of the Mittag-Leffler function
to fit data that manifest signs of stretched exponentials, oscillations or even
damped oscillations can be of use when describing economic relations and
phenomenons, such as the Phillips curve.
"
econ,Program Evaluation with Right-Censored Data,"  In a unified framework, we provide estimators and confidence bands for a
variety of treatment effects when the outcome of interest, typically a
duration, is subjected to right censoring. Our methodology accommodates
average, distributional, and quantile treatment effects under different
identifying assumptions including unconfoundedness, local treatment effects,
and nonlinear differences-in-differences. The proposed estimators are easy to
implement, have close-form representation, are fully data-driven upon
estimation of nuisance parameters, and do not rely on parametric distributional
assumptions, shape restrictions, or on restricting the potential treatment
effect heterogeneity across different subpopulations. These treatment effects
results are obtained as a consequence of more general results on two-step
Kaplan-Meier estimators that are of independent interest: we provide conditions
for applying (i) uniform law of large numbers, (ii) functional central limit
theorems, and (iii) we prove the validity of the ordinary nonparametric
bootstrap in a two-step estimation procedure where the outcome of interest may
be randomly censored.
"
econ,"Approximate Residual Balancing: De-Biased Inference of Average Treatment
  Effects in High Dimensions","  There are many settings where researchers are interested in estimating
average treatment effects and are willing to rely on the unconfoundedness
assumption, which requires that the treatment assignment be as good as random
conditional on pre-treatment variables. The unconfoundedness assumption is
often more plausible if a large number of pre-treatment variables are included
in the analysis, but this can worsen the performance of standard approaches to
treatment effect estimation. In this paper, we develop a method for de-biasing
penalized regression adjustments to allow sparse regression methods like the
lasso to be used for sqrt{n}-consistent inference of average treatment effects
in high-dimensional linear models. Given linearity, we do not need to assume
that the treatment propensities are estimable, or that the average treatment
effect is a sparse contrast of the outcome model parameters. Rather, in
addition standard assumptions used to make lasso regression on the outcome
model consistent under 1-norm error, we only require overlap, i.e., that the
propensity score be uniformly bounded away from 0 and 1. Procedurally, our
method combines balancing weights with a regularized regression adjustment.
"
econ,Monte Carlo Confidence Sets for Identified Sets,"  In complicated/nonlinear parametric models, it is generally hard to know
whether the model parameters are point identified. We provide computationally
attractive procedures to construct confidence sets (CSs) for identified sets of
full parameters and of subvectors in models defined through a likelihood or a
vector of moment equalities or inequalities. These CSs are based on level sets
of optimal sample criterion functions (such as likelihood or optimally-weighted
or continuously-updated GMM criterions). The level sets are constructed using
cutoffs that are computed via Monte Carlo (MC) simulations directly from the
quasi-posterior distributions of the criterions. We establish new Bernstein-von
Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions
of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified
regular models and some non-regular models. These results imply that our MC CSs
have exact asymptotic frequentist coverage for identified sets of full
parameters and of subvectors in partially-identified regular models, and have
valid but potentially conservative coverage in models with reduced-form
parameters on the boundary. Our MC CSs for identified sets of subvectors are
shown to have exact asymptotic coverage in models with singularities. We also
provide results on uniform validity of our CSs over classes of DGPs that
include point and partially identified models. We demonstrate good
finite-sample coverage properties of our procedures in two simulation
experiments. Finally, our procedures are applied to two non-trivial empirical
examples: an airline entry game and a model of trade flows.
"
econ,Economic Development and Inequality: a complex system analysis,"  By borrowing methods from complex system analysis, in this paper we analyze
the features of the complex relationship that links the development and the
industrialization of a country to economic inequality. In order to do this, we
identify industrialization as a combination of a monetary index, the GDP per
capita, and a recently introduced measure of the complexity of an economy, the
Fitness. At first we explore these relations on a global scale over the time
period 1990--2008 focusing on two different dimensions of inequality: the
capital share of income and a Theil measure of wage inequality. In both cases,
the movement of inequality follows a pattern similar to the one theorized by
Kuznets in the fifties. We then narrow down the object of study ad we
concentrate on wage inequality within the United States. By employing data on
wages and employment on the approximately 3100 US counties for the time
interval 1990--2014, we generalize the Fitness-Complexity algorithm for
counties and NAICS sectors, and we investigate wage inequality between
industrial sectors within counties. At this scale, in the early nineties we
recover a behavior similar to the global one. While, in more recent years, we
uncover a trend reversal: wage inequality monotonically increases as
industrialization levels grow. Hence at a county level, at net of the social
and institutional factors that differ among countries, we not only observe an
upturn in inequality but also a change in the structure of the relation between
wage inequality and development.
"
econ,Testing for Common Breaks in a Multiple Equations System,"  The issue addressed in this paper is that of testing for common breaks across
or within equations of a multivariate system. Our framework is very general and
allows integrated regressors and trends as well as stationary regressors. The
null hypothesis is that breaks in different parameters occur at common
locations and are separated by some positive fraction of the sample size unless
they occur across different equations. Under the alternative hypothesis, the
break dates across parameters are not the same and also need not be separated
by a positive fraction of the sample size whether within or across equations.
The test considered is the quasi-likelihood ratio test assuming normal errors,
though as usual the limit distribution of the test remains valid with
non-normal errors. Of independent interest, we provide results about the rate
of convergence of the estimates when searching over all possible partitions
subject only to the requirement that each regime contains at least as many
observations as some positive fraction of the sample size, allowing break dates
not separated by a positive fraction of the sample size across equations.
Simulations show that the test has good finite sample properties. We also
provide an application to issues related to level shifts and persistence for
various measures of inflation to illustrate its usefulness.
"
econ,A/B Testing of Auctions,"  For many application areas A/B testing, which partitions users of a system
into an A (control) and B (treatment) group to experiment between several
application designs, enables Internet companies to optimize their services to
the behavioral patterns of their users. Unfortunately, the A/B testing
framework cannot be applied in a straightforward manner to applications like
auctions where the users (a.k.a., bidders) submit bids before the partitioning
into the A and B groups is made. This paper combines auction theoretic modeling
with the A/B testing framework to develop methodology for A/B testing auctions.
The accuracy of our method %, assuming the auction is directly comparable to
ideal A/B testing where there is no interference between A and B. Our results
are based on an extension and improved analysis of the inference method of
Chawla et al. (2014).
"
econ,Nonparametric Analysis of Random Utility Models,"  This paper develops and implements a nonparametric test of Random Utility
Models. The motivating application is to test the null hypothesis that a sample
of cross-sectional demand distributions was generated by a population of
rational consumers. We test a necessary and sufficient condition for this that
does not rely on any restriction on unobserved heterogeneity or the number of
goods. We also propose and implement a control function approach to account for
endogenous expenditure. An econometric result of independent interest is a test
for linear inequality constraints when these are represented as the vertices of
a polyhedron rather than its faces. An empirical application to the U.K.
Household Expenditure Survey illustrates computational feasibility of the
method in demand problems with 5 goods.
"
econ,"Quantile Graphical Models: Prediction and Conditional Independence with
  Applications to Systemic Risk","  We propose two types of Quantile Graphical Models (QGMs) --- Conditional
Independence Quantile Graphical Models (CIQGMs) and Prediction Quantile
Graphical Models (PQGMs). CIQGMs characterize the conditional independence of
distributions by evaluating the distributional dependence structure at each
quantile index. As such, CIQGMs can be used for validation of the graph
structure in the causal graphical models (\cite{pearl2009causality,
robins1986new, heckman2015causal}). One main advantage of these models is that
we can apply them to large collections of variables driven by non-Gaussian and
non-separable shocks. PQGMs characterize the statistical dependencies through
the graphs of the best linear predictors under asymmetric loss functions. PQGMs
make weaker assumptions than CIQGMs as they allow for misspecification. Because
of QGMs' ability to handle large collections of variables and focus on specific
parts of the distributions, we could apply them to quantify tail
interdependence. The resulting tail risk network can be used for measuring
systemic risk contributions that help make inroads in understanding
international financial contagion and dependence structures of returns under
downside market movements.
  We develop estimation and inference methods for QGMs focusing on the
high-dimensional case, where the number of variables in the graph is large
compared to the number of observations. For CIQGMs, these methods and results
include valid simultaneous choices of penalty functions, uniform rates of
convergence, and confidence regions that are simultaneously valid. We also
derive analogous results for PQGMs, which include new results for penalized
quantile regressions in high-dimensional settings to handle misspecification,
many controls, and a continuum of additional conditioning events.
"
