category,title,abstract,clean_abstract
phys,A High Robustness and Low Cost Model for Cascading Failures,"  We study numerically the cascading failure problem by using artificially
created scale-free networks and the real network structure of the power grid.
The capacity for a vertex is assigned as a monotonically increasing function of
the load (or the betweenness centrality). Through the use of a simple
functional form with two free parameters, revealed is that it is indeed
possible to make networks more robust while spending less cost. We suggest that
our method to prevent cascade by protecting less vertices is particularly
important for the design of more robust real-world networks to cascading
failures.
",study numerically cascade failure problem artificially create scale free network real network structure power grid capacity vertex assign monotonically increase function load betweenness centrality use simple functional form free parameter reveal possible network robust spend cost suggest method prevent cascade protect vertex particularly important design robust real world network cascade failure
phys,Turbulent Diffusion of Lines and Circulations,"  We study material lines and passive vectors in a model of turbulent flow at
infinite-Reynolds number, the Kraichnan-Kazantsev ensemble of velocities that
are white-noise in time and rough (Hoelder continuous) in space. It is argued
that the phenomenon of ``spontaneous stochasticity'' generalizes to material
lines and that conservation of circulations generalizes to a ``martingale
property'' of the stochastic process of lines.
",study material line passive vector model turbulent flow infinite reynolds number kraichnan kazantsev ensemble velocity white noise time rough hoelder continuous space argue phenomenon ` ` spontaneous stochasticity generalize material line conservation circulation generalize ` ` martingale property stochastic process line
phys,Leaky modes of a left-handed slab,"  Using complex plane analysis we show that left-handed slab may support either
leaky slab waves, which are backward because of negative refraction, or leaky
surface waves, which are backward or forward depending on the propagation
direction of the surface wave itself. Moreover, there is a general connection
between the reflection coefficient of the left-handed slab and the one of the
corresponding right-handed slab (with opposite permittivity and permeability)
so that leaky slab modes are excited for the same angle of incidence of the
impinging beam for both structures. Many negative giant lateral shifts can be
explained by the excitation of these leaky modes.
",complex plane analysis left handed slab support leaky slab wave backward negative refraction leaky surface wave backward forward depend propagation direction surface wave general connection reflection coefficient left handed slab corresponding right handed slab opposite permittivity permeability leaky slab mode excited angle incidence impinge beam structure negative giant lateral shift explain excitation leaky mode
phys,Universe Without Singularities. A Group Approach to De Sitter Cosmology,"  In the last years the traditional scenario of Big Bang has been deeply
modified by the study of the quantum features of the Universe evolution,
proposing again the problem of using local physical laws on cosmic scale, with
particular regard to the cosmological constant role. The group extention method
shows that the De Sitter group univocally generalizes the Poincare group,
formally justifies the cosmological constant use and suggests a new
interpretation for Hartle-Hawking boundary conditions in Quantum Cosmology.
",year traditional scenario big bang deeply modify study quantum feature universe evolution propose problem local physical law cosmic scale particular regard cosmological constant role group extention method show de sitter group univocally generalize poincare group formally justify cosmological constant use suggest new interpretation hartle hawk boundary condition quantum cosmology
phys,Quantifying social group evolution,"  The rich set of interactions between individuals in the society results in
complex community structure, capturing highly connected circles of friends,
families, or professional cliques in a social network. Thanks to frequent
changes in the activity and communication patterns of individuals, the
associated social and communication network is subject to constant evolution.
Our knowledge of the mechanisms governing the underlying community dynamics is
limited, but is essential for a deeper understanding of the development and
self-optimisation of the society as a whole. We have developed a new algorithm
based on clique percolation, that allows, for the first time, to investigate
the time dependence of overlapping communities on a large scale and as such, to
uncover basic relationships characterising community evolution. Our focus is on
networks capturing the collaboration between scientists and the calls between
mobile phone users. We find that large groups persist longer if they are
capable of dynamically altering their membership, suggesting that an ability to
change the composition results in better adaptability. The behaviour of small
groups displays the opposite tendency, the condition for stability being that
their composition remains unchanged. We also show that the knowledge of the
time commitment of the members to a given community can be used for estimating
the community's lifetime. These findings offer a new view on the fundamental
differences between the dynamics of small groups and large institutions.
",rich set interaction individual society result complex community structure capture highly connect circle friend family professional clique social network thank frequent change activity communication pattern individual associate social communication network subject constant evolution knowledge mechanism govern underlie community dynamic limited essential deep understanding development self optimisation society develop new algorithm base clique percolation allow time investigate time dependence overlap community large scale uncover basic relationship characterise community evolution focus network capture collaboration scientist call mobile phone user find large group persist long capable dynamically alter membership suggest ability change composition result well adaptability behaviour small group display opposite tendency condition stability composition remain unchanged knowledge time commitment member give community estimate community lifetime finding offer new view fundamental difference dynamic small group large institution
phys,Eigen Equation of the Nonlinear Spinor,"  How to effectively solve the eigen solutions of the nonlinear spinor field
equation coupling with some other interaction fields is important to understand
the behavior of the elementary particles. In this paper, we derive a simplified
form of the eigen equation of the nonlinear spinor, and then propose a scheme
to solve their numerical solutions. This simplified equation has elegant and
neat structure, which is more convenient for both theoretical analysis and
numerical computation.
",effectively solve eigen solution nonlinear spinor field equation couple interaction field important understand behavior elementary particle paper derive simplified form eigen equation nonlinear spinor propose scheme solve numerical solution simplified equation elegant neat structure convenient theoretical analysis numerical computation
phys,Non-extensive thermodynamics of 1D systems with long-range interaction,"  A new approach to non-extensive thermodynamical systems with non-additive
energy and entropy is proposed. The main idea of the paper is based on the
statistical matching of the thermodynamical systems with the additive
multi-step Markov chains. This general approach is applied to the Ising spin
chain with long-range interaction between its elements. The asymptotical
expressions for the energy and entropy of the system are derived for the
limiting case of weak interaction. These thermodynamical quantities are found
to be non-proportional to the length of the system (number of its particle).
",new approach non extensive thermodynamical system non additive energy entropy propose main idea paper base statistical matching thermodynamical system additive multi step markov chain general approach apply ising spin chain long range interaction element asymptotical expression energy entropy system derive limiting case weak interaction thermodynamical quantity find non proportional length system number particle
phys,Simulation of Robustness against Lesions of Cortical Networks,"  Structure entails function and thus a structural description of the brain
will help to understand its function and may provide insights into many
properties of brain systems, from their robustness and recovery from damage, to
their dynamics and even their evolution. Advances in the analysis of complex
networks provide useful new approaches to understanding structural and
functional properties of brain networks. Structural properties of networks
recently described allow their characterization as small-world, random
(exponential) and scale-free. They complement the set of other properties that
have been explored in the context of brain connectivity, such as topology,
hodology, clustering, and hierarchical organization. Here we apply new network
analysis methods to cortical inter-areal connectivity networks for the cat and
macaque brains. We compare these corticocortical fibre networks to benchmark
rewired, small-world, scale-free and random networks, using two analysis
strategies, in which we measure the effects of the removal of nodes and
connections on the structural properties of the cortical networks. The brain
networks' structural decay is in most respects similar to that of scale-free
networks. The results implicate highly connected hub-nodes and bottleneck
connections as structural basis for some of the conditional robustness of brain
systems. This informs the understanding of the development of brain networks'
connectivity.
",structure entail function structural description brain help understand function provide insight property brain system robustness recovery damage dynamic evolution advance analysis complex network provide useful new approach understand structural functional property brain network structural property network recently describe allow characterization small world random exponential scale free complement set property explore context brain connectivity topology hodology clustering hierarchical organization apply new network analysis method cortical inter areal connectivity network cat macaque brain compare corticocortical fibre network benchmark rewire small world scale free random network analysis strategy measure effect removal node connection structural property cortical network brain network structural decay respect similar scale free network result implicate highly connect hub node bottleneck connection structural basis conditional robustness brain system inform understanding development brain network connectivity
phys,"Bibliometric statistical properties of the 100 largest European
  universities: prevalent scaling rules in the science system","  For the 100 largest European universities we studied the statistical
properties of bibliometric indicators related to research performance, field
citation density and journal impact. We find a size-dependent cumulative
advantage for the impact of universities in terms of total number of citations.
In previous work a similar scaling rule was found at the level of research
groups. Therefore we conjecture that this scaling rule is a prevalent property
of the science system. We observe that lower performance universities have a
larger size-dependent cumulative advantage for receiving citations than
top-performance universities. We also find that for the lower-performance
universities the fraction of not-cited publications decreases considerably with
size. Generally, the higher the average journal impact of the publications of a
university, the lower the number of not-cited publications. We find that the
average research performance does not dilute with size. Large top-performance
universities succeed in keeping a high performance over a broad range of
activities. This most probably is an indication of their scientific attractive
power. Next we find that particularly for the lower-performance universities
the field citation density provides a strong cumulative advantage in citations
per publication. The relation between number of citations and field citation
density found in this study can be considered as a second basic scaling rule of
the science system. Top-performance universities publish in journals with
significantly higher journal impact as compared to the lower performance
universities. We find a significant decrease of the fraction of self-citations
with increasing research performance, average field citation density, and
average journal impact.
",100 large european university study statistical property bibliometric indicator relate research performance field citation density journal impact find size dependent cumulative advantage impact university term total number citation previous work similar scaling rule find level research group conjecture scaling rule prevalent property science system observe low performance university large size dependent cumulative advantage receive citation performance university find low performance university fraction cite publication decrease considerably size generally high average journal impact publication university low number cite publication find average research performance dilute size large performance university succeed keep high performance broad range activity probably indication scientific attractive power find particularly low performance university field citation density provide strong cumulative advantage citation publication relation number citation field citation density find study consider second basic scaling rule science system performance university publish journal significantly high journal impact compare low performance university find significant decrease fraction self citation increase research performance average field citation density average journal impact
phys,Nuclear Spin Effects in Optical Lattice Clocks,"  We present a detailed experimental and theoretical study of the effect of
nuclear spin on the performance of optical lattice clocks. With a state-mixing
theory including spin-orbit and hyperfine interactions, we describe the origin
of the $^1S_0$-$^3P_0$ clock transition and the differential g-factor between
the two clock states for alkaline-earth(-like) atoms, using $^{87}$Sr as an
example. Clock frequency shifts due to magnetic and optical fields are
discussed with an emphasis on those relating to nuclear structure. An
experimental determination of the differential g-factor in $^{87}$Sr is
performed and is in good agreement with theory. The magnitude of the tensor
light shift on the clock states is also explored experimentally. State specific
measurements with controlled nuclear spin polarization are discussed as a
method to reduce the nuclear spin-related systematic effects to below
10$^{-17}$ in lattice clocks.
",present detailed experimental theoretical study effect nuclear spin performance optical lattice clock state mix theory include spin orbit hyperfine interaction describe origin $ ^1s_0$-$^3p_0 $ clock transition differential g factor clock state alkaline earth(-like atom $ ^{87}$sr example clock frequency shift magnetic optical field discuss emphasis relate nuclear structure experimental determination differential g factor $ ^{87}$sr perform good agreement theory magnitude tensor light shift clock state explore experimentally state specific measurement control nuclear spin polarization discuss method reduce nuclear spin relate systematic effect 10$^{-17}$ lattice clock
phys,Statistical analysis of weighted networks,"  The purpose of this paper is to assess the statistical characterization of
weighted networks in terms of the generalization of the relevant parameters,
namely average path length, degree distribution and clustering coefficient.
Although the degree distribution and the average path length admit
straightforward generalizations, for the clustering coefficient several
different definitions have been proposed in the literature. We examined the
different definitions and identified the similarities and differences between
them. In order to elucidate the significance of different definitions of the
weighted clustering coefficient, we studied their dependence on the weights of
the connections. For this purpose, we introduce the relative perturbation norm
of the weights as an index to assess the weight distribution. This study
revealed new interesting statistical regularities in terms of the relative
perturbation norm useful for the statistical characterization of weighted
graphs.
",purpose paper assess statistical characterization weight network term generalization relevant parameter average path length degree distribution cluster coefficient degree distribution average path length admit straightforward generalization cluster coefficient different definition propose literature examine different definition identify similarity difference order elucidate significance different definition weighted cluster coefficient study dependence weight connection purpose introduce relative perturbation norm weight index assess weight distribution study reveal new interesting statistical regularity term relative perturbation norm useful statistical characterization weight graph
phys,Genetic Optimization of Photonic Bandgap Structures,"  We investigate the use of a Genetic Algorithm (GA) to design a set of
photonic crystals (PCs) in one and two dimensions. Our flexible design
methodology allows us to optimize PC structures which are optimized for
specific objectives. In this paper, we report the results of several such
GA-based PC optimizations. We show that the GA performs well even in very
complex design spaces, and therefore has great potential for use as a robust
design tool in present and future applications.
",investigate use genetic algorithm ga design set photonic crystal pc dimension flexible design methodology allow optimize pc structure optimize specific objective paper report result ga base pc optimization ga perform complex design space great potential use robust design tool present future application
phys,"Interference effects in above-threshold ionization from diatomic
  molecules: determining the internuclear separation","  We calculate angle-resolved above-threshold ionization spectra for diatomic
molecules in linearly polarized laser fields, employing the strong-field
approximation. The interference structure resulting from the individual
contributions of the different scattering scenarios is discussed in detail,
with respect to the dependence on the internuclear distance and molecular
orientation. We show that, in general, the contributions from the processes in
which the electron is freed at one center and rescatters off the other obscure
the interference maxima and minima obtained from single-center processes.
However, around the boundary of the energy regions for which rescattering has a
classical counterpart, such processes play a negligible role and very clear
interference patterns are observed. In such energy regions, one is able to
infer the internuclear distance from the energy difference between adjacent
interference minima.
",calculate angle resolve threshold ionization spectra diatomic molecule linearly polarize laser field employ strong field approximation interference structure result individual contribution different scatter scenario discuss detail respect dependence internuclear distance molecular orientation general contribution process electron free center rescatter obscure interference maxima minima obtain single center process boundary energy region rescattering classical counterpart process play negligible role clear interference pattern observe energy region able infer internuclear distance energy difference adjacent interference minima
phys,Three dimensional cooling and trapping with a narrow line,"  The intercombination line of Strontium at 689nm is successfully used in laser
cooling to reach the photon recoil limit with Doppler cooling in a
magneto-optical traps (MOT). In this paper we present a systematic study of the
loading efficiency of such a MOT. Comparing the experimental results to a
simple model allows us to discuss the actual limitation of our apparatus. We
also study in detail the final MOT regime emphasizing the role of gravity on
the position, size and temperature along the vertical and horizontal
directions. At large laser detuning, one finds an unusual situation where
cooling and trapping occur in the presence of a high bias magnetic field.
",intercombination line strontium 689 nm successfully laser cooling reach photon recoil limit doppler cool magneto optical trap mot paper present systematic study loading efficiency mot compare experimental result simple model allow discuss actual limitation apparatus study detail final mot regime emphasize role gravity position size temperature vertical horizontal direction large laser detuning find unusual situation cool trap occur presence high bias magnetic field
phys,"New version announcement for TaylUR, an arbitrary-order diagonal
  automatic differentiation package for Fortran 95","  We present a new version of TaylUR, a Fortran 95 module to automatically
compute the numerical values of a complex-valued function's derivatives with
respect to several variables up to an arbitrary order in each variable, but
excluding mixed derivatives. The new version fixes a potentially serious bug in
the code for exponential-related functions that could corrupt the imaginary
parts of derivatives, as well as being compatible with a wider range of
compilers.
",present new version taylur fortran 95 module automatically compute numerical value complex value function derivative respect variable arbitrary order variable exclude mixed derivative new version fix potentially bug code exponential relate function corrupt imaginary part derivative compatible wide range compiler
phys,"Effective conservation of energy and momentum algorithm using switching
  potentials suitable for molecular dynamics simulation of thermodynamical
  systems","  During a crossover via a switching mechanism from one 2-body potential to
another as might be applied in modeling (chemical) reactions in the vicinity of
bond formation, energy violations would occur due to finite step size which
determines the trajectory of the particles relative to the potential
interactions of the unbonded state by numerical (e.g. Verlet) integration. This
problem is overcome by an algorithm which preserves the coordinates of the
system for each move, but corrects for energy discrepancies by ensuring both
energy and momentum conservation in the dynamics. The algorithm is tested for a
hysteresis loop reaction model with an without the implementation of the
algorithm. The tests involve checking the rate of energy flow out of the MD
simulation box; in the equilibrium state, no net rate of flows within
experimental error should be observed. The temperature and pressure of the box
should also be invariant within the range of fluctuation of these quantities.
It is demonstrated that the algorithm satisfies these criteria.
",crossover switching mechanism 2 body potential apply modeling chemical reaction vicinity bond formation energy violation occur finite step size determine trajectory particle relative potential interaction unbonded state numerical e.g. verlet integration problem overcome algorithm preserve coordinate system correct energy discrepancy ensure energy momentum conservation dynamic algorithm test hysteresis loop reaction model implementation algorithm test involve check rate energy flow md simulation box equilibrium state net rate flow experimental error observe temperature pressure box invariant range fluctuation quantity demonstrate algorithm satisfy criterion
phys,"Lattice Boltzmann inverse kinetic approach for the incompressible
  Navier-Stokes equations","  In spite of the large number of papers appeared in the past which are devoted
to the lattice Boltzmann (LB) methods, basic aspects of the theory still remain
unchallenged. An unsolved theoretical issue is related to the construction of a
discrete kinetic theory which yields \textit{exactly} the fluid equations,
i.e., is non-asymptotic (here denoted as \textit{LB inverse kinetic theory}).
The purpose of this paper is theoretical and aims at developing an inverse
kinetic approach of this type. In principle infinite solutions exist to this
problem but the freedom can be exploited in order to meet important
requirements. In particular, the discrete kinetic theory can be defined so that
it yields exactly the fluid equation also for arbitrary non-equilibrium (but
suitably smooth) kinetic distribution functions and arbitrarily close to the
boundary of the fluid domain. Unlike previous entropic LB methods the theorem
can be obtained without functional constraints on the class of the initial
distribution functions. Possible realizations of the theory and asymptotic
approximations are provided which permit to determine the fluid equations
\textit{with prescribed accuracy.} As a result, asymptotic accuracy estimates
of customary LB approaches and comparisons with the Chorin artificial
compressibility method are discussed.
",spite large number paper appear past devote lattice boltzmann lb method basic aspect theory remain unchallenged unsolved theoretical issue relate construction discrete kinetic theory yield \textit{exactly fluid equation i.e. non asymptotic denote \textit{lb inverse kinetic theory purpose paper theoretical aim develop inverse kinetic approach type principle infinite solution exist problem freedom exploit order meet important requirement particular discrete kinetic theory define yield exactly fluid equation arbitrary non equilibrium suitably smooth kinetic distribution function arbitrarily close boundary fluid domain unlike previous entropic lb method theorem obtain functional constraint class initial distribution function possible realization theory asymptotic approximation provide permit determine fluid equation \textit{with prescribed accuracy result asymptotic accuracy estimate customary lb approach comparison chorin artificial compressibility method discuss
phys,Pairwise comparisons of typological profiles (of languages),"  No abstract given; compares pairs of languages from World Atlas of Language
Structures.
",abstract give compare pair language world atlas language structures
phys,"Convergence of the discrete dipole approximation. I. Theoretical
  analysis","  We performed a rigorous theoretical convergence analysis of the discrete
dipole approximation (DDA). We prove that errors in any measured quantity are
bounded by a sum of a linear and quadratic term in the size of a dipole d, when
the latter is in the range of DDA applicability. Moreover, the linear term is
significantly smaller for cubically than for non-cubically shaped scatterers.
Therefore, for small d errors for cubically shaped particles are much smaller
than for non-cubically shaped. The relative importance of the linear term
decreases with increasing size, hence convergence of DDA for large enough
scatterers is quadratic in the common range of d. Extensive numerical
simulations were carried out for a wide range of d. Finally we discuss a number
of new developments in DDA and their consequences for convergence.
",perform rigorous theoretical convergence analysis discrete dipole approximation dda prove error measured quantity bound sum linear quadratic term size dipole d range dda applicability linear term significantly small cubically non cubically shaped scatterer small d error cubically shape particle small non cubically shape relative importance linear term decrease increase size convergence dda large scatterer quadratic common range d. extensive numerical simulation carry wide range d. finally discuss number new development dda consequence convergence
phys,A Multiphilic Descriptor for Chemical Reactivity and Selectivity,"  In line with the local philicity concept proposed by Chattaraj et al.
(Chattaraj, P. K.; Maiti, B.; Sarkar, U. J. Phys. Chem. A. 2003, 107, 4973) and
a dual descriptor derived by Toro-Labbe and coworkers (Morell, C.; Grand, A.;
Toro-Labbe, A. J. Phys. Chem. A. 2005, 109, 205), we propose a multiphilic
descriptor. It is defined as the difference between nucleophilic (Wk+) and
electrophilic (Wk-) condensed philicity functions. This descriptor is capable
of simultaneously explaining the nucleophilicity and electrophilicity of the
given atomic sites in the molecule. Variation of these quantities along the
path of a soft reaction is also analyzed. Predictive ability of this descriptor
has been successfully tested on the selected systems and reactions.
Corresponding force profiles are also analyzed in some representative cases.
Also, to study the intra- and intermolecular reactivities another related
descriptor namely, the nucleophilicity excess (DelW-+) for a nucleophile, over
the electrophilicity in it has been defined and tested on all-metal aromatic
compounds.
",line local philicity concept propose chattaraj et al chattaraj p. k. maiti b. sarkar u. j. phys chem a. 2003 107 4973 dual descriptor derive toro labbe coworker morell c. grand a. toro labbe a. j. phys chem a. 2005 109 205 propose multiphilic descriptor define difference nucleophilic wk+ electrophilic wk- condense philicity function descriptor capable simultaneously explain nucleophilicity electrophilicity give atomic site molecule variation quantity path soft reaction analyze predictive ability descriptor successfully test select system reaction correspond force profile analyze representative case study intra- intermolecular reactivity relate descriptor nucleophilicity excess delw-+ nucleophile electrophilicity define test metal aromatic compound
phys,A critical theory of quantum entanglement for the Hydrogen molecule,"  In this paper we investigate some entanglement properties for the Hydrogen
molecule considered as a two interacting spin 1/2 (qubit) model. The
entanglement related to the $H_{2}$ molecule is evaluated both using the von
Neumann entropy and the Concurrence and it is compared with the corresponding
quantities for the two interacting spin system. Many aspects of these functions
are examinated employing in part analytical and, essentially, numerical
techniques. We have compared analogous results obtained by Huang and Kais a few
years ago. In this respect, some possible controversial situations are
presented and discussed.
",paper investigate entanglement property hydrogen molecule consider interact spin 1/2 qubit model entanglement relate $ h_{2}$ molecule evaluate von neumann entropy concurrence compare correspond quantity interact spin system aspect function examinate employ analytical essentially numerical technique compare analogous result obtain huang kais year ago respect possible controversial situation present discuss
phys,"Transient behavior of surface plasmon polaritons scattered at a
  subwavelength groove","  We present a numerical study and analytical model of the optical near-field
diffracted in the vicinity of subwavelength grooves milled in silver surfaces.
The Green's tensor approach permits computation of the phase and amplitude
dependence of the diffracted wave as a function of the groove geometry. It is
shown that the field diffracted along the interface by the groove is equivalent
to replacing the groove by an oscillating dipolar line source. An analytic
expression is derived from the Green's function formalism, that reproduces well
the asymptotic surface plasmon polariton (SPP) wave as well as the transient
surface wave in the near-zone close to the groove. The agreement between this
model and the full simulation is very good, showing that the transient
""near-zone"" regime does not depend on the precise shape of the groove. Finally,
it is shown that a composite diffractive evanescent wave model that includes
the asymptotic SPP can describe the wavelength evolution in this transient
near-zone. Such a semi-analytical model may be useful for the design and
optimization of more elaborate photonic circuits whose behavior in large part
will be controlled by surface waves.
",present numerical study analytical model optical near field diffract vicinity subwavelength groove mill silver surface green tensor approach permit computation phase amplitude dependence diffract wave function groove geometry show field diffract interface groove equivalent replace groove oscillate dipolar line source analytic expression derive green function formalism reproduce asymptotic surface plasmon polariton spp wave transient surface wave near zone close groove agreement model simulation good show transient near zone regime depend precise shape groove finally show composite diffractive evanescent wave model include asymptotic spp describe wavelength evolution transient near zone semi analytical model useful design optimization elaborate photonic circuit behavior large control surface wave
phys,"Compounding Fields and Their Quantum Equations in the Trigintaduonion
  Space","  The 32-dimensional compounding fields and their quantum interplays in the
trigintaduonion space can be presented by analogy with octonion and sedenion
electromagnetic, gravitational, strong and weak interactions. In the
trigintaduonion fields which are associated with the electromagnetic,
gravitational, strong and weak interactions, the study deduces some conclusions
of field source particles (quarks and leptons) and intermediate particles which
are consistent with current some sorts of interaction theories. In the
trigintaduonion fields which are associated with the hyper-strong and
strong-weak fields, the paper draws some predicts and conclusions of the field
source particles (sub-quarks) and intermediate particles. The research results
show that there may exist some new particles in the nature.
",32 dimensional compounding field quantum interplay trigintaduonion space present analogy octonion sedenion electromagnetic gravitational strong weak interaction trigintaduonion field associate electromagnetic gravitational strong weak interaction study deduce conclusion field source particle quark lepton intermediate particle consistent current sort interaction theory trigintaduonion field associate hyper strong strong weak field paper draw predict conclusion field source particle sub quark intermediate particle research result exist new particle nature
phys,"New possible properties of atomic nuclei investigated by non linear
  methods: Fractal and recurrence quantification analysis","  For the first time we apply the methodologies of nonlinear analysis to
investigate atomic matter. We use these methods in the analysis of Atomic
Weights and of Mass Number of atomic nuclei. Using the AutoCorrelation Function
and Mutual Information we establish the presence of nonlinear effects in the
mechanism of increasing mass of atomic nuclei considered as a function of the
atomic number. We find that increasing mass is divergent, possibly chaotic. We
also investigate the possible existence of a Power Law for atomic nuclei and,
using also the technique of the variogram, we conclude that a fractal regime
could superintend to the mechanism of increasing mass for nuclei. Finally,
using the Hurst exponent, evidence is obtained that the mechanism of increasing
mass in atomic nuclei is in the fractional Brownian regime. The most
interesting results are obtained by using Recurrence Quantification Analysis
(RQA). New recurrences, psudoperiodicities, self-resemblance and class of
self-similarities are identified with values of determinism showing oscillating
values indicating the presence of more or less stability during the process of
increasing mass of atomic nuclei. In brief, new regimes of regularities are
identified for atomic nuclei that deserve to be studied by future researches.
In particular an accurate analysis of binding energy values by nonlinear
methods is further required.
",time apply methodology nonlinear analysis investigate atomic matter use method analysis atomic weights mass number atomic nucleus autocorrelation function mutual information establish presence nonlinear effect mechanism increase mass atomic nucleus consider function atomic number find increase mass divergent possibly chaotic investigate possible existence power law atomic nucleus technique variogram conclude fractal regime superintend mechanism increase mass nucleus finally hurst exponent evidence obtain mechanism increase mass atomic nucleus fractional brownian regime interesting result obtain recurrence quantification analysis rqa new recurrence psudoperiodicitie self resemblance class self similarity identify value determinism showing oscillate value indicate presence stability process increase mass atomic nucleus brief new regime regularity identify atomic nucleus deserve study future research particular accurate analysis bind energy value nonlinear method require
phys,Daemons and DAMA: Their Celestial-Mechanics Interrelations,"  The assumption of the capture by the Solar System of the electrically charged
Planckian DM objects (daemons) from the galactic disk is confirmed not only by
the St.Petersburg (SPb) experiments detecting particles with V<30 km/s. Here
the daemon approach is analyzed considering the positive model independent
result of the DAMA/NaI experiment. We explain the maximum in DAMA signals
observed in the May-June period to be associated with the formation behind the
Sun of a trail of daemons that the Sun captures into elongated orbits as it
moves to the apex. The range of significant 2-6-keV DAMA signals fits well the
iodine nuclei elastically knocked out of the NaI(Tl) scintillator by particles
falling on the Earth with V=30-50 km/s from strongly elongated heliocentric
orbits. The half-year periodicity of the slower daemons observed in SPb
originates from the transfer of particles that are deflected through ~90 deg
into near-Earth orbits each time the particles cross the outer reaches of the
Sun which had captured them. Their multi-loop (cross-like) trajectories
traverse many times the Earth's orbit in March and September, which increases
the probability for the particles to enter near-Earth orbits during this time.
Corroboration of celestial mechanics calculations with observations yields
~1e-19 cm2 for the cross section of daemon interaction with the solar matter.
",assumption capture solar system electrically charge planckian dm object daemon galactic disk confirm st. petersburg spb experiment detect particle v<30 km s. daemon approach analyze consider positive model independent result dama nai experiment explain maximum dama signal observe june period associate formation sun trail daemon sun capture elongate orbit move apex range significant 2 6 kev dama signal fit iodine nucleus elastically knock nai(tl scintillator particle fall earth v=30 50 km s strongly elongate heliocentric orbit half year periodicity slow daemon observe spb originate transfer particle deflect ~90 deg near earth orbit time particle cross outer reach sun capture multi loop cross like trajectory traverse time earth orbit march september increase probability particle enter near earth orbit time corroboration celestial mechanic calculation observation yield ~1e-19 cm2 cross section daemon interaction solar matter
phys,"Polymerization Force Driven Buckling of Microtubule Bundles Determines
  the Wavelength of Patterns Formed in Tubulin Solutions","  We present a model for the spontaneous formation of a striated pattern in
polymerizing microtubule solutions. It describes the buckling of a single
microtubule (MT) bundle within an elastic network formed by other similarly
aligned and buckling bundles and unaligned MTs. Phase contrast and polarization
microscopy studies of the temporal evolution of the pattern imply that the
polymerization of MTs within the bundles creates the driving compressional
force. Using the measured rate of buckling, the established MT force-velocity
curve and the pattern wavelength, we obtain reasonable estimates for the MT
bundle bending rigidity and the elastic constant of the network. The analysis
implies that the bundles buckle as solid rods.
",present model spontaneous formation striate pattern polymerize microtubule solution describe buckling single microtubule mt bundle elastic network form similarly aligned buckling bundle unaligned mt phase contrast polarization microscopy study temporal evolution pattern imply polymerization mt bundle create drive compressional force measured rate buckling establish mt force velocity curve pattern wavelength obtain reasonable estimate mt bundle bend rigidity elastic constant network analysis imply bundle buckle solid rod
phys,Approaching the Heisenberg limit in an atom laser,"  We present experimental and theoretical results showing the improved beam
quality and reduced divergence of an atom laser produced by an optical Raman
transition, compared to one produced by an RF transition. We show that Raman
outcoupling can eliminate the diverging lens effect that the condensate has on
the outcoupled atoms. This substantially improves the beam quality of the atom
laser, and the improvement may be greater than a factor of ten for experiments
with tight trapping potentials. We show that Raman outcoupling can produce atom
lasers whose quality is only limited by the wavefunction shape of the
condensate that produces them, typically a factor of 1.3 above the Heisenberg
limit.
",present experimental theoretical result show improve beam quality reduce divergence atom laser produce optical raman transition compare produce rf transition raman outcoupling eliminate diverge lens effect condensate outcoupled atom substantially improve beam quality atom laser improvement great factor experiment tight trap potential raman outcoupling produce atom laser quality limit wavefunction shape condensate produce typically factor 1.3 heisenberg limit
phys,"Protein and ionic surfactants - promoters and inhibitors of contact line
  pinning","  We report a new effect of surfactants in pinning a drop contact line,
specifically that lysozyme promotes while lauryl sulfate inhibits pinning. We
explain the pinning disparity assuming difference in wetting: the protein-laden
drop wets a ""clean"" surface and the surfactant-laden drop wets an
auto-precursored surface.
",report new effect surfactant pin drop contact line specifically lysozyme promote lauryl sulfate inhibits pin explain pin disparity assume difference wet protein laden drop wet clean surface surfactant laden drop wet auto precursore surface
phys,"Thermal decomposition of norbornane (bicyclo[2.2.1]heptane) dissolved in
  benzene. Experimental study and mechanism investigation","  The thermal decomposition of norbornane (dissolved in benzene) has been
studied in a jet stirred reactor at temperatures between 873 and 973 K, at
residence times ranging from 1 to 4 s and at atmospheric pressure, leading to
conversions from 0.04 to 22.6%. 25 reaction products were identified and
quantified by gas chromatography, amongst which the main ones are hydrogen,
ethylene and 1,3-cyclopentadiene. A mechanism investigation of the thermal
decomposition of the norbornane - benzene binary mixture has been performed.
Reactions involved in the mechanism have been reviewed: unimolecular
initiations 1 by C-C bond scission of norbornane, fate of the generated
diradicals, reactions of transfer and propagation of norbornyl radicals,
reactions of benzene and cross-coupling reactions.
","thermal decomposition norbornane dissolve benzene study jet stir reactor temperature 873 973 k residence time range 1 4 s atmospheric pressure lead conversion 0.04 22.6 25 reaction product identify quantify gas chromatography main one hydrogen ethylene 1,3 cyclopentadiene mechanism investigation thermal decomposition norbornane benzene binary mixture perform reaction involve mechanism review unimolecular initiation 1 c c bond scission norbornane fate generate diradical reaction transfer propagation norbornyl radical reaction benzene cross coupling reaction"
phys,"Monitoring spatially heterogeneous dynamics in a drying colloidal thin
  film","  We report on a new type of experiment that enables us to monitor spatially
and temporally heterogeneous dynamic properties in complex fluids. Our approach
is based on the analysis of near-field speckles produced by light diffusely
reflected from the superficial volume of a strongly scattering medium. By
periodic modulation of an incident speckle beam we obtain pixel-wise ensemble
averages of the structure function coefficient, a measure of the dynamic
activity. To illustrate the application of our approach we follow the different
stages in the drying process of a colloidal thin film. We show that we can
access ensemble averaged dynamic properties on length scales as small as ten
micrometers over the full field of view.
",report new type experiment enable monitor spatially temporally heterogeneous dynamic property complex fluid approach base analysis near field speckle produce light diffusely reflect superficial volume strongly scatter medium periodic modulation incident speckle beam obtain pixel wise ensemble average structure function coefficient measure dynamic activity illustrate application approach follow different stage dry process colloidal thin film access ensemble average dynamic property length scale small micrometer field view
phys,The Genetic Programming Collaboration Network and its Communities,"  Useful information about scientific collaboration structures and patterns can
be inferred from computer databases of published papers. The genetic
programming bibliography is the most complete reference of papers on GP\@. In
addition to locating publications, it contains coauthor and coeditor
relationships from which a more complete picture of the field emerges. We treat
these relationships as undirected small world graphs whose study reveals the
community structure of the GP collaborative social network. Automatic analysis
discovers new communities and highlights new facets of them. The investigation
reveals many similarities between GP and coauthorship networks in other
scientific fields but also some subtle differences such as a smaller central
network component and a high clustering.
",useful information scientific collaboration structure pattern infer computer database publish paper genetic programming bibliography complete reference paper gp\@. addition locate publication contain coauthor coeditor relationship complete picture field emerge treat relationship undirected small world graph study reveal community structure gp collaborative social network automatic analysis discover new community highlight new facet investigation reveal similarity gp coauthorship network scientific field subtle difference small central network component high clustering
phys,Effect of node deleting on network structure,"  The ever-increasing knowledge of the structure of various real-world networks
has uncovered their complex multi-mechanism-governed evolution processes.
Therefore, a better understanding of the structure and evolution of these
networked complex systems requires us to describe such processes in a more
detailed and realistic manner. In this paper, we introduce a new type of
network growth rule which comprises addition and deletion of nodes, and propose
an evolving network model to investigate the effect of node deleting on network
structure. It is found that, with the introduction of node deleting, network
structure is significantly transformed. In particular, degree distribution of
the network undergoes a transition from scale-free to exponential forms as the
intensity of node deleting increases. At the same time, nontrivial
disassortative degree correlation develops spontaneously as a natural result of
network evolution in the model. We also demonstrate that node deleting
introduced in the model does not destroy the connectedness of a growing network
so long as the increasing rate of edges is not excessively small. In addition,
it is found that node deleting will weaken but not eliminate the small-world
effect of a growing network, and generally it will decrease the clustering
coefficient in a network.
",increase knowledge structure real world network uncover complex multi mechanism govern evolution process well understanding structure evolution networked complex system require describe process detailed realistic manner paper introduce new type network growth rule comprise addition deletion node propose evolve network model investigate effect node delete network structure find introduction node deleting network structure significantly transform particular degree distribution network undergo transition scale free exponential form intensity node deleting increase time nontrivial disassortative degree correlation develop spontaneously natural result network evolution model demonstrate node deleting introduce model destroy connectedness grow network long increase rate edge excessively small addition find node deleting weaken eliminate small world effect grow network generally decrease cluster coefficient network
phys,"On the over-barrier reflection in quantum mechanics with multiple
  degrees of freedom","  We present an analytic example of two dimensional quantum mechanical system,
where the exponential suppression of the probability of over-barrier reflection
changes non-monotonically with energy. The suppression is minimal at certain
""optimal"" energies where reflection occurs with exponentially larger
probability than at other energies.
",present analytic example dimensional quantum mechanical system exponential suppression probability barrier reflection change non monotonically energy suppression minimal certain optimal energy reflection occur exponentially large probability energy
phys,Scalar potential model progress,"  Because observations of galaxies and clusters have been found inconsistent
with General Relativity (GR), the focus of effort in developing a Scalar
Potential Model (SPM) has been on the examination of galaxies and clusters. The
SPM has been found to be consistent with cluster cellular structure, the flow
of IGM from spiral galaxies to elliptical galaxies, intergalactic redshift
without an expanding universe, discrete redshift, rotation curve (RC) data
without dark matter, asymmetric RCs, galaxy central mass, galaxy central
velocity dispersion, and the Pioneer Anomaly. In addition, the SPM suggests a
model of past expansion, past contraction, and current expansion of the
universe. GR corresponds to the SPM in the limit in which the effect of the
Sources and Sinks approximate a flat scalar potential field such as between
clusters and on the solar system scale, which is small relative to the distance
to a Source.
",observation galaxy cluster find inconsistent general relativity gr focus effort develop scalar potential model spm examination galaxy cluster spm find consistent cluster cellular structure flow igm spiral galaxy elliptical galaxy intergalactic redshift expand universe discrete redshift rotation curve rc datum dark matter asymmetric rcs galaxy central mass galaxy central velocity dispersion pioneer anomaly addition spm suggest model past expansion past contraction current expansion universe gr correspond spm limit effect sources sinks approximate flat scalar potential field cluster solar system scale small relative distance source
phys,"Alternative Approaches to the Equilibrium Properties of Hard-Sphere
  Liquids","  An overview of some analytical approaches to the computation of the
structural and thermodynamic properties of single component and multicomponent
hard-sphere fluids is provided. For the structural properties, they yield a
thermodynamically consistent formulation, thus improving and extending the
known analytical results of the Percus-Yevick theory. Approximate expressions
for the contact values of the radial distribution functions and the
corresponding analytical equations of state are also discussed. Extensions of
this methodology to related systems, such as sticky hard spheres and
square-well fluids, as well as its use in connection with the perturbation
theory of fluids are briefly addressed.
",overview analytical approach computation structural thermodynamic property single component multicomponent hard sphere fluid provide structural property yield thermodynamically consistent formulation improve extend know analytical result percus yevick theory approximate expression contact value radial distribution function correspond analytical equation state discuss extension methodology relate system sticky hard sphere square fluid use connection perturbation theory fluid briefly address
phys,"Measurement of the Aerosol Phase Function at the Pierre Auger
  Observatory","  Air fluorescence detectors measure the energy of ultra-high energy cosmic
rays by collecting fluorescence light emitted from nitrogen molecules along the
extensive air shower cascade. To ensure a reliable energy determination, the
light signal needs to be corrected for atmospheric effects, which not only
attenuate the signal, but also produce a non-negligible background component
due to scattered Cherenkov light and multiple-scattered light. The correction
requires regular measurements of the aerosol attenuation length and the aerosol
phase function, defined as the probability of light scattered in a given
direction. At the Pierre Auger Observatory in Malargue, Argentina, the phase
function is measured on an hourly basis using two Aerosol Phase Function (APF)
light sources. These sources direct a UV light beam across the field of view of
the fluorescence detectors; the phase function can be extracted from the image
of the shots in the fluorescence detector cameras. This paper describes the
design, current status, standard operation procedure, and performance of the
APF system at the Pierre Auger Observatory.
",air fluorescence detector measure energy ultra high energy cosmic ray collect fluorescence light emit nitrogen molecule extensive air shower cascade ensure reliable energy determination light signal need correct atmospheric effect attenuate signal produce non negligible background component scatter cherenkov light multiple scatter light correction require regular measurement aerosol attenuation length aerosol phase function define probability light scatter give direction pierre auger observatory malargue argentina phase function measure hourly basis aerosol phase function apf light source source direct uv light beam field view fluorescence detector phase function extract image shot fluorescence detector camera paper describe design current status standard operation procedure performance apf system pierre auger observatory
phys,"Coupling of whispering-gallery modes in size-mismatched microdisk
  photonic molecules","  Mechanisms of whispering-gallery (WG) modes coupling in microdisk photonic
molecules (PMs) with slight and significant size mismatch are numerically
investigated. The results reveal two different scenarios of modes interaction
depending on the degree of this mismatch and offer new insight into how PM
parameters can be tuned to control and modify WG-modes wavelengths and
Q-factors. From a practical point of view, these findings offer a way to
fabricate PM microlaser structures that exhibit low thresholds and directional
emission, and at the same time are more tolerant to fabrication errors than
previously explored coupled-cavity structures composed of identical
microresonators.
",mechanism whispering gallery wg mode couple microdisk photonic molecule pms slight significant size mismatch numerically investigate result reveal different scenario mode interaction depend degree mismatch offer new insight pm parameter tune control modify wg mode wavelength q factor practical point view finding offer way fabricate pm microlaser structure exhibit low threshold directional emission time tolerant fabrication error previously explore couple cavity structure compose identical microresonator
phys,"Laser spectroscopy of hyperfine structure in highly-charged ions: a test
  of QED at high fields","  An overview is presented of laser spectroscopy experiments with cold,
trapped, highly-charged ions, which will be performed at the HITRAP facility at
GSI in Darmstadt (Germany). These high-resolution measurements of ground state
hyperfine splittings will be three orders of magnitude more precise than
previous measurements. Moreover, from a comparison of measurements of the
hyperfine splittings in hydrogen- and lithium-like ions of the same isotope,
QED effects at high electromagnetic fields can be determined within a few
percent. Several candidate ions suited for these laser spectroscopy studies are
presented.
",overview present laser spectroscopy experiment cold trap highly charge ion perform hitrap facility gsi darmstadt germany high resolution measurement ground state hyperfine splitting order magnitude precise previous measurement comparison measurement hyperfine splitting hydrogen- lithium like ion isotope qed effect high electromagnetic field determine percent candidate ion suit laser spectroscopy study present
phys,The Einstein-Varicak Correspondence on Relativistic Rigid Rotation,"  The historical significance of the problem of relativistic rigid rotation is
reviewed in light of recently published correspondence between Einstein and the
mathematician Vladimir Varicak from the years 1909 to 1913.
",historical significance problem relativistic rigid rotation review light recently publish correspondence einstein mathematician vladimir varicak year 1909 1913
phys,Real Options for Project Schedules (ROPS),"  Real Options for Project Schedules (ROPS) has three recursive
sampling/optimization shells. An outer Adaptive Simulated Annealing (ASA)
optimization shell optimizes parameters of strategic Plans containing multiple
Projects containing ordered Tasks. A middle shell samples probability
distributions of durations of Tasks. An inner shell samples probability
distributions of costs of Tasks. PATHTREE is used to develop options on
schedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to
develop a relative risk analysis among projects.
",real options project schedules rops recursive sampling optimization shell outer adaptive simulated annealing asa optimization shell optimize parameter strategic plan contain multiple project contain order tasks middle shell sample probability distribution duration tasks inner shell sample probability distribution cost tasks pathtree develop option schedule algorithms trading risk dimensions trd apply develop relative risk analysis project
phys,Failure of the work-Hamiltonian connection for free energy calculations,"  Extensions of statistical mechanics are routinely being used to infer free
energies from the work performed over single-molecule nonequilibrium
trajectories. A key element of this approach is the ubiquitous expression
dW/dt=\partial H(x,t)/ \partial t which connects the microscopic work W
performed by a time-dependent force on the coordinate x with the corresponding
Hamiltonian H(x,t) at time t. Here we show that this connection, as pivotal as
it is, cannot be used to estimate free energy changes. We discuss the
implications of this result for single-molecule experiments and atomistic
molecular simulations and point out possible avenues to overcome these
limitations.
",extensions statistical mechanic routinely infer free energy work perform single molecule nonequilibrium trajectory key element approach ubiquitous expression dw dt=\partial h(x t)/ \partial t connect microscopic work w perform time dependent force coordinate x correspond hamiltonian h(x t time t. connection pivotal estimate free energy change discuss implication result single molecule experiment atomistic molecular simulation point possible avenue overcome limitation
phys,"Time and motion in physics: the Reciprocity Principle, relativistic
  invariance of the lengths of rulers and time dilatation","  Ponderable objects moving in free space according to Newton's First Law
constitute both rulers and clocks when one such object is viewed from the rest
frame of another. Together with the Reciprocity Principle this is used to
demonstrate, in both Galilean and special relativity, the invariance of the
measured length of a ruler in motion. The different times: `proper', `improper'
and `apparent' appearing in different formulations of the relativistic time
dilatation relation are discussed and exemplified by experimental applications.
A non-intuitive `length expansion' effect predicted by the Reciprocity
Principle as a necessary consequence of time dilatation is pointed out
",ponderable object move free space accord newton law constitute ruler clock object view rest frame reciprocity principle demonstrate galilean special relativity invariance measure length ruler motion different time ` proper ` improper ` apparent appear different formulation relativistic time dilatation relation discuss exemplify experimental application non intuitive ` length expansion effect predict reciprocity principle necessary consequence time dilatation point
phys,"Proposal for an Enhanced Optical Cooling System Test in an Electron
  Storage Ring","  We are proposing to test experimentally the new idea of Enhanced Optical
Cooling (EOC) in an electron storage ring. This experiment will confirm new
fundamental processes in beam physics and will demonstrate new unique
possibilities with this cooling technique. It will open important applications
of EOC in nuclear physics, elementary particle physics and in Light Sources
(LS) based on high brightness electron and ion beams.
",propose test experimentally new idea enhanced optical cooling eoc electron storage ring experiment confirm new fundamental process beam physics demonstrate new unique possibility cool technique open important application eoc nuclear physics elementary particle physics light sources ls base high brightness electron ion beam
phys,Equation of state for dense hydrogen and plasma phase transition,"  We calculate the equation of state of dense hydrogen within the chemical
picture. Fluid variational theory is generalized for a multi-component system
of molecules, atoms, electrons, and protons. Chemical equilibrium is supposed
for the reactions dissociation and ionization. We identify the region of
thermodynamic instability which is related to the plasma phase transition. The
reflectivity is calculated along the Hugoniot curve and compared with
experimental results. The equation-of-state data is used to calculate the
pressure and temperature profiles for the interior of Jupiter.
",calculate equation state dense hydrogen chemical picture fluid variational theory generalize multi component system molecule atom electron proton chemical equilibrium suppose reaction dissociation ionization identify region thermodynamic instability relate plasma phase transition reflectivity calculate hugoniot curve compare experimental result equation state datum calculate pressure temperature profile interior jupiter
phys,Nova Geminorum 1912 and the Origin of the Idea of Gravitational Lensing,"  Einstein's early calculations of gravitational lensing, contained in a
scratch notebook and dated to the spring of 1912, are reexamined. A hitherto
unknown letter by Einstein suggests that he entertained the idea of explaining
the phenomenon of new stars by gravitational lensing in the fall of 1915 much
more seriously than was previously assumed. A reexamination of the relevant
calculations by Einstein shows that, indeed, at least some of them most likely
date from early October 1915. But in support of earlier historical
interpretation of Einstein's notes, it is argued that the appearance of Nova
Geminorum 1912 (DN Gem) in March 1912 may, in fact, provide a relevant context
and motivation for Einstein's lensing calculations on the occasion of his first
meeting with Erwin Freundlich during a visit in Berlin in April 1912. We also
comment on the significance of Einstein's consideration of gravitational
lensing in the fall of 1915 for the reconstruction of Einstein's final steps in
his path towards general relativity.
",einstein early calculation gravitational lense contain scratch notebook date spring 1912 reexamine hitherto unknown letter einstein suggest entertain idea explain phenomenon new star gravitational lense fall 1915 seriously previously assume reexamination relevant calculation einstein show likely date early october 1915 support early historical interpretation einstein note argue appearance nova geminorum 1912 dn gem march 1912 fact provide relevant context motivation einstein lense calculation occasion meeting erwin freundlich visit berlin april 1912 comment significance einstein consideration gravitational lense fall 1915 reconstruction einstein final step path general relativity
phys,"Rich methane premixed laminar flames doped by light unsaturated
  hydrocarbons - Part I : allene and propyne","  The structure of three laminar premixed rich flames has been investigated: a
pure methane flame and two methane flames doped by allene and propyne,
respectively. The gases of the three flames contain 20.9% (molar) of methane
and 33.4% of oxygen, corresponding to an equivalence ratio of 1.25 for the pure
methane flame. In both doped flames, 2.49% of C3H4 was added, corresponding to
a ratio C3H4/CH4 of 12% and an equivalence ratio of 1.55. The three flames have
been stabilized on a burner at a pressure of 6.7 kPa using argon as dilutant,
with a gas velocity at the burner of 36 cm/s at 333 K. The concentration
profiles of stable species were measured by gas chromatography after sampling
with a quartz microprobe. Quantified species included carbon monoxide and
dioxide, methane, oxygen, hydrogen, ethane, ethylene, acetylene, propyne,
allene, propene, propane, 1,2-butadiene, 1,3-butadiene, 1-butene, isobutene,
1-butyne, vinylacetylene, and benzene. The temperature was measured using a
PtRh (6%)-PtRh (30%) thermocouple settled inside the enclosure and ranged from
700 K close to the burner up to 1850 K. In order to model these new results,
some improvements have been made to a mechanism previously developed in our
laboratory for the reactions of C3-C4 unsaturated hydrocarbons. The main
reaction pathways of consumption of allene and propyne and of formation of C6
aromatic species have been derived from flow rate analyses.
","structure laminar premixed rich flame investigate pure methane flame methane flame dope allene propyne respectively gas flame contain 20.9 molar methane 33.4 oxygen correspond equivalence ratio 1.25 pure methane flame dope flame 2.49 c3h4 add correspond ratio c3h4 ch4 12 equivalence ratio 1.55 flame stabilize burner pressure 6.7 kpa argon dilutant gas velocity burner 36 cm s 333 k. concentration profile stable specie measure gas chromatography sample quartz microprobe quantified specie include carbon monoxide dioxide methane oxygen hydrogen ethane ethylene acetylene propyne allene propene propane 1,2 butadiene 1,3 butadiene 1 butene isobutene 1 butyne vinylacetylene benzene temperature measure ptrh 6%)-ptrh 30 thermocouple settle inside enclosure range 700 k close burner 1850 k. order model new result improvement mechanism previously develop laboratory reaction c3 c4 unsaturated hydrocarbon main reaction pathway consumption allene propyne formation c6 aromatic specie derive flow rate analysis"
phys,A non-perturbative proof of Bertrand's theorem,"  We discuss an alternative non-perturbative proof of Bertrand's theorem that
leads in a concise way directly to the two allowed fields: the newtonian and
the isotropic harmonic oscillator central fields.
",discuss alternative non perturbative proof bertrand theorem lead concise way directly allow field newtonian isotropic harmonic oscillator central field
phys,"The discrete dipole approximation for simulation of light scattering by
  particles much larger than the wavelength","  In this manuscript we investigate the capabilities of the Discrete Dipole
Approximation (DDA) to simulate scattering from particles that are much larger
than the wavelength of the incident light, and describe an optimized publicly
available DDA computer program that processes the large number of dipoles
required for such simulations. Numerical simulations of light scattering by
spheres with size parameters x up to 160 and 40 for refractive index m=1.05 and
2 respectively are presented and compared with exact results of the Mie theory.
Errors of both integral and angle-resolved scattering quantities generally
increase with m and show no systematic dependence on x. Computational times
increase steeply with both x and m, reaching values of more than 2 weeks on a
cluster of 64 processors. The main distinctive feature of the computer program
is the ability to parallelize a single DDA simulation over a cluster of
computers, which allows it to simulate light scattering by very large
particles, like the ones that are considered in this manuscript. Current
limitations and possible ways for improvement are discussed.
",manuscript investigate capability discrete dipole approximation dda simulate scatter particle large wavelength incident light describe optimize publicly available dda computer program process large number dipole require simulation numerical simulation light scatter sphere size parameter x 160 40 refractive index m=1.05 2 respectively present compare exact result mie theory error integral angle resolve scatter quantity generally increase m systematic dependence x. computational time increase steeply x m reach value 2 week cluster 64 processor main distinctive feature computer program ability parallelize single dda simulation cluster computer allow simulate light scattering large particle like one consider manuscript current limitation possible way improvement discuss
phys,Stock market return distributions: from past to present,"  We show that recent stock market fluctuations are characterized by the
cumulative distributions whose tails on short, minute time scales exhibit power
scaling with the scaling index alpha > 3 and this index tends to increase
quickly with decreasing sampling frequency. Our study is based on
high-frequency recordings of the S&P500, DAX and WIG20 indices over the
interval May 2004 - May 2006. Our findings suggest that dynamics of the
contemporary market may differ from the one observed in the past. This effect
indicates a constantly increasing efficiency of world markets.
",recent stock market fluctuation characterize cumulative distribution tail short minute time scale exhibit power scaling scale index alpha > 3 index tend increase quickly decrease sampling frequency study base high frequency recording s&p500 dax wig20 indice interval 2004 2006 finding suggest dynamic contemporary market differ observe past effect indicate constantly increase efficiency world market
phys,Extraction of physical laws from joint experimental data,"  The extraction of a physical law y=yo(x) from joint experimental data about x
and y is treated. The joint, the marginal and the conditional probability
density functions (PDF) are expressed by given data over an estimator whose
kernel is the instrument scattering function. As an optimal estimator of yo(x)
the conditional average is proposed. The analysis of its properties is based
upon a new definition of prediction quality. The joint experimental information
and the redundancy of joint measurements are expressed by the relative entropy.
With the number of experiments the redundancy on average increases, while the
experimental information converges to a certain limit value. The difference
between this limit value and the experimental information at a finite number of
data represents the discrepancy between the experimentally determined and the
true properties of the phenomenon. The sum of the discrepancy measure and the
redundancy is utilized as a cost function. By its minimum a reasonable number
of data for the extraction of the law yo(x) is specified. The mutual
information is defined by the marginal and the conditional PDFs of the
variables. The ratio between mutual information and marginal information is
used to indicate which variable is the independent one. The properties of the
introduced statistics are demonstrated on deterministically and randomly
related variables.
",extraction physical law y = yo(x joint experimental datum x y treat joint marginal conditional probability density function pdf express give datum estimator kernel instrument scattering function optimal estimator yo(x conditional average propose analysis property base new definition prediction quality joint experimental information redundancy joint measurement express relative entropy number experiment redundancy average increase experimental information converge certain limit value difference limit value experimental information finite number datum represent discrepancy experimentally determine true property phenomenon sum discrepancy measure redundancy utilize cost function minimum reasonable number datum extraction law yo(x specify mutual information define marginal conditional pdfs variable ratio mutual information marginal information indicate variable independent property introduce statistic demonstrate deterministically randomly related variable
phys,Semi-spheroidal Quantum Harmonic Oscillator,"  A new single-particle shell model is derived by solving the Schr\""odinger
equation for a semi-spheroidal potential well. Only the negative parity states
of the $Z(z)$ component of the wave function are allowed, so that new magic
numbers are obtained for oblate semi-spheroids, semi-sphere and prolate
semi-spheroids. The semi-spherical magic numbers are identical with those
obtained at the oblate spheroidal superdeformed shape: 2, 6, 14, 26, 44, 68,
100, 140, ... The superdeformed prolate magic numbers of the semi-spheroidal
shape are identical with those obtained at the spherical shape of the
spheroidal harmonic oscillator: 2, 8, 20, 40, 70, 112, 168 ...
","new single particle shell model derive solve schr\""odinger equation semi spheroidal potential negative parity state $ z(z)$ component wave function allow new magic number obtain oblate semi spheroid semi sphere prolate semi spheroid semi spherical magic number identical obtain oblate spheroidal superdeforme shape 2 6 14 26 44 68 100 140 superdeformed prolate magic number semi spheroidal shape identical obtain spherical shape spheroidal harmonic oscillator 2 8 20 40 70 112 168"
phys,"Convergence of the discrete dipole approximation. II. An extrapolation
  technique to increase the accuracy","  We propose an extrapolation technique that allows accuracy improvement of the
discrete dipole approximation computations. The performance of this technique
was studied empirically based on extensive simulations for 5 test cases using
many different discretizations. The quality of the extrapolation improves with
refining discretization reaching extraordinary performance especially for
cubically shaped particles. A two order of magnitude decrease of error was
demonstrated. We also propose estimates of the extrapolation error, which were
proven to be reliable. Finally we propose a simple method to directly separate
shape and discretization errors and illustrated this for one test case.
",propose extrapolation technique allow accuracy improvement discrete dipole approximation computation performance technique study empirically base extensive simulation 5 test case different discretization quality extrapolation improve refining discretization reach extraordinary performance especially cubically shape particle order magnitude decrease error demonstrate propose estimate extrapolation error prove reliable finally propose simple method directly separate shape discretization error illustrate test case
phys,Photon splitting in a laser field,"  Photon splitting due to vacuum polarization in a laser field is considered.
Using an operator technique, we derive the amplitudes for arbitrary strength,
spectral content and polarization of the laser field. The case of a
monochromatic circularly polarized laser field is studied in detail and the
amplitudes are obtained as three-fold integrals. The asymptotic behavior of the
amplitudes for various limits of interest are investigated also in the case of
a linearly polarized laser field. Using the obtained results, the possibility
of experimental observation of the process is discussed.
",photon splitting vacuum polarization laser field consider operator technique derive amplitude arbitrary strength spectral content polarization laser field case monochromatic circularly polarize laser field study detail amplitude obtain fold integral asymptotic behavior amplitude limit interest investigate case linearly polarize laser field obtain result possibility experimental observation process discuss
phys,"A Rigorous Time-Domain Analysis of Full--Wave Electromagnetic Cloaking
  (Invisibility)","  There is currently a great deal of interest in the theoretical and practical
possibility of cloaking objects from the observation by electromagnetic waves.
The basic idea of these invisibility devices \cite{glu1, glu2, le},\cite{pss1}
is to use anisotropic {\it transformation media} whose permittivity and
permeability $\var^{\lambda\nu}, \mu^{\lambda\nu}$, are obtained from the ones,
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$, of isotropic media, by singular
transformations of coordinates. In this paper we study electromagnetic cloaking
in the time-domain using the formalism of time-dependent scattering theory.
This formalism allows us to settle in an unambiguous way the mathematical
problems posed by the singularities of the inverse of the permittivity and the
permeability of the {\it transformation media} on the boundary of the cloaked
objects. We write Maxwell's equations in Schr\""odinger form with the
electromagnetic propagator playing the role of the Hamiltonian. We prove that
the electromagnetic propagator outside of the cloaked objects is essentially
self-adjoint. Moreover, the unique self-adjoint extension is unitarily
equivalent to the electromagnetic propagator in the medium
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$. Using this fact, and since the
coordinate transformation is the identity outside of a ball, we prove that the
scattering operator is the identity. Our results give a rigorous proof that the
construction of \cite{glu1, glu2, le}, \cite{pss1} perfectly cloaks passive and
active devices from observation by electromagnetic waves. Furthermore, we prove
cloaking for general anisotropic materials. In particular, our results prove
that it is possible to cloak objects inside general crystals.
","currently great deal interest theoretical practical possibility cloak object observation electromagnetic wave basic idea invisibility device \cite{glu1 glu2 le},\cite{pss1 use anisotropic \it transformation medium permittivity permeability $ \var^{\lambda\nu \mu^{\lambda\nu}$ obtain one $ \var_0^{\lambda\nu \mu^{\lambda\nu}_0 $ isotropic medium singular transformation coordinate paper study electromagnetic cloaking time domain formalism time dependent scattering theory formalism allow settle unambiguous way mathematical problem pose singularity inverse permittivity permeability \it transformation medium boundary cloaked object write maxwell equation schr\""odinger form electromagnetic propagator play role hamiltonian prove electromagnetic propagator outside cloaked object essentially self adjoint unique self adjoint extension unitarily equivalent electromagnetic propagator medium $ \var_0^{\lambda\nu \mu^{\lambda\nu}_0$. fact coordinate transformation identity outside ball prove scatter operator identity result rigorous proof construction \cite{glu1 glu2 le \cite{pss1 perfectly cloak passive active device observation electromagnetic wave furthermore prove cloaking general anisotropic material particular result prove possible cloak object inside general crystal"
phys,"Evolutionary Neural Gas (ENG): A Model of Self Organizing Network from
  Input Categorization","  Despite their claimed biological plausibility, most self organizing networks
have strict topological constraints and consequently they cannot take into
account a wide range of external stimuli. Furthermore their evolution is
conditioned by deterministic laws which often are not correlated with the
structural parameters and the global status of the network, as it should happen
in a real biological system. In nature the environmental inputs are noise
affected and fuzzy. Which thing sets the problem to investigate the possibility
of emergent behaviour in a not strictly constrained net and subjected to
different inputs. It is here presented a new model of Evolutionary Neural Gas
(ENG) with any topological constraints, trained by probabilistic laws depending
on the local distortion errors and the network dimension. The network is
considered as a population of nodes that coexist in an ecosystem sharing local
and global resources. Those particular features allow the network to quickly
adapt to the environment, according to its dimensions. The ENG model analysis
shows that the net evolves as a scale-free graph, and justifies in a deeply
physical sense- the term gas here used.
",despite claim biological plausibility self organize network strict topological constraint consequently account wide range external stimulus furthermore evolution condition deterministic law correlate structural parameter global status network happen real biological system nature environmental input noise affected fuzzy thing set problem investigate possibility emergent behaviour strictly constrain net subject different input present new model evolutionary neural gas eng topological constraint train probabilistic law depend local distortion error network dimension network consider population node coexist ecosystem share local global resource particular feature allow network quickly adapt environment accord dimension eng model analysis show net evolve scale free graph justifie deeply physical sense- term gas
phys,"Gravity-induced electric polarization of matter and planetary magnetic
  fields","  This paper has been withdrawn due to copyright reasons.
",paper withdraw copyright reason
phys,Polarization conversion in a silica microsphere,"  We experimentally demonstrate controlled polarization-selective phenomena in
a whispering gallery mode resonator. We observed efficient ($\approx 75 %$)
polarization conversion of light in a silica microsphere coupled to a tapered
optical fiber with proper optimization of the polarization of the propagating
light. A simple model treating the microsphere as a ring resonator provides a
good fit to the observed behavior.
",experimentally demonstrate control polarization selective phenomenon whisper gallery mode resonator observe efficient $ \approx 75 $ polarization conversion light silica microsphere couple tapered optical fiber proper optimization polarization propagate light simple model treat microsphere ring resonator provide good fit observed behavior
phys,Search for Chaotic Behavior in a Flapping Flag,"  We measured the correlation of the times between successive flaps of a flag
for a variety of wind speeds and found no evidence of low dimensional chaotic
behavior in the return maps of these times. We instead observed what is best
modeled as random times determined by an exponential distribution. This study
was done as an undergraduate experiment and illustrates the differences between
low dimensional chaotic and possibly higher dimensional chaotic systems.
",measure correlation time successive flap flag variety wind speed find evidence low dimensional chaotic behavior return map time instead observe well model random time determine exponential distribution study undergraduate experiment illustrate difference low dimensional chaotic possibly high dimensional chaotic system
phys,"Polarization properties of subwavelength hole arrays consisting of
  rectangular holes","  Influence of hole shape on extraordinary optical transmission was
investigated using hole arrays consisting of rectangular holes with different
aspect ratio. It was found that the transmission could be tuned continuously by
rotating the hole array. Further more, a phase was generated in this process,
and linear polarization states could be changed to elliptical polarization
states. This phase was correlated with the aspect ratio of the holes. An
intuitional model was presented to explain these results.
",influence hole shape extraordinary optical transmission investigate hole array consist rectangular hole different aspect ratio find transmission tune continuously rotate hole array phase generate process linear polarization state change elliptical polarization state phase correlate aspect ratio hole intuitional model present explain result
phys,Estimation of experimental data redundancy and related statistics,"  Redundancy of experimental data is the basic statistic from which the
complexity of a natural phenomenon and the proper number of experiments needed
for its exploration can be estimated. The redundancy is expressed by the
entropy of information pertaining to the probability density function of
experimental variables. Since the calculation of entropy is inconvenient due to
integration over a range of variables, an approximate expression for redundancy
is derived that includes only a sum over the set of experimental data about
these variables. The approximation makes feasible an efficient estimation of
the redundancy of data along with the related experimental information and
information cost function. From the experimental information the complexity of
the phenomenon can be simply estimated, while the proper number of experiments
needed for its exploration can be determined from the minimum of the cost
function. The performance of the approximate estimation of these statistics is
demonstrated on two-dimensional normally distributed random data.
",redundancy experimental datum basic statistic complexity natural phenomenon proper number experiment need exploration estimate redundancy express entropy information pertain probability density function experimental variable calculation entropy inconvenient integration range variable approximate expression redundancy derive include sum set experimental datum variable approximation make feasible efficient estimation redundancy datum related experimental information information cost function experimental information complexity phenomenon simply estimate proper number experiment need exploration determine minimum cost function performance approximate estimation statistic demonstrate dimensional normally distribute random datum
phys,Visualizing Teleportation,"  A novel way of picturing the processing of quantum information is described,
allowing a direct visualization of teleportation of quantum states and
providing a simple and intuitive understanding of this fascinating phenomenon.
The discussion is aimed at providing physicists a method of explaining
teleportation to non-scientists. The basic ideas of quantum physics are first
explained in lay terms, after which these ideas are used with a graphical
description, out of which teleportation arises naturally.
",novel way picture processing quantum information describe allow direct visualization teleportation quantum state provide simple intuitive understanding fascinating phenomenon discussion aim provide physicist method explain teleportation non scientist basic idea quantum physics explain lay term idea graphical description teleportation arise naturally
phys,"Nonlinear Dynamics of the Phonon Stimulated Emission in Microwave
  Solid-State Resonator of the Nonautonomous Phaser Generator","  The microwave phonon stimulated emission (SE) has been experimentally and
numerically investigated in a nonautonomous microwave acoustic quantum
generator, called also microwave phonon laser or phaser (see previous works
arXiv:cond-mat/0303188 ; arXiv:cond-mat/0402640 ; arXiv:nlin.CG/0703050)
Phenomena of branching and long-time refractority (absence of the reaction on
the external pulses) for deterministic chaotic and regular processes of SE were
observed in experiments with various levels of electromagnetic pumping. At the
pumping level growth, the clearly depined increasing of the number of
coexisting SE states has been observed both in real physical experiments and in
computer simulations. This confirms the analytical estimations of the branching
density in the phase space. The nature of the refractority of SE pulses is
closely connected with the pointed branching and reflects the crises of strange
attractors, i.e. their collisions with unstable periodic components of the
higher branches of SE states in the nonautonomous microwave phonon laser.
",microwave phonon stimulate emission se experimentally numerically investigate nonautonomous microwave acoustic quantum generator call microwave phonon laser phaser previous work arxiv cond mat/0303188 arxiv cond mat/0402640 arxiv nlin cg/0703050 phenomenon branching long time refractority absence reaction external pulse deterministic chaotic regular process se observe experiment level electromagnetic pumping pumping level growth clearly depine increase number coexist se state observe real physical experiment computer simulation confirm analytical estimation branching density phase space nature refractority se pulse closely connect pointed branching reflect crisis strange attractor i.e. collision unstable periodic component high branch se state nonautonomous microwave phonon laser
phys,"Analysis of the real estate market in Las Vegas: Bubble, seasonal
  patterns, and prediction of the CSW indexes","  We analyze 27 house price indexes of Las Vegas from Jun. 1983 to Mar. 2005,
corresponding to 27 different zip codes. These analyses confirm the existence
of a real-estate bubble, defined as a price acceleration faster than
exponential, which is found however to be confined to a rather limited time
interval in the recent past from approximately 2003 to mid-2004 and has
progressively transformed into a more normal growth rate comparable to
pre-bubble levels in 2005. There has been no bubble till 2002 except for a
medium-sized surge in 1990. In addition, we have identified a strong yearly
periodicity which provides a good potential for fine-tuned prediction from
month to month. A monthly monitoring using a model that we have developed could
confirm, by testing the intra-year structure, if indeed the market has returned
to ``normal'' or if more turbulence is expected ahead. We predict the evolution
of the indexes one year ahead, which is validated with new data up to Sep.
2006. The present analysis demonstrates the existence of very significant
variations at the local scale, in the sense that the bubble in Las Vegas seems
to have preceded the more global USA bubble and has ended approximately two
years earlier (mid 2004 for Las Vegas compared with mid-2006 for the whole of
the USA).
",analyze 27 house price index las vegas jun. 1983 mar. 2005 correspond 27 different zip code analysis confirm existence real estate bubble define price acceleration fast exponential find confine limited time interval recent past approximately 2003 mid-2004 progressively transform normal growth rate comparable pre bubble level 2005 bubble till 2002 medium sized surge 1990 addition identify strong yearly periodicity provide good potential fine tune prediction month month monthly monitoring model develop confirm test intra year structure market return ` ` normal turbulence expect ahead predict evolution index year ahead validate new datum sep. 2006 present analysis demonstrate existence significant variation local scale sense bubble las vegas precede global usa bubble end approximately year early mid 2004 las vegas compare mid-2006 usa
phys,"The evolution of the Earth-Moon system based on the dark matter field
  fluid model","  The evolution of Earth-Moon system is described by the dark matter field
fluid model proposed in the Meeting of Division of Particle and Field 2004,
American Physical Society. The current behavior of the Earth-Moon system agrees
with this model very well and the general pattern of the evolution of the
Moon-Earth system described by this model agrees with geological and fossil
evidence. The closest distance of the Moon to Earth was about 259000 km at 4.5
billion years ago, which is far beyond the Roche's limit. The result suggests
that the tidal friction may not be the primary cause for the evolution of the
Earth-Moon system. The average dark matter field fluid constant derived from
Earth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts
that the Mars's rotation is also slowing with the angular acceleration rate
about -4.38 x 10^(-22) rad s^(-2).
",evolution earth moon system describe dark matter field fluid model propose meeting division particle field 2004 american physical society current behavior earth moon system agree model general pattern evolution moon earth system describe model agree geological fossil evidence close distance moon earth 259000 km 4.5 billion year ago far roche limit result suggest tidal friction primary cause evolution earth moon system average dark matter field fluid constant derive earth moon system datum 4.39 x 10^(-22 s^(-1)m^(-1 model predict mars rotation slow angular acceleration rate -4.38 x 10^(-22 rad s^(-2
phys,"Approximate Selection Rule for Orbital Angular Momentum in Atomic
  Radiative Transitions","  We demonstrate that radiative transitions with \Delta l = - 1 are strongly
dominating for all values of n and l, except small region where l << n.
",demonstrate radiative transition \delta l = 1 strongly dominating value n l small region l < < n.
phys,Frequency modulation Fourier transform spectroscopy,"  A new method, FM-FTS, combining Frequency Modulation heterodyne laser
spectroscopy and Fourier Transform Spectroscopy is presented. It provides
simultaneous sensitive measurement of absorption and dispersion profiles with
broadband spectral coverage capabilities. Experimental demonstration is made on
the overtone spectrum of C2H2 in the 1.5 $\mu$m region.
",new method fm fts combine frequency modulation heterodyne laser spectroscopy fourier transform spectroscopy present provide simultaneous sensitive measurement absorption dispersion profile broadband spectral coverage capability experimental demonstration overtone spectrum c2h2 1.5 $ \mu$m region
phys,"Reciprocal Symmetry and Classical Discrete Oscillator Incorporating
  Half-Integral Energy Levels","  Classical oscillator differential equation is replaced by the corresponding
(finite time) difference equation. The equation is, then, symmetrized so that
it remains invariant under the change d going to -d, where d is the smallest
span of time. This symmetric equation has solutions, which come in reciprocally
related pairs. One member of a pair agrees with the classical solution and the
other is an oscillating solution and does not converge to a limit as d goes to
0. This solution contributes to oscillator energy a term which is a multiple of
half-integers.
",classical oscillator differential equation replace corresponding finite time difference equation equation symmetrize remain invariant change d go -d d small span time symmetric equation solution come reciprocally relate pair member pair agree classical solution oscillating solution converge limit d go 0 solution contribute oscillator energy term multiple half integer
phys,"Local-field effects in radiatively broadened magneto-dielectric media:
  negative refraction and absorption reduction","  We give a microscopic derivation of the Clausius-Mossotti relations for a
homogeneous and isotropic magneto-dielectric medium consisting of radiatively
broadened atomic oscillators. To this end the diagram series of electromagnetic
propagators is calculated exactly for an infinite bi-cubic lattice of
dielectric and magnetic dipoles for a lattice constant small compared to the
resonance wavelength $\lambda$. Modifications of transition frequencies and
linewidth of the elementary oscillators are taken into account in a
selfconsistent way by a proper incorporation of the singular self-interaction
terms. We show that in radiatively broadened media sufficiently close to the
free-space resonance the real part of the index of refraction approaches the
value -2 in the limit of $\rho \lambda^3 \gg 1$, where $\rho$ is the number
density of scatterers. Since at the same time the imaginary part vanishes as
$1/\rho$ local field effects can have important consequences for realizing
low-loss negative index materials.
",microscopic derivation clausius mossotti relation homogeneous isotropic magneto dielectric medium consist radiatively broaden atomic oscillator end diagram series electromagnetic propagator calculate exactly infinite bi cubic lattice dielectric magnetic dipole lattice constant small compare resonance wavelength $ \lambda$. modification transition frequency linewidth elementary oscillator take account selfconsistent way proper incorporation singular self interaction term radiatively broaden medium sufficiently close free space resonance real index refraction approach value -2 limit $ \rho \lambda^3 \gg 1 $ $ \rho$ number density scatterer time imaginary vanish $ 1/\rho$ local field effect important consequence realize low loss negative index material
phys,Shocks in nonlocal media,"  We investigate the formation of collisionless shocks along the spatial
profile of a gaussian laser beam propagating in nonlocal nonlinear media. For
defocusing nonlinearity the shock survives the smoothing effect of the nonlocal
response, though its dynamics is qualitatively affected by the latter, whereas
for focusing nonlinearity it dominates over filamentation. The patterns
observed in a thermal defocusing medium are interpreted in the framework of our
theory.
",investigate formation collisionless shock spatial profile gaussian laser beam propagate nonlocal nonlinear medium defocuse nonlinearity shock survive smooth effect nonlocal response dynamic qualitatively affect focus nonlinearity dominate filamentation pattern observe thermal defocuse medium interpret framework theory
phys,"Astrophysical gyrokinetics: kinetic and fluid turbulent cascades in
  magnetized weakly collisional plasmas","  We present a theoretical framework for plasma turbulence in astrophysical
plasmas (solar wind, interstellar medium, galaxy clusters, accretion disks).
The key assumptions are that the turbulence is anisotropic with respect to the
mean magnetic field and frequencies are low compared to the ion cyclotron
frequency. The energy injected at the outer scale scale has to be converted
into heat, which ultimately cannot be done without collisions. A KINETIC
CASCADE develops that brings the energy to collisional scales both in space and
velocity. Its nature depends on the physics of plasma fluctuations. In each of
the physically distinct scale ranges, the kinetic problem is systematically
reduced to a more tractable set of equations. In the ""inertial range"" above the
ion gyroscale, the kinetic cascade splits into a cascade of Alfvenic
fluctuations, which are governed by the RMHD equations at both the collisional
and collisionless scales, and a passive cascade of compressive fluctuations,
which obey a linear kinetic equation along the moving field lines associated
with the Alfvenic component. In the ""dissipation range"" between the ion and
electron gyroscales, there are again two cascades: the kinetic-Alfven-wave
(KAW) cascade governed by two fluid-like Electron RMHD equations and a passive
phase-space cascade of ion entropy fluctuations. The latter cascade brings the
energy of the inertial-range fluctuations that was damped by collisionless
wave-particle interaction at the ion gyroscale to collisional scales in the
phase space and leads to ion heating. The KAW energy is similarly damped at the
electron gyroscale and converted into electron heat. Kolmogorov-style scaling
relations are derived for these cascades. Astrophysical and space-physical
applications are discussed in detail.
",present theoretical framework plasma turbulence astrophysical plasma solar wind interstellar medium galaxy cluster accretion disk key assumption turbulence anisotropic respect mean magnetic field frequency low compare ion cyclotron frequency energy inject outer scale scale convert heat ultimately collision kinetic cascade develop bring energy collisional scale space velocity nature depend physics plasma fluctuation physically distinct scale range kinetic problem systematically reduce tractable set equation inertial range ion gyroscale kinetic cascade split cascade alfvenic fluctuation govern rmhd equation collisional collisionless scale passive cascade compressive fluctuation obey linear kinetic equation move field line associate alfvenic component dissipation range ion electron gyroscale cascade kinetic alfven wave kaw cascade govern fluid like electron rmhd equation passive phase space cascade ion entropy fluctuation cascade bring energy inertial range fluctuation damp collisionless wave particle interaction ion gyroscale collisional scale phase space lead ion heating kaw energy similarly damp electron gyroscale convert electron heat kolmogorov style scaling relation derive cascade astrophysical space physical application discuss detail
phys,Electromagnetic wormholes via handlebody constructions,"  Cloaking devices are prescriptions of electrostatic, optical or
electromagnetic parameter fields (conductivity $\sigma(x)$, index of refraction
$n(x)$, or electric permittivity $\epsilon(x)$ and magnetic permeability
$\mu(x)$) which are piecewise smooth on $\mathbb R^3$ and singular on a
hypersurface $\Sigma$, and such that objects in the region enclosed by $\Sigma$
are not detectable to external observation by waves. Here, we give related
constructions of invisible tunnels, which allow electromagnetic waves to pass
between possibly distant points, but with only the ends of the tunnels visible
to electromagnetic imaging. Effectively, these change the topology of space
with respect to solutions of Maxwell's equations, corresponding to attaching a
handlebody to $\mathbb R^3$. The resulting devices thus function as
electromagnetic wormholes.
",cloaking device prescription electrostatic optical electromagnetic parameter field conductivity $ \sigma(x)$ index refraction $ n(x)$ electric permittivity $ \epsilon(x)$ magnetic permeability $ \mu(x)$ piecewise smooth $ \mathbb r^3 $ singular hypersurface $ \sigma$ object region enclose $ \sigma$ detectable external observation wave related construction invisible tunnel allow electromagnetic wave pass possibly distant point end tunnel visible electromagnetic imaging effectively change topology space respect solution maxwell equation correspond attach handlebody $ \mathbb r^3$. result device function electromagnetic wormhole
phys,Some new experimental photonic flame effect features,"  The results of the spectral, energetical and temporal characteristics of
radiation in the presence of the photonic flame effect are presented.
Artificial opal posed on Cu plate at the temperature of liquid nitrogen boiling
point (77 K) being irradiated by nanosecond ruby laser pulse produces long-
term luminiscence with a duration till ten seconds with a finely structured
spectrum in the the antistocks part of the spectrum. Analogous visible
luminescence manifesting time delay appeared in other samples of the artificial
opals posed on the same plate. In the case of the opal infiltrated with
different nonlinear liquids the threshold of the luminiscence is reduced and
the spatial disribution of the bright emmiting area on the opal surface is
being changed. In the case of the putting the frozen nonlinear liquids on the
Cu plate long-term blue bright luminiscence took place in the frozen species of
the liquids. Temporal characteristics of this luminiscence are nearly the same
as in opal matrixes.
",result spectral energetical temporal characteristic radiation presence photonic flame effect present artificial opal pose cu plate temperature liquid nitrogen boiling point 77 k irradiate nanosecond ruby laser pulse produce long- term luminiscence duration till second finely structure spectrum antistock spectrum analogous visible luminescence manifest time delay appear sample artificial opal pose plate case opal infiltrate different nonlinear liquid threshold luminiscence reduce spatial disribution bright emmite area opal surface change case put frozen nonlinear liquid cu plate long term blue bright luminiscence take place frozen specie liquid temporal characteristic luminiscence nearly opal matrix
phys,"Birth, survival and death of languages by Monte Carlo simulation","  Simulations of physicists for the competition between adult languages since
2003 are reviewed. How many languages are spoken by how many people? How many
languages are contained in various language families? How do language
similarities decay with geographical distance, and what effects do natural
boundaries have? New simulations of bilinguality are given in an appendix.
",simulation physicist competition adult language 2003 review language speak people language contain language family language similarity decay geographical distance effect natural boundary new simulation bilinguality give appendix
phys,"Two-scale structure of the electron dissipation region during
  collisionless magnetic reconnection","  Particle in cell (PIC) simulations of collisionless magnetic reconnection are
presented that demonstrate that the electron dissipation region develops a
distinct two-scale structure along the outflow direction. The length of the
electron current layer is found to decrease with decreasing electron mass,
approaching the ion inertial length for a proton-electron plasma. A surprise,
however, is that the electrons form a high-velocity outflow jet that remains
decoupled from the magnetic field and extends large distances downstream from
the x-line. The rate of reconnection remains fast in very large systems,
independent of boundary conditions and the mass of electrons.
",particle cell pic simulation collisionless magnetic reconnection present demonstrate electron dissipation region develop distinct scale structure outflow direction length electron current layer find decrease decrease electron mass approach ion inertial length proton electron plasma surprise electron form high velocity outflow jet remains decouple magnetic field extend large distance downstream x line rate reconnection remain fast large system independent boundary condition mass electron
phys,"A general approach to statistical modeling of physical laws:
  nonparametric regression","  Statistical modeling of experimental physical laws is based on the
probability density function of measured variables. It is expressed by
experimental data via a kernel estimator. The kernel is determined objectively
by the scattering of data during calibration of experimental setup. A physical
law, which relates measured variables, is optimally extracted from experimental
data by the conditional average estimator. It is derived directly from the
kernel estimator and corresponds to a general nonparametric regression. The
proposed method is demonstrated by the modeling of a return map of noisy
chaotic data. In this example, the nonparametric regression is used to predict
a future value of chaotic time series from the present one. The mean predictor
error is used in the definition of predictor quality, while the redundancy is
expressed by the mean square distance between data points. Both statistics are
used in a new definition of predictor cost function. From the minimum of the
predictor cost function, a proper number of data in the model is estimated.
",statistical modeling experimental physical law base probability density function measured variable express experimental datum kernel estimator kernel determine objectively scattering datum calibration experimental setup physical law relate measure variable optimally extract experimental datum conditional average estimator derive directly kernel estimator correspond general nonparametric regression propose method demonstrate modeling return map noisy chaotic datum example nonparametric regression predict future value chaotic time series present mean predictor error definition predictor quality redundancy express mean square distance datum point statistic new definition predictor cost function minimum predictor cost function proper number datum model estimate
phys,Resonant activation in bistable semiconductor lasers,"  We theoretically investigate the possibility of observing resonant activation
in the hopping dynamics of two-mode semiconductor lasers. We present a series
of simulations of a rate-equations model under random and periodic modulation
of the bias current. In both cases, for an optimal choice of the modulation
time-scale, the hopping times between the stable lasing modes attain a minimum.
The simulation data are understood by means of an effective one-dimensional
Langevin equation with multiplicative fluctuations. Our conclusions apply to
both Edge Emitting and Vertical Cavity Lasers, thus opening the way to several
experimental tests in such optical systems.
",theoretically investigate possibility observe resonant activation hop dynamic mode semiconductor laser present series simulation rate equation model random periodic modulation bias current case optimal choice modulation time scale hop time stable lasing mode attain minimum simulation datum understand mean effective dimensional langevin equation multiplicative fluctuation conclusion apply edge emitting vertical cavity lasers open way experimental test optical system
phys,Intricate Knots in Proteins: Function and Evolution,"  A number of recently discovered protein structures incorporate a rather
unexpected structural feature: a knot in the polypeptide backbone. These knots
are extremely rare, but their occurrence is likely connected to protein
function in as yet unexplored fashion. Our analysis of the complete Protein
Data Bank reveals several new knots which, along with previously discovered
ones, can shed light on such connections. In particular, we identify the most
complex knot discovered to date in human ubiquitin hydrolase, and suggest that
its entangled topology protects it against unfolding and degradation by the
proteasome. Knots in proteins are typically preserved across species and
sometimes even across kingdoms. However, we also identify a knot which only
appears in some transcarbamylases while being absent in homologous proteins of
similar structure. The emergence of the knot is accompanied by a shift in the
enzymatic function of the protein. We suggest that the simple insertion of a
short DNA fragment into the gene may suffice to turn an unknotted into a
knotted structure in this protein.
",number recently discover protein structure incorporate unexpected structural feature knot polypeptide backbone knot extremely rare occurrence likely connect protein function unexplored fashion analysis complete protein data bank reveal new knot previously discover one shed light connection particular identify complex knot discover date human ubiquitin hydrolase suggest entangled topology protect unfolding degradation proteasome knot protein typically preserve specie kingdom identify knot appear transcarbamylase absent homologous protein similar structure emergence knot accompany shift enzymatic function protein suggest simple insertion short dna fragment gene suffice turn unknotted knot structure protein
phys,Universal Forces and the Dark Energy Problem,"  The Dark Energy problem is forcing us to re-examine our models and our
understanding of relativity and space-time. Here a novel idea of Fundamental
Forces is introduced. This allows us to perceive the General Theory of
Relativity and Einstein's Equation from a new pesrpective. In addition to
providing us with an improved understanding of space and time, it will be shown
how it leads to a resolution of the Dark Energy problem.
",dark energy problem force examine model understanding relativity space time novel idea fundamental forces introduce allow perceive general theory relativity einstein equation new pesrpective addition provide improved understanding space time show lead resolution dark energy problem
phys,Vacuum Structure and Potential,"  Based on overall experimental observations, especially the pair processes, I
developed a model structure of the vacuum along with a basic-particle formation
scheme begun in 2000 (with collaborator P-I Johansson). The model consists in
that the vacuum is, briefly, filled of neutral but polarizable vacuuons,
consisting each of a p-vaculeon and n- vaculeon of charges $+e$ and $-e$ of
zero rest masses but with spin motions, assumed interacting each other with a
Coulomb force. The model has been introduced in full in a book (Nova Sci, 2005)
and referred to in a number of journal/E-print papers. I outline in this easier
accessible paper the detailed derivation of the model and a corresponding
quantitative determination of the vacuuon size.
",base overall experimental observation especially pair process develop model structure vacuum basic particle formation scheme begin 2000 collaborator p johansson model consist vacuum briefly fill neutral polarizable vacuuon consist p vaculeon n- vaculeon charge $ + e$ $ -e$ zero rest masse spin motion assume interact coulomb force model introduce book nova sci 2005 refer number journal e print paper outline easy accessible paper detailed derivation model corresponding quantitative determination vacuuon size
phys,A POVM view of the ensemble approach to polarization optics,"  Statistical ensemble formalism of Kim, Mandel and Wolf (J. Opt. Soc. Am. A 4,
433 (1987)) offers a realistic model for characterizing the effect of
stochastic non-image forming optical media on the state of polarization of
transmittedlight. With suitable choice of the Jones ensemble, various Mueller
transformations - some of which have been unknown so far - are deduced. It is
observed that the ensemble approach is formally identical to the positive
operator valued measures (POVM) on the quantum density matrix. This
observation, in combination with the recent suggestion by Ahnert and Payne
(Phys. Rev. A 71, 012330, (2005)) - in the context of generalized quantum
measurement on single photon polarization states - that linear optics elements
can be employed in setting up all possible POVMs, enables us to propose a way
of realizing different types of Mueller devices.
",statistical ensemble formalism kim mandel wolf j. opt soc 4 433 1987 offer realistic model characterize effect stochastic non image form optical medium state polarization transmittedlight suitable choice jones ensemble mueller transformation unknown far deduce observe ensemble approach formally identical positive operator value measure povm quantum density matrix observation combination recent suggestion ahnert payne phys rev. 71 012330 2005 context generalized quantum measurement single photon polarization state linear optic element employ set possible povm enable propose way realize different type mueller device
phys,Quantum electromagnetic X-waves,"  We show that two distinct quantum states of the electromagnetic field can be
associated to a classical vector X wave or a propagation-invariant solution of
Maxwell equations. The difference between the two states is of pure quantum
mechanical origin since they are internally entangled and disentangled,
respectively and can be generated by different linear or nonlinear processes.
Detection and generation of Schr\""odinger-cat states comprising two entangled
X-waves and their possible applications are discussed.
","distinct quantum state electromagnetic field associate classical vector x wave propagation invariant solution maxwell equation difference state pure quantum mechanical origin internally entangle disentangle respectively generate different linear nonlinear process detection generation schr\""odinger cat state comprise entangle x wave possible application discuss"
phys,"Robust manipulation of electron spin coherence in an ensemble of singly
  charged quantum dots","  Using the recently reported mode locking effect we demonstrate a highly
robust control of electron spin coherence in an ensemble of (In,Ga)As quantum
dots during the single spin coherence time. The spin precession in a transverse
magnetic field can be fully controlled up to 25 K by the parameters of the
exciting pulsed laser protocol such as the pulse train sequence, leading to
adjustable quantum beat bursts in Faraday rotation. Flipping of the electron
spin precession phase was demonstrated by inverting the polarization within a
pulse doublet sequence.
",recently report mode locking effect demonstrate highly robust control electron spin coherence ensemble ga)as quantum dot single spin coherence time spin precession transverse magnetic field fully control 25 k parameter exciting pulse laser protocol pulse train sequence lead adjustable quantum beat burst faraday rotation flip electron spin precession phase demonstrate invert polarization pulse doublet sequence
phys,"General System theory, Like-Quantum Semantics and Fuzzy Sets","  It is outlined the possibility to extend the quantum formalism in relation to
the requirements of the general systems theory. It can be done by using a
quantum semantics arising from the deep logical structure of quantum theory. It
is so possible taking into account the logical openness relationship between
observer and system. We are going to show how considering the truth-values of
quantum propositions within the context of the fuzzy sets is here more useful
for systemics . In conclusion we propose an example of formal quantum
coherence.
",outline possibility extend quantum formalism relation requirement general system theory quantum semantic arise deep logical structure quantum theory possible take account logical openness relationship observer system go consider truth value quantum proposition context fuzzy set useful systemic conclusion propose example formal quantum coherence
phys,"Detailed kinetic study of the ring opening of cycloalkanes by CBS-QB3
  calculations","  This work reports a theoretical study of the gas phase unimolecular
decomposition of cyclobutane, cyclopentane and cyclohexane by means of quantum
chemical calculations. A biradical mechanism has been envisaged for each
cycloalkane, and the main routes for the decomposition of the biradicals formed
have been investigated at the CBS-QB3 level of theory. Thermochemical data
(\delta H^0_f, S^0, C^0_p) for all the involved species have been obtained by
means of isodesmic reactions. The contribution of hindered rotors has also been
included. Activation barriers of each reaction have been analyzed to assess the
1 energetically most favorable pathways for the decomposition of biradicals.
Rate constants have been derived for all elementary reactions using transition
state theory at 1 atm and temperatures ranging from 600 to 2000 K. Global rate
constant for the decomposition of the cyclic alkanes in molecular products have
been calculated. Comparison between calculated and experimental results allowed
to validate the theoretical approach. An important result is that the
rotational barriers between the conformers, which are usually neglected, are of
importance in decomposition rate of the largest biradicals. Ring strain
energies (RSE) in transition states for ring opening have been estimated and
show that the main part of RSE contained in the cyclic reactants is removed
upon the activation process.
",work report theoretical study gas phase unimolecular decomposition cyclobutane cyclopentane cyclohexane mean quantum chemical calculation biradical mechanism envisage cycloalkane main route decomposition biradical form investigate cbs qb3 level theory thermochemical datum \delta h^0_f s^0 c^0_p involve specie obtain mean isodesmic reaction contribution hinder rotor include activation barrier reaction analyze assess 1 energetically favorable pathway decomposition biradical rate constant derive elementary reaction transition state theory 1 atm temperature range 600 2000 k. global rate constant decomposition cyclic alkane molecular product calculate comparison calculated experimental result allow validate theoretical approach important result rotational barrier conformer usually neglect importance decomposition rate large biradical ring strain energy rse transition state ring opening estimate main rse contain cyclic reactant remove activation process
phys,Long-range correlation and multifractality in Bach's Inventions pitches,"  We show that it can be considered some of Bach pitches series as a stochastic
process with scaling behavior. Using multifractal deterend fluctuation analysis
(MF-DFA) method, frequency series of Bach pitches have been analyzed. In this
view we find same second moment exponents (after double profiling) in ranges
(1.7-1.8) in his works. Comparing MF-DFA results of original series to those
for shuffled and surrogate series we can distinguish multifractality due to
long-range correlations and a broad probability density function. Finally we
determine the scaling exponents and singularity spectrum. We conclude fat tail
has more effect in its multifractality nature than long-range correlations.
",consider bach pitch series stochastic process scale behavior multifractal deterend fluctuation analysis mf dfa method frequency series bach pitch analyze view find second moment exponent double profiling range 1.7 1.8 work compare mf dfa result original series shuffled surrogate series distinguish multifractality long range correlation broad probability density function finally determine scale exponent singularity spectrum conclude fat tail effect multifractality nature long range correlation
phys,"Molecular Synchronization Waves in Arrays of Allosterically Regulated
  Enzymes","  Spatiotemporal pattern formation in a product-activated enzymic reaction at
high enzyme concentrations is investigated. Stochastic simulations show that
catalytic turnover cycles of individual enzymes can become coherent and that
complex wave patterns of molecular synchronization can develop. The analysis
based on the mean-field approximation indicates that the observed patterns
result from the presence of Hopf and wave bifurcations in the considered
system.
",spatiotemporal pattern formation product activate enzymic reaction high enzyme concentration investigate stochastic simulation catalytic turnover cycle individual enzyme coherent complex wave pattern molecular synchronization develop analysis base mean field approximation indicate observed pattern result presence hopf wave bifurcation consider system
phys,Experimental modeling of physical laws,"  A physical law is represented by the probability distribution of a measured
variable. The probability density is described by measured data using an
estimator whose kernel is the instrument scattering function. The experimental
information and data redundancy are defined in terms of information entropy.
The model cost function, comprised of data redundancy and estimation error, is
minimized by the creation-annihilation process.
",physical law represent probability distribution measured variable probability density describe measure datum estimator kernel instrument scattering function experimental information datum redundancy define term information entropy model cost function comprise datum redundancy estimation error minimize creation annihilation process
phys,Modeling the field of laser welding melt pool by RBFNN,"  Efficient control of a laser welding process requires the reliable prediction
of process behavior. A statistical method of field modeling, based on
normalized RBFNN, can be successfully used to predict the spatiotemporal
dynamics of surface optical activity in the laser welding process. In this
article we demonstrate how to optimize RBFNN to maximize prediction quality.
Special attention is paid to the structure of sample vectors, which represent
the bridge between the field distributions in the past and future.
",efficient control laser welding process require reliable prediction process behavior statistical method field modeling base normalize rbfnn successfully predict spatiotemporal dynamic surface optical activity laser welding process article demonstrate optimize rbfnn maximize prediction quality special attention pay structure sample vector represent bridge field distribution past future
phys,Collective behavior of stock price movements in an emerging market,"  To investigate the universality of the structure of interactions in different
markets, we analyze the cross-correlation matrix C of stock price fluctuations
in the National Stock Exchange (NSE) of India. We find that this emerging
market exhibits strong correlations in the movement of stock prices compared to
developed markets, such as the New York Stock Exchange (NYSE). This is shown to
be due to the dominant influence of a common market mode on the stock prices.
By comparison, interactions between related stocks, e.g., those belonging to
the same business sector, are much weaker. This lack of distinct sector
identity in emerging markets is explicitly shown by reconstructing the network
of mutually interacting stocks. Spectral analysis of C for NSE reveals that,
the few largest eigenvalues deviate from the bulk of the spectrum predicted by
random matrix theory, but they are far fewer in number compared to, e.g., NYSE.
We show this to be due to the relative weakness of intra-sector interactions
between stocks, compared to the market mode, by modeling stock price dynamics
with a two-factor model. Our results suggest that the emergence of an internal
structure comprising multiple groups of strongly coupled components is a
signature of market development.
",investigate universality structure interaction different market analyze cross correlation matrix c stock price fluctuation national stock exchange nse india find emerge market exhibit strong correlation movement stock price compare developed market new york stock exchange nyse show dominant influence common market mode stock price comparison interaction related stock e.g. belong business sector weak lack distinct sector identity emerge market explicitly show reconstruct network mutually interact stock spectral analysis c nse reveal large eigenvalue deviate bulk spectrum predict random matrix theory far few number compare e.g. nyse relative weakness intra sector interaction stock compare market mode model stock price dynamic factor model result suggest emergence internal structure comprise multiple group strongly couple component signature market development
phys,The discrete dipole approximation: an overview and recent developments,"  We present a review of the discrete dipole approximation (DDA), which is a
general method to simulate light scattering by arbitrarily shaped particles. We
put the method in historical context and discuss recent developments, taking
the viewpoint of a general framework based on the integral equations for the
electric field. We review both the theory of the DDA and its numerical aspects,
the latter being of critical importance for any practical application of the
method. Finally, the position of the DDA among other methods of light
scattering simulation is shown and possible future developments are discussed.
",present review discrete dipole approximation dda general method simulate light scattering arbitrarily shape particle method historical context discuss recent development take viewpoint general framework base integral equation electric field review theory dda numerical aspect critical importance practical application method finally position dda method light scatter simulation show possible future development discuss
phys,On the dragging of light by a rotating medium,"  When light is passing through a rotating medium the optical polarisation is
rotated. Recently it has been reasoned that this rotation applies also to the
transmitted image (Padgett et al. 2006). We examine these two phenomena by
extending an analysis of Player (1976) to general electromagnetic fields. We
find that in this more general case the wave equation inside the rotating
medium has to be amended by a term which is connected to the orbital angular
momentum of the light. We show that optical spin and orbital angular momentum
account respectively for the rotation of the polarisation and the rotation of
the transmitted image.
",light pass rotate medium optical polarisation rotate recently reason rotation apply transmit image padgett et al 2006 examine phenomenon extend analysis player 1976 general electromagnetic field find general case wave equation inside rotating medium amend term connect orbital angular momentum light optical spin orbital angular momentum account respectively rotation polarisation rotation transmit image
phys,Intelligent Life in Cosmology,"  I shall present three arguments for the proposition that intelligent life is
very rare in the universe. First, I shall summarize the consensus opinion of
the founders of the Modern Synthesis (Simpson, Dobzhanski, and Mayr) that the
evolution of intelligent life is exceedingly improbable. Second, I shall
develop the Fermi Paradox: if they existed they'd be here. Third, I shall show
that if intelligent life were too common, it would use up all available
resources and die out. But I shall show that the quantum mechanical principle
of unitarity (actually a form of teleology!) requires intelligent life to
survive to the end of time. Finally, I shall argue that, if the universe is
indeed accelerating, then survival to the end of time requires that intelligent
life, though rare, to have evolved several times in the visible universe. I
shall argue that the acceleration is a consequence of the excess of matter over
antimatter in the universe. I shall suggest experiments to test these claims.
",shall present argument proposition intelligent life rare universe shall summarize consensus opinion founder modern synthesis simpson dobzhanski mayr evolution intelligent life exceedingly improbable second shall develop fermi paradox exist shall intelligent life common use available resource die shall quantum mechanical principle unitarity actually form teleology require intelligent life survive end time finally shall argue universe accelerate survival end time require intelligent life rare evolve time visible universe shall argue acceleration consequence excess matter antimatter universe shall suggest experiment test claim
phys,"On thermal effects in solid state lasers: the case of ytterbium-doped
  materials","  A review of theoretical and experimental studies of thermal effects in
solid-state lasers is presented, with a special focus on diode-pumped
ytterbium-doped materials. A large part of this review provides however general
information applicable to any kind of solid-state laser. Our aim here is not to
make a list of the techniques that have been used to minimize thermal effects,
but instead to give an overview of the theoretical aspects underneath, and give
a state-of-the-art of the tools at the disposal of the laser scientist to
measure thermal effects. After a presentation of some general properties of
Yb-doped materials, we address the issue of evaluating the temperature map in
Yb-doped laser crystals, both theoretically and experimentally. This is the
first step before studying the complex problem of thermal lensing (part III).
We will focus on some newly discussed aspects, like the definition of the
thermo-optic coefficient: we will highlight some misleading interpretations of
thermal lensing experiments due to the use of the dn/dT parameter in a context
where it is not relevant. Part IV will be devoted to a state-of-the-art of
experimental techniques used to measure thermal lensing. Eventually, in part V,
we will give some concrete examples in Yb-doped materials, where their
peculiarities will be pointed out.
",review theoretical experimental study thermal effect solid state laser present special focus diode pump ytterbium dope material large review provide general information applicable kind solid state laser aim list technique minimize thermal effect instead overview theoretical aspect underneath state art tool disposal laser scientist measure thermal effect presentation general property yb dope material address issue evaluate temperature map yb dope laser crystal theoretically experimentally step study complex problem thermal lense iii focus newly discuss aspect like definition thermo optic coefficient highlight misleading interpretation thermal lense experiment use dn dt parameter context relevant iv devoted state art experimental technique measure thermal lense eventually v concrete example yb dope material peculiarity point
phys,"Formation of density singularities in ideal hydrodynamics of freely
  cooling inelastic gases: a family of exact solutions","  We employ granular hydrodynamics to investigate a paradigmatic problem of
clustering of particles in a freely cooling dilute granular gas. We consider
large-scale hydrodynamic motions where the viscosity and heat conduction can be
neglected, and one arrives at the equations of ideal gas dynamics with an
additional term describing bulk energy losses due to inelastic collisions. We
employ Lagrangian coordinates and derive a broad family of exact non-stationary
analytical solutions that depend only on one spatial coordinate. These
solutions exhibit a new type of singularity, where the gas density blows up in
a finite time when starting from smooth initial conditions. The density blowups
signal formation of close-packed clusters of particles. As the density blow-up
time $t_c$ is approached, the maximum density exhibits a power law $\sim
(t_c-t)^{-2}$. The velocity gradient blows up as $\sim - (t_c-t)^{-1}$ while
the velocity itself remains continuous and develops a cusp (rather than a shock
discontinuity) at the singularity. The gas temperature vanishes at the
singularity, and the singularity follows the isobaric scenario: the gas
pressure remains finite and approximately uniform in space and constant in time
close to the singularity. An additional exact solution shows that the density
blowup, of the same type, may coexist with an ""ordinary"" shock, at which the
hydrodynamic fields are discontinuous but finite. We confirm stability of the
exact solutions with respect to small one-dimensional perturbations by solving
the ideal hydrodynamic equations numerically. Furthermore, numerical solutions
show that the local features of the density blowup hold universally,
independently of details of the initial and boundary conditions.
",employ granular hydrodynamic investigate paradigmatic problem clustering particle freely cool dilute granular gas consider large scale hydrodynamic motion viscosity heat conduction neglect arrive equation ideal gas dynamic additional term describe bulk energy loss inelastic collision employ lagrangian coordinate derive broad family exact non stationary analytical solution depend spatial coordinate solution exhibit new type singularity gas density blow finite time start smooth initial condition density blowup signal formation close pack cluster particle density blow time $ t_c$ approach maximum density exhibit power law $ \sim t_c t)^{-2}$. velocity gradient blow $ \sim t_c t)^{-1}$ velocity remain continuous develop cusp shock discontinuity singularity gas temperature vanish singularity singularity follow isobaric scenario gas pressure remain finite approximately uniform space constant time close singularity additional exact solution show density blowup type coexist ordinary shock hydrodynamic field discontinuous finite confirm stability exact solution respect small dimensional perturbation solve ideal hydrodynamic equation numerically furthermore numerical solution local feature density blowup hold universally independently detail initial boundary condition
phys,"Fluctuation-dissipation relation on a Melde string in a turbulent flow,
  considerations on a ""dynamical temperature""","  We report on measurements of the transverse fluctuations of a string in a
turbulent air jet flow. Harmonic modes are excited by the fluctuating drag
force, at different wave-numbers. This simple mechanical probe makes it
possible to measure excitations of the flow at specific scales, averaged over
space and time: it is a scale-resolved, global measurement. We also measure the
dissipation associated to the string motion, and we consider the ratio of the
fluctuations over dissipation (FDR). In an exploratory approach, we investigate
the concept of {\it effective temperature} defined through the FDR. We compare
our observations with other definitions of temperature in turbulence. From the
theory of Kolmogorov (1941), we derive the exponent -11/3 expected for the
spectrum of the fluctuations. This simple model and our experimental results
are in good agreement, over the range of wave-numbers, and Reynolds number
accessible ($74000 \leq Re \leq 170000$).
",report measurement transverse fluctuation string turbulent air jet flow harmonic mode excite fluctuate drag force different wave number simple mechanical probe make possible measure excitation flow specific scale average space time scale resolve global measurement measure dissipation associate string motion consider ratio fluctuation dissipation fdr exploratory approach investigate concept \it effective temperature define fdr compare observation definition temperature turbulence theory kolmogorov 1941 derive exponent -11/3 expect spectrum fluctuation simple model experimental result good agreement range wave number reynolds number accessible $ 74000 \leq \leq 170000 $
phys,"Experimental and theoretical study of light scattering by individual
  mature red blood cells by use of scanning flow cytometry and discrete dipole
  approximation","  Elastic light scattering by mature red blood cells (RBCs) was theoretically
and experimentally analyzed with the discrete dipole approximation (DDA) and
the scanning flow cytometry (SFC), respectively. SFC permits measurement of
angular dependence of light-scattering intensity (indicatrix) of single
particles. A mature RBC is modeled as a biconcave disk in DDA simulations of
light scattering. We have studied the effect of RBC orientation related to the
direction of the incident light upon the indicatrix. Numerical calculations of
indicatrices for several aspect ratios and volumes of RBC have been carried
out. Comparison of the simulated indicatrices and indicatrices measured by SFC
showed good agreement, validating the biconcave disk model for a mature RBC. We
simulated the light-scattering output signals from the SFC with the DDA for
RBCs modeled as a disk-sphere and as an oblate spheroid. The biconcave disk,
the disk-sphere, and the oblate spheroid models have been compared for two
orientations, i.e. face-on and rim-on incidence. Only the oblate spheroid model
for rim-on incidence gives results similar to the rigorous biconcave disk
model.
",elastic light scatter mature red blood cell rbcs theoretically experimentally analyze discrete dipole approximation dda scanning flow cytometry sfc respectively sfc permit measurement angular dependence light scatter intensity indicatrix single particle mature rbc model biconcave disk dda simulation light scattering study effect rbc orientation relate direction incident light indicatrix numerical calculation indicatrice aspect ratio volume rbc carry comparison simulate indicatrice indicatrice measure sfc show good agreement validate biconcave disk model mature rbc simulate light scatter output signal sfc dda rbcs model disk sphere oblate spheroid biconcave disk disk sphere oblate spheroid model compare orientation i.e. face rim incidence oblate spheroid model rim incidence give result similar rigorous biconcave disk model
phys,"Growing Networks: Limit in-degree distribution for arbitrary out-degree
  one","  We compute the stationary in-degree probability, $P_{in}(k)$, for a growing
network model with directed edges and arbitrary out-degree probability. In
particular, under preferential linking, we find that if the nodes have a light
tail (finite variance) out-degree distribution, then the corresponding
in-degree one behaves as $k^{-3}$. Moreover, for an out-degree distribution
with a scale invariant tail, $P_{out}(k)\sim k^{-\alpha}$, the corresponding
in-degree distribution has exactly the same asymptotic behavior only if
$2<\alpha<3$ (infinite variance). Similar results are obtained when
attractiveness is included. We also present some results on descriptive
statistics measures %descriptive statistics such as the correlation between the
number of in-going links, $D_{in}$, and outgoing links, $D_{out}$, and the
conditional expectation of $D_{in}$ given $D_{out}$, and we calculate these
measures for the WWW network. Finally, we present an application to the
scientific publications network. The results presented here can explain the
tail behavior of in/out-degree distribution observed in many real networks.
",compute stationary degree probability $ p_{in}(k)$ grow network model direct edge arbitrary degree probability particular preferential linking find node light tail finite variance degree distribution corresponding degree behave $ k^{-3}$. degree distribution scale invariant tail $ p_{out}(k)\sim k^{-\alpha}$ corresponding degree distribution exactly asymptotic behavior $ 2<\alpha<3 $ infinite variance similar result obtain attractiveness include present result descriptive statistic measure descriptive statistic correlation number go link $ d_{in}$ outgoing link $ d_{out}$ conditional expectation $ d_{in}$ give $ d_{out}$ calculate measure www network finally present application scientific publication network result present explain tail behavior degree distribution observe real network
phys,"On axisymmetric MHD equilibria with incompressible flows under side
  conditions","  Axisymmetric equilibria with incompressible flows of arbitrary direction are
studied in the framework of magnetohydrodynamics under a variety of physically
relevant side conditions. To this end a set of pertinent non-linear ODEs are
transformed to quasilinear ones and the respective initial value problem is
solved numerically with appropriately determined initial values near the
magnetic axis. Several equilibria are then constructed surface by surface. The
non field aligned flow results in novel configurations with a single magnetic
axis, toroidal shell configurations in which the plasma is confined within a
couple of magnetic surfaces and double shell-like configurations. In addition,
the flow affects the elongation and triangularity of the magnetic surfaces.
",axisymmetric equilibrium incompressible flow arbitrary direction study framework magnetohydrodynamic variety physically relevant condition end set pertinent non linear ode transform quasilinear one respective initial value problem solve numerically appropriately determined initial value near magnetic axis equilibrium construct surface surface non field align flow result novel configuration single magnetic axis toroidal shell configuration plasma confine couple magnetic surface double shell like configuration addition flow affect elongation triangularity magnetic surface
phys,"Rounding of first-order phase transitions and optimal cooperation in
  scale-free networks","  We consider the ferromagnetic large-$q$ state Potts model in complex evolving
networks, which is equivalent to an optimal cooperation problem, in which the
agents try to optimize the total sum of pair cooperation benefits and the
supports of independent projects. The agents are found to be typically of two
kinds: a fraction of $m$ (being the magnetization of the Potts model) belongs
to a large cooperating cluster, whereas the others are isolated one man's
projects. It is shown rigorously that the homogeneous model has a strongly
first-order phase transition, which turns to second-order for random
interactions (benefits), the properties of which are studied numerically on the
Barab\'asi-Albert network. The distribution of finite-size transition points is
characterized by a shift exponent, $1/\tilde{\nu}'=.26(1)$, and by a different
width exponent, $1/\nu'=.18(1)$, whereas the magnetization at the transition
point scales with the size of the network, $N$, as: $m\sim N^{-x}$, with
$x=.66(1)$.
",consider ferromagnetic large-$q$ state potts model complex evolving network equivalent optimal cooperation problem agent try optimize total sum pair cooperation benefit support independent project agent find typically kind fraction $ m$ magnetization potts model belong large cooperating cluster isolate man project show rigorously homogeneous model strongly order phase transition turn second order random interaction benefit property study numerically barab\'asi albert network distribution finite size transition point characterize shift exponent $ 1/\tilde{\nu}'=.26(1)$ different width exponent $ 1/\nu'=.18(1)$ magnetization transition point scale size network $ n$ $ m\sim n^{-x}$ $ x=.66(1)$.
phys,"Query on Negative Temperature, Internal Interactions and Decrease of
  Entropy","  After negative temperature is restated, we find that it will derive
necessarily decrease of entropy. Negative temperature is based on the Kelvin
scale and the condition dU>0 and dS<0. Conversely, there is also negative
temperature for dU<0 and dS>0. But, negative temperature is contradiction with
usual meaning of temperature and with some basic concepts of physics and
mathematics. It is a question in nonequilibrium thermodynamics. We proposed a
possibility of decrease of entropy due to fluctuation magnified and internal
interactions in some isolated systems. From this we discuss some possible
examples and theories.
",negative temperature restate find derive necessarily decrease entropy negative temperature base kelvin scale condition du>0 ds<0 conversely negative temperature du<0 ds>0 negative temperature contradiction usual meaning temperature basic concept physics mathematic question nonequilibrium thermodynamic propose possibility decrease entropy fluctuation magnify internal interaction isolate system discuss possible example theory
math,Spectral perturbation bounds for selfadjoint operators,"  We give general spectral and eigenvalue perturbation bounds for a selfadjoint
operator perturbed in the sense of the pseudo-Friedrichs extension. We also
give several generalisations of the aforementioned extension. The spectral
bounds for finite eigenvalues are obtained by using analyticity and
monotonicity properties (rather than variational principles) and they are
general enough to include eigenvalues in gaps of the essential spectrum.
",general spectral eigenvalue perturbation bound selfadjoint operator perturb sense pseudo friedrichs extension generalisation aforementioned extension spectral bound finite eigenvalue obtain analyticity monotonicity property variational principle general include eigenvalue gap essential spectrum
math,"Direct Theorems in the Theory of Approximation of the Banach Space
  Vectors by Entire Vectors of Exponential Type","  For an arbitrary operator A on a Banach space X which is a generator of
C_0-group with certain growth condition at the infinity, the direct theorems on
connection between the smoothness degree of a vector $x\in X$ with respect to
the operator A, the order of convergence to zero of the best approximation of x
by exponential type entire vectors for the operator A, and the k-module of
continuity are given. Obtained results allows to acquire Jackson-type
inequalities in many classic spaces of periodic functions and weighted $L_p$
spaces.
",arbitrary operator banach space x generator c_0 group certain growth condition infinity direct theorem connection smoothness degree vector $ x\in x$ respect operator order convergence zero good approximation x exponential type entire vector operator k module continuity give obtain result allow acquire jackson type inequality classic space periodic function weight $ l_p$ space
math,Generic character sheaves on disconnected groups and character values,"  We relate a generic character sheaf on a disconnected reductive group with a
character of a representation of the rational points of the group over a finite
field extending a result known in the connected case.
",relate generic character sheaf disconnected reductive group character representation rational point group finite field extend result know connected case
math,"Placeholder Substructures III: A Bit-String-Driven ''Recipe Theory'' for
  Infinite-Dimensional Zero-Divisor Spaces","  Zero-divisors (ZDs) derived by Cayley-Dickson Process (CDP) from
N-dimensional hypercomplex numbers (N a power of 2, at least 4) can represent
singularities and, as N approaches infinite, fractals -- and thereby,scale-free
networks. Any integer greater than 8 and not a power of 2 generates a
meta-fractal or ""Sky"" when it is interpreted as the ""strut constant"" (S) of an
ensemble of octahedral vertex figures called ""Box-Kites"" (the fundamental
building blocks of ZDs). Remarkably simple bit-manipulation rules or ""recipes""
provide tools for transforming one fractal genus into others within the context
of Wolfram's Class 4 complexity.
",zero divisor zds derive cayley dickson process cdp n dimensional hypercomplex number n power 2 4 represent singularity n approach infinite fractal scale free network integer great 8 power 2 generate meta fractal sky interpret strut constant s ensemble octahedral vertex figure call box kites fundamental building block zds remarkably simple bit manipulation rule recipe provide tool transform fractal genus context wolfram class 4 complexity
math,The local structure of conformally symmetric manifolds,"  This is a final step in a local classification of pseudo-Riemannian manifolds
with parallel Weyl tensor that are not conformally flat or locally symmetric.
",final step local classification pseudo riemannian manifold parallel weyl tensor conformally flat locally symmetric
math,K_0-theory of n-potents in rings and algebras,"  Let $n \geq 2$ be an integer. An \emph{$n$-potent} is an element $e$ of a
ring $R$ such that $e^n = e$. In this paper, we study $n$-potents in matrices
over $R$ and use them to construct an abelian group $K_0^n(R)$. If $A$ is a
complex algebra, there is a group isomorphism $K_0^n(A) \cong
\bigl(K_0(A)\bigr)^{n-1}$ for all $n \geq 2$. However, for algebras over
cyclotomic fields, this is not true in general. We consider $K_0^n$ as a
covariant functor, and show that it is also functorial for a generalization of
homomorphism called an \emph{$n$-homomorphism}.
",let $ n \geq 2 $ integer \emph{$n$-potent element $ e$ ring $ r$ $ e^n = e$. paper study $ n$-potent matrix $ r$ use construct abelian group $ k_0^n(r)$. $ a$ complex algebra group isomorphism $ k_0^n(a \cong \bigl(k_0(a)\bigr)^{n-1}$ $ n \geq 2$. algebra cyclotomic field true general consider $ k_0^n$ covariant functor functorial generalization homomorphism call \emph{$n$-homomorphism
math,"Renewals for exponentially increasing lifetimes, with an application to
  digital search trees","  We show that the number of renewals up to time $t$ exhibits distributional
fluctuations as $t\to\infty$ if the underlying lifetimes increase at an
exponential rate in a distributional sense. This provides a probabilistic
explanation for the asymptotics of insertion depth in random trees generated by
a bit-comparison strategy from uniform input; we also obtain a representation
for the resulting family of limit laws along subsequences. Our approach can
also be used to obtain rates of convergence.
",number renewal time $ t$ exhibit distributional fluctuation $ t\to\infty$ underlie lifetime increase exponential rate distributional sense provide probabilistic explanation asymptotic insertion depth random tree generate bit comparison strategy uniform input obtain representation result family limit law subsequence approach obtain rate convergence
math,Mathematics of thermoacoustic tomography,"  The paper presents a survey of mathematical problems, techniques, and
challenges arising in the Thermoacoustic and Photoacoustic Tomography.
",paper present survey mathematical problem technique challenge arise thermoacoustic photoacoustic tomography
math,Fractional WKB Approximation,"  Wentzel, Kramers, Brillouin (WKB) approximation for fractional systems is
investigated in this paper using the fractional calculus. In the fractional
case the wave function is constructed such that the phase factor is the same as
the Hamilton's principle function ""S"". To demonstrate our proposed approach two
examples are investigated in details.
",wentzel kramers brillouin wkb approximation fractional system investigate paper fractional calculus fractional case wave function construct phase factor hamilton principle function s demonstrate propose approach example investigate detail
math,Complete Shrinking Ricci Solitons have Finite Fundamental Group,"  We show that if a complete Riemannian manifold supports a vector field such
that the Ricci tensor plus the Lie derivative of the metric with respect to the
vector field has a positive lower bound, then the fundamental group is finite.
In particular, it follows that complete shrinking Ricci solitons and complete
smooth metric measure spaces with a positive lower bound on the Bakry-Emery
tensor have finite fundamental group. The method of proof is to generalize
arguments of Garcia-Rio and Fernandez-Lopez in the compact case.
",complete riemannian manifold support vector field ricci tensor plus lie derivative metric respect vector field positive lower bind fundamental group finite particular follow complete shrink ricci soliton complete smooth metric measure space positive lower bind bakry emery tensor finite fundamental group method proof generalize argument garcia rio fernandez lopez compact case
math,Algebraic geometry of Gaussian Bayesian networks,"  Conditional independence models in the Gaussian case are algebraic varieties
in the cone of positive definite covariance matrices. We study these varieties
in the case of Bayesian networks, with a view towards generalizing the
recursive factorization theorem to situations with hidden variables. In the
case when the underlying graph is a tree, we show that the vanishing ideal of
the model is generated by the conditional independence statements implied by
graph. We also show that the ideal of any Bayesian network is homogeneous with
respect to a multigrading induced by a collection of upstream random variables.
This has a number of important consequences for hidden variable models.
Finally, we relate the ideals of Bayesian networks to a number of classical
constructions in algebraic geometry including toric degenerations of the
Grassmannian, matrix Schubert varieties, and secant varieties.
",conditional independence model gaussian case algebraic variety cone positive definite covariance matrix study variety case bayesian network view generalize recursive factorization theorem situation hidden variable case underlying graph tree vanish ideal model generate conditional independence statement imply graph ideal bayesian network homogeneous respect multigrading induce collection upstream random variable number important consequence hide variable model finally relate ideal bayesian network number classical construction algebraic geometry include toric degeneration grassmannian matrix schubert variety secant variety
math,"Local well-posedness of nonlinear dispersive equations on modulation
  spaces","  By using tools of time-frequency analysis, we obtain some improved local
well-posedness results for the NLS, NLW and NLKG equations with Cauchy data in
modulation spaces $M{p, 1}_{0,s}$.
","tool time frequency analysis obtain improved local posedness result nls nlw nlkg equation cauchy datum modulation space $ m{p 1}_{0,s}$."
math,"Nonimmersions of RP^n implied by tmf, revisited","  In a 2002 paper, the authors and Bruner used the new spectrum tmf to obtain
some new nonimmersions of real projective spaces. In this note, we
complete/correct two oversights in that paper.
  The first is to note that in that paper a general nonimmersion result was
stated which yielded new nonimmersions for RP^n with n as small as 48, and yet
it was stated there that the first new result occurred when n=1536. Here we
give a simple proof of those overlooked results.
  Secondly, we fill in a gap in the proof of the 2002 paper. There it was
claimed that an axial map f must satisfy f^*(X)=X_1+X_2. We realized recently
that this is not clear. However, here we show that it is true up multiplication
by a unit in the appropriate ring, and so we retrieve all the nonimmersion
results claimed in the original paper.
  Finally, we present a complete determination of tmf^{8*}(RP^\infty\times
RP^\infty) and tmf^*(CP^\infty\times CP^\infty) in positive dimensions.
",2002 paper author bruner new spectrum tmf obtain new nonimmersion real projective space note complete correct oversight paper note paper general nonimmersion result state yield new nonimmersion rp^n n small 48 state new result occur n=1536 simple proof overlook result secondly fill gap proof 2002 paper claim axial map f satisfy f^*(x)=x_1+x_2 realize recently clear true multiplication unit appropriate ring retrieve nonimmersion result claim original paper finally present complete determination tmf^{8*}(rp^\infty\time rp^\infty tmf^*(cp^\infty\times cp^\infty positive dimension
math,Hamilton-Jacobi Fractional Sequential Mechanics,"  As a continuation of Rabei et al. work [11], the Hamilton- Jacobi partial
differential equation is generalized to be applicable for systems containing
fractional derivatives. The Hamilton- Jacobi function in configuration space is
obtained in a similar manner to the usual mechanics. Two problems are
considered to demonstrate the application of the formalism. The result found to
be in exact agreement with Agrawal's formalism.
",continuation rabei et al work 11 hamilton- jacobi partial differential equation generalize applicable system contain fractional derivative hamilton- jacobi function configuration space obtain similar manner usual mechanic problem consider demonstrate application formalism result find exact agreement agrawal formalism
math,"An equilibrium problem for the limiting eigenvalue distribution of
  banded Toeplitz matrices","  We study the limiting eigenvalue distribution of $n\times n$ banded Toeplitz
matrices as $n\to \infty$. From classical results of Schmidt-Spitzer and
Hirschman it is known that the eigenvalues accumulate on a special curve in the
complex plane and the normalized eigenvalue counting measure converges weakly
to a measure on this curve as $n\to\infty$. In this paper, we characterize the
limiting measure in terms of an equilibrium problem. The limiting measure is
one component of the unique vector of measures that minimes an energy
functional defined on admissible vectors of measures. In addition, we show that
each of the other components is the limiting measure of the normalized counting
measure on certain generalized eigenvalues.
",study limit eigenvalue distribution $ n\time n$ band toeplitz matrix $ n\to \infty$. classical result schmidt spitzer hirschman know eigenvalue accumulate special curve complex plane normalize eigenvalue counting measure converge weakly measure curve $ n\to\infty$. paper characterize limit measure term equilibrium problem limit measure component unique vector measure minime energy functional define admissible vector measure addition component limit measure normalize counting measure certain generalized eigenvalue
math,"Computing genus 2 Hilbert-Siegel modular forms over $\Q(\sqrt{5})$ via
  the Jacquet-Langlands correspondence","  In this paper we present an algorithm for computing Hecke eigensystems of
Hilbert-Siegel cusp forms over real quadratic fields of narrow class number
one. We give some illustrative examples using the quadratic field
$\Q(\sqrt{5})$. In those examples, we identify Hilbert-Siegel eigenforms that
are possible lifts from Hilbert eigenforms.
",paper present algorithm compute hecke eigensystem hilbert siegel cusp form real quadratic field narrow class number illustrative example quadratic field $ \q(\sqrt{5})$. example identify hilbert siegel eigenform possible lift hilbert eigenform
math,Stable algebras of entire functions,"  Suppose that $h$ and $g$ belong to the algebra $\B$ generated by the rational
functions and an entire function $f$ of finite order on ${\Bbb C}^n$ and that
$h/g$ has algebraic polar variety. We show that either $h/g\in\B$ or
$f=q_1e^p+q_2$, where $p$ is a polynomial and $q_1,q_2$ are rational functions.
In the latter case, $h/g$ belongs to the algebra generated by the rational
functions, $e^p$ and $e^{-p}$.
","suppose $ h$ $ g$ belong algebra $ \b$ generate rational function entire function $ f$ finite order $ \bbb c}^n$ $ h g$ algebraic polar variety $ h g\in\b$ $ f = q_1e^p+q_2 $ $ p$ polynomial $ q_1,q_2 $ rational function case $ h g$ belong algebra generate rational function $ e^p$ $ e^{-p}$."
math,Average optimality for risk-sensitive control with general state space,"  This paper deals with discrete-time Markov control processes on a general
state space. A long-run risk-sensitive average cost criterion is used as a
performance measure. The one-step cost function is nonnegative and possibly
unbounded. Using the vanishing discount factor approach, the optimality
inequality and an optimal stationary strategy for the decision maker are
established.
",paper deal discrete time markov control process general state space long run risk sensitive average cost criterion performance measure step cost function nonnegative possibly unbounded vanish discount factor approach optimality inequality optimal stationary strategy decision maker establish
math,Quivers with potentials and their representations I: Mutations,"  We study quivers with relations given by non-commutative analogs of Jacobian
ideals in the complete path algebra. This framework allows us to give a
representation-theoretic interpretation of quiver mutations at arbitrary
vertices. This gives a far-reaching generalization of
Bernstein-Gelfand-Ponomarev reflection functors. The motivations for this work
come from several sources: superpotentials in physics, Calabi-Yau algebras,
cluster algebras.
",study quiver relation give non commutative analog jacobian ideal complete path algebra framework allow representation theoretic interpretation quiver mutation arbitrary vertex give far reach generalization bernstein gelfand ponomarev reflection functor motivation work come source superpotential physic calabi yau algebra cluster algebra
math,Hyperbolicity in unbounded convex domains,"  We provide several equivalent characterizations of Kobayashi hyperbolicity in
unbounded convex domains in terms of peak and anti-peak functions at infinity,
affine lines, Bergman metric and iteration theory.
",provide equivalent characterization kobayashi hyperbolicity unbounded convex domain term peak anti peak function infinity affine line bergman metric iteration theory
math,"On the Nonexistence of Nontrivial Involutive n-Homomorphisms of
  C*-algebras","  An n-homomorphism between algebras is a linear map $\phi : A \to B$ such that
$\phi(a_1 ... a_n) = \phi(a_1)... \phi(a_n)$ for all elements $a_1, >..., a_n
\in A.$ Every homomorphism is an n-homomorphism, for all n >= 2, but the
converse is false, in general. Hejazian et al. [7] ask: Is every *-preserving
n-homomorphism between C*-algebras continuous? We answer their question in the
affirmative, but the even and odd n arguments are surprisingly disjoint. We
then use these results to prove stronger ones: If n >2 is even, then $\phi$ is
just an ordinary *-homomorphism. If n >= 3 is odd, then $\phi$ is a difference
of two orthogonal *-homomorphisms. Thus, there are no nontrivial *-linear
n-homomorphisms between C*-algebras.
",n homomorphism algebras linear map $ \phi \to b$ $ \phi(a_1 a_n = \phi(a_1 \phi(a_n)$ element $ a_1 > a_n \in a.$ homomorphism n homomorphism n > = 2 converse false general hejazian et al 7 ask -preserve n homomorphism c*-algebras continuous answer question affirmative odd n argument surprisingly disjoint use result prove strong one n > 2 $ \phi$ ordinary -homomorphism n > = 3 odd $ \phi$ difference orthogonal -homomorphism nontrivial -linear n homomorphism c*-algebras
math,"The classification of surfaces with p_g=q=1 isogenous to a product of
  curves","  A projective surface S is said to be isogenous to a product if there exist
two smooth curves C, F and a finite group G acting freely on C \times F so that
S=(C \times F)/G. In this paper we classify all surfaces with p_g=q=1 which are
isogenous to a product.
",projective surface s say isogenous product exist smooth curve c f finite group g act freely c \times f s=(c \times f)/g. paper classify surface p_g = q=1 isogenous product
math,Duality and Tameness,"  We prove a duality theorem for certain graded algebras and show by various
examples different kinds of failure of tameness of local cohomology.
",prove duality theorem certain grade algebra example different kind failure tameness local cohomology
math,M-regularity of the Fano surface,"  Let $(A,\Theta)$ be a principally polarised abelian variety, and let Y be a
subvariety. Pareschi and Popa conjectured that Y has minimal cohomology class
if and only if the structure sheaf of Y satisfies a property that they call
M-regularity.
  Let now X be a smooth cubic threefold. By a classical result due to Clemens
and Griffiths, its intermediate Jacobian J(X) is a principally polarised
abelian variety; furthermore the Fano surface of lines on X can be embedded in
J(X) and has minimal cohomology class. In this short note we show that its
structure sheaf is M-regular.
","let $ a,\theta)$ principally polarise abelian variety let y subvariety pareschi popa conjecture y minimal cohomology class structure sheaf y satisfy property m regularity let x smooth cubic threefold classical result clemens griffiths intermediate jacobian j(x principally polarise abelian variety furthermore fano surface line x embed j(x minimal cohomology class short note structure sheaf m regular"
math,Invariance and the twisted Chern character : a case study,"  We give details of the proof of the remark made in \cite{G2} that the Chern
characters of the canonical generators on the K homology of the quantum group
$SU_q(2)$ are not invariant under the natural $SU_q(2)$ coaction. Furthermore,
the conjecture made in \cite{G2} about the nontriviality of the twisted Chern
character coming from an odd equivariant spectral triple on $SU_q(2)$ is
settled in the affirmative.
",detail proof remark \cite{g2 chern character canonical generator k homology quantum group $ su_q(2)$ invariant natural $ su_q(2)$ coaction furthermore conjecture \cite{g2 nontriviality twisted chern character come odd equivariant spectral triple $ su_q(2)$ settle affirmative
math,Stability of a finite volume scheme for the incompressible fluids,"  We introduce a finite volume scheme for the two-dimensional incompressible
Navier-Stokes equations. We use a triangular mesh. The unknowns for the
velocity and pressure are respectively piecewise constant and affine. We use a
projection method to deal with the incompressibility constraint. We show that
the differential operators in the Navier-Stokes equations and their discrete
counterparts share similar properties. In particular we state an inf-sup
(Babuska-Brezzi) condition. Using these properties we infer the stability of
the scheme.
",introduce finite volume scheme dimensional incompressible navier stokes equation use triangular mesh unknown velocity pressure respectively piecewise constant affine use projection method deal incompressibility constraint differential operator navier stokes equation discrete counterpart share similar property particular state inf sup babuska brezzi condition property infer stability scheme
math,Energy conservation and Onsager's conjecture for the Euler equations,"  Onsager conjectured that weak solutions of the Euler equations for
incompressible fluids in 3D conserve energy only if they have a certain minimal
smoothness, (of order of 1/3 fractional derivatives) and that they dissipate
energy if they are rougher. In this paper we prove that energy is conserved for
velocities in the function space $B^{1/3}_{3,c(\NN)}$. We show that this space
is sharp in a natural sense. We phrase the energy spectrum in terms of the
Littlewood-Paley decomposition and show that the energy flux is controlled by
local interactions. This locality is shown to hold also for the helicity flux;
moreover, every weak solution of the Euler equations that belongs to
$B^{2/3}_{3,c(\NN)}$ conserves helicity. In contrast, in two dimensions, the
strong locality of the enstrophy holds only in the ultraviolet range.
","onsager conjecture weak solution euler equation incompressible fluid 3d conserve energy certain minimal smoothness order 1/3 fractional derivative dissipate energy rougher paper prove energy conserve velocity function space $ b^{1/3}_{3,c(\nn)}$. space sharp natural sense phrase energy spectrum term littlewood paley decomposition energy flux control local interaction locality show hold helicity flux weak solution euler equation belong $ b^{2/3}_{3,c(\nn)}$ conserve helicity contrast dimension strong locality enstrophy hold ultraviolet range"
math,"Penalization approach for mixed hyperbolic systems with constant
  coefficients satisfying a Uniform Lopatinski Condition","  In this paper, we describe a new, systematic and explicit way of
approximating solutions of mixed hyperbolic systems with constant coefficients
satisfying a Uniform Lopatinski Condition via different Penalization
approaches.
",paper describe new systematic explicit way approximating solution mixed hyperbolic system constant coefficient satisfy uniform lopatinski condition different penalization approach
math,"Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian
  Vector Broadcast Channels","  MIMO technology is one of the most significant advances in the past decade to
increase channel capacity and has a great potential to improve network capacity
for mesh networks. In a MIMO-based mesh network, the links outgoing from each
node sharing the common communication spectrum can be modeled as a Gaussian
vector broadcast channel. Recently, researchers showed that ``dirty paper
coding'' (DPC) is the optimal transmission strategy for Gaussian vector
broadcast channels. So far, there has been little study on how this fundamental
result will impact the cross-layer design for MIMO-based mesh networks. To fill
this gap, we consider the problem of jointly optimizing DPC power allocation in
the link layer at each node and multihop/multipath routing in a MIMO-based mesh
networks. It turns out that this optimization problem is a very challenging
non-convex problem. To address this difficulty, we transform the original
problem to an equivalent problem by exploiting the channel duality. For the
transformed problem, we develop an efficient solution procedure that integrates
Lagrangian dual decomposition method, conjugate gradient projection method
based on matrix differential calculus, cutting-plane method, and subgradient
method. In our numerical example, it is shown that we can achieve a network
performance gain of 34.4% by using DPC.
",mimo technology significant advance past decade increase channel capacity great potential improve network capacity mesh network mimo base mesh network link outgo node share common communication spectrum model gaussian vector broadcast channel recently researcher show ` ` dirty paper coding dpc optimal transmission strategy gaussian vector broadcast channel far little study fundamental result impact cross layer design mimo base mesh network fill gap consider problem jointly optimize dpc power allocation link layer node multihop multipath routing mimo base mesh network turn optimization problem challenging non convex problem address difficulty transform original problem equivalent problem exploit channel duality transform problem develop efficient solution procedure integrate lagrangian dual decomposition method conjugate gradient projection method base matrix differential calculus cutting plane method subgradient method numerical example show achieve network performance gain 34.4 dpc
math,"Transfinite diameter, Chebyshev constant and energy on locally compact
  spaces","  We study the relationship between transfinite diameter, Chebyshev constant
and Wiener energy in the abstract linear potential analytic setting pioneered
by Choquet, Fuglede and Ohtsuka. It turns out that, whenever the potential
theoretic kernel has the maximum principle, then all these quantities are equal
for all compact sets. For continuous kernels even the converse statement is
true: if the Chebyshev constant of any compact set coincides with its
transfinite diameter, the kernel must satisfy the maximum principle. An
abundance of examples is provided to show the sharpness of the results.
",study relationship transfinite diameter chebyshev constant wiener energy abstract linear potential analytic setting pioneer choquet fuglede ohtsuka turn potential theoretic kernel maximum principle quantity equal compact set continuous kernel converse statement true chebyshev constant compact set coincide transfinite diameter kernel satisfy maximum principle abundance example provide sharpness result
math,"Contrasting Two Transformation-Based Methods for Obtaining Absolute
  Extrema","  In this note we contrast two transformation-based methods to deduce absolute
extrema and the corresponding extremizers. Unlike variation-based methods, the
transformation-based ones of Carlson and Leitmann and the recent one of Silva
and Torres are direct in that they permit obtaining solutions by inspection.
",note contrast transformation base method deduce absolute extrema corresponding extremizer unlike variation base method transformation base one carlson leitmann recent silva torres direct permit obtain solution inspection
math,"A Direct Method for Solving Optimal Switching Problems of
  One-Dimensional Diffusions","  In this paper, we propose a direct solution method for optimal switching
problems of one-dimensional diffusions. This method is free from conjectures
about the form of the value function and switching strategies, or does not
require the proof of optimality through quasi-variational inequalities. The
direct method uses a general theory of optimal stopping problems for
one-dimensional diffusions and characterizes the value function as sets of the
smallest linear majorants in their respective transformed spaces.
",paper propose direct solution method optimal switching problem dimensional diffusion method free conjecture form value function switch strategy require proof optimality quasi variational inequality direct method use general theory optimal stopping problem dimensional diffusion characterize value function set small linear majorant respective transform space
math,Electromagnetic wormholes via handlebody constructions,"  Cloaking devices are prescriptions of electrostatic, optical or
electromagnetic parameter fields (conductivity $\sigma(x)$, index of refraction
$n(x)$, or electric permittivity $\epsilon(x)$ and magnetic permeability
$\mu(x)$) which are piecewise smooth on $\mathbb R^3$ and singular on a
hypersurface $\Sigma$, and such that objects in the region enclosed by $\Sigma$
are not detectable to external observation by waves. Here, we give related
constructions of invisible tunnels, which allow electromagnetic waves to pass
between possibly distant points, but with only the ends of the tunnels visible
to electromagnetic imaging. Effectively, these change the topology of space
with respect to solutions of Maxwell's equations, corresponding to attaching a
handlebody to $\mathbb R^3$. The resulting devices thus function as
electromagnetic wormholes.
",cloaking device prescription electrostatic optical electromagnetic parameter field conductivity $ \sigma(x)$ index refraction $ n(x)$ electric permittivity $ \epsilon(x)$ magnetic permeability $ \mu(x)$ piecewise smooth $ \mathbb r^3 $ singular hypersurface $ \sigma$ object region enclose $ \sigma$ detectable external observation wave related construction invisible tunnel allow electromagnetic wave pass possibly distant point end tunnel visible electromagnetic imaging effectively change topology space respect solution maxwell equation correspond attach handlebody $ \mathbb r^3$. result device function electromagnetic wormhole
math,Conformal Field Theory and Operator Algebras,"  We review recent progress in operator algebraic approach to conformal quantum
field theory. Our emphasis is on use of representation theory in classification
theory. This is based on a series of joint works with R. Longo.
",review recent progress operator algebraic approach conformal quantum field theory emphasis use representation theory classification theory base series joint work r. longo
math,Intersection Bodies and Generalized Cosine Transforms,"  Intersection bodies represent a remarkable class of geometric objects
associated with sections of star bodies and invoking
  Radon transforms, generalized cosine transforms, and the relevant Fourier
analysis. The main focus of this article is interrelation between generalized
cosine transforms of different kinds in the context of their application to
investigation of a certain family of intersection bodies, which we call
$\lam$-intersection bodies. The latter include $k$-intersection bodies (in the
sense of A. Koldobsky) and unit balls of finite-dimensional subspaces of
$L_p$-spaces. In particular, we show that restrictions onto lower dimensional
subspaces of the spherical Radon transforms and the generalized cosine
transforms preserve their integral-geometric structure. We apply this result to
the study of sections of $\lam$-intersection bodies. New characterizations of
this class of bodies are obtained and examples are given. We also review some
known facts and give them new proofs.
",intersection body represent remarkable class geometric object associate section star body invoke radon transform generalize cosine transform relevant fourier analysis main focus article interrelation generalize cosine transform different kind context application investigation certain family intersection body $ \lam$-intersection body include $ k$-intersection body sense a. koldobsky unit ball finite dimensional subspace $ l_p$-space particular restriction low dimensional subspace spherical radon transform generalized cosine transform preserve integral geometric structure apply result study section $ \lam$-intersection body new characterization class body obtain example give review know fact new proof
math,"Hecke-Clifford algebras and spin Hecke algebras I: the classical affine
  type","  Associated to the classical Weyl groups, we introduce the notion of
degenerate spin affine Hecke algebras and affine Hecke-Clifford algebras. For
these algebras, we establish the PBW properties, formulate the intertwiners,
and describe the centers. We further develop connections of these algebras with
the usual degenerate (i.e. graded) affine Hecke algebras of Lusztig by
introducing a notion of degenerate covering affine Hecke algebras.
",associate classical weyl group introduce notion degenerate spin affine hecke algebras affine hecke clifford algebra algebra establish pbw property formulate intertwiner describe center develop connection algebra usual degenerate i.e. grade affine hecke algebra lusztig introduce notion degenerate covering affine hecke algebras
math,A unified approach to SIC-POVMs and MUBs,"  A unified approach to (symmetric informationally complete) positive operator
valued measures and mutually unbiased bases is developed in this article. The
approach is based on the use of operator equivalents expanded in the enveloping
algebra of SU(2). Emphasis is put on similarities and differences between
SIC-POVMs and MUBs.
",unified approach symmetric informationally complete positive operator value measure mutually unbiased basis develop article approach base use operator equivalent expand enveloping algebra su(2 emphasis similarity difference sic povms mubs
math,Littlewood-Richardson polynomials,"  We introduce a family of rings of symmetric functions depending on an
infinite sequence of parameters. A distinguished basis of such a ring is
comprised by analogues of the Schur functions. The corresponding structure
coefficients are polynomials in the parameters which we call the
Littlewood-Richardson polynomials. We give a combinatorial rule for their
calculation by modifying an earlier result of B. Sagan and the author. The new
rule provides a formula for these polynomials which is manifestly positive in
the sense of W. Graham. We apply this formula for the calculation of the
product of equivariant Schubert classes on Grassmannians which implies a
stability property of the structure coefficients. The first manifestly positive
formula for such an expansion was given by A. Knutson and T. Tao by using
combinatorics of puzzles while the stability property was not apparent from
that formula. We also use the Littlewood-Richardson polynomials to describe the
multiplication rule in the algebra of the Casimir elements for the general
linear Lie algebra in the basis of the quantum immanants constructed by A.
Okounkov and G. Olshanski.
",introduce family ring symmetric function depend infinite sequence parameter distinguished basis ring comprise analogue schur function corresponding structure coefficient polynomial parameter littlewood richardson polynomial combinatorial rule calculation modify early result b. sagan author new rule provide formula polynomial manifestly positive sense w. graham apply formula calculation product equivariant schubert class grassmannians imply stability property structure coefficient manifestly positive formula expansion give a. knutson t. tao combinatoric puzzle stability property apparent formula use littlewood richardson polynomial describe multiplication rule algebra casimir element general linear lie algebra basis quantum immanant construct a. okounkov g. olshanski
math,"Some combinatorial aspects of differential operation compositions on
  space $R^n$","  In this paper we present a recurrent relation for counting meaningful
compositions of the higher-order differential operations on the space $R^{n}$
(n=3,4,...) and extract the non-trivial compositions of order higher than two.
","paper present recurrent relation count meaningful composition high order differential operation space $ r^{n}$ n=3,4 extract non trivial composition order high"
math,Curvature flows in semi-Riemannian manifolds,"  We prove that the limit hypersurfaces of converging curvature flows are
stable, if the initial velocity has a weak sign, and give a survey of the
existence and regularity results.
",prove limit hypersurface converging curvature flow stable initial velocity weak sign survey existence regularity result
math,On Ando's inequalities for convex and concave functions,"  For positive semidefinite matrices $A$ and $B$, Ando and Zhan proved the
inequalities $||| f(A)+f(B) ||| \ge ||| f(A+B) |||$ and $||| g(A)+g(B) ||| \le
||| g(A+B) |||$, for any unitarily invariant norm, and for any non-negative
operator monotone $f$ on $[0,\infty)$ with inverse function $g$. These
inequalities have very recently been generalised to non-negative concave
functions $f$ and non-negative convex functions $g$, by Bourin and Uchiyama,
and Kosem, respectively.
  In this paper we consider the related question whether the inequalities $|||
f(A)-f(B) ||| \le ||| f(|A-B|) |||$, and $||| g(A)-g(B) ||| \ge ||| g(|A-B|)
|||$, obtained by Ando, for operator monotone $f$ with inverse $g$, also have a
similar generalisation to non-negative concave $f$ and convex $g$. We answer
exactly this question, in the negative for general matrices, and affirmatively
in the special case when $A\ge ||B||$.
  In the course of this work, we introduce the novel notion of $Y$-dominated
majorisation between the spectra of two Hermitian matrices, where $Y$ is itself
a Hermitian matrix, and prove a certain property of this relation that allows
to strengthen the results of Bourin-Uchiyama and Kosem, mentioned above.
","positive semidefinite matrix $ a$ $ b$ ando zhan prove inequality $ ||| f(a)+f(b ||| \ge ||| f(a+b |||$ $ ||| g(a)+g(b ||| \le ||| g(a+b |||$ unitarily invariant norm non negative operator monotone $ f$ $ 0,\infty)$ inverse function $ g$. inequality recently generalise non negative concave function $ f$ non negative convex function $ g$ bourin uchiyama kosem respectively paper consider related question inequality $ ||| f(a)-f(b ||| \le ||| f(|a b| |||$ $ ||| g(a)-g(b ||| \ge ||| g(|a b| |||$ obtain ando operator monotone $ f$ inverse $ g$ similar generalisation non negative concave $ f$ convex $ g$. answer exactly question negative general matrix affirmatively special case $ a\ge ||b||$. course work introduce novel notion $ y$-dominate majorisation spectra hermitian matrix $ y$ hermitian matrix prove certain property relation allow strengthen result bourin uchiyama kosem mention"
math,On the total disconnectedness of the quotient Aubry set,"  In this paper we show that the quotient Aubry set associated to certain
Lagrangians is totally disconnected (i.e., every connected component consists
of a single point). Moreover, we discuss the relation between this problem and
a Morse-Sard type property for (difference of) critical subsolutions of
Hamilton-Jacobi equations.
",paper quotient aubry set associate certain lagrangians totally disconnected i.e. connected component consist single point discuss relation problem morse sard type property difference critical subsolution hamilton jacobi equation
math,"A geometric realization of sl(6,C)","  Given an orientable weakly self-dual manifold X of rank two, we build a
geometric realization of the Lie algebra sl(6,C) as a naturally defined algebra
L of endomorphisms of the space of differential forms of X. We provide an
explicit description of Serre generators in terms of natural generators of L.
This construction gives a bundle on X which is related to the search for a
natural Gauge theory on X. We consider this paper as a first step in the study
of a rich and interesting algebraic structure.
","give orientable weakly self dual manifold x rank build geometric realization lie algebra sl(6,c naturally define algebra l endomorphism space differential form x. provide explicit description serre generator term natural generator l. construction give bundle x relate search natural gauge theory x. consider paper step study rich interesting algebraic structure"
math,"Pfaffians, hafnians and products of real linear functionals","  We prove pfaffian and hafnian versions of Lieb's inequalities on determinants
and permanents of positive semi-definite matrices. We use the hafnian
inequality to improve the lower bound of R\'ev\'esz and Sarantopoulos on the
norm of a product of linear functionals on a real Euclidean space (this subject
is sometimes called the `real linear polarization constant' problem).
",prove pfaffian hafnian version lieb inequality determinant permanent positive semi definite matrix use hafnian inequality improve low bound r\'ev\'esz sarantopoulos norm product linear functional real euclidean space subject call ` real linear polarization constant problem
math,Groups with finitely many conjugacy classes and their automorphisms,"  We combine classical methods of combinatorial group theory with the theory of
small cancellations over relatively hyperbolic groups to construct finitely
generated torsion-free groups that have only finitely many classes of conjugate
elements. Moreover, we present several results concerning embeddings into such
groups.
  As another application of these techniques, we prove that every countable
group $C$ can be realized as a group of outer automorphisms of a group $N$,
where $N$ is a finitely generated group having Kazhdan's property (T) and
containing exactly two conjugacy classes.
",combine classical method combinatorial group theory theory small cancellation relatively hyperbolic group construct finitely generate torsion free group finitely class conjugate element present result concern embedding group application technique prove countable group $ c$ realize group outer automorphism group $ n$ $ n$ finitely generate group have kazhdan property t contain exactly conjugacy class
math,Sparsity-certifying Graph Decompositions,"  We describe a new algorithm, the $(k,\ell)$-pebble game with colors, and use
it obtain a characterization of the family of $(k,\ell)$-sparse graphs and
algorithmic solutions to a family of problems concerning tree decompositions of
graphs. Special instances of sparse graphs appear in rigidity theory and have
received increased attention in recent years. In particular, our colored
pebbles generalize and strengthen the previous results of Lee and Streinu and
give a new proof of the Tutte-Nash-Williams characterization of arboricity. We
also present a new decomposition that certifies sparsity based on the
$(k,\ell)$-pebble game with colors. Our work also exposes connections between
pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and
Westermann and Hendrickson.
","describe new algorithm $ k,\ell)$-pebble game color use obtain characterization family $ k,\ell)$-sparse graph algorithmic solution family problem concern tree decomposition graph special instance sparse graph appear rigidity theory receive increase attention recent year particular color pebble generalize strengthen previous result lee streinu new proof tutte nash williams characterization arboricity present new decomposition certify sparsity base $ k,\ell)$-pebble game color work expose connection pebble game algorithm previous sparse graph algorithm gabow gabow westermann hendrickson"
math,"Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless
  Networks","  This paper investigates the many-to-one throughput capacity (and by symmetry,
one-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has
generally been assumed in prior studies that the many-to-one throughput
capacity is upper-bounded by the link capacity L. Throughput capacity L is not
achievable under 802.11. This paper introduces the notion of ""canonical
networks"", which is a class of regularly-structured networks whose capacities
can be analyzed more easily than unstructured networks. We show that the
throughput capacity of canonical networks under 802.11 has an analytical upper
bound of 3L/4 when the source nodes are two or more hops away from the sink;
and simulated throughputs of 0.690L (0.740L) when the source nodes are many
hops away. We conjecture that 3L/4 is also the upper bound for general
networks. When all links have equal length, 2L/3 can be shown to be the upper
bound for general networks. Our simulations show that 802.11 networks with
random topologies operated with AODV routing can only achieve throughputs far
below the upper bounds. Fortunately, by properly selecting routes near the
gateway (or by properly positioning the relay nodes leading to the gateway) to
fashion after the structure of canonical networks, the throughput can be
improved significantly by more than 150%. Indeed, in a dense network, it is
worthwhile to deactivate some of the relay nodes near the sink judiciously.
",paper investigate throughput capacity symmetry throughput capacity ieee 802.11 multi hop network generally assume prior study throughput capacity upper bounded link capacity l. throughput capacity l achievable 802.11 paper introduce notion canonical network class regularly structure network capacity analyze easily unstructured network throughput capacity canonical network 802.11 analytical upper bind 3l/4 source node hop away sink simulate throughput 0.690l 0.740l source node hop away conjecture 3l/4 upper bind general network link equal length 2l/3 show upper bind general network simulation 802.11 network random topology operate aodv routing achieve throughput far upper bound fortunately properly select route near gateway properly position relay node lead gateway fashion structure canonical network throughput improve significantly 150 dense network worthwhile deactivate relay node near sink judiciously
math,"Glueball Masses in (2+1)-Dimensional Anisotropic Weakly-Coupled
  Yang-Mills Theory","  The confinement problem has been solved in the anisotropic (2+1)-dimensional
SU(N) Yang-Mills theory at weak coupling. In this paper, we find the low-lying
spectrum for N=2. The lightest excitations are pairs of fundamental particles
of the (1+1)-dimensional SU(2)XSU(2) principal chiral sigma model bound in a
linear potential, with a specified matching condition where the particles
overlap. This matching condition can be determined from the exactly-known
S-matrix for the sigma model.
",confinement problem solve anisotropic 2 + 1)-dimensional su(n yang mills theory weak coupling paper find low lie spectrum n=2 light excitation pair fundamental particle 1 + 1)-dimensional su(2)xsu(2 principal chiral sigma model bind linear potential specified matching condition particle overlap matching condition determine exactly know s matrix sigma model
math,Skew-Hadamard matrices of orders 188 and 388 exist,"  We construct several difference families on cyclic groups of orders 47 and
97, and use them to construct skew-Hadamard matrices of orders 188 and 388.
Such difference families and matrices are constructed here for the first time.
The matrices are constructed by using the Goethals-Seidel array.
",construct difference family cyclic group order 47 97 use construct skew hadamard matrix order 188 388 difference family matrix construct time matrix construct goethals seidel array
math,A note on higher-order differential operations,"  In this paper we consider successive iterations of the first-order
differential operations in space ${\bf R}^3.$
",paper consider successive iteration order differential operation space $ \bf r}^3.$
math,"Dual billiards, Fagnano orbits and regular polygons","  We study the notion of Fagnano orbits for dual polygonal billiards. We used
them to characterize regular polygons and we study the iteration of the
developing map.
",study notion fagnano orbit dual polygonal billiard characterize regular polygon study iteration develop map
math,"Approximate solutions to the Dirichlet problem for harmonic maps between
  hyperbolic spaces","  Our main result in this paper is the following: Given $H^m, H^n$ hyperbolic
spaces of dimensional $m$ and $n$ corresponding, and given a Holder function
$f=(s^1,...,f^{n-1}):\partial H^m\to \partial H^n$ between geometric boundaries
of $H^m$ and $H^n$. Then for each $\epsilon >0$ there exists a harmonic map
$u:H^m\to H^n$ which is continuous up to the boundary (in the sense of
Euclidean) and $u|_{\partial H^m}=(f^1,...,f^{n-1},\epsilon)$.
","main result paper follow give $ h^m h^n$ hyperbolic space dimensional $ m$ $ n$ correspond give holder function $ f=(s^1, ,f^{n-1}):\partial h^m\to \partial h^n$ geometric boundary $ h^m$ $ h^n$. $ \epsilon > 0 $ exist harmonic map $ u h^m\to h^n$ continuous boundary sense euclidean $ u|_{\partial h^m}=(f^1, ,f^{n-1},\epsilon)$."
math,A Global Approach to the Theory of Special Finsler Manifolds,"  The aim of the present paper is to provide a global presentation of the
theory of special Finsler manifolds. We introduce and investigate globally (or
intrinsically, free from local coordinates) many of the most important and most
commonly used special Finsler manifolds: locally Minkowskian, Berwald,
Landesberg, general Landesberg, $P$-reducible, $C$-reducible,
semi-$C$-reducible, quasi-$C$-reducible, $P^{*}$-Finsler, $C^{h}$-recurrent,
$C^{v}$-recurrent, $C^{0}$-recurrent, $S^{v}$-recurrent, $S^{v}$-recurrent of
the second order, $C_{2}$-like, $S_{3}$-like, $S_{4}$-like, $P_{2}$-like,
$R_{3}$-like, $P$-symmetric, $h$-isotropic, of scalar curvature, of constant
curvature, of $p$-scalar curvature, of $s$-$ps$-curvature. The global
definitions of these special Finsler manifolds are introduced. Various
relationships between the different types of the considered special Finsler
manifolds are found. Many local results, known in the literature, are proved
globally and several new results are obtained. As a by-product, interesting
identities and properties concerning the torsion tensor fields and the
curvature tensor fields are deduced. Although our investigation is entirely
global, we provide; for comparison reasons, an appendix presenting a local
counterpart of our global approach and the local definitions of the special
Finsler spaces considered.
",aim present paper provide global presentation theory special finsler manifold introduce investigate globally intrinsically free local coordinate important commonly special finsler manifold locally minkowskian berwald landesberg general landesberg $ p$-reducible $ c$ -reducible semi-$c$-reducible quasi-$c$-reducible $ p^{*}$-finsler $ c^{h}$-recurrent $ c^{v}$-recurrent $ c^{0}$-recurrent $ s^{v}$-recurrent $ s^{v}$-recurrent second order $ c_{2}$-like $ s_{3}$-like $ s_{4}$-like $ p_{2}$-like $ r_{3}$-like $ p$-symmetric $ h$-isotropic scalar curvature constant curvature $ p$-scalar curvature $ s$-$ps$-curvature global definition special finsler manifold introduce relationship different type consider special finsler manifold find local result know literature prove globally new result obtain product interesting identity property concern torsion tensor field curvature tensor field deduce investigation entirely global provide comparison reason appendix present local counterpart global approach local definition special finsler space consider
math,"Construction of Complete Embedded Self-Similar Surfaces under Mean
  Curvature Flow. Part II","  We study the Dirichlet problem associated to the equation for self-similar
surfaces for graphs over the Euclidean plane with a disk removed. We show the
existence of a solution provided the boundary conditions on the boundary circle
are small enough and satisfy some symmetries. This is the second step towards
the construction of new examples of complete embedded self similar surfaces
under mean curvature flow.
",study dirichlet problem associate equation self similar surface graph euclidean plane disk remove existence solution provide boundary condition boundary circle small satisfy symmetry second step construction new example complete embed self similar surface mean curvature flow
math,One-dimensional Brownian particle systems with rank dependent drifts,"  We study interacting systems of linear Brownian motions whose drift vector at
every time point is determined by the relative ranks of the coordinate
processes at that time. Our main objective has been to study the long range
behavior of the spacings between the Brownian motions arranged in increasing
order. For finitely many Brownian motions interacting in this manner, we
characterize drifts for which the family of laws of the vector of spacings is
tight, and show its convergence to a unique stationary joint distribution given
by independent exponential distributions with varying means. We also study one
particular countably infinite system, where only the minimum Brownian particle
gets a constant upward drift, and prove that independent and identically
distributed exponential spacings remain stationary under the dynamics of such a
process. Some related conjectures in this direction have also been discussed.
",study interact system linear brownian motion drift vector time point determine relative rank coordinate process time main objective study long range behavior spacing brownian motion arrange increase order finitely brownian motion interact manner characterize drift family law vector spacing tight convergence unique stationary joint distribution give independent exponential distribution varying mean study particular countably infinite system minimum brownian particle get constant upward drift prove independent identically distribute exponential spacing remain stationary dynamic process related conjecture direction discuss
math,"Strong Spherical Asymptotics for Rotor-Router Aggregation and the
  Divisible Sandpile","  The rotor-router model is a deterministic analogue of random walk. It can be
used to define a deterministic growth model analogous to internal DLA. We prove
that the asymptotic shape of this model is a Euclidean ball, in a sense which
is stronger than our earlier work. For the shape consisting of $n=\omega_d r^d$
sites, where $\omega_d$ is the volume of the unit ball in $\R^d$, we show that
the inradius of the set of occupied sites is at least $r-O(\log r)$, while the
outradius is at most $r+O(r^\alpha)$ for any $\alpha > 1-1/d$. For a related
model, the divisible sandpile, we show that the domain of occupied sites is a
Euclidean ball with error in the radius a constant independent of the total
mass. For the classical abelian sandpile model in two dimensions, with $n=\pi
r^2$ particles, we show that the inradius is at least $r/\sqrt{3}$, and the
outradius is at most $(r+o(r))/\sqrt{2}$. This improves on bounds of Le Borgne
and Rossin. Similar bounds apply in higher dimensions.
",rotor router model deterministic analogue random walk define deterministic growth model analogous internal dla prove asymptotic shape model euclidean ball sense strong early work shape consist $ n=\omega_d r^d$ site $ \omega_d$ volume unit ball $ \r^d$ inradius set occupy site $ r o(\log r)$ outradius $ r+o(r^\alpha)$ $ \alpha > 1 1 d$. related model divisible sandpile domain occupy site euclidean ball error radius constant independent total mass classical abelian sandpile model dimension $ n=\pi r^2 $ particle inradius $ r/\sqrt{3}$ outradius $ r+o(r))/\sqrt{2}$. improve bound le borgne rossin similar bound apply high dimension
math,"Late-time tails of a Yang-Mills field on Minkowski and Schwarzschild
  backgrounds","  We study the late-time behavior of spherically symmetric solutions of the
Yang-Mills equations on Minkowski and Schwarzschild backgrounds. Using
nonlinear perturbation theory we show in both cases that solutions having
smooth compactly supported initial data posses tails which decay as $t^{-4}$ at
timelike infinity. Moreover, for small initial data on Minkowski background we
derive the third-order formula for the amplitude of the tail and confirm
numerically its accuracy.
",study late time behavior spherically symmetric solution yang mills equation minkowski schwarzschild background nonlinear perturbation theory case solution having smooth compactly support initial datum posse tail decay $ t^{-4}$ timelike infinity small initial datum minkowski background derive order formula amplitude tail confirm numerically accuracy
math,Dimers on surface graphs and spin structures. II,"  In a previous paper, we showed how certain orientations of the edges of a
graph G embedded in a closed oriented surface S can be understood as discrete
spin structures on S. We then used this correspondence to give a geometric
proof of the Pfaffian formula for the partition function of the dimer model on
G. In the present article, we generalize these results to the case of compact
oriented surfaces with boundary. We also show how the operations of cutting and
gluing act on discrete spin structures and how they change the partition
function. These operations allow to reformulate the dimer model as a quantum
field theory on surface graphs.
",previous paper show certain orientation edge graph g embed closed orient surface s understand discrete spin structure s. correspondence geometric proof pfaffian formula partition function dimer model g. present article generalize result case compact orient surface boundary operation cut gluing act discrete spin structure change partition function operation allow reformulate dimer model quantum field theory surface graph
math,Frobenius-Schur indicators for semisimple Lie algebras,"  Let g be a finite dimensional complex semisimple Lie algebra, and let V be a
finite dimensional represenation of g. We give a closed formula for the mth
Frobenius-Schur indicator, m>1, of V in representation-theoretic terms. We
deduce that the indicators take integer values, and that for a large enough m,
the mth indicator of V equals the dimension of the zero weight space of V. For
the classical Lie algebras sl(n), so(2n), so(2n+1) and sp(2n), this is the case
for m greater or equal to 2n-1, 4n-5, 4n-3 and 2n+1, respectively.
",let g finite dimensional complex semisimple lie algebra let v finite dimensional represenation g. closed formula mth frobenius schur indicator m>1 v representation theoretic term deduce indicator integer value large m mth indicator v equal dimension zero weight space v. classical lie algebra sl(n so(2n so(2n+1 sp(2n case m great equal 2n-1 4n-5 4n-3 2n+1 respectively
math,"Generic representations of orthogonal groups: projective functors in the
  category Fquad","  In this paper, we continue the study of the category of functors Fquad,
associated to F_2-vector spaces equipped with a nondegenerate quadratic form,
initiated in two previous papers of the author. We define a filtration of the
standard projective objects in Fquad; this refines to give a decomposition into
indecomposable factors of the two first standard projective objects in Fquad.
As an application of these two decompositions, we give a complete description
of the polynomial functors of the category Fquad.
",paper continue study category functor fquad associate f_2 vector space equip nondegenerate quadratic form initiate previous paper author define filtration standard projective object fquad refine decomposition indecomposable factor standard projective object fquad application decomposition complete description polynomial functor category fquad
math,"On packet lengths and overhead for random linear coding over the erasure
  channel","  We assess the practicality of random network coding by illuminating the issue
of overhead and considering it in conjunction with increasingly long packets
sent over the erasure channel. We show that the transmission of increasingly
long packets, consisting of either of an increasing number of symbols per
packet or an increasing symbol alphabet size, results in a data rate
approaching zero over the erasure channel. This result is due to an erasure
probability that increases with packet length. Numerical results for a
particular modulation scheme demonstrate a data rate of approximately zero for
a large, but finite-length packet. Our results suggest a reduction in the
performance gains offered by random network coding.
",assess practicality random network code illuminate issue overhead consider conjunction increasingly long packet send erasure channel transmission increasingly long packet consist increase number symbol packet increase symbol alphabet size result data rate approach zero erasure channel result erasure probability increase packet length numerical result particular modulation scheme demonstrate data rate approximately zero large finite length packet result suggest reduction performance gain offer random network coding
math,Clustering in a stochastic model of one-dimensional gas,"  We give a quantitative analysis of clustering in a stochastic model of
one-dimensional gas. At time zero, the gas consists of $n$ identical particles
that are randomly distributed on the real line and have zero initial speeds.
Particles begin to move under the forces of mutual attraction. When particles
collide, they stick together forming a new particle, called cluster, whose mass
and speed are defined by the laws of conservation. We are interested in the
asymptotic behavior of $K_n(t)$ as $n\to \infty$, where $K_n(t)$ denotes the
number of clusters at time $t$ in the system with $n$ initial particles. Our
main result is a functional limit theorem for $K_n(t)$. Its proof is based on
the discovered localization property of the aggregation process, which states
that the behavior of each particle is essentially defined by the motion of
neighbor particles.
",quantitative analysis cluster stochastic model dimensional gas time zero gas consist $ n$ identical particle randomly distribute real line zero initial speed particle begin force mutual attraction particle collide stick form new particle call cluster mass speed define law conservation interested asymptotic behavior $ k_n(t)$ $ n\to \infty$ $ k_n(t)$ denote number cluster time $ t$ system $ n$ initial particle main result functional limit theorem $ k_n(t)$. proof base discover localization property aggregation process state behavior particle essentially define motion neighbor particle
math,Gorenstein locus of minuscule Schubert varieties,"  In this article, we describe explicitely the Gorenstein locus of all
minuscule Schubert varieties. This proves a special case of a conjecture of A.
Woo and A. Yong (see math.AG/0603273) on the Gorenstein locus of Schubert
varieties.
",article describe explicitely gorenstein locus minuscule schubert variety prove special case conjecture a. woo a. yong math ag/0603273 gorenstein locus schubert variety
math,Smooth maps with singularities of bounded K-codimensions,"  We will prove the relative homotopy principle for smooth maps with
singularities of a given {\cal K}-invariant class with a mild condition. We
next study a filtration of the group of homotopy self-equivalences of a given
manifold P by considering singularities of non-negative {\cal K}-codimensions.
",prove relative homotopy principle smooth map singularity give \cal k}-invariant class mild condition study filtration group homotopy self equivalence give manifold p consider singularity non negative \cal k}-codimension
math,Bursting Dynamics of the 3D Euler Equations in Cylindrical Domains,"  A class of three-dimensional initial data characterized by uniformly large
vorticity is considered for the Euler equations of incompressible fluids. The
fast singular oscillating limits of the Euler equations are studied for
parametrically resonant cylinders. Resonances of fast swirling Beltrami waves
deplete the Euler nonlinearity. The resonant Euler equations are systems of
three-dimensional rigid body equations, coupled or not. Some cases of these
resonant systems have homoclinic cycles, and orbits in the vicinity of these
homoclinic cycles lead to bursts of the Euler solution measured in Sobolev
norms of order higher than that corresponding to the enstrophy.
",class dimensional initial datum characterize uniformly large vorticity consider euler equation incompressible fluid fast singular oscillating limit euler equation study parametrically resonant cylinder resonance fast swirl beltrami wave deplete euler nonlinearity resonant euler equation system dimensional rigid body equation couple case resonant system homoclinic cycle orbit vicinity homoclinic cycle lead burst euler solution measure sobolev norm order high correspond enstrophy
math,Counting characters in linear group actions,"  Let $G$ be a finite group and $V$ be a finite $G$--module. We present upper
bounds for the cardinalities of certain subsets of $\Irr(GV)$, such as the set
of those $\chi\in\Irr(GV)$ such that, for a fixed $v\in V$, the restriction of
$\chi$ to $<v>$ is not a multiple of the regular character of $<v>$. These
results might be useful in attacking the non--coprime $k(GV)$--problem.
",let $ g$ finite group $ v$ finite $ g$--module present upper bound cardinality certain subset $ \irr(gv)$ set $ \chi\in\irr(gv)$ fix $ v\in v$ restriction $ \chi$ $ < v>$ multiple regular character $ < v>$. result useful attack non coprime $ k(gv)$--problem
math,Rigorous Results for the Periodic Oscillation of an Adiabatic Piston,"  We study a heavy piston of mass $M$ that moves in one dimension. The piston
separates two gas chambers, each of which contains finitely many ideal, unit
mass gas particles moving in $d$ dimensions, where $ d\geq 1$. Using averaging
techniques, we prove that the actual motions of the piston converge in
probability to the predicted averaged behavior on the time scale $M^ {1/2} $
when $M$ tends to infinity while the total energy of the system is bounded and
the number of gas particles is fixed. Neishtadt and Sinai previously pointed
out that an averaging theorem due to Anosov should extend to this situation.
  When $ d=1$, the gas particles move in just one dimension, and we prove that
the rate of convergence of the actual motions of the piston to its averaged
behavior is $\mathcal{O} (M^ {-1/2}) $ on the time scale $M^ {1/2} $. The
convergence is uniform over all initial conditions in a compact set. We also
investigate the piston system when the particle interactions have been
smoothed. The convergence to the averaged behavior again takes place uniformly,
both over initial conditions and over the amount of smoothing.
  In addition, we prove generalizations of our results to $N$ pistons
separating $N+1$ gas chambers. We also provide a general discussion of
averaging theory and the proofs of a number of previously known averaging
results. In particular, we include a new proof of Anosov's averaging theorem
for smooth systems that is primarily due to Dolgopyat.
",study heavy piston mass $ m$ move dimension piston separate gas chamber contain finitely ideal unit mass gas particle move $ d$ dimension $ d\geq 1$. averaging technique prove actual motion piston converge probability predict average behavior time scale $ m^ 1/2 $ $ m$ tend infinity total energy system bound number gas particle fix neishtadt sinai previously point averaging theorem anosov extend situation $ d=1 $ gas particle dimension prove rate convergence actual motion piston average behavior $ \mathcal{o m^ -1/2 $ time scale $ m^ 1/2 $ convergence uniform initial condition compact set investigate piston system particle interaction smooth convergence average behavior take place uniformly initial condition smoothing addition prove generalization result $ n$ piston separate $ n+1 $ gas chamber provide general discussion average theory proof number previously know averaging result particular include new proof anosov averaging theorem smooth system primarily dolgopyat
math,Thermodynamic Stability - A note on a footnote in Ruelle's book,"  Thermodynamic stable interaction pair potentials which are not of the form
``positive function + real continuous function of positive type'' are presented
in dimension one. Construction of such a potential in dimension two is
sketched. These constructions use only elementary calculations. The
mathematical background is discussed separately.
",thermodynamic stable interaction pair potential form ` ` positive function + real continuous function positive type present dimension construction potential dimension sketch construction use elementary calculation mathematical background discuss separately
math,"Computation of Power Loss in Likelihood Ratio Tests for Probability
  Densities Extended by Lehmann Alternatives","  We compute the loss of power in likelihood ratio tests when we test the
original parameter of a probability density extended by the first Lehmann
alternative.
",compute loss power likelihood ratio test test original parameter probability density extend lehmann alternative
math,On the polynomial automorphisms of a group,"  We prove that if a group is nilpotent (resp. metabelian), then so is the
subgroup of its automorphism group generated by all polynomial automorphisms.
",prove group nilpotent resp metabelian subgroup automorphism group generate polynomial automorphism
math,"Locating the peaks of least-energy solutions to a quasilinear elliptic
  Neumann problem","  In this paper we study the shape of least-energy solutions to a singularly
perturbed quasilinear problem with homogeneous Neumann boundary condition. We
use an intrinsic variation method to show that at limit, the global maximum
point of least-energy solutions goes to a point on the boundary faster than the
linear rate and this point on the boundary approaches to a point where the mean
curvature of the boundary achieves its maximum. We also give a complete proof
of exponential decay of least-energy solutions.
",paper study shape energy solution singularly perturb quasilinear problem homogeneous neumann boundary condition use intrinsic variation method limit global maximum point energy solution go point boundary fast linear rate point boundary approach point mean curvature boundary achieve maximum complete proof exponential decay energy solution
math,"A Rigorous Time-Domain Analysis of Full--Wave Electromagnetic Cloaking
  (Invisibility)","  There is currently a great deal of interest in the theoretical and practical
possibility of cloaking objects from the observation by electromagnetic waves.
The basic idea of these invisibility devices \cite{glu1, glu2, le},\cite{pss1}
is to use anisotropic {\it transformation media} whose permittivity and
permeability $\var^{\lambda\nu}, \mu^{\lambda\nu}$, are obtained from the ones,
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$, of isotropic media, by singular
transformations of coordinates. In this paper we study electromagnetic cloaking
in the time-domain using the formalism of time-dependent scattering theory.
This formalism allows us to settle in an unambiguous way the mathematical
problems posed by the singularities of the inverse of the permittivity and the
permeability of the {\it transformation media} on the boundary of the cloaked
objects. We write Maxwell's equations in Schr\""odinger form with the
electromagnetic propagator playing the role of the Hamiltonian. We prove that
the electromagnetic propagator outside of the cloaked objects is essentially
self-adjoint. Moreover, the unique self-adjoint extension is unitarily
equivalent to the electromagnetic propagator in the medium
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$. Using this fact, and since the
coordinate transformation is the identity outside of a ball, we prove that the
scattering operator is the identity. Our results give a rigorous proof that the
construction of \cite{glu1, glu2, le}, \cite{pss1} perfectly cloaks passive and
active devices from observation by electromagnetic waves. Furthermore, we prove
cloaking for general anisotropic materials. In particular, our results prove
that it is possible to cloak objects inside general crystals.
","currently great deal interest theoretical practical possibility cloak object observation electromagnetic wave basic idea invisibility device \cite{glu1 glu2 le},\cite{pss1 use anisotropic \it transformation medium permittivity permeability $ \var^{\lambda\nu \mu^{\lambda\nu}$ obtain one $ \var_0^{\lambda\nu \mu^{\lambda\nu}_0 $ isotropic medium singular transformation coordinate paper study electromagnetic cloaking time domain formalism time dependent scattering theory formalism allow settle unambiguous way mathematical problem pose singularity inverse permittivity permeability \it transformation medium boundary cloaked object write maxwell equation schr\""odinger form electromagnetic propagator play role hamiltonian prove electromagnetic propagator outside cloaked object essentially self adjoint unique self adjoint extension unitarily equivalent electromagnetic propagator medium $ \var_0^{\lambda\nu \mu^{\lambda\nu}_0$. fact coordinate transformation identity outside ball prove scatter operator identity result rigorous proof construction \cite{glu1 glu2 le \cite{pss1 perfectly cloak passive active device observation electromagnetic wave furthermore prove cloaking general anisotropic material particular result prove possible cloak object inside general crystal"
math,Enumerating limit groups,"  We prove that the set of limit groups is recursive, answering a question of
Delzant. One ingredient of the proof is the observation that a finitely
presented group with local retractions (a la Long and Reid) is coherent and,
furthermore, there exists an algorithm that computes presentations for finitely
generated subgroups. The other main ingredient is the ability to
algorithmically calculate centralizers in relatively hyperbolic groups.
Applications include the existence of recognition algorithms for limit groups
and free groups.
",prove set limit group recursive answer question delzant ingredient proof observation finitely present group local retraction la long reid coherent furthermore exist algorithm compute presentation finitely generate subgroup main ingredient ability algorithmically calculate centralizer relatively hyperbolic group application include existence recognition algorithm limit group free group
math,Learning from compressed observations,"  The problem of statistical learning is to construct a predictor of a random
variable $Y$ as a function of a related random variable $X$ on the basis of an
i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable
predictors are drawn from some specified class, and the goal is to approach
asymptotically the performance (expected loss) of the best predictor in the
class. We consider the setting in which one has perfect observation of the
$X$-part of the sample, while the $Y$-part has to be communicated at some
finite bit rate. The encoding of the $Y$-values is allowed to depend on the
$X$-values. Under suitable regularity conditions on the admissible predictors,
the underlying family of probability distributions and the loss function, we
give an information-theoretic characterization of achievable predictor
performance in terms of conditional distortion-rate functions. The ideas are
illustrated on the example of nonparametric regression in Gaussian noise.
",problem statistical learning construct predictor random variable $ y$ function relate random variable $ x$ basis i.i.d training sample joint distribution $ x y)$. allowable predictor draw specify class goal approach asymptotically performance expect loss good predictor class consider setting perfect observation $ x$-part sample $ y$-part communicate finite bit rate encoding $ y$-values allow depend $ x$-values suitable regularity condition admissible predictor underlie family probability distribution loss function information theoretic characterization achievable predictor performance term conditional distortion rate function idea illustrate example nonparametric regression gaussian noise
math,A procedure for finding the k-th power of a matrix,"  We give a new procedure in Maple for finding the k-th power of a martix. The
algorithm is based on the article [1].
",new procedure maple find k th power martix algorithm base article 1
math,The Arctic Circle Revisited,"  The problem of limit shapes in the six-vertex model with domain wall boundary
conditions is addressed by considering a specially tailored bulk correlation
function, the emptiness formation probability. A closed expression of this
correlation function is given, both in terms of certain determinant and
multiple integral, which allows for a systematic treatment of the limit shapes
of the model for full range of values of vertex weights. Specifically, we show
that for vertex weights corresponding to the free-fermion line on the phase
diagram, the emptiness formation probability is related to a one-matrix model
with a triple logarithmic singularity, or Triple Penner model. The saddle-point
analysis of this model leads to the Arctic Circle Theorem, and its
generalization to the Arctic Ellipses, known previously from domino tilings.
",problem limit shape vertex model domain wall boundary condition address consider specially tailor bulk correlation function emptiness formation probability closed expression correlation function give term certain determinant multiple integral allow systematic treatment limit shape model range value vertex weight specifically vertex weight correspond free fermion line phase diagram emptiness formation probability relate matrix model triple logarithmic singularity triple penner model saddle point analysis model lead arctic circle theorem generalization arctic ellipses know previously domino tiling
math,A limit relation for entropy and channel capacity per unit cost,"  In a quantum mechanical model, Diosi, Feldmann and Kosloff arrived at a
conjecture stating that the limit of the entropy of certain mixtures is the
relative entropy as system size goes to infinity. The conjecture is proven in
this paper for density matrices. The first proof is analytic and uses the
quantum law of large numbers. The second one clarifies the relation to channel
capacity per unit cost for classical-quantum channels. Both proofs lead to
generalization of the conjecture.
",quantum mechanical model diosi feldmann kosloff arrive conjecture state limit entropy certain mixture relative entropy system size go infinity conjecture prove paper density matrix proof analytic use quantum law large number second clarify relation channel capacity unit cost classical quantum channel proof lead generalization conjecture
math,Topological Free Entropy Dimension of in Unital C^*-algebras,"  The notion of topological free entropy dimension of $n-$tuples of elements in
a unital C$^*$ algebra was introduced by Voiculescu. In the paper, we compute
topological free entropy dimension of one self-adjoint element and topological
orbit dimension of one self-adjoint element in a unital C$^*$ algebra.
Moreover, we calculate the values of topological free entropy dimensions of
families of generators of some unital C$^*$ algebras (for example: irrational
rotation C$^*$ algebras or minimal tensor product of two reduced C$^*$ algebras
of free groups).
",notion topological free entropy dimension $ n-$tuple element unital c$ ^*$ algebra introduce voiculescu paper compute topological free entropy dimension self adjoint element topological orbit dimension self adjoint element unital c$ ^*$ algebra calculate value topological free entropy dimension family generator unital c$ ^*$ algebras example irrational rotation c$ ^*$ algebras minimal tensor product reduce c$ ^*$ algebra free group
math,Dynamical Objects for Cohomologically Expanding Maps,"  The goal of this paper is to construct invariant dynamical objects for a (not
necessarily invertible) smooth self map of a compact manifold. We prove a
result that takes advantage of differences in rates of expansion in the terms
of a sheaf cohomological long exact sequence to create unique lifts of finite
dimensional invariant subspaces of one term of the sequence to invariant
subspaces of the preceding term. This allows us to take invariant cohomological
classes and under the right circumstances construct unique currents of a given
type, including unique measures of a given type, that represent those classes
and are invariant under pullback. A dynamically interesting self map may have a
plethora of invariant measures, so the uniquess of the constructed currents is
important. It means that if local growth is not too big compared to the growth
rate of the cohomological class then the expanding cohomological class gives
sufficient ""marching orders"" to the system to prohibit the formation of any
other such invariant current of the same type (say from some local dynamical
subsystem). Because we use subsheaves of the sheaf of currents we give
conditions under which a subsheaf will have the same cohomology as the sheaf
containing it. Using a smoothing argument this allows us to show that the sheaf
cohomology of the currents under consideration can be canonically identified
with the deRham cohomology groups. Our main theorem can be applied in both the
smooth and holomorphic setting.
",goal paper construct invariant dynamical object necessarily invertible smooth self map compact manifold prove result take advantage difference rate expansion term sheaf cohomological long exact sequence create unique lift finite dimensional invariant subspace term sequence invariant subspace precede term allow invariant cohomological class right circumstance construct unique current give type include unique measure give type represent class invariant pullback dynamically interesting self map plethora invariant measure uniquess construct current important mean local growth big compare growth rate cohomological class expand cohomological class give sufficient marching order system prohibit formation invariant current type local dynamical subsystem use subsheave sheaf current condition subsheaf cohomology sheaf contain smooth argument allow sheaf cohomology current consideration canonically identify derham cohomology group main theorem apply smooth holomorphic setting
math,"Capacity of a Multiple-Antenna Fading Channel with a Quantized Precoding
  Matrix","  Given a multiple-input multiple-output (MIMO) channel, feedback from the
receiver can be used to specify a transmit precoding matrix, which selectively
activates the strongest channel modes. Here we analyze the performance of
Random Vector Quantization (RVQ), in which the precoding matrix is selected
from a random codebook containing independent, isotropically distributed
entries. We assume that channel elements are i.i.d. and known to the receiver,
which relays the optimal (rate-maximizing) precoder codebook index to the
transmitter using B bits. We first derive the large system capacity of
beamforming (rank-one precoding matrix) as a function of B, where large system
refers to the limit as B and the number of transmit and receive antennas all go
to infinity with fixed ratios. With beamforming RVQ is asymptotically optimal,
i.e., no other quantization scheme can achieve a larger asymptotic rate. The
performance of RVQ is also compared with that of a simpler reduced-rank scalar
quantization scheme in which the beamformer is constrained to lie in a random
subspace. We subsequently consider a precoding matrix with arbitrary rank, and
approximate the asymptotic RVQ performance with optimal and linear receivers
(matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show
that these approximations accurately predict the performance of finite-size
systems of interest. Given a target spectral efficiency, numerical examples
show that the amount of feedback required by the linear MMSE receiver is only
slightly more than that required by the optimal receiver, whereas the matched
filter can require significantly more feedback.
",give multiple input multiple output mimo channel feedback receiver specify transmit precoding matrix selectively activate strong channel mode analyze performance random vector quantization rvq precoding matrix select random codebook contain independent isotropically distribute entry assume channel element i.i.d know receiver relay optimal rate maximize precoder codebook index transmitter b bit derive large system capacity beamforme rank precoding matrix function b large system refer limit b number transmit receive antenna infinity fix ratio beamforme rvq asymptotically optimal i.e. quantization scheme achieve large asymptotic rate performance rvq compare simple reduced rank scalar quantization scheme beamformer constrain lie random subspace subsequently consider precoding matrix arbitrary rank approximate asymptotic rvq performance optimal linear receiver match filter minimum mean squared error mmse numerical example approximation accurately predict performance finite size system interest give target spectral efficiency numerical example feedback require linear mmse receiver slightly require optimal receiver match filter require significantly feedback
math,Non-monotone convergence in the quadratic Wasserstein distance,"  We give an easy counter-example to Problem 7.20 from C. Villani's book on
mass transport: in general, the quadratic Wasserstein distance between $n$-fold
normalized convolutions of two given measures fails to decrease monotonically.
",easy counter example problem 7.20 c. villani book mass transport general quadratic wasserstein distance $ n$-fold normalize convolution give measure fail decrease monotonically
math,"The exact asymptotic of the collision time tail distribution for
  independent Brownian particles with different drifts","  In this note we consider the time of the collision $\tau$ for $n$ independent
Brownian motions $X^1_t,...,X_t^n$ with drifts $a_1,...,a_n$, each starting
from $x=(x_1,...,x_n)$, where $x_1<...<x_n$. We show the exact asymptotics of
$P_x(\tau>t) = C h(x)t^{-\alpha}e^{-\gamma t}(1 + o(1))$ as $t\to\infty$ and
identify $C,h(x),\alpha,\gamma$ in terms of the drifts.
","note consider time collision $ \tau$ $ n$ independent brownian motion $ x^1_t, ,x_t^n$ drift $ a_1, ,a_n$ start $ x=(x_1, ,x_n)$ $ x_1< <x_n$. exact asymptotic $ p_x(\tau > t = c h(x)t^{-\alpha}e^{-\gamma t}(1 + o(1))$ $ t\to\infty$ identify $ c h(x),\alpha,\gamma$ term drift"
math,A Symplectic Test of the L-Functions Ratios Conjecture,"  Recently Conrey, Farmer and Zirnbauer conjectured formulas for the averages
over a family of ratios of products of shifted L-functions. Their L-functions
Ratios Conjecture predicts both the main and lower order terms for many
problems, ranging from n-level correlations and densities to mollifiers and
moments to vanishing at the central point. There are now many results showing
agreement between the main terms of number theory and random matrix theory;
however, there are very few families where the lower order terms are known.
These terms often depend on subtle arithmetic properties of the family, and
provide a way to break the universality of behavior. The L-functions Ratios
Conjecture provides a powerful and tractable way to predict these terms. We
test a specific case here, that of the 1-level density for the symplectic
family of quadratic Dirichlet characters arising from even fundamental
discriminants d \le X. For test functions supported in (-1/3, 1/3) we calculate
all the lower order terms up to size O(X^{-1/2+epsilon}) and observe perfect
agreement with the conjecture (for test functions supported in (-1, 1) we show
agreement up to errors of size O(X^{-epsilon}) for any epsilon). Thus for this
family and suitably restricted test functions, we completely verify the Ratios
Conjecture's prediction for the 1-level density.
",recently conrey farmer zirnbauer conjecture formula average family ratio product shift l function l function ratios conjecture predict main low order term problem range n level correlation density mollifier moment vanish central point result show agreement main term number theory random matrix theory family low order term know term depend subtle arithmetic property family provide way break universality behavior l function ratio conjecture provide powerful tractable way predict term test specific case 1 level density symplectic family quadratic dirichlet character arise fundamental discriminant d \le x. test function support -1/3 1/3 calculate low order term size o(x^{-1/2+epsilon observe perfect agreement conjecture test function support -1 1 agreement error size o(x^{-epsilon epsilon family suitably restrict test function completely verify ratios conjecture prediction 1 level density
math,Classification of superpotentials,"  We extend our previous classification of superpotentials of ``scalar
curvature type"" for the cohomogeneity one Ricci-flat equations. We now consider
the case not covered in our previous paper, i.e., when some weight vector of
the superpotential lies outside (a scaled translate of) the convex hull of the
weight vectors associated with the scalar curvature function of the principal
orbit. In this situation we show that either the isotropy representation has at
most 3 irreducible summands or the first order subsystem associated to the
superpotential is of the same form as the Calabi-Yau condition for submersion
type metrics on complex line bundles over a Fano K\""ahler-Einstein product.
","extend previous classification superpotential ` ` scalar curvature type cohomogeneity ricci flat equation consider case cover previous paper i.e. weight vector superpotential lie outside scale translate convex hull weight vector associate scalar curvature function principal orbit situation isotropy representation 3 irreducible summand order subsystem associate superpotential form calabi yau condition submersion type metric complex line bundle fano k\""ahler einstein product"
math,"On the Achievable Rate Regions for Interference Channels with Degraded
  Message Sets","  The interference channel with degraded message sets (IC-DMS) refers to a
communication model in which two senders attempt to communicate with their
respective receivers simultaneously through a common medium, and one of the
senders has complete and a priori (non-causal) knowledge about the message
being transmitted by the other. A coding scheme that collectively has
advantages of cooperative coding, collaborative coding, and dirty paper coding,
is developed for such a channel. With resorting to this coding scheme,
achievable rate regions of the IC-DMS in both discrete memoryless and Gaussian
cases are derived, which, in general, include several previously known rate
regions. Numerical examples for the Gaussian case demonstrate that in the
high-interference-gain regime, the derived achievable rate regions offer
considerable improvements over these existing results.
",interference channel degraded message set ic dms refer communication model sender attempt communicate respective receiver simultaneously common medium sender complete priori non causal knowledge message transmit code scheme collectively advantage cooperative coding collaborative coding dirty paper coding develop channel resort code scheme achievable rate region ic dms discrete memoryless gaussian case derive general include previously know rate region numerical example gaussian case demonstrate high interference gain regime derive achievable rate region offer considerable improvement exist result
math,"Weak and Strong Taylor methods for numerical solutions of stochastic
  differential equations","  We apply results of Malliavin-Thalmaier-Watanabe for strong and weak Taylor
expansions of solutions of perturbed stochastic differential equations (SDEs).
In particular, we work out weight expressions for the Taylor coefficients of
the expansion. The results are applied to LIBOR market models in order to deal
with the typical stochastic drift and with stochastic volatility. In contrast
to other accurate methods like numerical schemes for the full SDE, we obtain
easily tractable expressions for accurate pricing. In particular, we present an
easily tractable alternative to ``freezing the drift'' in LIBOR market models,
which has an accuracy similar to the full numerical scheme. Numerical examples
underline the results.
",apply result malliavin thalmaier watanabe strong weak taylor expansion solution perturb stochastic differential equation sde particular work weight expression taylor coefficient expansion result apply libor market model order deal typical stochastic drift stochastic volatility contrast accurate method like numerical scheme sde obtain easily tractable expression accurate pricing particular present easily tractable alternative ` ` freeze drift libor market model accuracy similar numerical scheme numerical example underline result
math,"Unit groups of integral finite group rings with no noncyclic abelian
  finite subgroups","  It is shown that in the units of augmentation one of an integral group ring
$\mathbb{Z} G$ of a finite group $G$, a noncyclic subgroup of order $p^{2}$,
for some odd prime $p$, exists only if such a subgroup exists in $G$. The
corresponding statement for $p=2$ holds by the Brauer--Suzuki theorem, as
recently observed by W. Kimmerle.
",show unit augmentation integral group ring $ \mathbb{z g$ finite group $ g$ noncyclic subgroup order $ p^{2}$ odd prime $ p$ exist subgroup exist $ g$. corresponding statement $ p=2 $ hold brauer suzuki theorem recently observe w. kimmerle
math,Convergence of a finite volume scheme for the incompressible fluids,"  We consider a finite volume scheme for the two-dimensional incompressible
Navier-Stokes equations. We use a triangular mesh. The unknowns for the
velocity and pressure are respectively piecewise constant and affine. We use a
projection method to deal with the incompressibility constraint. In a former
paper, the stability of the scheme has been proven. We infer from it its
convergence.
",consider finite volume scheme dimensional incompressible navier stokes equation use triangular mesh unknown velocity pressure respectively piecewise constant affine use projection method deal incompressibility constraint paper stability scheme prove infer convergence
math,Proper J-holomorphic discs in Stein domains of dimension 2,"  We prove the existence of global Bishop discs in a strictly pseudoconvex
Stein domain in an almost complex manifold of complex dimension 2.
",prove existence global bishop disc strictly pseudoconvex stein domain complex manifold complex dimension 2
math,"The Hardy-Lorentz Spaces $H^{p,q}(R^n)$","  In this paper we consider the Hardy-Lorentz spaces $H^{p,q}(R^n)$, with
$0<p\le 1$, $0<q\le \infty$. We discuss the atomic decomposition of the
elements in these spaces, their interpolation properties, and the behavior of
singular integrals and other operators acting on them.
",paper consider hardy lorentz space $ h^{p q}(r^n)$ $ 0 < p\le 1 $ $ 0 < q\le \infty$. discuss atomic decomposition element space interpolation property behavior singular integral operator act
math,"Distribution of integral Fourier Coefficients of a Modular Form of Half
  Integral Weight Modulo Primes","  Recently, Bruinier and Ono classified cusp forms $f(z) := \sum_{n=0}^{\infty}
a_f(n)q ^n \in S_{\lambda+1/2}(\Gamma_0(N),\chi)\cap \mathbb{Z}[[q]]$ that does
not satisfy a certain distribution property for modulo odd primes $p$. In this
paper, using Rankin-Cohen Bracket, we extend this result to modular forms of
half integral weight for primes $p \geq 5$. As applications of our main theorem
we derive distribution properties, for modulo primes $p\geq5$, of traces of
singular moduli and Hurwitz class number. We also study an analogue of Newman's
conjecture for overpartitions.
","recently bruinier ono classify cusp form $ f(z = \sum_{n=0}^{\infty a_f(n)q ^n \in s_{\lambda+1/2}(\gamma_0(n),\chi)\cap \mathbb{z}[[q]]$ satisfy certain distribution property modulo odd prime $ p$. paper rankin cohen bracket extend result modular form half integral weight prime $ p \geq 5$. application main theorem derive distribution property modulo prime $ p\geq5 $ trace singular modulus hurwitz class number study analogue newman conjecture overpartition"
math,Growing Perfect Decagonal Quasicrystals by Local Rules,"  A local growth algorithm for a decagonal quasicrystal is presented. We show
that a perfect Penrose tiling (PPT) layer can be grown on a decapod tiling
layer by a three dimensional (3D) local rule growth. Once a PPT layer begins to
form on the upper layer, successive 2D PPT layers can be added on top resulting
in a perfect decagonal quasicrystalline structure in bulk with a point defect
only on the bottom surface layer. Our growth rule shows that an ideal
quasicrystal structure can be constructed by a local growth algorithm in 3D,
contrary to the necessity of non-local information for a 2D PPT growth.
",local growth algorithm decagonal quasicrystal present perfect penrose tile ppt layer grow decapod tiling layer dimensional 3d local rule growth ppt layer begin form upper layer successive 2d ppt layer add result perfect decagonal quasicrystalline structure bulk point defect surface layer growth rule show ideal quasicrystal structure construct local growth algorithm 3d contrary necessity non local information 2d ppt growth
math,Maximum solutions of normalized Ricci flows on 4-manifolds,"  We consider maximum solution $g(t)$, $t\in [0, +\infty)$, to the normalized
Ricci flow. Among other things, we prove that, if $(M, \omega) $ is a smooth
compact symplectic 4-manifold such that $b_2^+(M)>1$ and let
$g(t),t\in[0,\infty)$, be a solution to (1.3) on $M$ whose Ricci curvature
satisfies that $|\text{Ric}(g(t))|\leq 3$ and additionally $\chi(M)=3 \tau
(M)>0$, then there exists an $m\in \mathbb{N}$, and a sequence of points
$\{x_{j,k}\in M\}$, $j=1, ..., m$, satisfying that, by passing to a
subsequence, $$(M, g(t_{k}+t), x_{1,k},..., x_{m,k})
\stackrel{d_{GH}}\longrightarrow (\coprod_{j=1}^m N_j, g_{\infty},
x_{1,\infty}, ...,, x_{m,\infty}),$$ $t\in [0, \infty)$, in the $m$-pointed
Gromov-Hausdorff sense for any sequence $t_{k}\longrightarrow \infty$, where
$(N_{j}, g_{\infty})$, $j=1,..., m$, are complete complex hyperbolic orbifolds
of complex dimension 2 with at most finitely many isolated orbifold points.
Moreover, the convergence is $C^{\infty}$ in the non-singular part of
$\coprod_1^m N_{j}$ and
$\text{Vol}_{g_{0}}(M)=\sum_{j=1}^{m}\text{Vol}_{g_{\infty}}(N_{j})$, where
$\chi(M)$ (resp. $\tau(M)$) is the Euler characteristic (resp. signature) of
$M$.
","consider maximum solution $ g(t)$ $ t\in 0 + \infty)$ normalize ricci flow thing prove $ m \omega $ smooth compact symplectic 4 manifold $ b_2^+(m)>1 $ let $ g(t),t\in[0,\infty)$ solution 1.3 $ m$ ricci curvature satisfie $ |\text{ric}(g(t))|\leq 3 $ additionally $ \chi(m)=3 \tau m)>0 $ exist $ m\in \mathbb{n}$ sequence point $ \{x_{j k}\in m\}$ $ j=1 m$ satisfy pass subsequence $ $ m g(t_{k}+t x_{1,k x_{m k \stackrel{d_{gh}}\longrightarrow \coprod_{j=1}^m n_j g_{\infty x_{1,\infty x_{m,\infty}),$$ $ t\in 0 \infty)$ $ m$-pointe gromov hausdorff sense sequence $ t_{k}\longrightarrow \infty$ $ n_{j g_{\infty})$ $ j=1 m$ complete complex hyperbolic orbifold complex dimension 2 finitely isolated orbifold point convergence $ c^{\infty}$ non singular $ \coprod_1^m n_{j}$ $ \text{vol}_{g_{0}}(m)=\sum_{j=1}^{m}\text{vol}_{g_{\infty}}(n_{j})$ $ \chi(m)$ resp $ \tau(m)$ euler characteristic resp signature $ m$."
math,Monoid generalizations of the Richard Thompson groups,"  The groups G_{k,1} of Richard Thompson and Graham Higman can be generalized
in a natural way to monoids, that we call M_{k,1}, and to inverse monoids,
called Inv_{k,1}; this is done by simply generalizing bijections to partial
functions or partial injective functions. The monoids M_{k,1} have connections
with circuit complexity (studied in another paper). Here we prove that M_{k,1}
and Inv_{k,1} are congruence-simple for all k. Their Green relations J and D
are characterized: M_{k,1} and Inv_{k,1} are J-0-simple, and they have k-1
non-zero D-classes. They are submonoids of the multiplicative part of the Cuntz
algebra O_k. They are finitely generated, and their word problem over any
finite generating set is in P. Their word problem is coNP-complete over certain
infinite generating sets.
  Changes in this version: Section 4 has been thoroughly revised, and errors
have been corrected; however, the main results of Section 4 do not change.
Sections 1, 2, and 3 are unchanged, except for the proof of Theorem 2.3, which
was incomplete; a complete proof was published in the Appendix of reference
[6], and is also given here.
","group g_{k,1 richard thompson graham higman generalize natural way monoid m_{k,1 inverse monoid call inv_{k,1 simply generalize bijection partial function partial injective function monoids m_{k,1 connection circuit complexity study paper prove m_{k,1 inv_{k,1 congruence simple k. green relation j d characterize m_{k,1 inv_{k,1 j-0 simple k-1 non zero d class submonoid multiplicative cuntz algebra o_k finitely generate word problem finite generating set p. word problem conp complete certain infinite generating set change version section 4 thoroughly revise error correct main result section 4 change section 1 2 3 unchanged proof theorem 2.3 incomplete complete proof publish appendix reference 6 give"
math,"Universal Source Coding for Monotonic and Fast Decaying Monotonic
  Distributions","  We study universal compression of sequences generated by monotonic
distributions. We show that for a monotonic distribution over an alphabet of
size $k$, each probability parameter costs essentially $0.5 \log (n/k^3)$ bits,
where $n$ is the coded sequence length, as long as $k = o(n^{1/3})$. Otherwise,
for $k = O(n)$, the total average sequence redundancy is $O(n^{1/3+\epsilon})$
bits overall. We then show that there exists a sub-class of monotonic
distributions over infinite alphabets for which redundancy of
$O(n^{1/3+\epsilon})$ bits overall is still achievable. This class contains
fast decaying distributions, including many distributions over the integers and
geometric distributions. For some slower decays, including other distributions
over the integers, redundancy of $o(n)$ bits overall is achievable, where a
method to compute specific redundancy rates for such distributions is derived.
The results are specifically true for finite entropy monotonic distributions.
Finally, we study individual sequence redundancy behavior assuming a sequence
is governed by a monotonic distribution. We show that for sequences whose
empirical distributions are monotonic, individual redundancy bounds similar to
those in the average case can be obtained. However, even if the monotonicity in
the empirical distribution is violated, diminishing per symbol individual
sequence redundancies with respect to the monotonic maximum likelihood
description length may still be achievable.
",study universal compression sequence generate monotonic distribution monotonic distribution alphabet size $ k$ probability parameter cost essentially $ 0.5 \log n k^3)$ bit $ n$ code sequence length long $ k = o(n^{1/3})$. $ k = o(n)$ total average sequence redundancy $ o(n^{1/3+\epsilon})$ bit overall exist sub class monotonic distribution infinite alphabet redundancy $ o(n^{1/3+\epsilon})$ bit overall achievable class contain fast decaying distribution include distribution integer geometric distribution slow decay include distribution integer redundancy $ o(n)$ bit overall achievable method compute specific redundancy rate distribution derive result specifically true finite entropy monotonic distribution finally study individual sequence redundancy behavior assume sequence govern monotonic distribution sequence empirical distribution monotonic individual redundancy bound similar average case obtain monotonicity empirical distribution violate diminish symbol individual sequence redundancy respect monotonic maximum likelihood description length achievable
math,On the Markov trace for Temperley--Lieb algebras of type $E_n$,"  We show that there is a unique Markov trace on the tower of Temperley--Lieb
type quotients of Hecke algebras of Coxeter type $E_n$ (for all $n \geq 6$). We
explain in detail how this trace may be computed easily using tom Dieck's
calculus of diagrams. As applications, we show how to use the trace to show
that the diagram representation is faithful, and to compute leading
coefficients of certain Kazhdan--Lusztig polynomials.
",unique markov trace tower temperley lieb type quotient hecke algebra coxeter type $ e_n$ $ n \geq 6 $ explain detail trace compute easily tom dieck calculus diagram application use trace diagram representation faithful compute lead coefficient certain kazhdan lusztig polynomial
math,Spline Single-Index Prediction Model,"  For the past two decades, single-index model, a special case of projection
pursuit regression, has proven to be an efficient way of coping with the high
dimensional problem in nonparametric regression. In this paper, based on weakly
dependent sample, we investigate the single-index prediction (SIP) model which
is robust against deviation from the single-index model. The single-index is
identified by the best approximation to the multivariate prediction function of
the response variable, regardless of whether the prediction function is a
genuine single-index function. A polynomial spline estimator is proposed for
the single-index prediction coefficients, and is shown to be root-n consistent
and asymptotically normal. An iterative optimization routine is used which is
sufficiently fast for the user to analyze large data of high dimension within
seconds. Simulation experiments have provided strong evidence that corroborates
with the asymptotic theory. Application of the proposed procedure to the rive
flow data of Iceland has yielded superior out-of-sample rolling forecasts.
",past decade single index model special case projection pursuit regression prove efficient way cope high dimensional problem nonparametric regression paper base weakly dependent sample investigate single index prediction sip model robust deviation single index model single index identify good approximation multivariate prediction function response variable regardless prediction function genuine single index function polynomial spline estimator propose single index prediction coefficient show root n consistent asymptotically normal iterative optimization routine sufficiently fast user analyze large datum high dimension second simulation experiment provide strong evidence corroborate asymptotic theory application propose procedure rive flow datum iceland yield superior sample rolling forecast
math,"Metropolis algorithm and equienergy sampling for two mean field spin
  systems","  In this paper we study the Metropolis algorithm in connection with two
mean--field spin systems, the so called mean--field Ising model and the
Blume--Emery--Griffiths model. In both this examples the naive choice of
proposal chain gives rise, for some parameters, to a slowly mixing Metropolis
chain, that is a chain whose spectral gap decreases exponentially fast (in the
dimension $N$ of the problem). Here we show how a slight variant in the
proposal chain can avoid this problem, keeping the mean computational cost
similar to the cost of the usual Metropolis. More precisely we prove that, with
a suitable variant in the proposal, the Metropolis chain has a spectral gap
which decreases polynomially in 1/N. Using some symmetry structure of the
energy, the method rests on allowing appropriate jumps within the energy level
of the starting state.
",paper study metropolis algorithm connection mean field spin system call mean field ising model blume emery griffiths model example naive choice proposal chain give rise parameter slowly mix metropolis chain chain spectral gap decrease exponentially fast dimension $ n$ problem slight variant proposal chain avoid problem keep mean computational cost similar cost usual metropolis precisely prove suitable variant proposal metropolis chain spectral gap decrease polynomially 1 n. symmetry structure energy method rest allow appropriate jump energy level starting state
math,"Optimal Routing for Decode-and-Forward based Cooperation in Wireless
  Networks","  We investigate cooperative wireless relay networks in which the nodes can
help each other in data transmission. We study different coding strategies in
the single-source single-destination network with many relay nodes. Given the
myriad of ways in which nodes can cooperate, there is a natural routing
problem, i.e., determining an ordered set of nodes to relay the data from the
source to the destination. We find that for a given route, the
decode-and-forward strategy, which is an information theoretic cooperative
coding strategy, achieves rates significantly higher than that achievable by
the usual multi-hop coding strategy, which is a point-to-point non-cooperative
coding strategy. We construct an algorithm to find an optimal route (in terms
of rate maximizing) for the decode-and-forward strategy. Since the algorithm
runs in factorial time in the worst case, we propose a heuristic algorithm that
runs in polynomial time. The heuristic algorithm outputs an optimal route when
the nodes transmit independent codewords. We implement these coding strategies
using practical low density parity check codes to compare the performance of
the strategies on different routes.
",investigate cooperative wireless relay network node help data transmission study different code strategy single source single destination network relay node give myriad way node cooperate natural routing problem i.e. determine order set node relay datum source destination find give route decode forward strategy information theoretic cooperative code strategy achieve rate significantly high achievable usual multi hop coding strategy point point non cooperative code strategy construct algorithm find optimal route term rate maximizing decode forward strategy algorithm run factorial time bad case propose heuristic algorithm run polynomial time heuristic algorithm output optimal route node transmit independent codeword implement code strategy practical low density parity check code compare performance strategy different route
math,$p$-Adic Haar multiresolution analysis,"  In this paper, the notion of {\em $p$-adic multiresolution analysis (MRA)} is
introduced. We use a ``natural'' refinement equation whose solution (a
refinable function) is the characteristic function of the unit disc. This
equation reflects the fact that the characteristic function of the unit disc is
the sum of $p$ characteristic functions of disjoint discs of radius $p^{-1}$.
The case $p=2$ is studied in detail. Our MRA is a 2-adic analog of the real
Haar MRA. But in contrast to the real setting, the refinable function
generating our Haar MRA is periodic with period 1, which never holds for real
refinable functions. This fact implies that there exist infinity many different
2-adic orthonormal wavelet bases in ${\cL}^2(\bQ_2)$ generated by the same Haar
MRA. All of these bases are constructed. Since $p$-adic pseudo-differential
operators are closely related to wavelet-type bases, our bases can be
intensively used for applications.
",paper notion \em $ p$-adic multiresolution analysis mra introduce use ` ` natural refinement equation solution refinable function characteristic function unit disc equation reflect fact characteristic function unit disc sum $ p$ characteristic function disjoint disc radius $ p^{-1}$. case $ p=2 $ study detail mra 2 adic analog real haar mra contrast real setting refinable function generate haar mra periodic period 1 hold real refinable function fact imply exist infinity different 2 adic orthonormal wavelet basis $ \cl}^2(\bq_2)$ generate haar mra basis construct $ p$-adic pseudo differential operator closely relate wavelet type basis basis intensively application
cs,The Genetic Programming Collaboration Network and its Communities,"  Useful information about scientific collaboration structures and patterns can
be inferred from computer databases of published papers. The genetic
programming bibliography is the most complete reference of papers on GP\@. In
addition to locating publications, it contains coauthor and coeditor
relationships from which a more complete picture of the field emerges. We treat
these relationships as undirected small world graphs whose study reveals the
community structure of the GP collaborative social network. Automatic analysis
discovers new communities and highlights new facets of them. The investigation
reveals many similarities between GP and coauthorship networks in other
scientific fields but also some subtle differences such as a smaller central
network component and a high clustering.
",useful information scientific collaboration structure pattern infer computer database publish paper genetic programming bibliography complete reference paper gp\@. addition locate publication contain coauthor coeditor relationship complete picture field emerge treat relationship undirected small world graph study reveal community structure gp collaborative social network automatic analysis discover new community highlight new facet investigation reveal similarity gp coauthorship network scientific field subtle difference small central network component high clustering
cs,Intricate Knots in Proteins: Function and Evolution,"  A number of recently discovered protein structures incorporate a rather
unexpected structural feature: a knot in the polypeptide backbone. These knots
are extremely rare, but their occurrence is likely connected to protein
function in as yet unexplored fashion. Our analysis of the complete Protein
Data Bank reveals several new knots which, along with previously discovered
ones, can shed light on such connections. In particular, we identify the most
complex knot discovered to date in human ubiquitin hydrolase, and suggest that
its entangled topology protects it against unfolding and degradation by the
proteasome. Knots in proteins are typically preserved across species and
sometimes even across kingdoms. However, we also identify a knot which only
appears in some transcarbamylases while being absent in homologous proteins of
similar structure. The emergence of the knot is accompanied by a shift in the
enzymatic function of the protein. We suggest that the simple insertion of a
short DNA fragment into the gene may suffice to turn an unknotted into a
knotted structure in this protein.
",number recently discover protein structure incorporate unexpected structural feature knot polypeptide backbone knot extremely rare occurrence likely connect protein function unexplored fashion analysis complete protein data bank reveal new knot previously discover one shed light connection particular identify complex knot discover date human ubiquitin hydrolase suggest entangled topology protect unfolding degradation proteasome knot protein typically preserve specie kingdom identify knot appear transcarbamylase absent homologous protein similar structure emergence knot accompany shift enzymatic function protein suggest simple insertion short dna fragment gene suffice turn unknotted knot structure protein
cs,The Complexity of HCP in Digraps with Degree Bound Two,"  The Hamiltonian cycle problem (HCP) in digraphs D with degree bound two is
solved by two mappings in this paper. The first bijection is between an
incidence matrix C_{nm} of simple digraph and an incidence matrix F of balanced
bipartite undirected graph G; The second mapping is from a perfect matching of
G to a cycle of D. It proves that the complexity of HCP in D is polynomial, and
finding a second non-isomorphism Hamiltonian cycle from a given Hamiltonian
digraph with degree bound two is also polynomial. Lastly it deduces P=NP base
on the results.
",hamiltonian cycle problem hcp digraph d degree bind solve mapping paper bijection incidence matrix c_{nm simple digraph incidence matrix f balanced bipartite undirected graph g second mapping perfect matching g cycle d. prove complexity hcp d polynomial find second non isomorphism hamiltonian cycle give hamiltonian digraph degree bind polynomial lastly deduce p = np base result
cs,"The discrete dipole approximation for simulation of light scattering by
  particles much larger than the wavelength","  In this manuscript we investigate the capabilities of the Discrete Dipole
Approximation (DDA) to simulate scattering from particles that are much larger
than the wavelength of the incident light, and describe an optimized publicly
available DDA computer program that processes the large number of dipoles
required for such simulations. Numerical simulations of light scattering by
spheres with size parameters x up to 160 and 40 for refractive index m=1.05 and
2 respectively are presented and compared with exact results of the Mie theory.
Errors of both integral and angle-resolved scattering quantities generally
increase with m and show no systematic dependence on x. Computational times
increase steeply with both x and m, reaching values of more than 2 weeks on a
cluster of 64 processors. The main distinctive feature of the computer program
is the ability to parallelize a single DDA simulation over a cluster of
computers, which allows it to simulate light scattering by very large
particles, like the ones that are considered in this manuscript. Current
limitations and possible ways for improvement are discussed.
",manuscript investigate capability discrete dipole approximation dda simulate scatter particle large wavelength incident light describe optimize publicly available dda computer program process large number dipole require simulation numerical simulation light scatter sphere size parameter x 160 40 refractive index m=1.05 2 respectively present compare exact result mie theory error integral angle resolve scatter quantity generally increase m systematic dependence x. computational time increase steeply x m reach value 2 week cluster 64 processor main distinctive feature computer program ability parallelize single dda simulation cluster computer allow simulate light scattering large particle like one consider manuscript current limitation possible way improvement discuss
cs,Resonant activation in bistable semiconductor lasers,"  We theoretically investigate the possibility of observing resonant activation
in the hopping dynamics of two-mode semiconductor lasers. We present a series
of simulations of a rate-equations model under random and periodic modulation
of the bias current. In both cases, for an optimal choice of the modulation
time-scale, the hopping times between the stable lasing modes attain a minimum.
The simulation data are understood by means of an effective one-dimensional
Langevin equation with multiplicative fluctuations. Our conclusions apply to
both Edge Emitting and Vertical Cavity Lasers, thus opening the way to several
experimental tests in such optical systems.
",theoretically investigate possibility observe resonant activation hop dynamic mode semiconductor laser present series simulation rate equation model random periodic modulation bias current case optimal choice modulation time scale hop time stable lasing mode attain minimum simulation datum understand mean effective dimensional langevin equation multiplicative fluctuation conclusion apply edge emitting vertical cavity lasers open way experimental test optical system
cs,"Proposal for an Enhanced Optical Cooling System Test in an Electron
  Storage Ring","  We are proposing to test experimentally the new idea of Enhanced Optical
Cooling (EOC) in an electron storage ring. This experiment will confirm new
fundamental processes in beam physics and will demonstrate new unique
possibilities with this cooling technique. It will open important applications
of EOC in nuclear physics, elementary particle physics and in Light Sources
(LS) based on high brightness electron and ion beams.
",propose test experimentally new idea enhanced optical cooling eoc electron storage ring experiment confirm new fundamental process beam physics demonstrate new unique possibility cool technique open important application eoc nuclear physics elementary particle physics light sources ls base high brightness electron ion beam
cs,"Lattice Boltzmann inverse kinetic approach for the incompressible
  Navier-Stokes equations","  In spite of the large number of papers appeared in the past which are devoted
to the lattice Boltzmann (LB) methods, basic aspects of the theory still remain
unchallenged. An unsolved theoretical issue is related to the construction of a
discrete kinetic theory which yields \textit{exactly} the fluid equations,
i.e., is non-asymptotic (here denoted as \textit{LB inverse kinetic theory}).
The purpose of this paper is theoretical and aims at developing an inverse
kinetic approach of this type. In principle infinite solutions exist to this
problem but the freedom can be exploited in order to meet important
requirements. In particular, the discrete kinetic theory can be defined so that
it yields exactly the fluid equation also for arbitrary non-equilibrium (but
suitably smooth) kinetic distribution functions and arbitrarily close to the
boundary of the fluid domain. Unlike previous entropic LB methods the theorem
can be obtained without functional constraints on the class of the initial
distribution functions. Possible realizations of the theory and asymptotic
approximations are provided which permit to determine the fluid equations
\textit{with prescribed accuracy.} As a result, asymptotic accuracy estimates
of customary LB approaches and comparisons with the Chorin artificial
compressibility method are discussed.
",spite large number paper appear past devote lattice boltzmann lb method basic aspect theory remain unchallenged unsolved theoretical issue relate construction discrete kinetic theory yield \textit{exactly fluid equation i.e. non asymptotic denote \textit{lb inverse kinetic theory purpose paper theoretical aim develop inverse kinetic approach type principle infinite solution exist problem freedom exploit order meet important requirement particular discrete kinetic theory define yield exactly fluid equation arbitrary non equilibrium suitably smooth kinetic distribution function arbitrarily close boundary fluid domain unlike previous entropic lb method theorem obtain functional constraint class initial distribution function possible realization theory asymptotic approximation provide permit determine fluid equation \textit{with prescribed accuracy result asymptotic accuracy estimate customary lb approach comparison chorin artificial compressibility method discuss
cs,"Formation of density singularities in ideal hydrodynamics of freely
  cooling inelastic gases: a family of exact solutions","  We employ granular hydrodynamics to investigate a paradigmatic problem of
clustering of particles in a freely cooling dilute granular gas. We consider
large-scale hydrodynamic motions where the viscosity and heat conduction can be
neglected, and one arrives at the equations of ideal gas dynamics with an
additional term describing bulk energy losses due to inelastic collisions. We
employ Lagrangian coordinates and derive a broad family of exact non-stationary
analytical solutions that depend only on one spatial coordinate. These
solutions exhibit a new type of singularity, where the gas density blows up in
a finite time when starting from smooth initial conditions. The density blowups
signal formation of close-packed clusters of particles. As the density blow-up
time $t_c$ is approached, the maximum density exhibits a power law $\sim
(t_c-t)^{-2}$. The velocity gradient blows up as $\sim - (t_c-t)^{-1}$ while
the velocity itself remains continuous and develops a cusp (rather than a shock
discontinuity) at the singularity. The gas temperature vanishes at the
singularity, and the singularity follows the isobaric scenario: the gas
pressure remains finite and approximately uniform in space and constant in time
close to the singularity. An additional exact solution shows that the density
blowup, of the same type, may coexist with an ""ordinary"" shock, at which the
hydrodynamic fields are discontinuous but finite. We confirm stability of the
exact solutions with respect to small one-dimensional perturbations by solving
the ideal hydrodynamic equations numerically. Furthermore, numerical solutions
show that the local features of the density blowup hold universally,
independently of details of the initial and boundary conditions.
",employ granular hydrodynamic investigate paradigmatic problem clustering particle freely cool dilute granular gas consider large scale hydrodynamic motion viscosity heat conduction neglect arrive equation ideal gas dynamic additional term describe bulk energy loss inelastic collision employ lagrangian coordinate derive broad family exact non stationary analytical solution depend spatial coordinate solution exhibit new type singularity gas density blow finite time start smooth initial condition density blowup signal formation close pack cluster particle density blow time $ t_c$ approach maximum density exhibit power law $ \sim t_c t)^{-2}$. velocity gradient blow $ \sim t_c t)^{-1}$ velocity remain continuous develop cusp shock discontinuity singularity gas temperature vanish singularity singularity follow isobaric scenario gas pressure remain finite approximately uniform space constant time close singularity additional exact solution show density blowup type coexist ordinary shock hydrodynamic field discontinuous finite confirm stability exact solution respect small dimensional perturbation solve ideal hydrodynamic equation numerically furthermore numerical solution local feature density blowup hold universally independently detail initial boundary condition
cs,"A Hierarchical Approach for Dependability Analysis of a Commercial
  Cache-Based RAID Storage Architecture","  We present a hierarchical simulation approach for the dependability analysis
and evaluation of a highly available commercial cache-based RAID storage
system. The archi-tecture is complex and includes several layers of
overlap-ping error detection and recovery mechanisms. Three ab-straction levels
have been developed to model the cache architecture, cache operations, and
error detection and recovery mechanism. The impact of faults and errors
oc-curring in the cache and in the disks is analyzed at each level of the
hierarchy. A simulation submodel is associated with each abstraction level. The
models have been devel-oped using DEPEND, a simulation-based environment for
system-level dependability analysis, which provides facili-ties to inject
faults into a functional behavior model, to simulate error detection and
recovery mechanisms, and to evaluate quantitative measures. Several fault
models are defined for each submodel to simulate cache component failures, disk
failures, transmission errors, and data errors in the cache memory and in the
disks. Some of the parame-ters characterizing fault injection in a given
submodel cor-respond to probabilities evaluated from the simulation of the
lower-level submodel. Based on the proposed method-ology, we evaluate and
analyze 1) the system behavior un-der a real workload and high error rate
(focusing on error bursts), 2) the coverage of the error detection mechanisms
implemented in the system and the error latency distribu-tions, and 3) the
accumulation of errors in the cache and in the disks.
",present hierarchical simulation approach dependability analysis evaluation highly available commercial cache base raid storage system archi tecture complex include layer overlap ping error detection recovery mechanism ab straction level develop model cache architecture cache operation error detection recovery mechanism impact fault error oc curre cache disk analyze level hierarchy simulation submodel associate abstraction level model devel oped depend simulation base environment system level dependability analysis provide facili tie inject fault functional behavior model simulate error detection recovery mechanism evaluate quantitative measure fault model define submodel simulate cache component failure disk failure transmission error datum error cache memory disk parame ter characterize fault injection give submodel cor respond probability evaluate simulation low level submodel base propose method ology evaluate analyze 1 system behavior un der real workload high error rate focus error burst 2 coverage error detection mechanism implement system error latency distribu tion 3 accumulation error cache disk
cs,An architecture-based dependability modeling framework using AADL,"  For efficiency reasons, the software system designers' will is to use an
integrated set of methods and tools to describe specifications and designs, and
also to perform analyses such as dependability, schedulability and performance.
AADL (Architecture Analysis and Design Language) has proved to be efficient for
software architecture modeling. In addition, AADL was designed to accommodate
several types of analyses. This paper presents an iterative dependency-driven
approach for dependability modeling using AADL. It is illustrated on a small
example. This approach is part of a complete framework that allows the
generation of dependability analysis and evaluation models from AADL models to
support the analysis of software and system architectures, in critical
application domains.
",efficiency reason software system designer use integrate set method tool describe specification design perform analysis dependability schedulability performance aadl architecture analysis design language prove efficient software architecture modeling addition aadl design accommodate type analysis paper present iterative dependency drive approach dependability modeling aadl illustrate small example approach complete framework allow generation dependability analysis evaluation model aadl model support analysis software system architecture critical application domain
cs,Quantum electromagnetic X-waves,"  We show that two distinct quantum states of the electromagnetic field can be
associated to a classical vector X wave or a propagation-invariant solution of
Maxwell equations. The difference between the two states is of pure quantum
mechanical origin since they are internally entangled and disentangled,
respectively and can be generated by different linear or nonlinear processes.
Detection and generation of Schr\""odinger-cat states comprising two entangled
X-waves and their possible applications are discussed.
","distinct quantum state electromagnetic field associate classical vector x wave propagation invariant solution maxwell equation difference state pure quantum mechanical origin internally entangle disentangle respectively generate different linear nonlinear process detection generation schr\""odinger cat state comprise entangle x wave possible application discuss"
cs,Three dimensional cooling and trapping with a narrow line,"  The intercombination line of Strontium at 689nm is successfully used in laser
cooling to reach the photon recoil limit with Doppler cooling in a
magneto-optical traps (MOT). In this paper we present a systematic study of the
loading efficiency of such a MOT. Comparing the experimental results to a
simple model allows us to discuss the actual limitation of our apparatus. We
also study in detail the final MOT regime emphasizing the role of gravity on
the position, size and temperature along the vertical and horizontal
directions. At large laser detuning, one finds an unusual situation where
cooling and trapping occur in the presence of a high bias magnetic field.
",intercombination line strontium 689 nm successfully laser cooling reach photon recoil limit doppler cool magneto optical trap mot paper present systematic study loading efficiency mot compare experimental result simple model allow discuss actual limitation apparatus study detail final mot regime emphasize role gravity position size temperature vertical horizontal direction large laser detuning find unusual situation cool trap occur presence high bias magnetic field
cs,"Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian
  Vector Broadcast Channels","  MIMO technology is one of the most significant advances in the past decade to
increase channel capacity and has a great potential to improve network capacity
for mesh networks. In a MIMO-based mesh network, the links outgoing from each
node sharing the common communication spectrum can be modeled as a Gaussian
vector broadcast channel. Recently, researchers showed that ``dirty paper
coding'' (DPC) is the optimal transmission strategy for Gaussian vector
broadcast channels. So far, there has been little study on how this fundamental
result will impact the cross-layer design for MIMO-based mesh networks. To fill
this gap, we consider the problem of jointly optimizing DPC power allocation in
the link layer at each node and multihop/multipath routing in a MIMO-based mesh
networks. It turns out that this optimization problem is a very challenging
non-convex problem. To address this difficulty, we transform the original
problem to an equivalent problem by exploiting the channel duality. For the
transformed problem, we develop an efficient solution procedure that integrates
Lagrangian dual decomposition method, conjugate gradient projection method
based on matrix differential calculus, cutting-plane method, and subgradient
method. In our numerical example, it is shown that we can achieve a network
performance gain of 34.4% by using DPC.
",mimo technology significant advance past decade increase channel capacity great potential improve network capacity mesh network mimo base mesh network link outgo node share common communication spectrum model gaussian vector broadcast channel recently researcher show ` ` dirty paper coding dpc optimal transmission strategy gaussian vector broadcast channel far little study fundamental result impact cross layer design mimo base mesh network fill gap consider problem jointly optimize dpc power allocation link layer node multihop multipath routing mimo base mesh network turn optimization problem challenging non convex problem address difficulty transform original problem equivalent problem exploit channel duality transform problem develop efficient solution procedure integrate lagrangian dual decomposition method conjugate gradient projection method base matrix differential calculus cutting plane method subgradient method numerical example show achieve network performance gain 34.4 dpc
cs,Scalar potential model progress,"  Because observations of galaxies and clusters have been found inconsistent
with General Relativity (GR), the focus of effort in developing a Scalar
Potential Model (SPM) has been on the examination of galaxies and clusters. The
SPM has been found to be consistent with cluster cellular structure, the flow
of IGM from spiral galaxies to elliptical galaxies, intergalactic redshift
without an expanding universe, discrete redshift, rotation curve (RC) data
without dark matter, asymmetric RCs, galaxy central mass, galaxy central
velocity dispersion, and the Pioneer Anomaly. In addition, the SPM suggests a
model of past expansion, past contraction, and current expansion of the
universe. GR corresponds to the SPM in the limit in which the effect of the
Sources and Sinks approximate a flat scalar potential field such as between
clusters and on the solar system scale, which is small relative to the distance
to a Source.
",observation galaxy cluster find inconsistent general relativity gr focus effort develop scalar potential model spm examination galaxy cluster spm find consistent cluster cellular structure flow igm spiral galaxy elliptical galaxy intergalactic redshift expand universe discrete redshift rotation curve rc datum dark matter asymmetric rcs galaxy central mass galaxy central velocity dispersion pioneer anomaly addition spm suggest model past expansion past contraction current expansion universe gr correspond spm limit effect sources sinks approximate flat scalar potential field cluster solar system scale small relative distance source
cs,A Multiphilic Descriptor for Chemical Reactivity and Selectivity,"  In line with the local philicity concept proposed by Chattaraj et al.
(Chattaraj, P. K.; Maiti, B.; Sarkar, U. J. Phys. Chem. A. 2003, 107, 4973) and
a dual descriptor derived by Toro-Labbe and coworkers (Morell, C.; Grand, A.;
Toro-Labbe, A. J. Phys. Chem. A. 2005, 109, 205), we propose a multiphilic
descriptor. It is defined as the difference between nucleophilic (Wk+) and
electrophilic (Wk-) condensed philicity functions. This descriptor is capable
of simultaneously explaining the nucleophilicity and electrophilicity of the
given atomic sites in the molecule. Variation of these quantities along the
path of a soft reaction is also analyzed. Predictive ability of this descriptor
has been successfully tested on the selected systems and reactions.
Corresponding force profiles are also analyzed in some representative cases.
Also, to study the intra- and intermolecular reactivities another related
descriptor namely, the nucleophilicity excess (DelW-+) for a nucleophile, over
the electrophilicity in it has been defined and tested on all-metal aromatic
compounds.
",line local philicity concept propose chattaraj et al chattaraj p. k. maiti b. sarkar u. j. phys chem a. 2003 107 4973 dual descriptor derive toro labbe coworker morell c. grand a. toro labbe a. j. phys chem a. 2005 109 205 propose multiphilic descriptor define difference nucleophilic wk+ electrophilic wk- condense philicity function descriptor capable simultaneously explain nucleophilicity electrophilicity give atomic site molecule variation quantity path soft reaction analyze predictive ability descriptor successfully test select system reaction correspond force profile analyze representative case study intra- intermolecular reactivity relate descriptor nucleophilicity excess delw-+ nucleophile electrophilicity define test metal aromatic compound
cs,The World as Evolving Information,"  This paper discusses the benefits of describing the world as information,
especially in the study of the evolution of life and cognition. Traditional
studies encounter problems because it is difficult to describe life and
cognition in terms of matter and energy, since their laws are valid only at the
physical scale. However, if matter and energy, as well as life and cognition,
are described in terms of information, evolution can be described consistently
as information becoming more complex.
  The paper presents eight tentative laws of information, valid at multiple
scales, which are generalizations of Darwinian, cybernetic, thermodynamic,
psychological, philosophical, and complexity principles. These are further used
to discuss the notions of life, cognition and their evolution.
",paper discuss benefit describe world information especially study evolution life cognition traditional study encounter problem difficult describe life cognition term matter energy law valid physical scale matter energy life cognition describe term information evolution describe consistently information complex paper present tentative law information valid multiple scale generalization darwinian cybernetic thermodynamic psychological philosophical complexity principle discuss notion life cognition evolution
cs,Approaching the Heisenberg limit in an atom laser,"  We present experimental and theoretical results showing the improved beam
quality and reduced divergence of an atom laser produced by an optical Raman
transition, compared to one produced by an RF transition. We show that Raman
outcoupling can eliminate the diverging lens effect that the condensate has on
the outcoupled atoms. This substantially improves the beam quality of the atom
laser, and the improvement may be greater than a factor of ten for experiments
with tight trapping potentials. We show that Raman outcoupling can produce atom
lasers whose quality is only limited by the wavefunction shape of the
condensate that produces them, typically a factor of 1.3 above the Heisenberg
limit.
",present experimental theoretical result show improve beam quality reduce divergence atom laser produce optical raman transition compare produce rf transition raman outcoupling eliminate diverge lens effect condensate outcoupled atom substantially improve beam quality atom laser improvement great factor experiment tight trap potential raman outcoupling produce atom laser quality limit wavefunction shape condensate produce typically factor 1.3 heisenberg limit
cs,"Geometric Complexity Theory VI: the flip via saturated and positive
  integer programming in representation theory and algebraic geometry","  This article belongs to a series on geometric complexity theory (GCT), an
approach to the P vs. NP and related problems through algebraic geometry and
representation theory. The basic principle behind this approach is called the
flip. In essence, it reduces the negative hypothesis in complexity theory (the
lower bound problems), such as the P vs. NP problem in characteristic zero, to
the positive hypothesis in complexity theory (the upper bound problems):
specifically, to showing that the problems of deciding nonvanishing of the
fundamental structural constants in representation theory and algebraic
geometry, such as the well known plethysm constants--or rather certain relaxed
forms of these decision probelms--belong to the complexity class P. In this
article, we suggest a plan for implementing the flip, i.e., for showing that
these relaxed decision problems belong to P. This is based on the reduction of
the preceding complexity-theoretic positive hypotheses to mathematical
positivity hypotheses: specifically, to showing that there exist positive
formulae--i.e. formulae with nonnegative coefficients--for the structural
constants under consideration and certain functions associated with them. These
turn out be intimately related to the similar positivity properties of the
Kazhdan-Lusztig polynomials and the multiplicative structural constants of the
canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum
groups. The known proofs of these positivity properties depend on the Riemann
hypothesis over finite fields and the related results. Thus the reduction here,
in conjunction with the flip, in essence, says that the validity of the P vs.
NP conjecture in characteristic zero is intimately linked to the Riemann
hypothesis over finite fields and related problems.
",article belong series geometric complexity theory gct approach p vs. np relate problem algebraic geometry representation theory basic principle approach call flip essence reduce negative hypothesis complexity theory low bind problem p vs. np problem characteristic zero positive hypothesis complexity theory upper bind problem specifically show problem decide nonvanishing fundamental structural constant representation theory algebraic geometry know plethysm constant certain relaxed form decision probelm belong complexity class p. article suggest plan implement flip i.e. show relaxed decision problem belong p. base reduction precede complexity theoretic positive hypothesis mathematical positivity hypothesis specifically show exist positive formulae i.e. formulae nonnegative coefficient structural constant consideration certain function associate turn intimately relate similar positivity property kazhdan lusztig polynomial multiplicative structural constant canonical global crystal basis theory drinfeld jimbo quantum group known proof positivity property depend riemann hypothesis finite field related result reduction conjunction flip essence say validity p vs. np conjecture characteristic zero intimately link riemann hypothesis finite field related problem
cs,Universal Forces and the Dark Energy Problem,"  The Dark Energy problem is forcing us to re-examine our models and our
understanding of relativity and space-time. Here a novel idea of Fundamental
Forces is introduced. This allows us to perceive the General Theory of
Relativity and Einstein's Equation from a new pesrpective. In addition to
providing us with an improved understanding of space and time, it will be shown
how it leads to a resolution of the Dark Energy problem.
",dark energy problem force examine model understanding relativity space time novel idea fundamental forces introduce allow perceive general theory relativity einstein equation new pesrpective addition provide improved understanding space time show lead resolution dark energy problem
cs,On Punctured Pragmatic Space-Time Codes in Block Fading Channel,"  This paper considers the use of punctured convolutional codes to obtain
pragmatic space-time trellis codes over block-fading channel. We show that good
performance can be achieved even when puncturation is adopted and that we can
still employ the same Viterbi decoder of the convolutional mother code by using
approximated metrics without increasing the complexity of the decoding
operations.
",paper consider use punctured convolutional code obtain pragmatic space time trellis code block fade channel good performance achieve puncturation adopt employ viterbi decoder convolutional mother code approximate metric increase complexity decoding operation
cs,"Approximate Selection Rule for Orbital Angular Momentum in Atomic
  Radiative Transitions","  We demonstrate that radiative transitions with \Delta l = - 1 are strongly
dominating for all values of n and l, except small region where l << n.
",demonstrate radiative transition \delta l = 1 strongly dominating value n l small region l < < n.
cs,Inapproximability of Maximum Weighted Edge Biclique and Its Applications,"  Given a bipartite graph $G = (V_1,V_2,E)$ where edges take on {\it both}
positive and negative weights from set $\mathcal{S}$, the {\it maximum weighted
edge biclique} problem, or $\mathcal{S}$-MWEB for short, asks to find a
bipartite subgraph whose sum of edge weights is maximized. This problem has
various applications in bioinformatics, machine learning and databases and its
(in)approximability remains open. In this paper, we show that for a wide range
of choices of $\mathcal{S}$, specifically when $| \frac{\min\mathcal{S}} {\max
\mathcal{S}} | \in \Omega(\eta^{\delta-1/2}) \cap O(\eta^{1/2-\delta})$ (where
$\eta = \max\{|V_1|, |V_2|\}$, and $\delta \in (0,1/2]$), no polynomial time
algorithm can approximate $\mathcal{S}$-MWEB within a factor of $n^{\epsilon}$
for some $\epsilon > 0$ unless $\mathsf{RP = NP}$. This hardness result gives
justification of the heuristic approaches adopted for various applied problems
in the aforementioned areas, and indicates that good approximation algorithms
are unlikely to exist. Specifically, we give two applications by showing that:
1) finding statistically significant biclusters in the SAMBA model, proposed in
\cite{Tan02} for the analysis of microarray data, is
$n^{\epsilon}$-inapproximable; and 2) no polynomial time algorithm exists for
the Minimum Description Length with Holes problem \cite{Bu05} unless
$\mathsf{RP=NP}$.
","give bipartite graph $ g = v_1,v_2,e)$ edge \it positive negative weight set $ \mathcal{s}$ \it maximum weight edge biclique problem $ \mathcal{s}$-mweb short ask find bipartite subgraph sum edge weight maximize problem application bioinformatics machine learning database in)approximability remain open paper wide range choice $ \mathcal{s}$ specifically $ | \frac{\min\mathcal{s \max \mathcal{s | \in \omega(\eta^{\delta-1/2 \cap o(\eta^{1/2-\delta})$ $ \eta = \max\{|v_1| |v_2|\}$ $ \delta \in 0,1/2]$ polynomial time algorithm approximate $ \mathcal{s}$-mweb factor $ n^{\epsilon}$ $ \epsilon > 0 $ $ \mathsf{rp = np}$. hardness result give justification heuristic approach adopt apply problem aforementioned area indicate good approximation algorithm unlikely exist specifically application show 1 find statistically significant bicluster samba model propose \cite{tan02 analysis microarray datum $ n^{\epsilon}$-inapproximable 2 polynomial time algorithm exist minimum description length holes problem \cite{bu05 $ \mathsf{rp = np}$."
cs,"Monitoring spatially heterogeneous dynamics in a drying colloidal thin
  film","  We report on a new type of experiment that enables us to monitor spatially
and temporally heterogeneous dynamic properties in complex fluids. Our approach
is based on the analysis of near-field speckles produced by light diffusely
reflected from the superficial volume of a strongly scattering medium. By
periodic modulation of an incident speckle beam we obtain pixel-wise ensemble
averages of the structure function coefficient, a measure of the dynamic
activity. To illustrate the application of our approach we follow the different
stages in the drying process of a colloidal thin film. We show that we can
access ensemble averaged dynamic properties on length scales as small as ten
micrometers over the full field of view.
",report new type experiment enable monitor spatially temporally heterogeneous dynamic property complex fluid approach base analysis near field speckle produce light diffusely reflect superficial volume strongly scatter medium periodic modulation incident speckle beam obtain pixel wise ensemble average structure function coefficient measure dynamic activity illustrate application approach follow different stage dry process colloidal thin film access ensemble average dynamic property length scale small micrometer field view
cs,"Convergence of the discrete dipole approximation. II. An extrapolation
  technique to increase the accuracy","  We propose an extrapolation technique that allows accuracy improvement of the
discrete dipole approximation computations. The performance of this technique
was studied empirically based on extensive simulations for 5 test cases using
many different discretizations. The quality of the extrapolation improves with
refining discretization reaching extraordinary performance especially for
cubically shaped particles. A two order of magnitude decrease of error was
demonstrated. We also propose estimates of the extrapolation error, which were
proven to be reliable. Finally we propose a simple method to directly separate
shape and discretization errors and illustrated this for one test case.
",propose extrapolation technique allow accuracy improvement discrete dipole approximation computation performance technique study empirically base extensive simulation 5 test case different discretization quality extrapolation improve refining discretization reach extraordinary performance especially cubically shape particle order magnitude decrease error demonstrate propose estimate extrapolation error prove reliable finally propose simple method directly separate shape discretization error illustrate test case
cs,Non-extensive thermodynamics of 1D systems with long-range interaction,"  A new approach to non-extensive thermodynamical systems with non-additive
energy and entropy is proposed. The main idea of the paper is based on the
statistical matching of the thermodynamical systems with the additive
multi-step Markov chains. This general approach is applied to the Ising spin
chain with long-range interaction between its elements. The asymptotical
expressions for the energy and entropy of the system are derived for the
limiting case of weak interaction. These thermodynamical quantities are found
to be non-proportional to the length of the system (number of its particle).
",new approach non extensive thermodynamical system non additive energy entropy propose main idea paper base statistical matching thermodynamical system additive multi step markov chain general approach apply ising spin chain long range interaction element asymptotical expression energy entropy system derive limiting case weak interaction thermodynamical quantity find non proportional length system number particle
cs,Equation of state for dense hydrogen and plasma phase transition,"  We calculate the equation of state of dense hydrogen within the chemical
picture. Fluid variational theory is generalized for a multi-component system
of molecules, atoms, electrons, and protons. Chemical equilibrium is supposed
for the reactions dissociation and ionization. We identify the region of
thermodynamic instability which is related to the plasma phase transition. The
reflectivity is calculated along the Hugoniot curve and compared with
experimental results. The equation-of-state data is used to calculate the
pressure and temperature profiles for the interior of Jupiter.
",calculate equation state dense hydrogen chemical picture fluid variational theory generalize multi component system molecule atom electron proton chemical equilibrium suppose reaction dissociation ionization identify region thermodynamic instability relate plasma phase transition reflectivity calculate hugoniot curve compare experimental result equation state datum calculate pressure temperature profile interior jupiter
cs,Leaky modes of a left-handed slab,"  Using complex plane analysis we show that left-handed slab may support either
leaky slab waves, which are backward because of negative refraction, or leaky
surface waves, which are backward or forward depending on the propagation
direction of the surface wave itself. Moreover, there is a general connection
between the reflection coefficient of the left-handed slab and the one of the
corresponding right-handed slab (with opposite permittivity and permeability)
so that leaky slab modes are excited for the same angle of incidence of the
impinging beam for both structures. Many negative giant lateral shifts can be
explained by the excitation of these leaky modes.
",complex plane analysis left handed slab support leaky slab wave backward negative refraction leaky surface wave backward forward depend propagation direction surface wave general connection reflection coefficient left handed slab corresponding right handed slab opposite permittivity permeability leaky slab mode excited angle incidence impinge beam structure negative giant lateral shift explain excitation leaky mode
cs,Estimation of experimental data redundancy and related statistics,"  Redundancy of experimental data is the basic statistic from which the
complexity of a natural phenomenon and the proper number of experiments needed
for its exploration can be estimated. The redundancy is expressed by the
entropy of information pertaining to the probability density function of
experimental variables. Since the calculation of entropy is inconvenient due to
integration over a range of variables, an approximate expression for redundancy
is derived that includes only a sum over the set of experimental data about
these variables. The approximation makes feasible an efficient estimation of
the redundancy of data along with the related experimental information and
information cost function. From the experimental information the complexity of
the phenomenon can be simply estimated, while the proper number of experiments
needed for its exploration can be determined from the minimum of the cost
function. The performance of the approximate estimation of these statistics is
demonstrated on two-dimensional normally distributed random data.
",redundancy experimental datum basic statistic complexity natural phenomenon proper number experiment need exploration estimate redundancy express entropy information pertain probability density function experimental variable calculation entropy inconvenient integration range variable approximate expression redundancy derive include sum set experimental datum variable approximation make feasible efficient estimation redundancy datum related experimental information information cost function experimental information complexity phenomenon simply estimate proper number experiment need exploration determine minimum cost function performance approximate estimation statistic demonstrate dimensional normally distribute random datum
cs,"Rich methane premixed laminar flames doped by light unsaturated
  hydrocarbons - Part I : allene and propyne","  The structure of three laminar premixed rich flames has been investigated: a
pure methane flame and two methane flames doped by allene and propyne,
respectively. The gases of the three flames contain 20.9% (molar) of methane
and 33.4% of oxygen, corresponding to an equivalence ratio of 1.25 for the pure
methane flame. In both doped flames, 2.49% of C3H4 was added, corresponding to
a ratio C3H4/CH4 of 12% and an equivalence ratio of 1.55. The three flames have
been stabilized on a burner at a pressure of 6.7 kPa using argon as dilutant,
with a gas velocity at the burner of 36 cm/s at 333 K. The concentration
profiles of stable species were measured by gas chromatography after sampling
with a quartz microprobe. Quantified species included carbon monoxide and
dioxide, methane, oxygen, hydrogen, ethane, ethylene, acetylene, propyne,
allene, propene, propane, 1,2-butadiene, 1,3-butadiene, 1-butene, isobutene,
1-butyne, vinylacetylene, and benzene. The temperature was measured using a
PtRh (6%)-PtRh (30%) thermocouple settled inside the enclosure and ranged from
700 K close to the burner up to 1850 K. In order to model these new results,
some improvements have been made to a mechanism previously developed in our
laboratory for the reactions of C3-C4 unsaturated hydrocarbons. The main
reaction pathways of consumption of allene and propyne and of formation of C6
aromatic species have been derived from flow rate analyses.
","structure laminar premixed rich flame investigate pure methane flame methane flame dope allene propyne respectively gas flame contain 20.9 molar methane 33.4 oxygen correspond equivalence ratio 1.25 pure methane flame dope flame 2.49 c3h4 add correspond ratio c3h4 ch4 12 equivalence ratio 1.55 flame stabilize burner pressure 6.7 kpa argon dilutant gas velocity burner 36 cm s 333 k. concentration profile stable specie measure gas chromatography sample quartz microprobe quantified specie include carbon monoxide dioxide methane oxygen hydrogen ethane ethylene acetylene propyne allene propene propane 1,2 butadiene 1,3 butadiene 1 butene isobutene 1 butyne vinylacetylene benzene temperature measure ptrh 6%)-ptrh 30 thermocouple settle inside enclosure range 700 k close burner 1850 k. order model new result improvement mechanism previously develop laboratory reaction c3 c4 unsaturated hydrocarbon main reaction pathway consumption allene propyne formation c6 aromatic specie derive flow rate analysis"
cs,"On Almost Periodicity Criteria for Morphic Sequences in Some Particular
  Cases","  In some particular cases we give criteria for morphic sequences to be almost
periodic (=uniformly recurrent). Namely, we deal with fixed points of
non-erasing morphisms and with automatic sequences. In both cases a
polynomial-time algorithm solving the problem is found. A result more or less
supporting the conjecture of decidability of the general problem is given.
",particular case criterion morphic sequence periodic (= uniformly recurrent deal fix point non erasing morphism automatic sequence case polynomial time algorithm solve problem find result support conjecture decidability general problem give
cs,"Polarization properties of subwavelength hole arrays consisting of
  rectangular holes","  Influence of hole shape on extraordinary optical transmission was
investigated using hole arrays consisting of rectangular holes with different
aspect ratio. It was found that the transmission could be tuned continuously by
rotating the hole array. Further more, a phase was generated in this process,
and linear polarization states could be changed to elliptical polarization
states. This phase was correlated with the aspect ratio of the holes. An
intuitional model was presented to explain these results.
",influence hole shape extraordinary optical transmission investigate hole array consist rectangular hole different aspect ratio find transmission tune continuously rotate hole array phase generate process linear polarization state change elliptical polarization state phase correlate aspect ratio hole intuitional model present explain result
cs,"Empirical analysis and statistical modeling of attack processes based on
  honeypots","  Honeypots are more and more used to collect data on malicious activities on
the Internet and to better understand the strategies and techniques used by
attackers to compromise target systems. Analysis and modeling methodologies are
needed to support the characterization of attack processes based on the data
collected from the honeypots. This paper presents some empirical analyses based
on the data collected from the Leurr{\'e}.com honeypot platforms deployed on
the Internet and presents some preliminary modeling studies aimed at fulfilling
such objectives.
",honeypot collect datum malicious activity internet well understand strategy technique attacker compromise target system analysis modeling methodology need support characterization attack process base datum collect honeypot paper present empirical analysis base datum collect leurr{\'e}.com honeypot platform deploy internet present preliminary modeling study aim fulfil objective
cs,Failure of the work-Hamiltonian connection for free energy calculations,"  Extensions of statistical mechanics are routinely being used to infer free
energies from the work performed over single-molecule nonequilibrium
trajectories. A key element of this approach is the ubiquitous expression
dW/dt=\partial H(x,t)/ \partial t which connects the microscopic work W
performed by a time-dependent force on the coordinate x with the corresponding
Hamiltonian H(x,t) at time t. Here we show that this connection, as pivotal as
it is, cannot be used to estimate free energy changes. We discuss the
implications of this result for single-molecule experiments and atomistic
molecular simulations and point out possible avenues to overcome these
limitations.
",extensions statistical mechanic routinely infer free energy work perform single molecule nonequilibrium trajectory key element approach ubiquitous expression dw dt=\partial h(x t)/ \partial t connect microscopic work w perform time dependent force coordinate x correspond hamiltonian h(x t time t. connection pivotal estimate free energy change discuss implication result single molecule experiment atomistic molecular simulation point possible avenue overcome limitation
cs,"Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless
  Networks","  This paper investigates the many-to-one throughput capacity (and by symmetry,
one-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has
generally been assumed in prior studies that the many-to-one throughput
capacity is upper-bounded by the link capacity L. Throughput capacity L is not
achievable under 802.11. This paper introduces the notion of ""canonical
networks"", which is a class of regularly-structured networks whose capacities
can be analyzed more easily than unstructured networks. We show that the
throughput capacity of canonical networks under 802.11 has an analytical upper
bound of 3L/4 when the source nodes are two or more hops away from the sink;
and simulated throughputs of 0.690L (0.740L) when the source nodes are many
hops away. We conjecture that 3L/4 is also the upper bound for general
networks. When all links have equal length, 2L/3 can be shown to be the upper
bound for general networks. Our simulations show that 802.11 networks with
random topologies operated with AODV routing can only achieve throughputs far
below the upper bounds. Fortunately, by properly selecting routes near the
gateway (or by properly positioning the relay nodes leading to the gateway) to
fashion after the structure of canonical networks, the throughput can be
improved significantly by more than 150%. Indeed, in a dense network, it is
worthwhile to deactivate some of the relay nodes near the sink judiciously.
",paper investigate throughput capacity symmetry throughput capacity ieee 802.11 multi hop network generally assume prior study throughput capacity upper bounded link capacity l. throughput capacity l achievable 802.11 paper introduce notion canonical network class regularly structure network capacity analyze easily unstructured network throughput capacity canonical network 802.11 analytical upper bind 3l/4 source node hop away sink simulate throughput 0.690l 0.740l source node hop away conjecture 3l/4 upper bind general network link equal length 2l/3 show upper bind general network simulation 802.11 network random topology operate aodv routing achieve throughput far upper bound fortunately properly select route near gateway properly position relay node lead gateway fashion structure canonical network throughput improve significantly 150 dense network worthwhile deactivate relay node near sink judiciously
cs,Visualizing Teleportation,"  A novel way of picturing the processing of quantum information is described,
allowing a direct visualization of teleportation of quantum states and
providing a simple and intuitive understanding of this fascinating phenomenon.
The discussion is aimed at providing physicists a method of explaining
teleportation to non-scientists. The basic ideas of quantum physics are first
explained in lay terms, after which these ideas are used with a graphical
description, out of which teleportation arises naturally.
",novel way picture processing quantum information describe allow direct visualization teleportation quantum state provide simple intuitive understanding fascinating phenomenon discussion aim provide physicist method explain teleportation non scientist basic idea quantum physics explain lay term idea graphical description teleportation arise naturally
cs,"Molecular Synchronization Waves in Arrays of Allosterically Regulated
  Enzymes","  Spatiotemporal pattern formation in a product-activated enzymic reaction at
high enzyme concentrations is investigated. Stochastic simulations show that
catalytic turnover cycles of individual enzymes can become coherent and that
complex wave patterns of molecular synchronization can develop. The analysis
based on the mean-field approximation indicates that the observed patterns
result from the presence of Hopf and wave bifurcations in the considered
system.
",spatiotemporal pattern formation product activate enzymic reaction high enzyme concentration investigate stochastic simulation catalytic turnover cycle individual enzyme coherent complex wave pattern molecular synchronization develop analysis base mean field approximation indicate observed pattern result presence hopf wave bifurcation consider system
cs,"Two-scale structure of the electron dissipation region during
  collisionless magnetic reconnection","  Particle in cell (PIC) simulations of collisionless magnetic reconnection are
presented that demonstrate that the electron dissipation region develops a
distinct two-scale structure along the outflow direction. The length of the
electron current layer is found to decrease with decreasing electron mass,
approaching the ion inertial length for a proton-electron plasma. A surprise,
however, is that the electrons form a high-velocity outflow jet that remains
decoupled from the magnetic field and extends large distances downstream from
the x-line. The rate of reconnection remains fast in very large systems,
independent of boundary conditions and the mass of electrons.
",particle cell pic simulation collisionless magnetic reconnection present demonstrate electron dissipation region develop distinct scale structure outflow direction length electron current layer find decrease decrease electron mass approach ion inertial length proton electron plasma surprise electron form high velocity outflow jet remains decouple magnetic field extend large distance downstream x line rate reconnection remain fast large system independent boundary condition mass electron
cs,Shocks in nonlocal media,"  We investigate the formation of collisionless shocks along the spatial
profile of a gaussian laser beam propagating in nonlocal nonlinear media. For
defocusing nonlinearity the shock survives the smoothing effect of the nonlocal
response, though its dynamics is qualitatively affected by the latter, whereas
for focusing nonlinearity it dominates over filamentation. The patterns
observed in a thermal defocusing medium are interpreted in the framework of our
theory.
",investigate formation collisionless shock spatial profile gaussian laser beam propagate nonlocal nonlinear medium defocuse nonlinearity shock survive smooth effect nonlocal response dynamic qualitatively affect focus nonlinearity dominate filamentation pattern observe thermal defocuse medium interpret framework theory
cs,Nuclear Spin Effects in Optical Lattice Clocks,"  We present a detailed experimental and theoretical study of the effect of
nuclear spin on the performance of optical lattice clocks. With a state-mixing
theory including spin-orbit and hyperfine interactions, we describe the origin
of the $^1S_0$-$^3P_0$ clock transition and the differential g-factor between
the two clock states for alkaline-earth(-like) atoms, using $^{87}$Sr as an
example. Clock frequency shifts due to magnetic and optical fields are
discussed with an emphasis on those relating to nuclear structure. An
experimental determination of the differential g-factor in $^{87}$Sr is
performed and is in good agreement with theory. The magnitude of the tensor
light shift on the clock states is also explored experimentally. State specific
measurements with controlled nuclear spin polarization are discussed as a
method to reduce the nuclear spin-related systematic effects to below
10$^{-17}$ in lattice clocks.
",present detailed experimental theoretical study effect nuclear spin performance optical lattice clock state mix theory include spin orbit hyperfine interaction describe origin $ ^1s_0$-$^3p_0 $ clock transition differential g factor clock state alkaline earth(-like atom $ ^{87}$sr example clock frequency shift magnetic optical field discuss emphasis relate nuclear structure experimental determination differential g factor $ ^{87}$sr perform good agreement theory magnitude tensor light shift clock state explore experimentally state specific measurement control nuclear spin polarization discuss method reduce nuclear spin relate systematic effect 10$^{-17}$ lattice clock
cs,Turbulent Diffusion of Lines and Circulations,"  We study material lines and passive vectors in a model of turbulent flow at
infinite-Reynolds number, the Kraichnan-Kazantsev ensemble of velocities that
are white-noise in time and rough (Hoelder continuous) in space. It is argued
that the phenomenon of ``spontaneous stochasticity'' generalizes to material
lines and that conservation of circulations generalizes to a ``martingale
property'' of the stochastic process of lines.
",study material line passive vector model turbulent flow infinite reynolds number kraichnan kazantsev ensemble velocity white noise time rough hoelder continuous space argue phenomenon ` ` spontaneous stochasticity generalize material line conservation circulation generalize ` ` martingale property stochastic process line
cs,"Interference effects in above-threshold ionization from diatomic
  molecules: determining the internuclear separation","  We calculate angle-resolved above-threshold ionization spectra for diatomic
molecules in linearly polarized laser fields, employing the strong-field
approximation. The interference structure resulting from the individual
contributions of the different scattering scenarios is discussed in detail,
with respect to the dependence on the internuclear distance and molecular
orientation. We show that, in general, the contributions from the processes in
which the electron is freed at one center and rescatters off the other obscure
the interference maxima and minima obtained from single-center processes.
However, around the boundary of the energy regions for which rescattering has a
classical counterpart, such processes play a negligible role and very clear
interference patterns are observed. In such energy regions, one is able to
infer the internuclear distance from the energy difference between adjacent
interference minima.
",calculate angle resolve threshold ionization spectra diatomic molecule linearly polarize laser field employ strong field approximation interference structure result individual contribution different scatter scenario discuss detail respect dependence internuclear distance molecular orientation general contribution process electron free center rescatter obscure interference maxima minima obtain single center process boundary energy region rescattering classical counterpart process play negligible role clear interference pattern observe energy region able infer internuclear distance energy difference adjacent interference minima
cs,"A Low Complexity Algorithm and Architecture for Systematic Encoding of
  Hermitian Codes","  We present an algorithm for systematic encoding of Hermitian codes. For a
Hermitian code defined over GF(q^2), the proposed algorithm achieves a run time
complexity of O(q^2) and is suitable for VLSI implementation. The encoder
architecture uses as main blocks q varying-rate Reed-Solomon encoders and
achieves a space complexity of O(q^2) in terms of finite field multipliers and
memory elements.
",present algorithm systematic encoding hermitian code hermitian code define gf(q^2 propose algorithm achieve run time complexity o(q^2 suitable vlsi implementation encoder architecture use main block q varying rate reed solomon encoder achieve space complexity o(q^2 term finite field multiplier memory element
cs,Photon splitting in a laser field,"  Photon splitting due to vacuum polarization in a laser field is considered.
Using an operator technique, we derive the amplitudes for arbitrary strength,
spectral content and polarization of the laser field. The case of a
monochromatic circularly polarized laser field is studied in detail and the
amplitudes are obtained as three-fold integrals. The asymptotic behavior of the
amplitudes for various limits of interest are investigated also in the case of
a linearly polarized laser field. Using the obtained results, the possibility
of experimental observation of the process is discussed.
",photon splitting vacuum polarization laser field consider operator technique derive amplitude arbitrary strength spectral content polarization laser field case monochromatic circularly polarize laser field study detail amplitude obtain fold integral asymptotic behavior amplitude limit interest investigate case linearly polarize laser field obtain result possibility experimental observation process discuss
cs,On the dragging of light by a rotating medium,"  When light is passing through a rotating medium the optical polarisation is
rotated. Recently it has been reasoned that this rotation applies also to the
transmitted image (Padgett et al. 2006). We examine these two phenomena by
extending an analysis of Player (1976) to general electromagnetic fields. We
find that in this more general case the wave equation inside the rotating
medium has to be amended by a term which is connected to the orbital angular
momentum of the light. We show that optical spin and orbital angular momentum
account respectively for the rotation of the polarisation and the rotation of
the transmitted image.
",light pass rotate medium optical polarisation rotate recently reason rotation apply transmit image padgett et al 2006 examine phenomenon extend analysis player 1976 general electromagnetic field find general case wave equation inside rotating medium amend term connect orbital angular momentum light optical spin orbital angular momentum account respectively rotation polarisation rotation transmit image
cs,Frequency modulation Fourier transform spectroscopy,"  A new method, FM-FTS, combining Frequency Modulation heterodyne laser
spectroscopy and Fourier Transform Spectroscopy is presented. It provides
simultaneous sensitive measurement of absorption and dispersion profiles with
broadband spectral coverage capabilities. Experimental demonstration is made on
the overtone spectrum of C2H2 in the 1.5 $\mu$m region.
",new method fm fts combine frequency modulation heterodyne laser spectroscopy fourier transform spectroscopy present provide simultaneous sensitive measurement absorption dispersion profile broadband spectral coverage capability experimental demonstration overtone spectrum c2h2 1.5 $ \mu$m region
cs,A non-perturbative proof of Bertrand's theorem,"  We discuss an alternative non-perturbative proof of Bertrand's theorem that
leads in a concise way directly to the two allowed fields: the newtonian and
the isotropic harmonic oscillator central fields.
",discuss alternative non perturbative proof bertrand theorem lead concise way directly allow field newtonian isotropic harmonic oscillator central field
cs,"Gravity-induced electric polarization of matter and planetary magnetic
  fields","  This paper has been withdrawn due to copyright reasons.
",paper withdraw copyright reason
cs,Differential Recursion and Differentially Algebraic Functions,"  Moore introduced a class of real-valued ""recursive"" functions by analogy with
Kleene's formulation of the standard recursive functions. While his concise
definition inspired a new line of research on analog computation, it contains
some technical inaccuracies. Focusing on his ""primitive recursive"" functions,
we pin down what is problematic and discuss possible attempts to remove the
ambiguity regarding the behavior of the differential recursion operator on
partial functions. It turns out that in any case the purported relation to
differentially algebraic functions, and hence to Shannon's model of analog
computation, fails.
",moore introduce class real value recursive function analogy kleene formulation standard recursive function concise definition inspire new line research analog computation contain technical inaccuracy focus primitive recursive function pin problematic discuss possible attempt remove ambiguity behavior differential recursion operator partial function turn case purport relation differentially algebraic function shannon model analog computation fail
cs,"Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo
  Codes","  It has been observed that particular rate-1/2 partially systematic parallel
concatenated convolutional codes (PCCCs) can achieve a lower error floor than
that of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can
only be identified by means of an exhaustive search, whilst convergence towards
low bit error probabilities can be problematic when the systematic output of a
rate-1/2 partially systematic PCCC is heavily punctured. In this paper, we
present and study a family of rate-1/2 partially systematic PCCCs, which we
call pseudo-randomly punctured codes. We evaluate their bit error rate
performance and we show that they always yield a lower error floor than that of
their rate-1/3 parent codes. Furthermore, we compare analytic results to
simulations and we demonstrate that their performance converges towards the
error floor region, owning to the moderate puncturing of their systematic
output. Consequently, we propose pseudo-random puncturing as a means of
improving the bandwidth efficiency of a PCCC and simultaneously lowering its
error floor.
",observe particular rate-1/2 partially systematic parallel concatenate convolutional code pccc achieve low error floor rate-1/3 parent code good puncture pattern identify mean exhaustive search whilst convergence low bit error probability problematic systematic output rate-1/2 partially systematic pccc heavily puncture paper present study family rate-1/2 partially systematic pccc pseudo randomly punctured code evaluate bit error rate performance yield low error floor rate-1/3 parent code furthermore compare analytic result simulation demonstrate performance converge error floor region own moderate puncturing systematic output consequently propose pseudo random puncturing means improve bandwidth efficiency pccc simultaneously lower error floor
cs,Lessons Learned from the deployment of a high-interaction honeypot,"  This paper presents an experimental study and the lessons learned from the
observation of the attackers when logged on a compromised machine. The results
are based on a six months period during which a controlled experiment has been
run with a high interaction honeypot. We correlate our findings with those
obtained with a worldwide distributed system of lowinteraction honeypots.
",paper present experimental study lesson learn observation attacker log compromise machine result base month period control experiment run high interaction honeypot correlate finding obtain worldwide distribute system lowinteraction honeypot
cs,"Availability assessment of SunOS/Solaris Unix Systems based on Syslogd
  and wtmpx logfiles : a case study","  This paper presents a measurement-based availability assessment study using
field data collected during a 4-year period from 373 SunOS/Solaris Unix
workstations and servers interconnected through a local area network. We focus
on the estimation of machine uptimes, downtimes and availability based on the
identification of failures that caused total service loss. Data corresponds to
syslogd event logs that contain a large amount of information about the normal
activity of the studied systems as well as their behavior in the presence of
failures. It is widely recognized that the information contained in such event
logs might be incomplete or imperfect. The solution investigated in this paper
to address this problem is based on the use of auxiliary sources of data
obtained from wtmpx files maintained by the SunOS/Solaris Unix operating
system. The results obtained suggest that the combined use of wtmpx and syslogd
log files provides more complete information on the state of the target systems
that is useful to provide availability estimations that better reflect reality.
",paper present measurement base availability assessment study field datum collect 4 year period 373 sunos solaris unix workstation server interconnect local area network focus estimation machine uptime downtime availability base identification failure cause total service loss datum correspond syslogd event log contain large information normal activity study system behavior presence failure widely recognize information contain event log incomplete imperfect solution investigate paper address problem base use auxiliary source datum obtain wtmpx file maintain sunos solaris unix operating system result obtained suggest combined use wtmpx syslogd log file provide complete information state target system useful provide availability estimation well reflect reality
cs,Statistical analysis of weighted networks,"  The purpose of this paper is to assess the statistical characterization of
weighted networks in terms of the generalization of the relevant parameters,
namely average path length, degree distribution and clustering coefficient.
Although the degree distribution and the average path length admit
straightforward generalizations, for the clustering coefficient several
different definitions have been proposed in the literature. We examined the
different definitions and identified the similarities and differences between
them. In order to elucidate the significance of different definitions of the
weighted clustering coefficient, we studied their dependence on the weights of
the connections. For this purpose, we introduce the relative perturbation norm
of the weights as an index to assess the weight distribution. This study
revealed new interesting statistical regularities in terms of the relative
perturbation norm useful for the statistical characterization of weighted
graphs.
",purpose paper assess statistical characterization weight network term generalization relevant parameter average path length degree distribution cluster coefficient degree distribution average path length admit straightforward generalization cluster coefficient different definition propose literature examine different definition identify similarity difference order elucidate significance different definition weighted cluster coefficient study dependence weight connection purpose introduce relative perturbation norm weight index assess weight distribution study reveal new interesting statistical regularity term relative perturbation norm useful statistical characterization weight graph
cs,A High Robustness and Low Cost Model for Cascading Failures,"  We study numerically the cascading failure problem by using artificially
created scale-free networks and the real network structure of the power grid.
The capacity for a vertex is assigned as a monotonically increasing function of
the load (or the betweenness centrality). Through the use of a simple
functional form with two free parameters, revealed is that it is indeed
possible to make networks more robust while spending less cost. We suggest that
our method to prevent cascade by protecting less vertices is particularly
important for the design of more robust real-world networks to cascading
failures.
",study numerically cascade failure problem artificially create scale free network real network structure power grid capacity vertex assign monotonically increase function load betweenness centrality use simple functional form free parameter reveal possible network robust spend cost suggest method prevent cascade protect vertex particularly important design robust real world network cascade failure
cs,"Birth, survival and death of languages by Monte Carlo simulation","  Simulations of physicists for the competition between adult languages since
2003 are reviewed. How many languages are spoken by how many people? How many
languages are contained in various language families? How do language
similarities decay with geographical distance, and what effects do natural
boundaries have? New simulations of bilinguality are given in an appendix.
",simulation physicist competition adult language 2003 review language speak people language contain language family language similarity decay geographical distance effect natural boundary new simulation bilinguality give appendix
cs,"On the Achievable Rate Regions for Interference Channels with Degraded
  Message Sets","  The interference channel with degraded message sets (IC-DMS) refers to a
communication model in which two senders attempt to communicate with their
respective receivers simultaneously through a common medium, and one of the
senders has complete and a priori (non-causal) knowledge about the message
being transmitted by the other. A coding scheme that collectively has
advantages of cooperative coding, collaborative coding, and dirty paper coding,
is developed for such a channel. With resorting to this coding scheme,
achievable rate regions of the IC-DMS in both discrete memoryless and Gaussian
cases are derived, which, in general, include several previously known rate
regions. Numerical examples for the Gaussian case demonstrate that in the
high-interference-gain regime, the derived achievable rate regions offer
considerable improvements over these existing results.
",interference channel degraded message set ic dms refer communication model sender attempt communicate respective receiver simultaneously common medium sender complete priori non causal knowledge message transmit code scheme collectively advantage cooperative coding collaborative coding dirty paper coding develop channel resort code scheme achievable rate region ic dms discrete memoryless gaussian case derive general include previously know rate region numerical example gaussian case demonstrate high interference gain regime derive achievable rate region offer considerable improvement exist result
cs,Eigen Equation of the Nonlinear Spinor,"  How to effectively solve the eigen solutions of the nonlinear spinor field
equation coupling with some other interaction fields is important to understand
the behavior of the elementary particles. In this paper, we derive a simplified
form of the eigen equation of the nonlinear spinor, and then propose a scheme
to solve their numerical solutions. This simplified equation has elegant and
neat structure, which is more convenient for both theoretical analysis and
numerical computation.
",effectively solve eigen solution nonlinear spinor field equation couple interaction field important understand behavior elementary particle paper derive simplified form eigen equation nonlinear spinor propose scheme solve numerical solution simplified equation elegant neat structure convenient theoretical analysis numerical computation
cs,"Intelligent location of simultaneously active acoustic emission sources:
  Part I","  The intelligent acoustic emission locator is described in Part I, while Part
II discusses blind source separation, time delay estimation and location of two
simultaneously active continuous acoustic emission sources.
  The location of acoustic emission on complicated aircraft frame structures is
a difficult problem of non-destructive testing. This article describes an
intelligent acoustic emission source locator. The intelligent locator comprises
a sensor antenna and a general regression neural network, which solves the
location problem based on learning from examples. Locator performance was
tested on different test specimens. Tests have shown that the accuracy of
location depends on sound velocity and attenuation in the specimen, the
dimensions of the tested area, and the properties of stored data. The location
accuracy achieved by the intelligent locator is comparable to that obtained by
the conventional triangulation method, while the applicability of the
intelligent locator is more general since analysis of sonic ray paths is
avoided. This is a promising method for non-destructive testing of aircraft
frame structures by the acoustic emission method.
",intelligent acoustic emission locator describe ii discuss blind source separation time delay estimation location simultaneously active continuous acoustic emission source location acoustic emission complicated aircraft frame structure difficult problem non destructive testing article describe intelligent acoustic emission source locator intelligent locator comprise sensor antenna general regression neural network solve location problem base learn example locator performance test different test specimen test show accuracy location depend sound velocity attenuation speciman dimension test area property store datum location accuracy achieve intelligent locator comparable obtain conventional triangulation method applicability intelligent locator general analysis sonic ray path avoid promising method non destructive testing aircraft frame structure acoustic emission method
cs,Genetic Optimization of Photonic Bandgap Structures,"  We investigate the use of a Genetic Algorithm (GA) to design a set of
photonic crystals (PCs) in one and two dimensions. Our flexible design
methodology allows us to optimize PC structures which are optimized for
specific objectives. In this paper, we report the results of several such
GA-based PC optimizations. We show that the GA performs well even in very
complex design spaces, and therefore has great potential for use as a robust
design tool in present and future applications.
",investigate use genetic algorithm ga design set photonic crystal pc dimension flexible design methodology allow optimize pc structure optimize specific objective paper report result ga base pc optimization ga perform complex design space great potential use robust design tool present future application
cs,"Robust manipulation of electron spin coherence in an ensemble of singly
  charged quantum dots","  Using the recently reported mode locking effect we demonstrate a highly
robust control of electron spin coherence in an ensemble of (In,Ga)As quantum
dots during the single spin coherence time. The spin precession in a transverse
magnetic field can be fully controlled up to 25 K by the parameters of the
exciting pulsed laser protocol such as the pulse train sequence, leading to
adjustable quantum beat bursts in Faraday rotation. Flipping of the electron
spin precession phase was demonstrated by inverting the polarization within a
pulse doublet sequence.
",recently report mode locking effect demonstrate highly robust control electron spin coherence ensemble ga)as quantum dot single spin coherence time spin precession transverse magnetic field fully control 25 k parameter exciting pulse laser protocol pulse train sequence lead adjustable quantum beat burst faraday rotation flip electron spin precession phase demonstrate invert polarization pulse doublet sequence
cs,"Coupling of whispering-gallery modes in size-mismatched microdisk
  photonic molecules","  Mechanisms of whispering-gallery (WG) modes coupling in microdisk photonic
molecules (PMs) with slight and significant size mismatch are numerically
investigated. The results reveal two different scenarios of modes interaction
depending on the degree of this mismatch and offer new insight into how PM
parameters can be tuned to control and modify WG-modes wavelengths and
Q-factors. From a practical point of view, these findings offer a way to
fabricate PM microlaser structures that exhibit low thresholds and directional
emission, and at the same time are more tolerant to fabrication errors than
previously explored coupled-cavity structures composed of identical
microresonators.
",mechanism whispering gallery wg mode couple microdisk photonic molecule pms slight significant size mismatch numerically investigate result reveal different scenario mode interaction depend degree mismatch offer new insight pm parameter tune control modify wg mode wavelength q factor practical point view finding offer way fabricate pm microlaser structure exhibit low threshold directional emission time tolerant fabrication error previously explore couple cavity structure compose identical microresonator
cs,"Capacity of a Multiple-Antenna Fading Channel with a Quantized Precoding
  Matrix","  Given a multiple-input multiple-output (MIMO) channel, feedback from the
receiver can be used to specify a transmit precoding matrix, which selectively
activates the strongest channel modes. Here we analyze the performance of
Random Vector Quantization (RVQ), in which the precoding matrix is selected
from a random codebook containing independent, isotropically distributed
entries. We assume that channel elements are i.i.d. and known to the receiver,
which relays the optimal (rate-maximizing) precoder codebook index to the
transmitter using B bits. We first derive the large system capacity of
beamforming (rank-one precoding matrix) as a function of B, where large system
refers to the limit as B and the number of transmit and receive antennas all go
to infinity with fixed ratios. With beamforming RVQ is asymptotically optimal,
i.e., no other quantization scheme can achieve a larger asymptotic rate. The
performance of RVQ is also compared with that of a simpler reduced-rank scalar
quantization scheme in which the beamformer is constrained to lie in a random
subspace. We subsequently consider a precoding matrix with arbitrary rank, and
approximate the asymptotic RVQ performance with optimal and linear receivers
(matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show
that these approximations accurately predict the performance of finite-size
systems of interest. Given a target spectral efficiency, numerical examples
show that the amount of feedback required by the linear MMSE receiver is only
slightly more than that required by the optimal receiver, whereas the matched
filter can require significantly more feedback.
",give multiple input multiple output mimo channel feedback receiver specify transmit precoding matrix selectively activate strong channel mode analyze performance random vector quantization rvq precoding matrix select random codebook contain independent isotropically distribute entry assume channel element i.i.d know receiver relay optimal rate maximize precoder codebook index transmitter b bit derive large system capacity beamforme rank precoding matrix function b large system refer limit b number transmit receive antenna infinity fix ratio beamforme rvq asymptotically optimal i.e. quantization scheme achieve large asymptotic rate performance rvq compare simple reduced rank scalar quantization scheme beamformer constrain lie random subspace subsequently consider precoding matrix arbitrary rank approximate asymptotic rvq performance optimal linear receiver match filter minimum mean squared error mmse numerical example approximation accurately predict performance finite size system interest give target spectral efficiency numerical example feedback require linear mmse receiver slightly require optimal receiver match filter require significantly feedback
cs,"A general approach to statistical modeling of physical laws:
  nonparametric regression","  Statistical modeling of experimental physical laws is based on the
probability density function of measured variables. It is expressed by
experimental data via a kernel estimator. The kernel is determined objectively
by the scattering of data during calibration of experimental setup. A physical
law, which relates measured variables, is optimally extracted from experimental
data by the conditional average estimator. It is derived directly from the
kernel estimator and corresponds to a general nonparametric regression. The
proposed method is demonstrated by the modeling of a return map of noisy
chaotic data. In this example, the nonparametric regression is used to predict
a future value of chaotic time series from the present one. The mean predictor
error is used in the definition of predictor quality, while the redundancy is
expressed by the mean square distance between data points. Both statistics are
used in a new definition of predictor cost function. From the minimum of the
predictor cost function, a proper number of data in the model is estimated.
",statistical modeling experimental physical law base probability density function measured variable express experimental datum kernel estimator kernel determine objectively scattering datum calibration experimental setup physical law relate measure variable optimally extract experimental datum conditional average estimator derive directly kernel estimator correspond general nonparametric regression propose method demonstrate modeling return map noisy chaotic datum example nonparametric regression predict future value chaotic time series present mean predictor error definition predictor quality redundancy express mean square distance datum point statistic new definition predictor cost function minimum predictor cost function proper number datum model estimate
cs,"Local-field effects in radiatively broadened magneto-dielectric media:
  negative refraction and absorption reduction","  We give a microscopic derivation of the Clausius-Mossotti relations for a
homogeneous and isotropic magneto-dielectric medium consisting of radiatively
broadened atomic oscillators. To this end the diagram series of electromagnetic
propagators is calculated exactly for an infinite bi-cubic lattice of
dielectric and magnetic dipoles for a lattice constant small compared to the
resonance wavelength $\lambda$. Modifications of transition frequencies and
linewidth of the elementary oscillators are taken into account in a
selfconsistent way by a proper incorporation of the singular self-interaction
terms. We show that in radiatively broadened media sufficiently close to the
free-space resonance the real part of the index of refraction approaches the
value -2 in the limit of $\rho \lambda^3 \gg 1$, where $\rho$ is the number
density of scatterers. Since at the same time the imaginary part vanishes as
$1/\rho$ local field effects can have important consequences for realizing
low-loss negative index materials.
",microscopic derivation clausius mossotti relation homogeneous isotropic magneto dielectric medium consist radiatively broaden atomic oscillator end diagram series electromagnetic propagator calculate exactly infinite bi cubic lattice dielectric magnetic dipole lattice constant small compare resonance wavelength $ \lambda$. modification transition frequency linewidth elementary oscillator take account selfconsistent way proper incorporation singular self interaction term radiatively broaden medium sufficiently close free space resonance real index refraction approach value -2 limit $ \rho \lambda^3 \gg 1 $ $ \rho$ number density scatterer time imaginary vanish $ 1/\rho$ local field effect important consequence realize low loss negative index material
cs,Architecture for Pseudo Acausal Evolvable Embedded Systems,"  Advances in semiconductor technology are contributing to the increasing
complexity in the design of embedded systems. Architectures with novel
techniques such as evolvable nature and autonomous behavior have engrossed lot
of attention. This paper demonstrates conceptually evolvable embedded systems
can be characterized basing on acausal nature. It is noted that in acausal
systems, future input needs to be known, here we make a mechanism such that the
system predicts the future inputs and exhibits pseudo acausal nature. An
embedded system that uses theoretical framework of acausality is proposed. Our
method aims at a novel architecture that features the hardware evolability and
autonomous behavior alongside pseudo acausality. Various aspects of this
architecture are discussed in detail along with the limitations.
",advances semiconductor technology contribute increase complexity design embed system architecture novel technique evolvable nature autonomous behavior engross lot attention paper demonstrate conceptually evolvable embed system characterize base acausal nature note acausal system future input need know mechanism system predict future input exhibit pseudo acausal nature embed system use theoretical framework acausality propose method aim novel architecture feature hardware evolability autonomous behavior alongside pseudo acausality aspect architecture discuss detail limitation
cs,A POVM view of the ensemble approach to polarization optics,"  Statistical ensemble formalism of Kim, Mandel and Wolf (J. Opt. Soc. Am. A 4,
433 (1987)) offers a realistic model for characterizing the effect of
stochastic non-image forming optical media on the state of polarization of
transmittedlight. With suitable choice of the Jones ensemble, various Mueller
transformations - some of which have been unknown so far - are deduced. It is
observed that the ensemble approach is formally identical to the positive
operator valued measures (POVM) on the quantum density matrix. This
observation, in combination with the recent suggestion by Ahnert and Payne
(Phys. Rev. A 71, 012330, (2005)) - in the context of generalized quantum
measurement on single photon polarization states - that linear optics elements
can be employed in setting up all possible POVMs, enables us to propose a way
of realizing different types of Mueller devices.
",statistical ensemble formalism kim mandel wolf j. opt soc 4 433 1987 offer realistic model characterize effect stochastic non image form optical medium state polarization transmittedlight suitable choice jones ensemble mueller transformation unknown far deduce observe ensemble approach formally identical positive operator value measure povm quantum density matrix observation combination recent suggestion ahnert payne phys rev. 71 012330 2005 context generalized quantum measurement single photon polarization state linear optic element employ set possible povm enable propose way realize different type mueller device
cs,Reducing SAT to 2-SAT,"  Description of a polynomial time reduction of SAT to 2-SAT of polynomial
size.
",description polynomial time reduction sat 2 sat polynomial size
cs,Optimal Synthesis of Multiple Algorithms,"  In this paper we give a definition of ""algorithm,"" ""finite algorithm,""
""equivalent algorithms,"" and what it means for a single algorithm to dominate a
set of algorithms. We define a derived algorithm which may have a smaller mean
execution time than any of its component algorithms. We give an explicit
expression for the mean execution time (when it exists) of the derived
algorithm. We give several illustrative examples of derived algorithms with two
component algorithms. We include mean execution time solutions for
two-algorithm processors whose joint density of execution times are of several
general forms. For the case in which the joint density for a two-algorithm
processor is a step function, we give a maximum-likelihood estimation scheme
with which to analyze empirical processing time data.
",paper definition algorithm finite algorithm equivalent algorithm mean single algorithm dominate set algorithm define derived algorithm small mean execution time component algorithm explicit expression mean execution time exist derive algorithm illustrative example derive algorithm component algorithm include mean execution time solution algorithm processor joint density execution time general form case joint density algorithm processor step function maximum likelihood estimation scheme analyze empirical processing time datum
cs,Revisiting the Issues On Netflow Sample and Export Performance,"  The high volume of packets and packet rates of traffic on some router links
makes it exceedingly difficult for routers to examine every packet in order to
keep detailed statistics about the traffic which is traversing the router.
Sampling is commonly applied on routers in order to limit the load incurred by
the collection of information that the router has to undertake when evaluating
flow information for monitoring purposes. The sampling process in nearly all
cases is a deterministic process of choosing 1 in every N packets on a
per-interface basis, and then forming the flow statistics based on the
collected sampled statistics. Even though this sampling may not be significant
for some statistics, such as packet rate, others can be severely distorted.
However, it is important to consider the sampling techniques and their relative
accuracy when applied to different traffic patterns. The main disadvantage of
sampling is the loss of accuracy in the collected trace when compared to the
original traffic stream. To date there has not been a detailed analysis of the
impact of sampling at a router in various traffic profiles and flow criteria.
In this paper, we assess the performance of the sampling process as used in
NetFlow in detail, and we discuss some techniques for the compensation of loss
of monitoring detail.
",high volume packet packet rate traffic router link make exceedingly difficult router examine packet order detailed statistic traffic traverse router sampling commonly apply router order limit load incur collection information router undertake evaluate flow information monitor purpose sampling process nearly case deterministic process choose 1 n packet interface basis form flow statistic base collect sample statistic sampling significant statistic packet rate severely distort important consider sample technique relative accuracy apply different traffic pattern main disadvantage sampling loss accuracy collect trace compare original traffic stream date detailed analysis impact sample router traffic profile flow criterion paper assess performance sampling process netflow detail discuss technique compensation loss monitor detail
cs,"Thermal decomposition of norbornane (bicyclo[2.2.1]heptane) dissolved in
  benzene. Experimental study and mechanism investigation","  The thermal decomposition of norbornane (dissolved in benzene) has been
studied in a jet stirred reactor at temperatures between 873 and 973 K, at
residence times ranging from 1 to 4 s and at atmospheric pressure, leading to
conversions from 0.04 to 22.6%. 25 reaction products were identified and
quantified by gas chromatography, amongst which the main ones are hydrogen,
ethylene and 1,3-cyclopentadiene. A mechanism investigation of the thermal
decomposition of the norbornane - benzene binary mixture has been performed.
Reactions involved in the mechanism have been reviewed: unimolecular
initiations 1 by C-C bond scission of norbornane, fate of the generated
diradicals, reactions of transfer and propagation of norbornyl radicals,
reactions of benzene and cross-coupling reactions.
","thermal decomposition norbornane dissolve benzene study jet stir reactor temperature 873 973 k residence time range 1 4 s atmospheric pressure lead conversion 0.04 22.6 25 reaction product identify quantify gas chromatography main one hydrogen ethylene 1,3 cyclopentadiene mechanism investigation thermal decomposition norbornane benzene binary mixture perform reaction involve mechanism review unimolecular initiation 1 c c bond scission norbornane fate generate diradical reaction transfer propagation norbornyl radical reaction benzene cross coupling reaction"
cs,"Laser spectroscopy of hyperfine structure in highly-charged ions: a test
  of QED at high fields","  An overview is presented of laser spectroscopy experiments with cold,
trapped, highly-charged ions, which will be performed at the HITRAP facility at
GSI in Darmstadt (Germany). These high-resolution measurements of ground state
hyperfine splittings will be three orders of magnitude more precise than
previous measurements. Moreover, from a comparison of measurements of the
hyperfine splittings in hydrogen- and lithium-like ions of the same isotope,
QED effects at high electromagnetic fields can be determined within a few
percent. Several candidate ions suited for these laser spectroscopy studies are
presented.
",overview present laser spectroscopy experiment cold trap highly charge ion perform hitrap facility gsi darmstadt germany high resolution measurement ground state hyperfine splitting order magnitude precise previous measurement comparison measurement hyperfine splitting hydrogen- lithium like ion isotope qed effect high electromagnetic field determine percent candidate ion suit laser spectroscopy study present
cs,Collective behavior of stock price movements in an emerging market,"  To investigate the universality of the structure of interactions in different
markets, we analyze the cross-correlation matrix C of stock price fluctuations
in the National Stock Exchange (NSE) of India. We find that this emerging
market exhibits strong correlations in the movement of stock prices compared to
developed markets, such as the New York Stock Exchange (NYSE). This is shown to
be due to the dominant influence of a common market mode on the stock prices.
By comparison, interactions between related stocks, e.g., those belonging to
the same business sector, are much weaker. This lack of distinct sector
identity in emerging markets is explicitly shown by reconstructing the network
of mutually interacting stocks. Spectral analysis of C for NSE reveals that,
the few largest eigenvalues deviate from the bulk of the spectrum predicted by
random matrix theory, but they are far fewer in number compared to, e.g., NYSE.
We show this to be due to the relative weakness of intra-sector interactions
between stocks, compared to the market mode, by modeling stock price dynamics
with a two-factor model. Our results suggest that the emergence of an internal
structure comprising multiple groups of strongly coupled components is a
signature of market development.
",investigate universality structure interaction different market analyze cross correlation matrix c stock price fluctuation national stock exchange nse india find emerge market exhibit strong correlation movement stock price compare developed market new york stock exchange nyse show dominant influence common market mode stock price comparison interaction related stock e.g. belong business sector weak lack distinct sector identity emerge market explicitly show reconstruct network mutually interact stock spectral analysis c nse reveal large eigenvalue deviate bulk spectrum predict random matrix theory far few number compare e.g. nyse relative weakness intra sector interaction stock compare market mode model stock price dynamic factor model result suggest emergence internal structure comprise multiple group strongly couple component signature market development
cs,Electromagnetic wormholes via handlebody constructions,"  Cloaking devices are prescriptions of electrostatic, optical or
electromagnetic parameter fields (conductivity $\sigma(x)$, index of refraction
$n(x)$, or electric permittivity $\epsilon(x)$ and magnetic permeability
$\mu(x)$) which are piecewise smooth on $\mathbb R^3$ and singular on a
hypersurface $\Sigma$, and such that objects in the region enclosed by $\Sigma$
are not detectable to external observation by waves. Here, we give related
constructions of invisible tunnels, which allow electromagnetic waves to pass
between possibly distant points, but with only the ends of the tunnels visible
to electromagnetic imaging. Effectively, these change the topology of space
with respect to solutions of Maxwell's equations, corresponding to attaching a
handlebody to $\mathbb R^3$. The resulting devices thus function as
electromagnetic wormholes.
",cloaking device prescription electrostatic optical electromagnetic parameter field conductivity $ \sigma(x)$ index refraction $ n(x)$ electric permittivity $ \epsilon(x)$ magnetic permeability $ \mu(x)$ piecewise smooth $ \mathbb r^3 $ singular hypersurface $ \sigma$ object region enclose $ \sigma$ detectable external observation wave related construction invisible tunnel allow electromagnetic wave pass possibly distant point end tunnel visible electromagnetic imaging effectively change topology space respect solution maxwell equation correspond attach handlebody $ \mathbb r^3$. result device function electromagnetic wormhole
cs,"On thermal effects in solid state lasers: the case of ytterbium-doped
  materials","  A review of theoretical and experimental studies of thermal effects in
solid-state lasers is presented, with a special focus on diode-pumped
ytterbium-doped materials. A large part of this review provides however general
information applicable to any kind of solid-state laser. Our aim here is not to
make a list of the techniques that have been used to minimize thermal effects,
but instead to give an overview of the theoretical aspects underneath, and give
a state-of-the-art of the tools at the disposal of the laser scientist to
measure thermal effects. After a presentation of some general properties of
Yb-doped materials, we address the issue of evaluating the temperature map in
Yb-doped laser crystals, both theoretically and experimentally. This is the
first step before studying the complex problem of thermal lensing (part III).
We will focus on some newly discussed aspects, like the definition of the
thermo-optic coefficient: we will highlight some misleading interpretations of
thermal lensing experiments due to the use of the dn/dT parameter in a context
where it is not relevant. Part IV will be devoted to a state-of-the-art of
experimental techniques used to measure thermal lensing. Eventually, in part V,
we will give some concrete examples in Yb-doped materials, where their
peculiarities will be pointed out.
",review theoretical experimental study thermal effect solid state laser present special focus diode pump ytterbium dope material large review provide general information applicable kind solid state laser aim list technique minimize thermal effect instead overview theoretical aspect underneath state art tool disposal laser scientist measure thermal effect presentation general property yb dope material address issue evaluate temperature map yb dope laser crystal theoretically experimentally step study complex problem thermal lense iii focus newly discuss aspect like definition thermo optic coefficient highlight misleading interpretation thermal lense experiment use dn dt parameter context relevant iv devoted state art experimental technique measure thermal lense eventually v concrete example yb dope material peculiarity point
cs,"Protein and ionic surfactants - promoters and inhibitors of contact line
  pinning","  We report a new effect of surfactants in pinning a drop contact line,
specifically that lysozyme promotes while lauryl sulfate inhibits pinning. We
explain the pinning disparity assuming difference in wetting: the protein-laden
drop wets a ""clean"" surface and the surfactant-laden drop wets an
auto-precursored surface.
",report new effect surfactant pin drop contact line specifically lysozyme promote lauryl sulfate inhibits pin explain pin disparity assume difference wet protein laden drop wet clean surface surfactant laden drop wet auto precursore surface
cs,Pairwise comparisons of typological profiles (of languages),"  No abstract given; compares pairs of languages from World Atlas of Language
Structures.
",abstract give compare pair language world atlas language structures
cs,"A Rigorous Time-Domain Analysis of Full--Wave Electromagnetic Cloaking
  (Invisibility)","  There is currently a great deal of interest in the theoretical and practical
possibility of cloaking objects from the observation by electromagnetic waves.
The basic idea of these invisibility devices \cite{glu1, glu2, le},\cite{pss1}
is to use anisotropic {\it transformation media} whose permittivity and
permeability $\var^{\lambda\nu}, \mu^{\lambda\nu}$, are obtained from the ones,
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$, of isotropic media, by singular
transformations of coordinates. In this paper we study electromagnetic cloaking
in the time-domain using the formalism of time-dependent scattering theory.
This formalism allows us to settle in an unambiguous way the mathematical
problems posed by the singularities of the inverse of the permittivity and the
permeability of the {\it transformation media} on the boundary of the cloaked
objects. We write Maxwell's equations in Schr\""odinger form with the
electromagnetic propagator playing the role of the Hamiltonian. We prove that
the electromagnetic propagator outside of the cloaked objects is essentially
self-adjoint. Moreover, the unique self-adjoint extension is unitarily
equivalent to the electromagnetic propagator in the medium
$\var_0^{\lambda\nu}, \mu^{\lambda\nu}_0$. Using this fact, and since the
coordinate transformation is the identity outside of a ball, we prove that the
scattering operator is the identity. Our results give a rigorous proof that the
construction of \cite{glu1, glu2, le}, \cite{pss1} perfectly cloaks passive and
active devices from observation by electromagnetic waves. Furthermore, we prove
cloaking for general anisotropic materials. In particular, our results prove
that it is possible to cloak objects inside general crystals.
","currently great deal interest theoretical practical possibility cloak object observation electromagnetic wave basic idea invisibility device \cite{glu1 glu2 le},\cite{pss1 use anisotropic \it transformation medium permittivity permeability $ \var^{\lambda\nu \mu^{\lambda\nu}$ obtain one $ \var_0^{\lambda\nu \mu^{\lambda\nu}_0 $ isotropic medium singular transformation coordinate paper study electromagnetic cloaking time domain formalism time dependent scattering theory formalism allow settle unambiguous way mathematical problem pose singularity inverse permittivity permeability \it transformation medium boundary cloaked object write maxwell equation schr\""odinger form electromagnetic propagator play role hamiltonian prove electromagnetic propagator outside cloaked object essentially self adjoint unique self adjoint extension unitarily equivalent electromagnetic propagator medium $ \var_0^{\lambda\nu \mu^{\lambda\nu}_0$. fact coordinate transformation identity outside ball prove scatter operator identity result rigorous proof construction \cite{glu1 glu2 le \cite{pss1 perfectly cloak passive active device observation electromagnetic wave furthermore prove cloaking general anisotropic material particular result prove possible cloak object inside general crystal"
cs,"Fluctuation-dissipation relation on a Melde string in a turbulent flow,
  considerations on a ""dynamical temperature""","  We report on measurements of the transverse fluctuations of a string in a
turbulent air jet flow. Harmonic modes are excited by the fluctuating drag
force, at different wave-numbers. This simple mechanical probe makes it
possible to measure excitations of the flow at specific scales, averaged over
space and time: it is a scale-resolved, global measurement. We also measure the
dissipation associated to the string motion, and we consider the ratio of the
fluctuations over dissipation (FDR). In an exploratory approach, we investigate
the concept of {\it effective temperature} defined through the FDR. We compare
our observations with other definitions of temperature in turbulence. From the
theory of Kolmogorov (1941), we derive the exponent -11/3 expected for the
spectrum of the fluctuations. This simple model and our experimental results
are in good agreement, over the range of wave-numbers, and Reynolds number
accessible ($74000 \leq Re \leq 170000$).
",report measurement transverse fluctuation string turbulent air jet flow harmonic mode excite fluctuate drag force different wave number simple mechanical probe make possible measure excitation flow specific scale average space time scale resolve global measurement measure dissipation associate string motion consider ratio fluctuation dissipation fdr exploratory approach investigate concept \it effective temperature define fdr compare observation definition temperature turbulence theory kolmogorov 1941 derive exponent -11/3 expect spectrum fluctuation simple model experimental result good agreement range wave number reynolds number accessible $ 74000 \leq \leq 170000 $
cs,"Compounding Fields and Their Quantum Equations in the Trigintaduonion
  Space","  The 32-dimensional compounding fields and their quantum interplays in the
trigintaduonion space can be presented by analogy with octonion and sedenion
electromagnetic, gravitational, strong and weak interactions. In the
trigintaduonion fields which are associated with the electromagnetic,
gravitational, strong and weak interactions, the study deduces some conclusions
of field source particles (quarks and leptons) and intermediate particles which
are consistent with current some sorts of interaction theories. In the
trigintaduonion fields which are associated with the hyper-strong and
strong-weak fields, the paper draws some predicts and conclusions of the field
source particles (sub-quarks) and intermediate particles. The research results
show that there may exist some new particles in the nature.
",32 dimensional compounding field quantum interplay trigintaduonion space present analogy octonion sedenion electromagnetic gravitational strong weak interaction trigintaduonion field associate electromagnetic gravitational strong weak interaction study deduce conclusion field source particle quark lepton intermediate particle consistent current sort interaction theory trigintaduonion field associate hyper strong strong weak field paper draw predict conclusion field source particle sub quark intermediate particle research result exist new particle nature
cs,"Effective conservation of energy and momentum algorithm using switching
  potentials suitable for molecular dynamics simulation of thermodynamical
  systems","  During a crossover via a switching mechanism from one 2-body potential to
another as might be applied in modeling (chemical) reactions in the vicinity of
bond formation, energy violations would occur due to finite step size which
determines the trajectory of the particles relative to the potential
interactions of the unbonded state by numerical (e.g. Verlet) integration. This
problem is overcome by an algorithm which preserves the coordinates of the
system for each move, but corrects for energy discrepancies by ensuring both
energy and momentum conservation in the dynamics. The algorithm is tested for a
hysteresis loop reaction model with an without the implementation of the
algorithm. The tests involve checking the rate of energy flow out of the MD
simulation box; in the equilibrium state, no net rate of flows within
experimental error should be observed. The temperature and pressure of the box
should also be invariant within the range of fluctuation of these quantities.
It is demonstrated that the algorithm satisfies these criteria.
",crossover switching mechanism 2 body potential apply modeling chemical reaction vicinity bond formation energy violation occur finite step size determine trajectory particle relative potential interaction unbonded state numerical e.g. verlet integration problem overcome algorithm preserve coordinate system correct energy discrepancy ensure energy momentum conservation dynamic algorithm test hysteresis loop reaction model implementation algorithm test involve check rate energy flow md simulation box equilibrium state net rate flow experimental error observe temperature pressure box invariant range fluctuation quantity demonstrate algorithm satisfy criterion
cs,"New version announcement for TaylUR, an arbitrary-order diagonal
  automatic differentiation package for Fortran 95","  We present a new version of TaylUR, a Fortran 95 module to automatically
compute the numerical values of a complex-valued function's derivatives with
respect to several variables up to an arbitrary order in each variable, but
excluding mixed derivatives. The new version fixes a potentially serious bug in
the code for exponential-related functions that could corrupt the imaginary
parts of derivatives, as well as being compatible with a wider range of
compilers.
",present new version taylur fortran 95 module automatically compute numerical value complex value function derivative respect variable arbitrary order variable exclude mixed derivative new version fix potentially bug code exponential relate function corrupt imaginary part derivative compatible wide range compiler
cs,"Nonlinear Dynamics of the Phonon Stimulated Emission in Microwave
  Solid-State Resonator of the Nonautonomous Phaser Generator","  The microwave phonon stimulated emission (SE) has been experimentally and
numerically investigated in a nonautonomous microwave acoustic quantum
generator, called also microwave phonon laser or phaser (see previous works
arXiv:cond-mat/0303188 ; arXiv:cond-mat/0402640 ; arXiv:nlin.CG/0703050)
Phenomena of branching and long-time refractority (absence of the reaction on
the external pulses) for deterministic chaotic and regular processes of SE were
observed in experiments with various levels of electromagnetic pumping. At the
pumping level growth, the clearly depined increasing of the number of
coexisting SE states has been observed both in real physical experiments and in
computer simulations. This confirms the analytical estimations of the branching
density in the phase space. The nature of the refractority of SE pulses is
closely connected with the pointed branching and reflects the crises of strange
attractors, i.e. their collisions with unstable periodic components of the
higher branches of SE states in the nonautonomous microwave phonon laser.
",microwave phonon stimulate emission se experimentally numerically investigate nonautonomous microwave acoustic quantum generator call microwave phonon laser phaser previous work arxiv cond mat/0303188 arxiv cond mat/0402640 arxiv nlin cg/0703050 phenomenon branching long time refractority absence reaction external pulse deterministic chaotic regular process se observe experiment level electromagnetic pumping pumping level growth clearly depine increase number coexist se state observe real physical experiment computer simulation confirm analytical estimation branching density phase space nature refractority se pulse closely connect pointed branching reflect crisis strange attractor i.e. collision unstable periodic component high branch se state nonautonomous microwave phonon laser
cs,Semi-spheroidal Quantum Harmonic Oscillator,"  A new single-particle shell model is derived by solving the Schr\""odinger
equation for a semi-spheroidal potential well. Only the negative parity states
of the $Z(z)$ component of the wave function are allowed, so that new magic
numbers are obtained for oblate semi-spheroids, semi-sphere and prolate
semi-spheroids. The semi-spherical magic numbers are identical with those
obtained at the oblate spheroidal superdeformed shape: 2, 6, 14, 26, 44, 68,
100, 140, ... The superdeformed prolate magic numbers of the semi-spheroidal
shape are identical with those obtained at the spherical shape of the
spheroidal harmonic oscillator: 2, 8, 20, 40, 70, 112, 168 ...
","new single particle shell model derive solve schr\""odinger equation semi spheroidal potential negative parity state $ z(z)$ component wave function allow new magic number obtain oblate semi spheroid semi sphere prolate semi spheroid semi spherical magic number identical obtain oblate spheroidal superdeforme shape 2 6 14 26 44 68 100 140 superdeformed prolate magic number semi spheroidal shape identical obtain spherical shape spheroidal harmonic oscillator 2 8 20 40 70 112 168"
cs,P-adic arithmetic coding,"  A new incremental algorithm for data compression is presented. For a sequence
of input symbols algorithm incrementally constructs a p-adic integer number as
an output. Decoding process starts with less significant part of a p-adic
integer and incrementally reconstructs a sequence of input symbols. Algorithm
is based on certain features of p-adic numbers and p-adic norm. p-adic coding
algorithm may be considered as of generalization a popular compression
technique - arithmetic coding algorithms. It is shown that for p = 2 the
algorithm works as integer variant of arithmetic coding; for a special class of
models it gives exactly the same codes as Huffman's algorithm, for another
special model and a specific alphabet it gives Golomb-Rice codes.
",new incremental algorithm datum compression present sequence input symbol algorithm incrementally construct p adic integer number output decode process start significant p adic integer incrementally reconstruct sequence input symbol algorithm base certain feature p adic number p adic norm p adic coding algorithm consider generalization popular compression technique arithmetic code algorithm show p = 2 algorithm work integer variant arithmetic coding special class model give exactly code huffman algorithm special model specific alphabet give golomb rice code
cs,Long-range correlation and multifractality in Bach's Inventions pitches,"  We show that it can be considered some of Bach pitches series as a stochastic
process with scaling behavior. Using multifractal deterend fluctuation analysis
(MF-DFA) method, frequency series of Bach pitches have been analyzed. In this
view we find same second moment exponents (after double profiling) in ranges
(1.7-1.8) in his works. Comparing MF-DFA results of original series to those
for shuffled and surrogate series we can distinguish multifractality due to
long-range correlations and a broad probability density function. Finally we
determine the scaling exponents and singularity spectrum. We conclude fat tail
has more effect in its multifractality nature than long-range correlations.
",consider bach pitch series stochastic process scale behavior multifractal deterend fluctuation analysis mf dfa method frequency series bach pitch analyze view find second moment exponent double profiling range 1.7 1.8 work compare mf dfa result original series shuffled surrogate series distinguish multifractality long range correlation broad probability density function finally determine scale exponent singularity spectrum conclude fat tail effect multifractality nature long range correlation
cs,"Polymerization Force Driven Buckling of Microtubule Bundles Determines
  the Wavelength of Patterns Formed in Tubulin Solutions","  We present a model for the spontaneous formation of a striated pattern in
polymerizing microtubule solutions. It describes the buckling of a single
microtubule (MT) bundle within an elastic network formed by other similarly
aligned and buckling bundles and unaligned MTs. Phase contrast and polarization
microscopy studies of the temporal evolution of the pattern imply that the
polymerization of MTs within the bundles creates the driving compressional
force. Using the measured rate of buckling, the established MT force-velocity
curve and the pattern wavelength, we obtain reasonable estimates for the MT
bundle bending rigidity and the elastic constant of the network. The analysis
implies that the bundles buckle as solid rods.
",present model spontaneous formation striate pattern polymerize microtubule solution describe buckling single microtubule mt bundle elastic network form similarly aligned buckling bundle unaligned mt phase contrast polarization microscopy study temporal evolution pattern imply polymerization mt bundle create drive compressional force measured rate buckling establish mt force velocity curve pattern wavelength obtain reasonable estimate mt bundle bend rigidity elastic constant network analysis imply bundle buckle solid rod
cs,Sparsely-spread CDMA - a statistical mechanics based analysis,"  Sparse Code Division Multiple Access (CDMA), a variation on the standard CDMA
method in which the spreading (signature) matrix contains only a relatively
small number of non-zero elements, is presented and analysed using methods of
statistical physics. The analysis provides results on the performance of
maximum likelihood decoding for sparse spreading codes in the large system
limit. We present results for both cases of regular and irregular spreading
matrices for the binary additive white Gaussian noise channel (BIAWGN) with a
comparison to the canonical (dense) random spreading code.
",sparse code division multiple access cdma variation standard cdma method spread signature matrix contain relatively small number non zero element present analyse method statistical physics analysis provide result performance maximum likelihood decode sparse spread code large system limit present result case regular irregular spreading matrix binary additive white gaussian noise channel biawgn comparison canonical dense random spread code
cs,Refuting the Pseudo Attack on the REESSE1+ Cryptosystem,"  We illustrate through example 1 and 2 that the condition at theorem 1 in [8]
dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does
not hold, namely the condition Z/M - L/Ak < 1/(2 Ak^2) is not sufficient for
f(i) + f(j) = f(k). Illuminate through an analysis and ex.3 that there is a
logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4
to be invalid. Demonstrate through ex.4 and 5 that each or the combination of
qu+1 > qu * D at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) +
f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4
and alg.2 based on table 1 are disordered and wrong logically. Further,
manifest through a repeated experiment and ex.5 that the data at table 2 is
falsified, and the example in [8] is woven elaborately. We explain why Cx = Ax
* W^f(x) (% M) is changed to Cx = (Ax * W^f(x))^d (% M) in REESSE1+ v2.1. To
the signature fraud, we point out that [8] misunderstands the existence of T^-1
and Q^-1 % (M-1), and forging of Q can be easily avoided through moving H.
Therefore, the conclusion of [8] that REESSE1+ is not secure at all (which
connotes that [8] can extract a related private key from any public key in
REESSE1+) is fully incorrect, and as long as the parameter Omega is fitly
selected, REESSE1+ with Cx = Ax * W^f(x) (% M) is secure.
",illustrate example 1 2 condition theorem 1 8 dissatisfie necessity converse proposition fact 1.1 8 hold condition z m l ak < 1/(2 ak^2 sufficient f(i + f(j = f(k illuminate analysis ex.3 logic error deduction fact 1.2 cause fact 1.2 1.3 4 invalid demonstrate ex.4 5 combination qu+1 > qu d fact 4 table 1 fact 2.2 sufficient f(i + f(j = f(k property 1 2 3 4 5 invalid alg.1 base fact 4 alg.2 base table 1 disorder wrong logically manifest repeat experiment ex.5 datum table 2 falsify example 8 weave elaborately explain cx = ax w^f(x m change cx = ax w^f(x))^d m reesse1 + v2.1 signature fraud point 8 misunderstand existence t^-1 q^-1 m-1 forge q easily avoid move h. conclusion 8 reesse1 + secure connote 8 extract relate private key public key reesse1 + fully incorrect long parameter omega fitly select reesse1 + cx = ax w^f(x m secure
cs,"Convergence of the discrete dipole approximation. I. Theoretical
  analysis","  We performed a rigorous theoretical convergence analysis of the discrete
dipole approximation (DDA). We prove that errors in any measured quantity are
bounded by a sum of a linear and quadratic term in the size of a dipole d, when
the latter is in the range of DDA applicability. Moreover, the linear term is
significantly smaller for cubically than for non-cubically shaped scatterers.
Therefore, for small d errors for cubically shaped particles are much smaller
than for non-cubically shaped. The relative importance of the linear term
decreases with increasing size, hence convergence of DDA for large enough
scatterers is quadratic in the common range of d. Extensive numerical
simulations were carried out for a wide range of d. Finally we discuss a number
of new developments in DDA and their consequences for convergence.
",perform rigorous theoretical convergence analysis discrete dipole approximation dda prove error measured quantity bound sum linear quadratic term size dipole d range dda applicability linear term significantly small cubically non cubically shaped scatterer small d error cubically shape particle small non cubically shape relative importance linear term decrease increase size convergence dda large scatterer quadratic common range d. extensive numerical simulation carry wide range d. finally discuss number new development dda consequence convergence
cs,The Einstein-Varicak Correspondence on Relativistic Rigid Rotation,"  The historical significance of the problem of relativistic rigid rotation is
reviewed in light of recently published correspondence between Einstein and the
mathematician Vladimir Varicak from the years 1909 to 1913.
",historical significance problem relativistic rigid rotation review light recently publish correspondence einstein mathematician vladimir varicak year 1909 1913
cs,"Alternative Approaches to the Equilibrium Properties of Hard-Sphere
  Liquids","  An overview of some analytical approaches to the computation of the
structural and thermodynamic properties of single component and multicomponent
hard-sphere fluids is provided. For the structural properties, they yield a
thermodynamically consistent formulation, thus improving and extending the
known analytical results of the Percus-Yevick theory. Approximate expressions
for the contact values of the radial distribution functions and the
corresponding analytical equations of state are also discussed. Extensions of
this methodology to related systems, such as sticky hard spheres and
square-well fluids, as well as its use in connection with the perturbation
theory of fluids are briefly addressed.
",overview analytical approach computation structural thermodynamic property single component multicomponent hard sphere fluid provide structural property yield thermodynamically consistent formulation improve extend know analytical result percus yevick theory approximate expression contact value radial distribution function correspond analytical equation state discuss extension methodology relate system sticky hard sphere square fluid use connection perturbation theory fluid briefly address
cs,Daemons and DAMA: Their Celestial-Mechanics Interrelations,"  The assumption of the capture by the Solar System of the electrically charged
Planckian DM objects (daemons) from the galactic disk is confirmed not only by
the St.Petersburg (SPb) experiments detecting particles with V<30 km/s. Here
the daemon approach is analyzed considering the positive model independent
result of the DAMA/NaI experiment. We explain the maximum in DAMA signals
observed in the May-June period to be associated with the formation behind the
Sun of a trail of daemons that the Sun captures into elongated orbits as it
moves to the apex. The range of significant 2-6-keV DAMA signals fits well the
iodine nuclei elastically knocked out of the NaI(Tl) scintillator by particles
falling on the Earth with V=30-50 km/s from strongly elongated heliocentric
orbits. The half-year periodicity of the slower daemons observed in SPb
originates from the transfer of particles that are deflected through ~90 deg
into near-Earth orbits each time the particles cross the outer reaches of the
Sun which had captured them. Their multi-loop (cross-like) trajectories
traverse many times the Earth's orbit in March and September, which increases
the probability for the particles to enter near-Earth orbits during this time.
Corroboration of celestial mechanics calculations with observations yields
~1e-19 cm2 for the cross section of daemon interaction with the solar matter.
",assumption capture solar system electrically charge planckian dm object daemon galactic disk confirm st. petersburg spb experiment detect particle v<30 km s. daemon approach analyze consider positive model independent result dama nai experiment explain maximum dama signal observe june period associate formation sun trail daemon sun capture elongate orbit move apex range significant 2 6 kev dama signal fit iodine nucleus elastically knock nai(tl scintillator particle fall earth v=30 50 km s strongly elongate heliocentric orbit half year periodicity slow daemon observe spb originate transfer particle deflect ~90 deg near earth orbit time particle cross outer reach sun capture multi loop cross like trajectory traverse time earth orbit march september increase probability particle enter near earth orbit time corroboration celestial mechanic calculation observation yield ~1e-19 cm2 cross section daemon interaction solar matter
cs,"Measurement of the Aerosol Phase Function at the Pierre Auger
  Observatory","  Air fluorescence detectors measure the energy of ultra-high energy cosmic
rays by collecting fluorescence light emitted from nitrogen molecules along the
extensive air shower cascade. To ensure a reliable energy determination, the
light signal needs to be corrected for atmospheric effects, which not only
attenuate the signal, but also produce a non-negligible background component
due to scattered Cherenkov light and multiple-scattered light. The correction
requires regular measurements of the aerosol attenuation length and the aerosol
phase function, defined as the probability of light scattered in a given
direction. At the Pierre Auger Observatory in Malargue, Argentina, the phase
function is measured on an hourly basis using two Aerosol Phase Function (APF)
light sources. These sources direct a UV light beam across the field of view of
the fluorescence detectors; the phase function can be extracted from the image
of the shots in the fluorescence detector cameras. This paper describes the
design, current status, standard operation procedure, and performance of the
APF system at the Pierre Auger Observatory.
",air fluorescence detector measure energy ultra high energy cosmic ray collect fluorescence light emit nitrogen molecule extensive air shower cascade ensure reliable energy determination light signal need correct atmospheric effect attenuate signal produce non negligible background component scatter cherenkov light multiple scatter light correction require regular measurement aerosol attenuation length aerosol phase function define probability light scatter give direction pierre auger observatory malargue argentina phase function measure hourly basis aerosol phase function apf light source source direct uv light beam field view fluorescence detector phase function extract image shot fluorescence detector camera paper describe design current status standard operation procedure performance apf system pierre auger observatory
cs,"Sensor Networks with Random Links: Topology Design for Distributed
  Consensus","  In a sensor network, in practice, the communication among sensors is subject
to:(1) errors or failures at random times; (3) costs; and(2) constraints since
sensors and networks operate under scarce resources, such as power, data rate,
or communication. The signal-to-noise ratio (SNR) is usually a main factor in
determining the probability of error (or of communication failure) in a link.
These probabilities are then a proxy for the SNR under which the links operate.
The paper studies the problem of designing the topology, i.e., assigning the
probabilities of reliable communication among sensors (or of link failures) to
maximize the rate of convergence of average consensus, when the link
communication costs are taken into account, and there is an overall
communication budget constraint. To consider this problem, we address a number
of preliminary issues: (1) model the network as a random topology; (2)
establish necessary and sufficient conditions for mean square sense (mss) and
almost sure (a.s.) convergence of average consensus when network links fail;
and, in particular, (3) show that a necessary and sufficient condition for both
mss and a.s. convergence is for the algebraic connectivity of the mean graph
describing the network topology to be strictly positive. With these results, we
formulate topology design, subject to random link failures and to a
communication cost constraint, as a constrained convex optimization problem to
which we apply semidefinite programming techniques. We show by an extensive
numerical study that the optimal design improves significantly the convergence
speed of the consensus algorithm and can achieve the asymptotic performance of
a non-random network at a fraction of the communication cost.
",sensor network practice communication sensor subject to:(1 error failure random time 3 cost and(2 constraint sensor network operate scarce resource power datum rate communication signal noise ratio snr usually main factor determine probability error communication failure link probability proxy snr link operate paper study problem design topology i.e. assign probability reliable communication sensor link failure maximize rate convergence average consensus link communication cost take account overall communication budget constraint consider problem address number preliminary issue 1 model network random topology 2 establish necessary sufficient condition mean square sense mss sure a.s convergence average consensus network link fail particular 3 necessary sufficient condition mss a.s convergence algebraic connectivity mean graph describe network topology strictly positive result formulate topology design subject random link failure communication cost constraint constrained convex optimization problem apply semidefinite programming technique extensive numerical study optimal design improve significantly convergence speed consensus algorithm achieve asymptotic performance non random network fraction communication cost
cs,Learning from compressed observations,"  The problem of statistical learning is to construct a predictor of a random
variable $Y$ as a function of a related random variable $X$ on the basis of an
i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable
predictors are drawn from some specified class, and the goal is to approach
asymptotically the performance (expected loss) of the best predictor in the
class. We consider the setting in which one has perfect observation of the
$X$-part of the sample, while the $Y$-part has to be communicated at some
finite bit rate. The encoding of the $Y$-values is allowed to depend on the
$X$-values. Under suitable regularity conditions on the admissible predictors,
the underlying family of probability distributions and the loss function, we
give an information-theoretic characterization of achievable predictor
performance in terms of conditional distortion-rate functions. The ideas are
illustrated on the example of nonparametric regression in Gaussian noise.
",problem statistical learning construct predictor random variable $ y$ function relate random variable $ x$ basis i.i.d training sample joint distribution $ x y)$. allowable predictor draw specify class goal approach asymptotically performance expect loss good predictor class consider setting perfect observation $ x$-part sample $ y$-part communicate finite bit rate encoding $ y$-values allow depend $ x$-values suitable regularity condition admissible predictor underlie family probability distribution loss function information theoretic characterization achievable predictor performance term conditional distortion rate function idea illustrate example nonparametric regression gaussian noise
cs,"Astrophysical gyrokinetics: kinetic and fluid turbulent cascades in
  magnetized weakly collisional plasmas","  We present a theoretical framework for plasma turbulence in astrophysical
plasmas (solar wind, interstellar medium, galaxy clusters, accretion disks).
The key assumptions are that the turbulence is anisotropic with respect to the
mean magnetic field and frequencies are low compared to the ion cyclotron
frequency. The energy injected at the outer scale scale has to be converted
into heat, which ultimately cannot be done without collisions. A KINETIC
CASCADE develops that brings the energy to collisional scales both in space and
velocity. Its nature depends on the physics of plasma fluctuations. In each of
the physically distinct scale ranges, the kinetic problem is systematically
reduced to a more tractable set of equations. In the ""inertial range"" above the
ion gyroscale, the kinetic cascade splits into a cascade of Alfvenic
fluctuations, which are governed by the RMHD equations at both the collisional
and collisionless scales, and a passive cascade of compressive fluctuations,
which obey a linear kinetic equation along the moving field lines associated
with the Alfvenic component. In the ""dissipation range"" between the ion and
electron gyroscales, there are again two cascades: the kinetic-Alfven-wave
(KAW) cascade governed by two fluid-like Electron RMHD equations and a passive
phase-space cascade of ion entropy fluctuations. The latter cascade brings the
energy of the inertial-range fluctuations that was damped by collisionless
wave-particle interaction at the ion gyroscale to collisional scales in the
phase space and leads to ion heating. The KAW energy is similarly damped at the
electron gyroscale and converted into electron heat. Kolmogorov-style scaling
relations are derived for these cascades. Astrophysical and space-physical
applications are discussed in detail.
",present theoretical framework plasma turbulence astrophysical plasma solar wind interstellar medium galaxy cluster accretion disk key assumption turbulence anisotropic respect mean magnetic field frequency low compare ion cyclotron frequency energy inject outer scale scale convert heat ultimately collision kinetic cascade develop bring energy collisional scale space velocity nature depend physics plasma fluctuation physically distinct scale range kinetic problem systematically reduce tractable set equation inertial range ion gyroscale kinetic cascade split cascade alfvenic fluctuation govern rmhd equation collisional collisionless scale passive cascade compressive fluctuation obey linear kinetic equation move field line associate alfvenic component dissipation range ion electron gyroscale cascade kinetic alfven wave kaw cascade govern fluid like electron rmhd equation passive phase space cascade ion entropy fluctuation cascade bring energy inertial range fluctuation damp collisionless wave particle interaction ion gyroscale collisional scale phase space lead ion heating kaw energy similarly damp electron gyroscale convert electron heat kolmogorov style scaling relation derive cascade astrophysical space physical application discuss detail
cs,"Analysis of the real estate market in Las Vegas: Bubble, seasonal
  patterns, and prediction of the CSW indexes","  We analyze 27 house price indexes of Las Vegas from Jun. 1983 to Mar. 2005,
corresponding to 27 different zip codes. These analyses confirm the existence
of a real-estate bubble, defined as a price acceleration faster than
exponential, which is found however to be confined to a rather limited time
interval in the recent past from approximately 2003 to mid-2004 and has
progressively transformed into a more normal growth rate comparable to
pre-bubble levels in 2005. There has been no bubble till 2002 except for a
medium-sized surge in 1990. In addition, we have identified a strong yearly
periodicity which provides a good potential for fine-tuned prediction from
month to month. A monthly monitoring using a model that we have developed could
confirm, by testing the intra-year structure, if indeed the market has returned
to ``normal'' or if more turbulence is expected ahead. We predict the evolution
of the indexes one year ahead, which is validated with new data up to Sep.
2006. The present analysis demonstrates the existence of very significant
variations at the local scale, in the sense that the bubble in Las Vegas seems
to have preceded the more global USA bubble and has ended approximately two
years earlier (mid 2004 for Las Vegas compared with mid-2006 for the whole of
the USA).
",analyze 27 house price index las vegas jun. 1983 mar. 2005 correspond 27 different zip code analysis confirm existence real estate bubble define price acceleration fast exponential find confine limited time interval recent past approximately 2003 mid-2004 progressively transform normal growth rate comparable pre bubble level 2005 bubble till 2002 medium sized surge 1990 addition identify strong yearly periodicity provide good potential fine tune prediction month month monthly monitoring model develop confirm test intra year structure market return ` ` normal turbulence expect ahead predict evolution index year ahead validate new datum sep. 2006 present analysis demonstrate existence significant variation local scale sense bubble las vegas precede global usa bubble end approximately year early mid 2004 las vegas compare mid-2006 usa
cs,Simulation of Robustness against Lesions of Cortical Networks,"  Structure entails function and thus a structural description of the brain
will help to understand its function and may provide insights into many
properties of brain systems, from their robustness and recovery from damage, to
their dynamics and even their evolution. Advances in the analysis of complex
networks provide useful new approaches to understanding structural and
functional properties of brain networks. Structural properties of networks
recently described allow their characterization as small-world, random
(exponential) and scale-free. They complement the set of other properties that
have been explored in the context of brain connectivity, such as topology,
hodology, clustering, and hierarchical organization. Here we apply new network
analysis methods to cortical inter-areal connectivity networks for the cat and
macaque brains. We compare these corticocortical fibre networks to benchmark
rewired, small-world, scale-free and random networks, using two analysis
strategies, in which we measure the effects of the removal of nodes and
connections on the structural properties of the cortical networks. The brain
networks' structural decay is in most respects similar to that of scale-free
networks. The results implicate highly connected hub-nodes and bottleneck
connections as structural basis for some of the conditional robustness of brain
systems. This informs the understanding of the development of brain networks'
connectivity.
",structure entail function structural description brain help understand function provide insight property brain system robustness recovery damage dynamic evolution advance analysis complex network provide useful new approach understand structural functional property brain network structural property network recently describe allow characterization small world random exponential scale free complement set property explore context brain connectivity topology hodology clustering hierarchical organization apply new network analysis method cortical inter areal connectivity network cat macaque brain compare corticocortical fibre network benchmark rewire small world scale free random network analysis strategy measure effect removal node connection structural property cortical network brain network structural decay respect similar scale free network result implicate highly connect hub node bottleneck connection structural basis conditional robustness brain system inform understanding development brain network connectivity
cs,Sparsity-certifying Graph Decompositions,"  We describe a new algorithm, the $(k,\ell)$-pebble game with colors, and use
it obtain a characterization of the family of $(k,\ell)$-sparse graphs and
algorithmic solutions to a family of problems concerning tree decompositions of
graphs. Special instances of sparse graphs appear in rigidity theory and have
received increased attention in recent years. In particular, our colored
pebbles generalize and strengthen the previous results of Lee and Streinu and
give a new proof of the Tutte-Nash-Williams characterization of arboricity. We
also present a new decomposition that certifies sparsity based on the
$(k,\ell)$-pebble game with colors. Our work also exposes connections between
pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and
Westermann and Hendrickson.
","describe new algorithm $ k,\ell)$-pebble game color use obtain characterization family $ k,\ell)$-sparse graph algorithmic solution family problem concern tree decomposition graph special instance sparse graph appear rigidity theory receive increase attention recent year particular color pebble generalize strengthen previous result lee streinu new proof tutte nash williams characterization arboricity present new decomposition certify sparsity base $ k,\ell)$-pebble game color work expose connection pebble game algorithm previous sparse graph algorithm gabow gabow westermann hendrickson"
cs,"The evolution of the Earth-Moon system based on the dark matter field
  fluid model","  The evolution of Earth-Moon system is described by the dark matter field
fluid model proposed in the Meeting of Division of Particle and Field 2004,
American Physical Society. The current behavior of the Earth-Moon system agrees
with this model very well and the general pattern of the evolution of the
Moon-Earth system described by this model agrees with geological and fossil
evidence. The closest distance of the Moon to Earth was about 259000 km at 4.5
billion years ago, which is far beyond the Roche's limit. The result suggests
that the tidal friction may not be the primary cause for the evolution of the
Earth-Moon system. The average dark matter field fluid constant derived from
Earth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts
that the Mars's rotation is also slowing with the angular acceleration rate
about -4.38 x 10^(-22) rad s^(-2).
",evolution earth moon system describe dark matter field fluid model propose meeting division particle field 2004 american physical society current behavior earth moon system agree model general pattern evolution moon earth system describe model agree geological fossil evidence close distance moon earth 259000 km 4.5 billion year ago far roche limit result suggest tidal friction primary cause evolution earth moon system average dark matter field fluid constant derive earth moon system datum 4.39 x 10^(-22 s^(-1)m^(-1 model predict mars rotation slow angular acceleration rate -4.38 x 10^(-22 rad s^(-2
cs,"Time and motion in physics: the Reciprocity Principle, relativistic
  invariance of the lengths of rulers and time dilatation","  Ponderable objects moving in free space according to Newton's First Law
constitute both rulers and clocks when one such object is viewed from the rest
frame of another. Together with the Reciprocity Principle this is used to
demonstrate, in both Galilean and special relativity, the invariance of the
measured length of a ruler in motion. The different times: `proper', `improper'
and `apparent' appearing in different formulations of the relativistic time
dilatation relation are discussed and exemplified by experimental applications.
A non-intuitive `length expansion' effect predicted by the Reciprocity
Principle as a necessary consequence of time dilatation is pointed out
",ponderable object move free space accord newton law constitute ruler clock object view rest frame reciprocity principle demonstrate galilean special relativity invariance measure length ruler motion different time ` proper ` improper ` apparent appear different formulation relativistic time dilatation relation discuss exemplify experimental application non intuitive ` length expansion effect predict reciprocity principle necessary consequence time dilatation point
q-bio,The World as Evolving Information,"  This paper discusses the benefits of describing the world as information,
especially in the study of the evolution of life and cognition. Traditional
studies encounter problems because it is difficult to describe life and
cognition in terms of matter and energy, since their laws are valid only at the
physical scale. However, if matter and energy, as well as life and cognition,
are described in terms of information, evolution can be described consistently
as information becoming more complex.
  The paper presents eight tentative laws of information, valid at multiple
scales, which are generalizations of Darwinian, cybernetic, thermodynamic,
psychological, philosophical, and complexity principles. These are further used
to discuss the notions of life, cognition and their evolution.
",paper discuss benefit describe world information especially study evolution life cognition traditional study encounter problem difficult describe life cognition term matter energy law valid physical scale matter energy life cognition describe term information evolution describe consistently information complex paper present tentative law information valid multiple scale generalization darwinian cybernetic thermodynamic psychological philosophical complexity principle discuss notion life cognition evolution
q-bio,Complexities of Human Promoter Sequences,"  By means of the diffusion entropy approach, we detect the scale-invariance
characteristics embedded in the 4737 human promoter sequences. The exponent for
the scale-invariance is in a wide range of $[ {0.3,0.9} ]$, which centered at
$\delta_c = 0.66$. The distribution of the exponent can be separated into left
and right branches with respect to the maximum. The left and right branches are
asymmetric and can be fitted exactly with Gaussian form with different widths,
respectively.
","mean diffusion entropy approach detect scale invariance characteristic embed 4737 human promoter sequence exponent scale invariance wide range $ 0.3,0.9 $ center $ \delta_c = 0.66$. distribution exponent separate left right branch respect maximum left right branch asymmetric fit exactly gaussian form different width respectively"
q-bio,"Evolutionary Neural Gas (ENG): A Model of Self Organizing Network from
  Input Categorization","  Despite their claimed biological plausibility, most self organizing networks
have strict topological constraints and consequently they cannot take into
account a wide range of external stimuli. Furthermore their evolution is
conditioned by deterministic laws which often are not correlated with the
structural parameters and the global status of the network, as it should happen
in a real biological system. In nature the environmental inputs are noise
affected and fuzzy. Which thing sets the problem to investigate the possibility
of emergent behaviour in a not strictly constrained net and subjected to
different inputs. It is here presented a new model of Evolutionary Neural Gas
(ENG) with any topological constraints, trained by probabilistic laws depending
on the local distortion errors and the network dimension. The network is
considered as a population of nodes that coexist in an ecosystem sharing local
and global resources. Those particular features allow the network to quickly
adapt to the environment, according to its dimensions. The ENG model analysis
shows that the net evolves as a scale-free graph, and justifies in a deeply
physical sense- the term gas here used.
",despite claim biological plausibility self organize network strict topological constraint consequently account wide range external stimulus furthermore evolution condition deterministic law correlate structural parameter global status network happen real biological system nature environmental input noise affected fuzzy thing set problem investigate possibility emergent behaviour strictly constrain net subject different input present new model evolutionary neural gas eng topological constraint train probabilistic law depend local distortion error network dimension network consider population node coexist ecosystem share local global resource particular feature allow network quickly adapt environment accord dimension eng model analysis show net evolve scale free graph justifie deeply physical sense- term gas
q-bio,A remark on the number of steady states in a multiple futile cycle,"  The multisite phosphorylation-dephosphorylation cycle is a motif repeatedly
used in cell signaling. This motif itself can generate a variety of dynamic
behaviors like bistability and ultrasensitivity without direct positive
feedbacks. In this paper, we study the number of positive steady states of a
general multisite phosphorylation-dephosphorylation cycle, and how the number
of positive steady states varies by changing the biological parameters. We show
analytically that (1) for some parameter ranges, there are at least n+1 (if n
is even) or n (if n is odd) steady states; (2) there never are more than 2n-1
steady states (in particular, this implies that for n=2, including single
levels of MAPK cascades, there are at most three steady states); (3) for
parameters near the standard Michaelis-Menten quasi-steady state conditions,
there are at most n+1 steady states; and (4) for parameters far from the
standard Michaelis-Menten quasi-steady state conditions, there is at most one
steady state.
",multisite phosphorylation dephosphorylation cycle motif repeatedly cell signal motif generate variety dynamic behavior like bistability ultrasensitivity direct positive feedback paper study number positive steady state general multisite phosphorylation dephosphorylation cycle number positive steady state vary change biological parameter analytically 1 parameter range n+1 n n n odd steady state 2 2n-1 steady state particular imply n=2 include single level mapk cascade steady state 3 parameter near standard michaelis menten quasi steady state condition n+1 steady state 4 parameter far standard michaelis menten quasi steady state condition steady state
q-bio,Parsimony via concensus,"  The parsimony score of a character on a tree equals the number of state
changes required to fit that character onto the tree. We show that for
unordered, reversible characters this score equals the number of tree
rearrangements required to fit the tree onto the character. We discuss
implications of this connection for the debate over the use of consensus trees
or total evidence, and show how it provides a link between incongruence of
characters and recombination.
",parsimony score character tree equal number state change require fit character tree unordered reversible character score equal number tree rearrangement require fit tree character discuss implication connection debate use consensus tree total evidence provide link incongruence character recombination
q-bio,"Emergence of spatiotemporal chaos driven by far-field breakup of spiral
  waves in the plankton ecological systems","  Alexander B. Medvinsky \emph{et al} [A. B. Medvinsky, I. A. Tikhonova, R. R.
Aliev, B.-L. Li, Z.-S. Lin, and H. Malchow, Phys. Rev. E \textbf{64}, 021915
(2001)] and Marcus R. Garvie \emph{et al} [M. R. Garvie and C. Trenchea, SIAM
J. Control. Optim. \textbf{46}, 775-791 (2007)] shown that the minimal
spatially extended reaction-diffusion model of phytoplankton-zooplankton can
exhibit both regular, chaotic behavior, and spatiotemporal patterns in a patchy
environment. Based on that, the spatial plankton model is furtherly
investigated by means of computer simulations and theoretical analysis in the
present paper when its parameters would be expected in the case of mixed
Turing-Hopf bifurcation region. Our results show that the spiral waves exist in
that region and the spatiotemporal chaos emerge, which arise from the far-field
breakup of the spiral waves over large ranges of diffusion coefficients of
phytoplankton and zooplankton. Moreover, the spatiotemporal chaos arising from
the far-field breakup of spiral waves does not gradually involve the whole
space within that region. Our results are confirmed by means of computation
spectra and nonlinear bifurcation of wave trains. Finally, we give some
explanations about the spatially structured patterns from the community level.
",alexander b. medvinsky \emph{et al a. b. medvinsky i. a. tikhonova r. r. aliev b.-l. li z.-s. lin h. malchow phys rev. e \textbf{64 021915 2001 marcus r. garvie \emph{et al m. r. garvie c. trenchea siam j. control optim \textbf{46 775 791 2007 show minimal spatially extend reaction diffusion model phytoplankton zooplankton exhibit regular chaotic behavior spatiotemporal pattern patchy environment base spatial plankton model furtherly investigate mean computer simulation theoretical analysis present paper parameter expect case mixed turing hopf bifurcation region result spiral wave exist region spatiotemporal chaos emerge arise far field breakup spiral wave large range diffusion coefficient phytoplankton zooplankton spatiotemporal chaos arise far field breakup spiral wave gradually involve space region result confirm mean computation spectra nonlinear bifurcation wave train finally explanation spatially structure pattern community level
q-bio,"A Finite Element framework for computation of protein normal modes and
  mechanical response","  A coarse-grained computational procedure based on the Finite Element Method
is proposed to calculate the normal modes and mechanical response of proteins
and their supramolecular assemblies. Motivated by the elastic network model,
proteins are modeled as homogeneous isotropic elastic solids with volume
defined by their solvent-excluded surface. The discretized Finite Element
representation is obtained using a surface simplification algorithm that
facilitates the generation of models of arbitrary prescribed spatial
resolution. The procedure is applied to compute the normal modes of a mutant of
T4 phage lysozyme and of filamentous actin, as well as the critical Euler
buckling load of the latter when subject to axial compression. Results compare
favorably with all-atom normal mode analysis, the Rotation Translation Blocks
procedure, and experiment. The proposed methodology establishes a computational
framework for the calculation of protein mechanical response that facilitates
the incorporation of specific atomic-level interactions into the model,
including aqueous-electrolyte-mediated electrostatic effects. The procedure is
equally applicable to proteins with known atomic coordinates as it is to
electron density maps of proteins, protein complexes, and supramolecular
assemblies of unknown atomic structure.
",coarse grain computational procedure base finite element method propose calculate normal mode mechanical response protein supramolecular assembly motivate elastic network model protein model homogeneous isotropic elastic solid volume define solvent exclude surface discretized finite element representation obtain surface simplification algorithm facilitate generation model arbitrary prescribe spatial resolution procedure apply compute normal mode mutant t4 phage lysozyme filamentous actin critical euler buckling load subject axial compression result compare favorably atom normal mode analysis rotation translation block procedure experiment proposed methodology establish computational framework calculation protein mechanical response facilitate incorporation specific atomic level interaction model include aqueous electrolyte mediate electrostatic effect procedure equally applicable protein know atomic coordinate electron density map protein protein complex supramolecular assembly unknown atomic structure
q-bio,Intricate Knots in Proteins: Function and Evolution,"  A number of recently discovered protein structures incorporate a rather
unexpected structural feature: a knot in the polypeptide backbone. These knots
are extremely rare, but their occurrence is likely connected to protein
function in as yet unexplored fashion. Our analysis of the complete Protein
Data Bank reveals several new knots which, along with previously discovered
ones, can shed light on such connections. In particular, we identify the most
complex knot discovered to date in human ubiquitin hydrolase, and suggest that
its entangled topology protects it against unfolding and degradation by the
proteasome. Knots in proteins are typically preserved across species and
sometimes even across kingdoms. However, we also identify a knot which only
appears in some transcarbamylases while being absent in homologous proteins of
similar structure. The emergence of the knot is accompanied by a shift in the
enzymatic function of the protein. We suggest that the simple insertion of a
short DNA fragment into the gene may suffice to turn an unknotted into a
knotted structure in this protein.
",number recently discover protein structure incorporate unexpected structural feature knot polypeptide backbone knot extremely rare occurrence likely connect protein function unexplored fashion analysis complete protein data bank reveal new knot previously discover one shed light connection particular identify complex knot discover date human ubiquitin hydrolase suggest entangled topology protect unfolding degradation proteasome knot protein typically preserve specie kingdom identify knot appear transcarbamylase absent homologous protein similar structure emergence knot accompany shift enzymatic function protein suggest simple insertion short dna fragment gene suffice turn unknotted knot structure protein
q-bio,Origin of adaptive mutants: a quantum measurement?,"  This is a supplement to the paper arXiv:q-bio/0701050, containing the text of
correspondence sent to Nature in 1990.
",supplement paper arxiv q bio/0701050 contain text correspondence send nature 1990
q-bio,Simulation of Robustness against Lesions of Cortical Networks,"  Structure entails function and thus a structural description of the brain
will help to understand its function and may provide insights into many
properties of brain systems, from their robustness and recovery from damage, to
their dynamics and even their evolution. Advances in the analysis of complex
networks provide useful new approaches to understanding structural and
functional properties of brain networks. Structural properties of networks
recently described allow their characterization as small-world, random
(exponential) and scale-free. They complement the set of other properties that
have been explored in the context of brain connectivity, such as topology,
hodology, clustering, and hierarchical organization. Here we apply new network
analysis methods to cortical inter-areal connectivity networks for the cat and
macaque brains. We compare these corticocortical fibre networks to benchmark
rewired, small-world, scale-free and random networks, using two analysis
strategies, in which we measure the effects of the removal of nodes and
connections on the structural properties of the cortical networks. The brain
networks' structural decay is in most respects similar to that of scale-free
networks. The results implicate highly connected hub-nodes and bottleneck
connections as structural basis for some of the conditional robustness of brain
systems. This informs the understanding of the development of brain networks'
connectivity.
",structure entail function structural description brain help understand function provide insight property brain system robustness recovery damage dynamic evolution advance analysis complex network provide useful new approach understand structural functional property brain network structural property network recently describe allow characterization small world random exponential scale free complement set property explore context brain connectivity topology hodology clustering hierarchical organization apply new network analysis method cortical inter areal connectivity network cat macaque brain compare corticocortical fibre network benchmark rewire small world scale free random network analysis strategy measure effect removal node connection structural property cortical network brain network structural decay respect similar scale free network result implicate highly connect hub node bottleneck connection structural basis conditional robustness brain system inform understanding development brain network connectivity
q-bio,"Molecular Synchronization Waves in Arrays of Allosterically Regulated
  Enzymes","  Spatiotemporal pattern formation in a product-activated enzymic reaction at
high enzyme concentrations is investigated. Stochastic simulations show that
catalytic turnover cycles of individual enzymes can become coherent and that
complex wave patterns of molecular synchronization can develop. The analysis
based on the mean-field approximation indicates that the observed patterns
result from the presence of Hopf and wave bifurcations in the considered
system.
",spatiotemporal pattern formation product activate enzymic reaction high enzyme concentration investigate stochastic simulation catalytic turnover cycle individual enzyme coherent complex wave pattern molecular synchronization develop analysis base mean field approximation indicate observed pattern result presence hopf wave bifurcation consider system
q-bio,"Optimal stimulus and noise distributions for information transmission
  via suprathreshold stochastic resonance","  Suprathreshold stochastic resonance (SSR) is a form of noise enhanced signal
transmission that occurs in a parallel array of independently noisy identical
threshold nonlinearities, including model neurons. Unlike most forms of
stochastic resonance, the output response to suprathreshold random input
signals of arbitrary magnitude is improved by the presence of even small
amounts of noise. In this paper the information transmission performance of SSR
in the limit of a large array size is considered. Using a relationship between
Shannon's mutual information and Fisher information, a sufficient condition for
optimality, i.e. channel capacity, is derived. It is shown that capacity is
achieved when the signal distribution is Jeffrey's prior, as formed from the
noise distribution, or when the noise distribution depends on the signal
distribution via a cosine relationship. These results provide theoretical
verification and justification for previous work in both computational
neuroscience and electronics.
",suprathreshold stochastic resonance ssr form noise enhance signal transmission occur parallel array independently noisy identical threshold nonlinearitie include model neuron unlike form stochastic resonance output response suprathreshold random input signal arbitrary magnitude improve presence small amount noise paper information transmission performance ssr limit large array size consider relationship shannon mutual information fisher information sufficient condition optimality i.e. channel capacity derive show capacity achieve signal distribution jeffrey prior form noise distribution noise distribution depend signal distribution cosine relationship result provide theoretical verification justification previous work computational neuroscience electronic
q-bio,Behavioral response to strong aversive stimuli: A neurodynamical model,"  In this paper a theoretical model of functioning of a neural circuit during a
behavioral response has been proposed. A neural circuit can be thought of as a
directed multigraph whose each vertex is a neuron and each edge is a synapse.
It has been assumed in this paper that the behavior of such circuits is
manifested through the collective behavior of neurons belonging to that
circuit. Behavioral information of each neuron is contained in the coefficients
of the fast Fourier transform (FFT) over the output spike train. Those
coefficients form a vector in a multidimensional vector space. Behavioral
dynamics of a neuronal network in response to strong aversive stimuli has been
studied in a vector space in which a suitable pseudometric has been defined.
The neurodynamical model of network behavior has been formulated in terms of
existing memory, synaptic plasticity and feelings. The model has an analogy in
classical electrostatics, by which the notion of force and potential energy has
been introduced. Since the model takes input from each neuron in a network and
produces a behavior as the output, it would be extremely difficult or may even
be impossible to implement. But with the help of the model a possible
explanation for an hitherto unexplained neurological observation in human brain
has been offered. The model is compatible with a recent model of sequential
behavioral dynamics. The model is based on electrophysiology, but its relevance
to hemodynamics has been outlined.
",paper theoretical model functioning neural circuit behavioral response propose neural circuit think direct multigraph vertex neuron edge synapse assume paper behavior circuit manifest collective behavior neuron belong circuit behavioral information neuron contain coefficient fast fourier transform fft output spike train coefficient form vector multidimensional vector space behavioral dynamic neuronal network response strong aversive stimulus study vector space suitable pseudometric define neurodynamical model network behavior formulate term exist memory synaptic plasticity feeling model analogy classical electrostatic notion force potential energy introduce model take input neuron network produce behavior output extremely difficult impossible implement help model possible explanation hitherto unexplained neurological observation human brain offer model compatible recent model sequential behavioral dynamic model base electrophysiology relevance hemodynamic outline
q-bio,Evolutionary games on minimally structured populations,"  Population structure induced by both spatial embedding and more general
networks of interaction, such as model social networks, have been shown to have
a fundamental effect on the dynamics and outcome of evolutionary games. These
effects have, however, proved to be sensitive to the details of the underlying
topology and dynamics. Here we introduce a minimal population structure that is
described by two distinct hierarchical levels of interaction. We believe this
model is able to identify effects of spatial structure that do not depend on
the details of the topology. We derive the dynamics governing the evolution of
a system starting from fundamental individual level stochastic processes
through two successive meanfield approximations. In our model of population
structure the topology of interactions is described by only two parameters: the
effective population size at the local scale and the relative strength of local
dynamics to global mixing. We demonstrate, for example, the existence of a
continuous transition leading to the dominance of cooperation in populations
with hierarchical levels of unstructured mixing as the benefit to cost ratio
becomes smaller then the local population size. Applying our model of spatial
structure to the repeated prisoner's dilemma we uncover a novel and
counterintuitive mechanism by which the constant influx of defectors sustains
cooperation. Further exploring the phase space of the repeated prisoner's
dilemma and also of the ""rock-paper-scissor"" game we find indications of rich
structure and are able to reproduce several effects observed in other models
with explicit spatial embedding, such as the maintenance of biodiversity and
the emergence of global oscillations.
",population structure induce spatial embed general network interaction model social network show fundamental effect dynamic outcome evolutionary game effect prove sensitive detail underlie topology dynamic introduce minimal population structure describe distinct hierarchical level interaction believe model able identify effect spatial structure depend detail topology derive dynamic govern evolution system start fundamental individual level stochastic process successive meanfield approximation model population structure topology interaction describe parameter effective population size local scale relative strength local dynamic global mixing demonstrate example existence continuous transition lead dominance cooperation population hierarchical level unstructured mixing benefit cost ratio small local population size apply model spatial structure repeat prisoner dilemma uncover novel counterintuitive mechanism constant influx defector sustain cooperation explore phase space repeat prisoner dilemma rock paper scissor game find indication rich structure able reproduce effect observe model explicit spatial embed maintenance biodiversity emergence global oscillation
q-bio,"Symmetries by base substitutions in the genetic code predict 2' or 3'
  aminoacylation of tRNAs","  This letter reports complete sets of two-fold symmetries between partitions
of the universal genetic code. By substituting bases at each position of the
codons according to a fixed rule, it happens that properties of the degeneracy
pattern or of tRNA aminoacylation specificity are exchanged.
",letter report complete set fold symmetry partition universal genetic code substitute basis position codon accord fix rule happen property degeneracy pattern trna aminoacylation specificity exchange
q-bio,Annealed importance sampling of dileucine peptide,"  Annealed importance sampling is a means to assign equilibrium weights to a
nonequilibrium sample that was generated by a simulated annealing protocol. The
weights may then be used to calculate equilibrium averages, and also serve as
an ``adiabatic signature'' of the chosen cooling schedule. In this paper we
demonstrate the method on the 50-atom dileucine peptide, showing that
equilibrium distributions are attained for manageable cooling schedules. For
this system, as naively implemented here, the method is modestly more efficient
than constant temperature simulation. However, the method is worth considering
whenever any simulated heating or cooling is performed (as is often done at the
beginning of a simulation project, or during an NMR structure calculation), as
it is simple to implement and requires minimal additional CPU expense.
Furthermore, the naive implementation presented here can be improved.
",anneal importance sampling means assign equilibrium weight nonequilibrium sample generate simulated anneal protocol weight calculate equilibrium average serve ` ` adiabatic signature choose cool schedule paper demonstrate method 50 atom dileucine peptide show equilibrium distribution attain manageable cool schedule system naively implement method modestly efficient constant temperature simulation method worth consider simulated heating cooling perform beginning simulation project nmr structure calculation simple implement require minimal additional cpu expense furthermore naive implementation present improve
q-bio,"Quantitative Resolution to some ""Absolute Discrepancies"" in Cancer
  Theories: a View from Phage lambda Genetic Switch","  Is it possible to understand cancer? Or more specifically, is it possible to
understand cancer from genetic side? There already many answers in literature.
The most optimistic one has claimed that it is mission-possible. Duesberg and
his colleagues reviewed the impressive amount of research results on cancer
accumulated over 100 years. It confirms the a general opinion that considering
all available experimental results and clinical observations there is no cancer
theory without major difficulties, including the prevailing gene-based cancer
theories. They have then listed 9 ""absolute discrepancies"" for such cancer
theory. In this letter the quantitative evidence against one of their major
reasons for dismissing mutation cancer theory, by both in vivo experiment and a
first principle computation, is explicitly pointed out.
",possible understand cancer specifically possible understand cancer genetic answer literature optimistic claim mission possible duesberg colleague review impressive research result cancer accumulate 100 year confirm general opinion consider available experimental result clinical observation cancer theory major difficulty include prevail gene base cancer theory list 9 absolute discrepancy cancer theory letter quantitative evidence major reason dismiss mutation cancer theory vivo experiment principle computation explicitly point
q-bio,"An individual based model with global competition interaction:
  fluctuations effects in pattern formation","  We present some numerical results obtained from a simple individual based
model that describes clustering of organisms caused by competition. Our aim is
to show how, even when a deterministic description developed for continuum
models predicts no pattern formation, an individual based model displays well
defined patterns, as a consequence of fluctuations effects caused by the
discrete nature of the interacting agents.
",present numerical result obtain simple individual base model describe cluster organism cause competition aim deterministic description develop continuum model predict pattern formation individual base model display define pattern consequence fluctuation effect cause discrete nature interact agent
q-bio,Velocity oscillations in actin-based motility,"  We present a simple and generic theoretical description of actin-based
motility, where polymerization of filaments maintains propulsion. The dynamics
is driven by polymerization kinetics at the filaments' free ends, crosslinking
of the actin network, attachment and detachment of filaments to the obstacle
interfaces and entropic forces. We show that spontaneous oscillations in the
velocity emerge in a broad range of parameter values, and compare our findings
with experiments.
",present simple generic theoretical description actin base motility polymerization filament maintain propulsion dynamic drive polymerization kinetic filament free end crosslinke actin network attachment detachment filament obstacle interface entropic force spontaneous oscillation velocity emerge broad range parameter value compare finding experiment
q-bio,Optimal flexibility for conformational transitions in macromolecules,"  Conformational transitions in macromolecular complexes often involve the
reorientation of lever-like structures. Using a simple theoretical model, we
show that the rate of such transitions is drastically enhanced if the lever is
bendable, e.g. at a localized ""hinge''. Surprisingly, the transition is fastest
with an intermediate flexibility of the hinge. In this intermediate regime, the
transition rate is also least sensitive to the amount of ""cargo'' attached to
the lever arm, which could be exploited by molecular motors. To explain this
effect, we generalize the Kramers-Langer theory for multi-dimensional barrier
crossing to configuration dependent mobility matrices.
",conformational transition macromolecular complex involve reorientation lever like structure simple theoretical model rate transition drastically enhance lever bendable e.g. localized hinge surprisingly transition fast intermediate flexibility hinge intermediate regime transition rate sensitive cargo attach lever arm exploit molecular motor explain effect generalize kramers langer theory multi dimensional barrier crossing configuration dependent mobility matrix
q-bio,"AFM Imaging of SWI/SNF action: mapping the nucleosome remodeling and
  sliding","  We propose a combined experimental (Atomic Force Microscopy) and theoretical
study of the structural and dynamical properties of nucleosomes. In contrast to
biochemical approaches, this method allows to determine simultaneously the DNA
complexed length distribution and nucleosome position in various contexts.
First, we show that differences in the nucleo-proteic structure observed
between conventional H2A and H2A.Bbd variant nucleosomes induce quantitative
changes in the in the length distribution of DNA complexed with histones. Then,
the sliding action of remodeling complex SWI/SNF is characterized through the
evolution of the nucleosome position and wrapped DNA length mapping. Using a
linear energetic model for the distribution of DNA complexed length, we extract
the net wrapping energy of DNA onto the histone octamer, and compare it to
previous studies.
",propose combine experimental atomic force microscopy theoretical study structural dynamical property nucleosome contrast biochemical approach method allow determine simultaneously dna complexe length distribution nucleosome position contexts difference nucleo proteic structure observe conventional h2a h2a.bbd variant nucleosome induce quantitative change length distribution dna complexe histone slide action remodel complex swi snf characterize evolution nucleosome position wrap dna length mapping linear energetic model distribution dna complexe length extract net wrap energy dna histone octamer compare previous study
q-bio,"A practical guide to stochastic simulations of reaction-diffusion
  processes","  A practical introduction to stochastic modelling of reaction-diffusion
processes is presented. No prior knowledge of stochastic simulations is
assumed. The methods are explained using illustrative examples. The article
starts with the classical Gillespie algorithm for the stochastic modelling of
chemical reactions. Then stochastic algorithms for modelling molecular
diffusion are given. Finally, basic stochastic reaction-diffusion methods are
presented. The connections between stochastic simulations and deterministic
models are explained and basic mathematical tools (e.g. chemical master
equation) are presented. The article concludes with an overview of more
advanced methods and problems.
",practical introduction stochastic modelling reaction diffusion process present prior knowledge stochastic simulation assume method explain illustrative example article start classical gillespie algorithm stochastic modelling chemical reaction stochastic algorithm model molecular diffusion give finally basic stochastic reaction diffusion method present connection stochastic simulation deterministic model explain basic mathematical tool e.g. chemical master equation present article conclude overview advanced method problem
q-bio,Fast recursive filters for simulating nonlinear dynamic systems,"  A fast and accurate computational scheme for simulating nonlinear dynamic
systems is presented. The scheme assumes that the system can be represented by
a combination of components of only two different types: first-order low-pass
filters and static nonlinearities. The parameters of these filters and
nonlinearities may depend on system variables, and the topology of the system
may be complex, including feedback. Several examples taken from neuroscience
are given: phototransduction, photopigment bleaching, and spike generation
according to the Hodgkin-Huxley equations. The scheme uses two slightly
different forms of autoregressive filters, with an implicit delay of zero for
feedforward control and an implicit delay of half a sample distance for
feedback control. On a fairly complex model of the macaque retinal horizontal
cell it computes, for a given level of accuracy, 1-2 orders of magnitude faster
than 4th-order Runge-Kutta. The computational scheme has minimal memory
requirements, and is also suited for computation on a stream processor, such as
a GPU (Graphical Processing Unit).
",fast accurate computational scheme simulate nonlinear dynamic system present scheme assume system represent combination component different type order low pass filter static nonlinearitie parameter filter nonlinearitie depend system variable topology system complex include feedback example take neuroscience give phototransduction photopigment bleaching spike generation accord hodgkin huxley equation scheme use slightly different form autoregressive filter implicit delay zero feedforward control implicit delay half sample distance feedback control fairly complex model macaque retinal horizontal cell compute give level accuracy 1 2 order magnitude fast 4th order runge kutta computational scheme minimal memory requirement suit computation stream processor gpu graphical processing unit
q-bio,Stochastic fluctuations in metabolic pathways,"  Fluctuations in the abundance of molecules in the living cell may affect its
growth and well being. For regulatory molecules (e.g., signaling proteins or
transcription factors), fluctuations in their expression can affect the levels
of downstream targets in a network. Here, we develop an analytic framework to
investigate the phenomenon of noise correlation in molecular networks.
Specifically, we focus on the metabolic network, which is highly inter-linked,
and noise properties may constrain its structure and function. Motivated by the
analogy between the dynamics of a linear metabolic pathway and that of the
exactly soluable linear queueing network or, alternatively, a mass transfer
system, we derive a plethora of results concerning fluctuations in the
abundance of intermediate metabolites in various common motifs of the metabolic
network. For all but one case examined, we find the steady-state fluctuation in
different nodes of the pathways to be effectively uncorrelated. Consequently,
fluctuations in enzyme levels only affect local properties and do not propagate
elsewhere into metabolic networks, and intermediate metabolites can be freely
shared by different reactions. Our approach may be applicable to study
metabolic networks with more complex topologies, or protein signaling networks
which are governed by similar biochemical reactions. Possible implications for
bioinformatic analysis of metabolimic data are discussed.
",fluctuations abundance molecule live cell affect growth regulatory molecule e.g. signal protein transcription factor fluctuation expression affect level downstream target network develop analytic framework investigate phenomenon noise correlation molecular network specifically focus metabolic network highly inter link noise property constrain structure function motivate analogy dynamic linear metabolic pathway exactly soluable linear queue network alternatively mass transfer system derive plethora result concern fluctuation abundance intermediate metabolite common motif metabolic network case examine find steady state fluctuation different node pathway effectively uncorrelated consequently fluctuation enzyme level affect local property propagate metabolic network intermediate metabolite freely share different reaction approach applicable study metabolic network complex topology protein signal network govern similar biochemical reaction possible implication bioinformatic analysis metabolimic datum discuss
q-bio,Holographic bound and protein linguistics,"  The holographic bound in physics constrains the complexity of life. The
finite storage capability of information in the observable universe requires
the protein linguistics in the evolution of life. We find that the evolution of
genetic code determines the variance of amino acid frequencies and genomic GC
content among species. The elegant linguistic mechanism is confirmed by the
experimental observations based on all known entire proteomes.
",holographic bind physics constrain complexity life finite storage capability information observable universe require protein linguistic evolution life find evolution genetic code determine variance amino acid frequency genomic gc content specie elegant linguistic mechanism confirm experimental observation base know entire proteome
q-bio,Unifying Evolutionary and Network Dynamics,"  Many important real-world networks manifest ""small-world"" properties such as
scale-free degree distributions, small diameters, and clustering. The most
common model of growth for these networks is ""preferential attachment"", where
nodes acquire new links with probability proportional to the number of links
they already have. We show that preferential attachment is a special case of
the process of molecular evolution. We present a new single-parameter model of
network growth that unifies varieties of preferential attachment with the
quasispecies equation (which models molecular evolution), and also with the
Erdos-Renyi random graph model. We suggest some properties of evolutionary
models that might be applied to the study of networks. We also derive the form
of the degree distribution resulting from our algorithm, and we show through
simulations that the process also models aspects of network growth. The
unification allows mathematical machinery developed for evolutionary dynamics
to be applied in the study of network dynamics, and vice versa.
",important real world network manifest small world property scale free degree distribution small diameter cluster common model growth network preferential attachment node acquire new link probability proportional number link preferential attachment special case process molecular evolution present new single parameter model network growth unify variety preferential attachment quasispecie equation model molecular evolution erdos renyi random graph model suggest property evolutionary model apply study network derive form degree distribution result algorithm simulation process model aspect network growth unification allow mathematical machinery develop evolutionary dynamic apply study network dynamic vice versa
q-bio,Bone Cancer Rates in Dinosaurs Compared with Modern Vertebrates,"  Data on the prevalence of bone cancer in dinosaurs is available from past
radiological examination of preserved bones. We statistically test this data
for consistency with rates extrapolated from information on bone cancer in
modern vertebrates, and find that there is no evidence of a different rate.
Thus, this test provides no support for a possible role of ionizing radiation
in the K-T extinction event.
",datum prevalence bone cancer dinosaur available past radiological examination preserve bone statistically test datum consistency rate extrapolate information bone cancer modern vertebrate find evidence different rate test provide support possible role ionize radiation k t extinction event
q-bio,"Evolution favors protein mutational robustness in sufficiently large
  populations","  BACKGROUND: An important question is whether evolution favors properties such
as mutational robustness or evolvability that do not directly benefit any
individual, but can influence the course of future evolution. Functionally
similar proteins can differ substantially in their robustness to mutations and
capacity to evolve new functions, but it has remained unclear whether any of
these differences might be due to evolutionary selection for these properties.
  RESULTS: Here we use laboratory experiments to demonstrate that evolution
favors protein mutational robustness if the evolving population is sufficiently
large. We neutrally evolve cytochrome P450 proteins under identical selection
pressures and mutation rates in populations of different sizes, and show that
proteins from the larger and thus more polymorphic population tend towards
higher mutational robustness. Proteins from the larger population also evolve
greater stability, a biophysical property that is known to enhance both
mutational robustness and evolvability. The excess mutational robustness and
stability is well described by existing mathematical theories, and can be
quantitatively related to the way that the proteins occupy their neutral
network.
  CONCLUSIONS: Our work is the first experimental demonstration of the general
tendency of evolution to favor mutational robustness and protein stability in
highly polymorphic populations. We suggest that this phenomenon may contribute
to the mutational robustness and evolvability of viruses and bacteria that
exist in large populations.
",background important question evolution favor property mutational robustness evolvability directly benefit individual influence course future evolution functionally similar protein differ substantially robustness mutation capacity evolve new function remain unclear difference evolutionary selection property result use laboratory experiment demonstrate evolution favor protein mutational robustness evolve population sufficiently large neutrally evolve cytochrome p450 protein identical selection pressure mutation rate population different size protein large polymorphic population tend high mutational robustness protein large population evolve great stability biophysical property know enhance mutational robustness evolvability excess mutational robustness stability describe exist mathematical theory quantitatively relate way protein occupy neutral network conclusion work experimental demonstration general tendency evolution favor mutational robustness protein stability highly polymorphic population suggest phenomenon contribute mutational robustness evolvability virus bacteria exist large population
q-bio,On restrictions of balanced 2-interval graphs,"  The class of 2-interval graphs has been introduced for modelling scheduling
and allocation problems, and more recently for specific bioinformatic problems.
Some of those applications imply restrictions on the 2-interval graphs, and
justify the introduction of a hierarchy of subclasses of 2-interval graphs that
generalize line graphs: balanced 2-interval graphs, unit 2-interval graphs, and
(x,x)-interval graphs. We provide instances that show that all the inclusions
are strict. We extend the NP-completeness proof of recognizing 2-interval
graphs to the recognition of balanced 2-interval graphs. Finally we give hints
on the complexity of unit 2-interval graphs recognition, by studying
relationships with other graph classes: proper circular-arc, quasi-line graphs,
K_{1,5}-free graphs, ...
","class 2 interval graph introduce modelling scheduling allocation problem recently specific bioinformatic problem application imply restriction 2 interval graph justify introduction hierarchy subclass 2 interval graph generalize line graph balanced 2 interval graph unit 2 interval graph x x)-interval graph provide instance inclusion strict extend np completeness proof recognize 2 interval graph recognition balanced 2 interval graph finally hint complexity unit 2 interval graph recognition study relationship graph class proper circular arc quasi line graph k_{1,5}-free graph"
q-bio,"Phylogenetic mixtures on a single tree can mimic a tree of another
  topology","  Phylogenetic mixtures model the inhomogeneous molecular evolution commonly
observed in data. The performance of phylogenetic reconstruction methods where
the underlying data is generated by a mixture model has stimulated considerable
recent debate. Much of the controversy stems from simulations of mixture model
data on a given tree topology for which reconstruction algorithms output a tree
of a different topology; these findings were held up to show the shortcomings
of particular tree reconstruction methods. In so doing, the underlying
assumption was that mixture model data on one topology can be distinguished
from data evolved on an unmixed tree of another topology given enough data and
the ``correct'' method. Here we show that this assumption can be false. For
biologists our results imply that, for example, the combined data from two
genes whose phylogenetic trees differ only in terms of branch lengths can
perfectly fit a tree of a different topology.
",phylogenetic mixture model inhomogeneous molecular evolution commonly observe datum performance phylogenetic reconstruction method underlie datum generate mixture model stimulate considerable recent debate controversy stem simulation mixture model datum give tree topology reconstruction algorithm output tree different topology finding hold shortcoming particular tree reconstruction method underlie assumption mixture model datum topology distinguish datum evolve unmixed tree topology give datum ` ` correct method assumption false biologist result imply example combine datum gene phylogenetic tree differ term branch length perfectly fit tree different topology
q-bio,Multi-Agent Approach to the Self-Organization of Networks,"  Is it possible to link a set of nodes without using preexisting positional
information or any kind of long-range attraction of the nodes? Can the process
of generating positional information, i.e. the detection of ``unknown'' nodes
and the estabishment of chemical gradients, \emph{and} the process of network
formation, i.e. the establishment of links between nodes, occur in parallel, on
a comparable time scale, as a process of co-evolution?
  The paper discusses a model where the generation of relevant information for
establishing the links between nodes results from the interaction of many
\emph{agents}, i.e. subunits of the system that are capable of performing some
activities. Their collective interaction is based on (indirect) communication,
which also includes memory effects and the dissemination of information in the
system. The relevant (``pragmatic'') information that leads to the
establishment of the links then emerges from an evolutionary interplay of
selection and reamplification.
",possible link set node preexist positional information kind long range attraction node process generate positional information i.e. detection ` ` unknown node estabishment chemical gradient \emph{and process network formation i.e. establishment link node occur parallel comparable time scale process co evolution paper discuss model generation relevant information establish link node result interaction \emph{agent i.e. subunit system capable perform activity collective interaction base indirect communication include memory effect dissemination information system relevant ` ` pragmatic information lead establishment link emerge evolutionary interplay selection reamplification
q-bio,"Observation of Multiple folding Pathways of beta-hairpin Trpzip2 from
  Independent Continuous Folding Trajectories","  We report 10 successfully folding events of trpzip2 by molecular dynamics
simulation. It is found that the trizip2 can fold into its native state through
different zipper pathways, depending on the ways of forming hydrophobic core.
We also find a very fast non-zipper pathway. This indicates that there may be
no inconsistencies in the current pictures of beta-hairpin folding mechanisms.
These pathways occur with different probabilities. zip-out is the most probable
one. This may explain the recent experiment that the turn formation is the
rate-limiting step for beta-hairpin folding.
",report 10 successfully fold event trpzip2 molecular dynamic simulation find trizip2 fold native state different zipper pathway depend way form hydrophobic core find fast non zipper pathway indicate inconsistency current picture beta hairpin folding mechanism pathway occur different probability zip probable explain recent experiment turn formation rate limit step beta hairpin folding
q-bio,Boolean network model predicts cell cycle sequence of fission yeast,"  A Boolean network model of the cell-cycle regulatory network of fission yeast
(Schizosaccharomyces Pombe) is constructed solely on the basis of the known
biochemical interaction topology. Simulating the model in the computer,
faithfully reproduces the known sequence of regulatory activity patterns along
the cell cycle of the living cell. Contrary to existing differential equation
models, no parameters enter the model except the structure of the regulatory
circuitry. The dynamical properties of the model indicate that the biological
dynamical sequence is robustly implemented in the regulatory network, with the
biological stationary state G1 corresponding to the dominant attractor in state
space, and with the biological regulatory sequence being a strongly attractive
trajectory. Comparing the fission yeast cell-cycle model to a similar model of
the corresponding network in S. cerevisiae, a remarkable difference in
circuitry, as well as dynamics is observed. While the latter operates in a
strongly damped mode, driven by external excitation, the S. pombe network
represents an auto-excited system with external damping.
",boolean network model cell cycle regulatory network fission yeast schizosaccharomyces pombe construct solely basis know biochemical interaction topology simulate model computer faithfully reproduce know sequence regulatory activity pattern cell cycle live cell contrary exist differential equation model parameter enter model structure regulatory circuitry dynamical property model indicate biological dynamical sequence robustly implement regulatory network biological stationary state g1 correspond dominant attractor state space biological regulatory sequence strongly attractive trajectory compare fission yeast cell cycle model similar model correspond network s. cerevisiae remarkable difference circuitry dynamic observe operate strongly damp mode drive external excitation s. pombe network represent auto excite system external damping
q-bio,A tree without leaves,"  The puzzle presented by the famous stumps of Gilboa, New York, finds a
solution in the discovery of two fossil specimens that allow the entire
structure of these early trees to be reconstructed.
",puzzle present famous stump gilboa new york find solution discovery fossil specimen allow entire structure early tree reconstruct
q-bio,"Multilevel Deconstruction of the In Vivo Behavior of Looped DNA-Protein
  Complexes","  Protein-DNA complexes with loops play a fundamental role in a wide variety of
cellular processes, ranging from the regulation of DNA transcription to
telomere maintenance. As ubiquitous as they are, their precise in vivo
properties and their integration into the cellular function still remain
largely unexplored. Here, we present a multilevel approach that efficiently
connects in both directions molecular properties with cell physiology and use
it to characterize the molecular properties of the looped DNA-lac repressor
complex while functioning in vivo. The properties we uncover include the
presence of two representative conformations of the complex, the stabilization
of one conformation by DNA architectural proteins, and precise values of the
underlying twisting elastic constants and bending free energies. Incorporation
of all this molecular information into gene-regulation models reveals an
unprecedented versatility of looped DNA-protein complexes at shaping the
properties of gene expression.
",protein dna complex loop play fundamental role wide variety cellular process range regulation dna transcription telomere maintenance ubiquitous precise vivo property integration cellular function remain largely unexplored present multilevel approach efficiently connect direction molecular property cell physiology use characterize molecular property loop dna lac repressor complex function vivo property uncover include presence representative conformation complex stabilization conformation dna architectural protein precise value underlie twist elastic constant bend free energy incorporation molecular information gene regulation model reveal unprecedented versatility loop dna protein complex shape property gene expression
q-bio,Mismatch Repair Error Implies Chargaff's Second Parity Rule,"  Chargaff's second parity rule holds empirically for most types of DNA that
along single strands of DNA the base contents are equal for complimentary
bases, A = T, G = C. A Markov chain model is constructed to track the evolution
of any single base position along single strands of genomes whose organisms are
equipped with replication mismatch repair. Under the key assumptions that
mismatch error rates primarily depend the number of hydrogen bonds of
nucleotides and that the mismatch repairing process itself makes strand
recognition error, the model shows that the steady state probabilities for any
base position to take on one of the 4 nucleotide bases are equal for
complimentary bases. As a result, Chargaff's second parity rule is the
manifestation of the Law of Large Number acting on the steady state
probabilities. More importantly, because the model pinpoints mismatch repair as
a basis of the rule, it is suitable for experimental verification.
",chargaff second parity rule hold empirically type dna single strand dna base content equal complimentary basis = t g = c. markov chain model construct track evolution single base position single strand genome organism equip replication mismatch repair key assumption mismatch error rate primarily depend number hydrogen bond nucleotide mismatch repair process make strand recognition error model show steady state probability base position 4 nucleotide basis equal complimentary basis result chargaff second parity rule manifestation law large number act steady state probability importantly model pinpoint mismatch repair basis rule suitable experimental verification
q-bio,"Considering the Case for Biodiversity Cycles: Reexamining the Evidence
  for Periodicity in the Fossil Record","  Medvedev and Melott (2007) have suggested that periodicity in fossil
biodiversity may be induced by cosmic rays which vary as the Solar System
oscillates normal to the galactic disk. We re-examine the evidence for a 62
million year (Myr) periodicity in biodiversity throughout the Phanerozoic
history of animal life reported by Rohde & Mueller (2005), as well as related
questions of periodicity in origination and extinction. We find that the signal
is robust against variations in methods of analysis, and is based on
fluctuations in the Paleozoic and a substantial part of the Mesozoic.
Examination of origination and extinction is somewhat ambiguous, with results
depending upon procedure. Origination and extinction intensity as defined by RM
may be affected by an artifact at 27 Myr in the duration of stratigraphic
intervals. Nevertheless, when a procedure free of this artifact is implemented,
the 27 Myr periodicity appears in origination, suggesting that the artifact may
ultimately be based on a signal in the data. A 62 Myr feature appears in
extinction, when this same procedure is used. We conclude that evidence for a
periodicity at 62 Myr is robust, and evidence for periodicity at approximately
27 Myr is also present, albeit more ambiguous.
",medvedev melott 2007 suggest periodicity fossil biodiversity induce cosmic ray vary solar system oscillate normal galactic disk examine evidence 62 million year myr periodicity biodiversity phanerozoic history animal life report rohde mueller 2005 related question periodicity origination extinction find signal robust variation method analysis base fluctuation paleozoic substantial mesozoic examination origination extinction somewhat ambiguous result depend procedure origination extinction intensity define rm affect artifact 27 myr duration stratigraphic interval procedure free artifact implement 27 myr periodicity appear origination suggest artifact ultimately base signal datum 62 myr feature appear extinction procedure conclude evidence periodicity 62 myr robust evidence periodicity approximately 27 myr present albeit ambiguous
q-bio,"A quantitative study on the growth variability of tumour cell clones in
  vitro","  Objectives: In this study, we quantify the growth variability of tumour cell
clones from a human leukemia cell line. Materials and methods: We have used
microplate spectrophotometry to measure the growth kinetics of hundreds of
individual cell clones from the Molt3 cell line. The growth rate of each clonal
population has been estimated by fitting experimental data with the logistic
equation. Results: The growth rates were observed to vary among different
clones. Up to six clones with a growth rate above or below the mean growth rate
of the parent population were further cloned and the growth rates of their
offsprings were measured. The distribution of the growth rates of the subclones
did not significantly differ from that of the parent population thus suggesting
that growth variability has an epigenetic origin. To explain the observed
distributions of clonal growth rates we have developed a probabilistic model
assuming that the fluctuations in the number of mitochondria through successive
cell cycles are the leading cause of growth variability. For fitting purposes,
we have estimated experimentally by flow cytometry the maximum average number
of mitochondria in Molt3 cells. The model fits nicely the observed
distributions of growth rates, however, cells in which the mitochondria were
rendered non functional (rho-0 cells) showed only a 30% reduction in the clonal
growth variability with respect to normal cells. Conclusions: A tumor cell
population is a dynamic ensemble of clones with highly variable growth rate. At
least part of this variability is due to fluctuations in the number of
mitochondria.
",objective study quantify growth variability tumour cell clone human leukemia cell line material method microplate spectrophotometry measure growth kinetic hundred individual cell clone molt3 cell line growth rate clonal population estimate fitting experimental datum logistic equation result growth rate observe vary different clone clone growth rate mean growth rate parent population clone growth rate offspring measure distribution growth rate subclone significantly differ parent population suggest growth variability epigenetic origin explain observed distribution clonal growth rate develop probabilistic model assume fluctuation number mitochondria successive cell cycle lead cause growth variability fitting purpose estimate experimentally flow cytometry maximum average number mitochondria molt3 cell model fit nicely observed distribution growth rate cell mitochondria render non functional rho-0 cell show 30 reduction clonal growth variability respect normal cell conclusion tumor cell population dynamic ensemble clone highly variable growth rate variability fluctuation number mitochondria
q-bio,Fourier Analysis of Biological Evolution: Concept of Selection Moment,"  Secondary structure elements of many protein families exhibit differential
conservation on their opposing faces. Amphipathic helices and beta-sheets by
definition possess this property, and play crucial functional roles. This type
of evolutionary trajectory of a protein family is usually critical to the
functions of the protein family, as well as in creating functions within
subfamilies. That is, differential conservation maintains properties of a
protein structure related to its orientation, and that are important in
packing, recognition, and catalysis. Here I define and formulate a new concept,
called the selection moment, that detects this evolutionary process in protein
sequences. A treatment of its various applications is detailed.
",secondary structure element protein family exhibit differential conservation opposing face amphipathic helix beta sheet definition possess property play crucial functional role type evolutionary trajectory protein family usually critical function protein family create function subfamily differential conservation maintain property protein structure relate orientation important packing recognition catalysis define formulate new concept call selection moment detect evolutionary process protein sequence treatment application detail
q-bio,Inferring dynamic genetic networks with low order independencies,"  In this paper, we propose a novel inference method for dynamic genetic
networks which makes it possible to face with a number of time measurements n
much smaller than the number of genes p. The approach is based on the concept
of low order conditional dependence graph that we extend here in the case of
Dynamic Bayesian Networks. Most of our results are based on the theory of
graphical models associated with the Directed Acyclic Graphs (DAGs). In this
way, we define a minimal DAG G which describes exactly the full order
conditional dependencies given the past of the process. Then, to face with the
large p and small n estimation case, we propose to approximate DAG G by
considering low order conditional independencies. We introduce partial qth
order conditional dependence DAGs G(q) and analyze their probabilistic
properties. In general, DAGs G(q) differ from DAG G but still reflect relevant
dependence facts for sparse networks such as genetic networks. By using this
approximation, we set out a non-bayesian inference method and demonstrate the
effectiveness of this approach on both simulated and real data analysis. The
inference procedure is implemented in the R package 'G1DBN' freely available
from the CRAN archive.
",paper propose novel inference method dynamic genetic network make possible face number time measurement n small number gene p. approach base concept low order conditional dependence graph extend case dynamic bayesian networks result base theory graphical model associate directed acyclic graphs dags way define minimal dag g describe exactly order conditional dependency give past process face large p small n estimation case propose approximate dag g consider low order conditional independency introduce partial qth order conditional dependence dag g(q analyze probabilistic property general dag g(q differ dag g reflect relevant dependence fact sparse network genetic network approximation set non bayesian inference method demonstrate effectiveness approach simulated real datum analysis inference procedure implement r package g1dbn freely available cran archive
q-bio,"Inferring DNA sequences from mechanical unzipping data: the
  large-bandwidth case","  The complementary strands of DNA molecules can be separated when stretched
apart by a force; the unzipping signal is correlated to the base content of the
sequence but is affected by thermal and instrumental noise. We consider here
the ideal case where opening events are known to a very good time resolution
(very large bandwidth), and study how the sequence can be reconstructed from
the unzipping data. Our approach relies on the use of statistical Bayesian
inference and of Viterbi decoding algorithm. Performances are studied
numerically on Monte Carlo generated data, and analytically. We show how
multiple unzippings of the same molecule may be exploited to improve the
quality of the prediction, and calculate analytically the number of required
unzippings as a function of the bandwidth, the sequence content, the elasticity
parameters of the unzipped strands.
",complementary strand dna molecule separate stretch apart force unzipping signal correlate base content sequence affect thermal instrumental noise consider ideal case opening event know good time resolution large bandwidth study sequence reconstruct unzip datum approach rely use statistical bayesian inference viterbi decode algorithm performance study numerically monte carlo generate datum analytically multiple unzipping molecule exploit improve quality prediction calculate analytically number require unzipping function bandwidth sequence content elasticity parameter unzipped strand
q-bio,Deterministic characterization of stochastic genetic circuits,"  For cellular biochemical reaction systems where the numbers of molecules is
small, significant noise is associated with chemical reaction events. This
molecular noise can give rise to behavior that is very different from the
predictions of deterministic rate equation models. Unfortunately, there are few
analytic methods for examining the qualitative behavior of stochastic systems.
Here we describe such a method that extends deterministic analysis to include
leading-order corrections due to the molecular noise. The method allows the
steady-state behavior of the stochastic model to be easily computed,
facilitates the mapping of stability phase diagrams that include stochastic
effects and reveals how model parameters affect noise susceptibility, in a
manner not accessible to numerical simulation. By way of illustration we
consider two genetic circuits: a bistable positive-feedback loop and a
negative-feedback oscillator. We find in the positive feedback circuit that
translational activation leads to a far more stable system than transcriptional
control. Conversely, in a negative-feedback loop triggered by a
positive-feedback switch, the stochasticity of transcriptional control is
harnessed to generate reproducible oscillations.
",cellular biochemical reaction system number molecule small significant noise associate chemical reaction event molecular noise rise behavior different prediction deterministic rate equation model unfortunately analytic method examine qualitative behavior stochastic system describe method extend deterministic analysis include lead order correction molecular noise method allow steady state behavior stochastic model easily compute facilitate mapping stability phase diagram include stochastic effect reveal model parameter affect noise susceptibility manner accessible numerical simulation way illustration consider genetic circuit bistable positive feedback loop negative feedback oscillator find positive feedback circuit translational activation lead far stable system transcriptional control conversely negative feedback loop trigger positive feedback switch stochasticity transcriptional control harness generate reproducible oscillation
q-bio,"Hedging our bets: the expected contribution of species to future
  phylogenetic diversity","  If predictions for species extinctions hold, then the `tree of life' today
may be quite different to that in (say) 100 years. We describe a technique to
quantify how much each species is likely to contribute to future biodiversity,
as measured by its expected contribution to phylogenetic diversity. Our
approach considers all possible scenarios for the set of species that will be
extant at some future time, and weights them according to their likelihood
under an independent (but not identical) distribution on species extinctions.
Although the number of extinction scenarios can typically be very large, we
show that there is a simple algorithm that will quickly compute this index. The
method is implemented and applied to the prosimian primates as a test case, and
the associated species ranking is compared to a related measure (the `Shapley
index'). We describe indices for rooted and unrooted trees, and a modification
that also includes the focal taxon's probability of extinction, making it
directly comparable to some new conservation metrics.
",prediction species extinction hold ` tree life today different 100 year describe technique quantify specie likely contribute future biodiversity measure expect contribution phylogenetic diversity approach consider possible scenario set specie extant future time weight accord likelihood independent identical distribution species extinction number extinction scenario typically large simple algorithm quickly compute index method implement apply prosimian primate test case associate species ranking compare related measure ` shapley index describe index rooted unrooted tree modification include focal taxon probability extinction make directly comparable new conservation metric
q-bio,"Modeling transcription factor binding events to DNA using a random
  walker/jumper representation on a 1D/2D lattice with different affinity sites","  Surviving in a diverse environment requires corresponding organism responses.
At the cellular level, such adjustment relies on the transcription factors
(TFs) which must rapidly find their target sequences amidst a vast amount of
non-relevant sequences on DNA molecules. Whether these transcription factors
locate their target sites through a 1D or 3D pathway is still a matter of
speculation. It has been suggested that the optimum search time is when the
protein equally shares its search time between 1D and 3D diffusions. In this
paper, we study the above problem using a Monte Carlo simulation by considering
a very simple physical model. A 1D strip, representing a DNA, with a number of
low affinity sites, corresponding to non-target sites, and high affinity sites,
corresponding to target sites, is considered and later extended to a 2D strip.
We study the 1D and 3D exploration pathways, and combinations of the two modes
by considering three different types of molecules: a walker that randomly walks
along the strip with no dissociation; a jumper that represents dissociation and
then re-association of a TF with the strip at later time at a distant site; and
a hopper that is similar to the jumper but it dissociates and then
re-associates at a faster rate than the jumper. We analyze the final
probability distribution of molecules for each case and find that TFs can
locate their targets fast enough even if they spend 15% of their search time
diffusing freely in the solution. This indeed agrees with recent experimental
results obtained by Elf et al. 2007 and is in contrast with theoretical
expectation.
",survive diverse environment require correspond organism response cellular level adjustment rely transcription factor tfs rapidly find target sequence amidst vast non relevant sequence dna molecule transcription factor locate target site 1d 3d pathway matter speculation suggest optimum search time protein equally share search time 1d 3d diffusion paper study problem monte carlo simulation consider simple physical model 1d strip represent dna number low affinity site correspond non target site high affinity site correspond target site consider later extend 2d strip study 1d 3d exploration pathway combination mode consider different type molecule walker randomly walk strip dissociation jumper represent dissociation association tf strip later time distant site hopper similar jumper dissociate associate fast rate jumper analyze final probability distribution molecule case find tfs locate target fast spend 15 search time diffuse freely solution agree recent experimental result obtain elf et al 2007 contrast theoretical expectation
q-bio,On Gene Duplication Models for Evolving Regulatory Networks,"  Background: Duplication of genes is important for evolution of molecular
networks. Many authors have therefore considered gene duplication as a driving
force in shaping the topology of molecular networks. In particular it has been
noted that growth via duplication would act as an implicit way of preferential
attachment, and thereby provide the observed broad degree distributions of
molecular networks.
  Results: We extend current models of gene duplication and rewiring by
including directions and the fact that molecular networks are not a result of
unidirectional growth. We introduce upstream sites and downstream shapes to
quantify potential links during duplication and rewiring. We find that this in
itself generates the observed scaling of transcription factors for genome sites
in procaryotes. The dynamical model can generate a scale-free degree
distribution, p(k)&prop; 1/k^&gamma;, with exponent &gamma;=1 in the
non-growing case, and with &gamma;>1 when the network is growing.
  Conclusions: We find that duplication of genes followed by substantial
recombination of upstream regions could generate main features of genetic
regulatory networks. Our steady state degree distribution is however to broad
to be consistent with data, thereby suggesting that selective pruning acts as a
main additional constraint on duplicated genes. Our analysis shows that gene
duplication can only be a main cause for the observed broad degree
distributions, if there is also substantial recombinations between upstream
regions of genes.
",background duplication gene important evolution molecular network author consider gene duplication driving force shape topology molecular network particular note growth duplication act implicit way preferential attachment provide observe broad degree distribution molecular network result extend current model gene duplication rewire include direction fact molecular network result unidirectional growth introduce upstream site downstream shape quantify potential link duplication rewiring find generate observed scaling transcription factor genome site procaryote dynamical model generate scale free degree distribution p(k)&prop 1 k^&gamma exponent gamma;=1 non growing case gamma;>1 network grow conclusion find duplication gene follow substantial recombination upstream region generate main feature genetic regulatory network steady state degree distribution broad consistent datum suggest selective pruning act main additional constraint duplicate gene analysis show gene duplication main cause observe broad degree distribution substantial recombination upstream region gene
q-bio,Fundamental Limits to Position Determination by Concentration Gradients,"  Position determination in biological systems is often achieved through
protein concentration gradients. Measuring the local concentration of such a
protein with a spatially-varying distribution allows the measurement of
position within the system. In order for these systems to work effectively,
position determination must be robust to noise. Here, we calculate fundamental
limits to the precision of position determination by concentration gradients
due to unavoidable biochemical noise perturbing the gradients. We focus on
gradient proteins with first order reaction kinetics. Systems of this type have
been experimentally characterised in both developmental and cell biology
settings. For a single gradient we show that, through time-averaging, great
precision can potentially be achieved even with very low protein copy numbers.
As a second example, we investigate the ability of a system with oppositely
directed gradients to find its centre. With this mechanism, positional
precision close to the centre improves more slowly with increasing averaging
time, and so longer averaging times or higher copy numbers are required for
high precision. For both single and double gradients, we demonstrate the
existence of optimal length scales for the gradients, where precision is
maximized, as well as analyzing how precision depends on the size of the
concentration measuring apparatus. Our results provide fundamental constraints
on the positional precision supplied by concentration gradients in various
contexts, including both in developmental biology and also within a single
cell.
",position determination biological system achieve protein concentration gradient measure local concentration protein spatially vary distribution allow measurement position system order system work effectively position determination robust noise calculate fundamental limit precision position determination concentration gradient unavoidable biochemical noise perturb gradient focus gradient protein order reaction kinetic system type experimentally characterise developmental cell biology setting single gradient time averaging great precision potentially achieve low protein copy number second example investigate ability system oppositely direct gradient find centre mechanism positional precision close centre improve slowly increase averaging time long average time high copy number require high precision single double gradient demonstrate existence optimal length scale gradient precision maximize analyze precision depend size concentration measure apparatus result provide fundamental constraint positional precision supply concentration gradient contexts include developmental biology single cell
q-bio,"Efficient model chemistries for peptides. I. Split-valence Gaussian
  basis sets and the heterolevel approximation in RHF and MP2","  We present an exhaustive study of more than 250 ab initio potential energy
surfaces (PESs) of the model dipeptide HCO-L-Ala-NH2. The model chemistries
(MCs) used are constructed as homo- and heterolevels involving possibly
different RHF and MP2 calculations for the geometry and the energy. The basis
sets used belong to a sample of 39 selected representants from Pople's
split-valence families, ranging from the small 3-21G to the large
6-311++G(2df,2pd). The reference PES to which the rest are compared is the
MP2/6-311++G(2df,2pd) homolevel, which, as far as we are aware, is the more
accurate PES of a dipeptide in the literature. The aim of the study presented
is twofold: On the one hand, the evaluation of the influence of polarization
and diffuse functions in the basis set, distinguishing between those placed at
1st-row atoms and those placed at hydrogens, as well as the effect of different
contraction and valence splitting schemes. On the other hand, the investigation
of the heterolevel assumption, which is defined here to be that which states
that heterolevel MCs are more efficient than homolevel MCs. The heterolevel
approximation is very commonly used in the literature, but it is seldom
checked. As far as we know, the only tests for peptides or related systems,
have been performed using a small number of conformers, and this is the first
time that this potentially very economical approximation is tested in full
PESs. In order to achieve these goals, all data sets have been compared and
analyzed in a way which captures the nearness concept in the space of MCs.
","present exhaustive study 250 ab initio potential energy surface pess model dipeptide hco l ala nh2 model chemistry mcs construct homo- heterolevels involve possibly different rhf mp2 calculation geometry energy basis set belong sample 39 select representant pople split valence family range small 3 21 g large 6 311++g(2df,2pd reference pes rest compare mp2/6 311++g(2df,2pd homolevel far aware accurate pes dipeptide literature aim study present twofold hand evaluation influence polarization diffuse function basis set distinguish place 1st row atom place hydrogen effect different contraction valence splitting scheme hand investigation heterolevel assumption define state heterolevel mc efficient homolevel mc heterolevel approximation commonly literature seldom check far know test peptide related system perform small number conformer time potentially economical approximation test pes order achieve goal data set compare analyze way capture nearness concept space mcs"
q-bio,"A Model of Late Long-Term Potentiation Simulates Aspects of Memory
  Maintenance","  Late long-term potentiation (L-LTP) appears essential for the formation of
long-term memory, with memories at least partly encoded by patterns of
strengthened synapses. How memories are preserved for months or years, despite
molecular turnover, is not well understood. Ongoing recurrent neuronal
activity, during memory recall or during sleep, has been hypothesized to
preferentially potentiate strong synapses, preserving memories. This hypothesis
has not been evaluated in the context of a mathematical model representing
biochemical pathways important for L-LTP. I incorporated ongoing activity into
two such models: a reduced model that represents some of the essential
biochemical processes, and a more detailed published model. The reduced model
represents synaptic tagging and gene induction intuitively, and the detailed
model adds activation of essential kinases by Ca. Ongoing activity was modeled
as continual brief elevations of [Ca]. In each model, two stable states of
synaptic weight resulted. Positive feedback between synaptic weight and the
amplitude of ongoing Ca transients underlies this bistability. A tetanic or
theta-burst stimulus switches a model synapse from a low weight to a high
weight stabilized by ongoing activity. Bistability was robust to parameter
variations. Simulations illustrated that prolonged decreased activity reset
synapses to low weights, suggesting a plausible forgetting mechanism. However,
episodic activity with shorter inactive intervals maintained strong synapses.
Both models support experimental predictions. Tests of these predictions are
expected to further understanding of how neuronal activity is coupled to
maintenance of synaptic strength.
",late long term potentiation l ltp appear essential formation long term memory memory partly encode pattern strengthen synapsis memory preserve month year despite molecular turnover understand ongoing recurrent neuronal activity memory recall sleep hypothesize preferentially potentiate strong synapsis preserve memory hypothesis evaluate context mathematical model represent biochemical pathway important l ltp incorporate ongoing activity model reduced model represent essential biochemical process detailed publish model reduced model represent synaptic tagging gene induction intuitively detailed model add activation essential kinase ongoing activity model continual brief elevation model stable state synaptic weight result positive feedback synaptic weight amplitude ongoing transient underlie bistability tetanic theta burst stimulus switch model synapse low weight high weight stabilize ongoing activity bistability robust parameter variation simulation illustrate prolong decrease activity reset synapsis low weight suggest plausible forgetting mechanism episodic activity short inactive interval maintain strong synapsis model support experimental prediction test prediction expect understanding neuronal activity couple maintenance synaptic strength
q-bio,"Propagation of external regulation and asynchronous dynamics in random
  Boolean networks","  Boolean Networks and their dynamics are of great interest as abstract
modeling schemes in various disciplines, ranging from biology to computer
science. Whereas parallel update schemes have been studied extensively in past
years, the level of understanding of asynchronous updates schemes is still very
poor. In this paper we study the propagation of external information given by
regulatory input variables into a random Boolean network. We compute both
analytically and numerically the time evolution and the asymptotic behavior of
this propagation of external regulation (PER). In particular, this allows us to
identify variables which are completely determined by this external
information. All those variables in the network which are not directly fixed by
PER form a core which contains in particular all non-trivial feedback loops. We
design a message-passing approach allowing to characterize the statistical
properties of these cores in dependence of the Boolean network and the external
condition. At the end we establish a link between PER dynamics and the full
random asynchronous dynamics of a Boolean network.
",boolean networks dynamic great interest abstract modeling scheme discipline range biology computer science parallel update scheme study extensively past year level understanding asynchronous update scheme poor paper study propagation external information give regulatory input variable random boolean network compute analytically numerically time evolution asymptotic behavior propagation external regulation particular allow identify variable completely determine external information variable network directly fix form core contain particular non trivial feedback loop design message pass approach allow characterize statistical property core dependence boolean network external condition end establish link dynamic random asynchronous dynamic boolean network
q-bio,"Inverse Geometric Approach to the Simulation of the Circular Growth. The
  Case of Multicellular Tumor Spheroids","  We demonstrate the power of the genetic algorithms to construct the cellular
automata model simulating the growth of 2-dimensional close-to-circular
clusters revealing the desired properties, such as the growth rate and, at the
same time, the fractal behavior of their contours. The possible application of
the approach in the field of tumor modeling is outlined.
",demonstrate power genetic algorithm construct cellular automata model simulate growth 2 dimensional close circular cluster reveal desire property growth rate time fractal behavior contour possible application approach field tumor modeling outline
q-bio,Clustering Coefficients of Protein-Protein Interaction Networks,"  The properties of certain networks are determined by hidden variables that
are not explicitly measured. The conditional probability (propagator) that a
vertex with a given value of the hidden variable is connected to k of other
vertices determines all measurable properties. We study hidden variable models
and find an averaging approximation that enables us to obtain a general
analytical result for the propagator. Analytic results showing the validity of
the approximation are obtained. We apply hidden variable models to
protein-protein interaction networks (PINs) in which the hidden variable is the
association free-energy, determined by distributions that depend on
biochemistry and evolution. We compute degree distributions as well as
clustering coefficients of several PINs of different species; good agreement
with measured data is obtained. For the human interactome two different
parameter sets give the same degree distributions, but the computed clustering
coefficients differ by a factor of about two. This shows that degree
distributions are not sufficient to determine the properties of PINs.
",property certain network determine hide variable explicitly measure conditional probability propagator vertex give value hide variable connect k vertex determine measurable property study hidden variable model find averaging approximation enable obtain general analytical result propagator analytic result show validity approximation obtain apply hidden variable model protein protein interaction network pins hide variable association free energy determine distribution depend biochemistry evolution compute degree distribution cluster coefficient pin different specie good agreement measured datum obtain human interactome different parameter set degree distribution computed cluster coefficient differ factor show degree distribution sufficient determine property pin
q-bio,Efficiency and versatility of distal multisite transcription regulation,"  Transcription regulation typically involves the binding of proteins over long
distances on multiple DNA sites that are brought close to each other by the
formation of DNA loops. The inherent complexity of the assembly of regulatory
complexes on looped DNA challenges the understanding of even the simplest
genetic systems, including the prototypical lac operon. Here we implement a
scalable quantitative computational approach to analyze systems regulated
through multiple DNA sites with looping. Our approach applied to the lac operon
accurately predicts the transcription rate over five orders of magnitude for
wild type and seven mutants accounting for all the combinations of deletions of
the three operators. A quantitative analysis of the model reveals that the
presence of three operators provides a mechanism to combine robust repression
with sensitive induction, two seemingly mutually exclusive properties that are
required for optimal functioning of metabolic switches.
",transcription regulation typically involve binding protein long distance multiple dna site bring close formation dna loop inherent complexity assembly regulatory complex loop dna challenge understanding simple genetic system include prototypical lac operon implement scalable quantitative computational approach analyze system regulate multiple dna site loop approach apply lac operon accurately predict transcription rate order magnitude wild type seven mutant account combination deletion operator quantitative analysis model reveal presence operator provide mechanism combine robust repression sensitive induction seemingly mutually exclusive property require optimal functioning metabolic switch
q-bio,"Network Growth with Preferential Attachment for High Indegree and Low
  Outdegree","  We study the growth of a directed transportation network, such as a food web,
in which links carry resources. We propose a growth process in which new nodes
(or species) preferentially attach to existing nodes with high indegree (in
food-web language, number of prey) and low outdegree (or number of predators).
This scheme, which we call inverse preferential attachment, is intended to
maximize the amount of resources available to each new node. We show that the
outdegree (predator) distribution decays at least exponentially fast for large
outdegree and is continuously tunable between an exponential distribution and a
delta function. The indegree (prey) distribution is poissonian in the
large-network limit.
",study growth direct transportation network food web link carry resource propose growth process new node specie preferentially attach exist node high indegree food web language number prey low outdegree number predator scheme inverse preferential attachment intend maximize resource available new node outdegree predator distribution decay exponentially fast large outdegree continuously tunable exponential distribution delta function indegree prey distribution poissonian large network limit
q-bio,"A new chaotic attractor in a basic multi-strain epidemiological model
  with temporary cross-immunity","  An epidemic multi-strain model with temporary cross-immunity shows chaos,
even in a previously unexpected parameter region. Especially dengue fever
models with strong enhanced infectivity on secondary infection have previously
shown deterministic chaos motivated by experimental findings of
antibody-dependent-enhancement (ADE). Including temporary cross-immunity in
such models, which is common knowledge among field researchers in dengue, we
find a deterministically chaotic attractor in the more realistic parameter
region of reduced infectivity on secondary infection (''inverse ADE'' parameter
region). This is realistic for dengue fever since on second infection people
are more likely to be hospitalized, hence do not contribute to the force of
infection as much as people with first infection.
  Our finding has wider implications beyond dengue in any multi-strain
epidemiological systems with altered infectivity upon secondary infection,
since we can relax the condition of rather high infectivity on secondary
infection previously required for deterministic chaos. For dengue the finding
of wide ranges of chaotic attractors open new ways to analysis of existing data
sets.
",epidemic multi strain model temporary cross immunity show chaos previously unexpected parameter region especially dengue fever model strong enhance infectivity secondary infection previously show deterministic chaos motivate experimental finding antibody dependent enhancement ade include temporary cross immunity model common knowledge field researcher dengue find deterministically chaotic attractor realistic parameter region reduce infectivity secondary infection inverse ade parameter region realistic dengue fever second infection people likely hospitalize contribute force infection people infection finding wide implication dengue multi strain epidemiological system altered infectivity secondary infection relax condition high infectivity secondary infection previously require deterministic chaos dengue finding wide range chaotic attractor open new way analysis exist datum set
q-bio,Converting genetic network oscillations into somite spatial pattern,"  In most vertebrate species, the body axis is generated by the formation of
repeated transient structures called somites. This spatial periodicity in
somitogenesis has been related to the temporally sustained oscillations in
certain mRNAs and their associated gene products in the cells forming the
presomatic mesoderm. The mechanism underlying these oscillations have been
identified as due to the delays involved in the synthesis of mRNA and
translation into protein molecules [J. Lewis, Current Biol. {\bf 13}, 1398
(2003)]. In addition, in the zebrafish embryo intercellular Notch signalling
couples these oscillators and a longitudinal positional information signal in
the form of an Fgf8 gradient exists that could be used to transform these
coupled temporal oscillations into the observed spatial periodicity of somites.
Here we consider a simple model based on this known biology and study its
consequences for somitogenesis. Comparison is made with the known properties of
somite formation in the zebrafish embryo . We also study the effects of
localized Fgf8 perturbations on somite patterning.
",vertebrate specie body axis generate formation repeat transient structure call somite spatial periodicity somitogenesis relate temporally sustained oscillation certain mrnas associate gene product cell form presomatic mesoderm mechanism underlie oscillation identify delay involve synthesis mrna translation protein molecule j. lewis current biol \bf 13 1398 2003 addition zebrafish embryo intercellular notch signal couple oscillator longitudinal positional information signal form fgf8 gradient exist transform couple temporal oscillation observed spatial periodicity somite consider simple model base know biology study consequence somitogenesis comparison known property somite formation zebrafish embryo study effect localized fgf8 perturbation somite pattern
q-bio,Primordial Evolution in the Finitary Process Soup,"  A general and basic model of primordial evolution--a soup of reacting
finitary and discrete processes--is employed to identify and analyze
fundamental mechanisms that generate and maintain complex structures in
prebiotic systems. The processes--$\epsilon$-machines as defined in
computational mechanics--and their interaction networks both provide well
defined notions of structure. This enables us to quantitatively demonstrate
hierarchical self-organization in the soup in terms of complexity. We found
that replicating processes evolve the strategy of successively building higher
levels of organization by autocatalysis. Moreover, this is facilitated by local
components that have low structural complexity, but high generality. In effect,
the finitary process soup spontaneously evolves a selection pressure that
favors such components. In light of the finitary process soup's generality,
these results suggest a fundamental law of hierarchical systems: global
complexity requires local simplicity.
",general basic model primordial evolution soup react finitary discrete process employ identify analyze fundamental mechanism generate maintain complex structure prebiotic system processes--$\epsilon$-machine define computational mechanic interaction network provide define notion structure enable quantitatively demonstrate hierarchical self organization soup term complexity find replicate process evolve strategy successively build high level organization autocatalysis facilitate local component low structural complexity high generality effect finitary process soup spontaneously evolve selection pressure favor component light finitary process soup generality result suggest fundamental law hierarchical system global complexity require local simplicity
q-bio,Coupling of transverse and longitudinal response in stiff polymers,"  The time-dependent transverse response of stiff polymers, represented as
weakly-bending wormlike chains (WLCs), is well-understood on the linear level,
where transverse degrees of freedom evolve independently from the longitudinal
ones. We show that, beyond a characteristic time scale, the nonlinear coupling
of transverse and longitudinal motion in an inextensible WLC significantly
weakens the polymer response compared to the widely used linear response
predictions. The corresponding feedback mechanism is rationalized by scaling
arguments and quantified by a multiple scale approach that exploits an inherent
separation of transverse and longitudinal correlation length scales. Crossover
scaling laws and exact analytical and numerical solutions for characteristic
response quantities are derived for different experimentally relevant setups.
Our findings are applicable to cytoskeletal filaments as well as DNA under
tension.
",time dependent transverse response stiff polymer represent weakly bend wormlike chain wlcs understand linear level transverse degree freedom evolve independently longitudinal one characteristic time scale nonlinear coupling transverse longitudinal motion inextensible wlc significantly weaken polymer response compare widely linear response prediction correspond feedback mechanism rationalize scale argument quantify multiple scale approach exploit inherent separation transverse longitudinal correlation length scale crossover scale law exact analytical numerical solution characteristic response quantity derive different experimentally relevant setup finding applicable cytoskeletal filament dna tension
q-bio,"Linked by Loops: Network Structure and Switch Integration in Complex
  Dynamical Systems","  Simple nonlinear dynamical systems with multiple stable stationary states are
often taken as models for switchlike biological systems. This paper considers
the interaction of multiple such simple multistable systems when they are
embedded together into a larger dynamical ""supersystem."" Attention is focused
on the network structure of the resulting set of coupled differential
equations, and the consequences of this structure on the propensity of the
embedded switches to act independently versus cooperatively. Specifically, it
is argued that both larger average and larger variance of the node degree
distribution lead to increased switch independence. Given the frequency of
empirical observations of high variance degree distributions (e.g., power-law)
in biological networks, it is suggested that the results presented here may aid
in identifying switch-integrating subnetworks as comparatively homogenous,
low-degree, substructures. Potential applications to ecological problems such
as the relationship of stability and complexity are also briefly discussed.
",simple nonlinear dynamical system multiple stable stationary state take model switchlike biological system paper consider interaction multiple simple multistable system embed large dynamical supersystem attention focus network structure result set couple differential equation consequence structure propensity embed switch act independently versus cooperatively specifically argue large average large variance node degree distribution lead increase switch independence give frequency empirical observation high variance degree distribution e.g. power law biological network suggest result present aid identify switch integrate subnetwork comparatively homogenous low degree substructure potential application ecological problem relationship stability complexity briefly discuss
q-bio,"Pr\'evention des escarres chez les parapl\'egiques : une nouvelle
  approche par \'electrostimulation linguale","  Pressure ulcers are recognized as a major health issue in individuals with
spinal cord injuries and new approaches to prevent this pathology are
necessary. An innovative health strategy is being developed through the use of
computer and sensory substitution via the tongue in order to compensate for the
sensory loss in the buttock area for individuals with paraplegia. This sensory
compensation will enable individuals with spinal cord injuries to be aware of a
localized excess of pressure at the skin/seat interface and, consequently, will
enable them to prevent the formation of pressure ulcers by relieving the
cutaneous area of suffering. This work reports an initial evaluation of this
approach and the feasibility of creating an adapted behavior, with a change in
pressure as a response to electro-stimulated information on the tongue.
Obtained during a clinical study in 10 healthy seated subjects, the first
results are encouraging, with 92% success in 100 performed tests. These
results, which have to be completed and validated in the paraplegic population,
may lead to a new approach to education in health to prevent the formation of
pressure ulcers within this population. Keywords: Spinal Cord Injuries,
Pressure Ulcer, Sensory Substitution, Health Education, Biomedical Informatics.
",pressure ulcer recognize major health issue individual spinal cord injury new approach prevent pathology necessary innovative health strategy develop use computer sensory substitution tongue order compensate sensory loss buttock area individual paraplegia sensory compensation enable individual spinal cord injury aware localize excess pressure skin seat interface consequently enable prevent formation pressure ulcer relieve cutaneous area suffering work report initial evaluation approach feasibility create adapt behavior change pressure response electro stimulate information tongue obtain clinical study 10 healthy seat subject result encouraging 92 success 100 perform test result complete validate paraplegic population lead new approach education health prevent formation pressure ulcer population keyword spinal cord injuries pressure ulcer sensory substitution health education biomedical informatics
q-bio,Delay estimation in a two-node acyclic network,"  Linear measures such as cross-correlation have been used successfully to
determine time delays from the given processes. Such an analysis often precedes
identifying possible causal relationships between the observed processes. The
present study investigates the impact of a positively correlated driver whose
correlation function decreases monotonically with lag on the delay estimation
in a two-node acyclic network with one and two-delays. It is shown that
cross-correlation analysis of the given processes can result in spurious
identification of multiple delays between the driver and the dependent
processes. Subsequently, delay estimation of increment process as opposed to
the original process under certain implicit constraints is explored.
Short-range and long-range correlated driver processes along with those of
their coarse-grained counterparts are considered.
",linear measure cross correlation successfully determine time delay give process analysis precede identify possible causal relationship observed process present study investigate impact positively correlate driver correlation function decrease monotonically lag delay estimation node acyclic network delay show cross correlation analysis give process result spurious identification multiple delay driver dependent process subsequently delay estimation increment process oppose original process certain implicit constraint explore short range long range correlate driver process coarse grain counterpart consider
q-bio,Non-coding DNA programs express adaptation and its universal law,"  Significant fraction (98.5% in humans) of most animal genomes is non- coding
dark matter. Its largely unknown function (1-5) is related to programming
(rather than to spontaneous mutations) of accurate adaptation to rapidly
changing environment. Programmed adaptation to the same universal law for
non-competing animals from anaerobic yeast to human is revealed in the study of
their extensively quantified mortality (6-21). Adaptation of animals with
removed non-coding DNA fractions may specify their contribution to genomic
programming. Emergence of new adaptation programs and their (non-Mendelian)
heredity may be studied in antibiotic mini-extinctions (22-24). On a large
evolutionary scale rapid universal adaptation was vital for survival, and
evolved, in otherwise lethal for diverse species major mass extinctions
(25-28). Evolutionary and experimental data corroborate these conclusions
(6-21, 29-32). Universal law implies certain biological universality of diverse
species, thus quantifies applicability of animal models to humans). Genomic
adaptation programming calls for unusual approach to its study and implies
unanticipated perspectives, in particular, directed biological changes.
",significant fraction 98.5 human animal genome non- code dark matter largely unknown function 1 5 relate programming spontaneous mutation accurate adaptation rapidly change environment program adaptation universal law non competing animal anaerobic yeast human reveal study extensively quantify mortality 6 21 adaptation animal removed non coding dna fraction specify contribution genomic programming emergence new adaptation program non mendelian heredity study antibiotic mini extinction 22 24 large evolutionary scale rapid universal adaptation vital survival evolve lethal diverse specie major mass extinction 25 28 evolutionary experimental datum corroborate conclusion 6 21 29 32 universal law imply certain biological universality diverse specie quantifie applicability animal model human genomic adaptation programming call unusual approach study imply unanticipated perspective particular direct biological change
q-bio,Extracting falsifiable predictions from sloppy models,"  Successful predictions are among the most compelling validations of any
model. Extracting falsifiable predictions from nonlinear multiparameter models
is complicated by the fact that such models are commonly sloppy, possessing
sensitivities to different parameter combinations that range over many decades.
Here we discuss how sloppiness affects the sorts of data that best constrain
model predictions, makes linear uncertainty approximations dangerous, and
introduces computational difficulties in Monte-Carlo uncertainty analysis. We
also present a useful test problem and suggest refinements to the standards by
which models are communicated.
",successful prediction compelling validation model extract falsifiable prediction nonlinear multiparameter model complicate fact model commonly sloppy possess sensitivity different parameter combination range decade discuss sloppiness affect sort datum good constrain model prediction make linear uncertainty approximation dangerous introduce computational difficulty monte carlo uncertainty analysis present useful test problem suggest refinement standard model communicate
q-bio,"Predicting the connectivity of primate cortical networks from
  topological and spatial node properties","  The organization of the connectivity between mammalian cortical areas has
become a major subject of study, because of its important role in scaffolding
the macroscopic aspects of animal behavior and intelligence. In this study we
present a computational reconstruction approach to the problem of network
organization, by considering the topological and spatial features of each area
in the primate cerebral cortex as subsidy for the reconstruction of the global
cortical network connectivity. Starting with all areas being disconnected,
pairs of areas with similar sets of features are linked together, in an attempt
to recover the original network structure. Inferring primate cortical
connectivity from the properties of the nodes, remarkably good reconstructions
of the global network organization could be obtained, with the topological
features allowing slightly superior accuracy to the spatial ones. Analogous
reconstruction attempts for the C. elegans neuronal network resulted in
substantially poorer recovery, indicating that cortical area interconnections
are relatively stronger related to the considered topological and spatial
properties than neuronal projections in the nematode. The close relationship
between area-based features and global connectivity may hint on developmental
rules and constraints for cortical networks. Particularly, differences between
the predictions from topological and spatial properties, together with the
poorer recovery resulting from spatial properties, indicate that the
organization of cortical networks is not entirely determined by spatial
constraints.
",organization connectivity mammalian cortical area major subject study important role scaffold macroscopic aspect animal behavior intelligence study present computational reconstruction approach problem network organization consider topological spatial feature area primate cerebral cortex subsidy reconstruction global cortical network connectivity start area disconnect pair area similar set feature link attempt recover original network structure inferring primate cortical connectivity property node remarkably good reconstruction global network organization obtain topological feature allow slightly superior accuracy spatial one analogous reconstruction attempt c. elegans neuronal network result substantially poor recovery indicate cortical area interconnection relatively strong relate consider topological spatial property neuronal projection nematode close relationship area base feature global connectivity hint developmental rule constraint cortical network particularly difference prediction topological spatial property poor recovery result spatial property indicate organization cortical network entirely determine spatial constraint
q-bio,A balanced memory network,"  A fundamental problem in neuroscience is understanding how working memory --
the ability to store information at intermediate timescales, like 10s of
seconds -- is implemented in realistic neuronal networks. The most likely
candidate mechanism is the attractor network, and a great deal of effort has
gone toward investigating it theoretically. Yet, despite almost a quarter
century of intense work, attractor networks are not fully understood. In
particular, there are still two unanswered questions. First, how is it that
attractor networks exhibit irregular firing, as is observed experimentally
during working memory tasks? And second, how many memories can be stored under
biologically realistic conditions? Here we answer both questions by studying an
attractor neural network in which inhibition and excitation balance each other.
Using mean field analysis, we derive a three-variable description of attractor
networks. From this description it follows that irregular firing can exist only
if the number of neurons involved in a memory is large. The same mean field
analysis also shows that the number of memories that can be stored in a network
scales with the number of excitatory connections, a result that has been
suggested for simple models but never shown for realistic ones. Both of these
predictions are verified using simulations with large networks of spiking
neurons.
",fundamental problem neuroscience understand work memory ability store information intermediate timescale like 10 second implement realistic neuronal network likely candidate mechanism attractor network great deal effort go investigate theoretically despite quarter century intense work attractor network fully understand particular unanswered question attractor network exhibit irregular firing observe experimentally work memory task second memory store biologically realistic condition answer question study attractor neural network inhibition excitation balance mean field analysis derive variable description attractor network description follow irregular firing exist number neuron involve memory large mean field analysis show number memory store network scale number excitatory connection result suggest simple model show realistic one prediction verify simulation large network spike neuron
q-bio,"An Adaptive Strategy for the Classification of G-Protein Coupled
  Receptors","  One of the major problems in computational biology is the inability of
existing classification models to incorporate expanding and new domain
knowledge. This problem of static classification models is addressed in this
paper by the introduction of incremental learning for problems in
bioinformatics. Many machine learning tools have been applied to this problem
using static machine learning structures such as neural networks or support
vector machines that are unable to accommodate new information into their
existing models. We utilize the fuzzy ARTMAP as an alternate machine learning
system that has the ability of incrementally learning new data as it becomes
available. The fuzzy ARTMAP is found to be comparable to many of the widespread
machine learning systems. The use of an evolutionary strategy in the selection
and combination of individual classifiers into an ensemble system, coupled with
the incremental learning ability of the fuzzy ARTMAP is proven to be suitable
as a pattern classifier. The algorithm presented is tested using data from the
G-Coupled Protein Receptors Database and shows good accuracy of 83%. The system
presented is also generally applicable, and can be used in problems in genomics
and proteomics.
",major problem computational biology inability exist classification model incorporate expand new domain knowledge problem static classification model address paper introduction incremental learning problem bioinformatics machine learn tool apply problem static machine learn structure neural network support vector machine unable accommodate new information exist model utilize fuzzy artmap alternate machine learning system ability incrementally learn new datum available fuzzy artmap find comparable widespread machine learning system use evolutionary strategy selection combination individual classifier ensemble system couple incremental learning ability fuzzy artmap prove suitable pattern classifier algorithm present test datum g couple protein receptors database show good accuracy 83 system present generally applicable problem genomic proteomic
q-bio,"Erwin Schroedinger, Francis Crick and epigenetic stability","  Schroedinger's book 'What is Life?' is widely credited for having played a
crucial role in development of molecular and cellular biology. My essay
revisits the issues raised by this book from the modern perspective of
epigenetics and systems biology. I contrast two classes of potential mechanisms
of epigenetic stability: 'epigenetic templating' and 'systems biology'
approaches, and consider them from the point of view expressed by Schroedinger.
I also discuss how quantum entanglement, a nonclassical feature of quantum
mechanics, can help to address the 'problem of small numbers' that lead
Schroedinger to promote the idea of molecular code-script for explanation of
stability of biological order.
",schroedinger book life widely credit having play crucial role development molecular cellular biology essay revisit issue raise book modern perspective epigenetic system biology contrast class potential mechanism epigenetic stability epigenetic templating system biology approach consider point view express schroedinger discuss quantum entanglement nonclassical feature quantum mechanic help address problem small number lead schroedinger promote idea molecular code script explanation stability biological order
q-bio,The Worm-Like Chain Theory And Bending Of Short DNA,"  The probability distributions for bending angles in double helical DNA
obtained in all-atom molecular dynamics simulations are compared with
theoretical predictions. The computed distributions remarkably agree with the
worm-like chain theory for double helices of one helical turn and longer, and
qualitatively differ from predictions of the semi-elastic chain model. The
computed data exhibit only small anomalies in the apparent flexibility of short
DNA and cannot account for the recently reported AFM data (Wiggins et al,
Nature nanotechnology 1, 137 (2006)). It is possible that the current atomistic
DNA models miss some essential mechanisms of DNA bending on intermediate length
scales. Analysis of bent DNA structures reveals, however, that the bending
motion is structurally heterogeneous and directionally anisotropic on the
intermediate length scales where the experimental anomalies were detected.
These effects are essential for interpretation of the experimental data and
they also can be responsible for the apparent discrepancy.
",probability distribution bend angle double helical dna obtain atom molecular dynamic simulation compare theoretical prediction computed distribution remarkably agree worm like chain theory double helix helical turn long qualitatively differ prediction semi elastic chain model compute datum exhibit small anomaly apparent flexibility short dna account recently report afm datum wiggins et al nature nanotechnology 1 137 2006 possible current atomistic dna model miss essential mechanism dna bending intermediate length scale analysis bent dna structure reveal bending motion structurally heterogeneous directionally anisotropic intermediate length scale experimental anomaly detect effect essential interpretation experimental datum responsible apparent discrepancy
q-bio,"Effect of beta-Dystroglycan Processing on Utrophin / DP116 Anchorage in
  Normal and MDX Mouse Schwann Cell Membrane","  In the peripheral nervous system, utrophin and the short dystrophin isoform
(Dp116) are co-localized at the outermost layer of the myelin sheath of nerve
fibers; together with the dystroglycan complex. In peripheral nerve, matrix
metalloproteinase (MMP) creates a 30 kDa fragment of beta-dystroglycan, leading
to a disruption of the link between the extracellular matrix and the cell
membrane. Here we asked if the processing of the beta-dystroglycan could
influence the anchorage of Dp116 or/and utrophin in normal and mdx Schwann cell
membrane. We showed that MMP-9 was more activated in mdx nerve than in
wild-type one. This activation leads to an accumulation of the 30 kDa
beta-dystroglycan isoform and have an impact on the anchorage of Dp116 and
utrophin isoforms in mdx Schwann cells membrane. Our results showed that Dp116
had greater affinity to the full length form of beta-dystroglycan than the 30
kDa form. Moreover, we showed for the first time that the short isoform of
utrophin (Up71) was over-expressed in mdx Schwann cells compared to wild-type.
In addition, this utrophin isoform (Up71) seems to have greater affinity to the
30 kDa beta-dystroglycan which could explain a more stabilization of this 30
kDa at the membrane compartment. Our results highlight the potential
participation of the short utrophin isoform and the cleaved form of
beta-dystroglycan in mdx Schwann cell membrane architecture.
",peripheral nervous system utrophin short dystrophin isoform dp116 co localize outermost layer myelin sheath nerve fiber dystroglycan complex peripheral nerve matrix metalloproteinase mmp create 30 kda fragment beta dystroglycan lead disruption link extracellular matrix cell membrane ask processing beta dystroglycan influence anchorage dp116 utrophin normal mdx schwann cell membrane show mmp-9 activate mdx nerve wild type activation lead accumulation 30 kda beta dystroglycan isoform impact anchorage dp116 utrophin isoform mdx schwann cell membrane result show dp116 great affinity length form beta dystroglycan 30 kda form show time short isoform utrophin up71 express mdx schwann cell compare wild type addition utrophin isoform up71 great affinity 30 kda beta dystroglycan explain stabilization 30 kda membrane compartment result highlight potential participation short utrophin isoform cleaved form beta dystroglycan mdx schwann cell membrane architecture
q-bio,Python Unleashed on Systems Biology,"  We have built an open-source software system for the modeling of biomolecular
reaction networks, SloppyCell, which is written in Python and makes substantial
use of third-party libraries for numerics, visualization, and parallel
programming. We highlight here some of the powerful features that Python
provides that enable SloppyCell to do dynamic code synthesis, symbolic
manipulation, and parallel exploration of complex parameter spaces.
",build open source software system modeling biomolecular reaction network sloppycell write python make substantial use party library numeric visualization parallel programming highlight powerful feature python provide enable sloppycell dynamic code synthesis symbolic manipulation parallel exploration complex parameter space
q-bio,A generic mechanism for adaptive growth rate regulation,"  How can a microorganism adapt to a variety of environmental conditions
despite there exists a limited number of signal transduction machineries? We
show that for any growing cells whose gene expression is under stochastic
fluctuations, adaptive cellular state is inevitably selected by noise, even
without specific signal transduction network for it. In general, changes in
protein concentration in a cell are given by its synthesis minus dilution and
degradation, both of which are proportional to the rate of cell growth. In an
adaptive state with a higher growth speed, both terms are large and balanced.
Under the presence of noise in gene expression, the adaptive state is less
affected by stochasticity since both the synthesis and dilution terms are
large, while for a non-adaptive state both the terms are smaller so that cells
are easily kicked out of the original state by noise. Hence, escape time from a
cellular state and the cellular growth rate are negatively correlated. This
leads to a selection of adaptive states with higher growth rates, and model
simulations confirm this selection to take place in general. The results
suggest a general form of adaptation that has never been brought to light - a
process that requires no specific machineries for sensory adaptation. The
present scheme may help explain a wide range of cellular adaptive responses
including the metabolic flux optimization for maximal cell growth.
",microorganism adapt variety environmental condition despite exist limited number signal transduction machinery grow cell gene expression stochastic fluctuation adaptive cellular state inevitably select noise specific signal transduction network general change protein concentration cell give synthesis minus dilution degradation proportional rate cell growth adaptive state high growth speed term large balanced presence noise gene expression adaptive state affect stochasticity synthesis dilution term large non adaptive state term small cell easily kick original state noise escape time cellular state cellular growth rate negatively correlate lead selection adaptive state high growth rate model simulation confirm selection place general result suggest general form adaptation bring light process require specific machinery sensory adaptation present scheme help explain wide range cellular adaptive response include metabolic flux optimization maximal cell growth
q-bio,"Artificial Tongue-Placed Tactile Biofeedback for perceptual
  supplementation: application to human disability and biomedical engineering","  The present paper aims at introducing the innovative technologies, based on
the concept of ""sensory substitution"" or ""perceptual supplementation"", we are
developing in the fields of human disability and biomedical engineering.
Precisely, our goal is to design, develop and validate practical assistive
biomedical and/technical devices and/or rehabilitating procedures for persons
with disabilities, using artificial tongue-placed tactile biofeedback systems.
Proposed applications are dealing with: (1) pressure sores prevention in case
of spinal cord injuries (persons with paraplegia, or tetraplegia); (2) ankle
proprioceptive acuity improvement for driving assistance in older and/or
disabled adults; and (3) balance control improvement to prevent fall in older
and/or disabled adults. This paper presents results of three feasibility
studies performed on young healthy adults.
",present paper aim introduce innovative technology base concept sensory substitution perceptual supplementation develop field human disability biomedical engineering precisely goal design develop validate practical assistive biomedical technical device and/or rehabilitate procedure person disability artificial tongue place tactile biofeedback system propose application deal 1 pressure sore prevention case spinal cord injury person paraplegia tetraplegia 2 ankle proprioceptive acuity improvement drive assistance old and/or disabled adult 3 balance control improvement prevent fall old and/or disabled adult paper present result feasibility study perform young healthy adult
q-bio,Multiple pattern matching: A Markov chain approach,"  RNA motifs typically consist of short, modular patterns that include base
pairs formed within and between modules. Estimating the abundance of these
patterns is of fundamental importance for assessing the statistical
significance of matches in genomewide searches, and for predicting whether a
given function has evolved many times in different species or arose from a
single common ancestor. In this manuscript, we review in an integrated and
self-contained manner some basic concepts of automata theory, generating
functions and transfer matrix methods that are relevant to pattern analysis in
biological sequences. We formalize, in a general framework, the concept of
Markov chain embedding to analyze patterns in random strings produced by a
memoryless source. This conceptualization, together with the capability of
automata to recognize complicated patterns, allows a systematic analysis of
problems related to the occurrence and frequency of patterns in random strings.
The applications we present focus on the concept of synchronization of
automata, as well as automata used to search for a finite number of keywords
(including sets of patterns generated according to base pairing rules) in a
general text.
",rna motif typically consist short modular pattern include base pair form module estimate abundance pattern fundamental importance assess statistical significance match genomewide search predict give function evolve time different specie arise single common ancestor manuscript review integrated self contain manner basic concept automata theory generate function transfer matrix method relevant pattern analysis biological sequence formalize general framework concept markov chain embed analyze pattern random string produce memoryless source conceptualization capability automata recognize complicated pattern allow systematic analysis problem relate occurrence frequency pattern random string application present focus concept synchronization automata automata search finite number keyword include set pattern generate accord base pairing rule general text
q-bio,Network Structure of Protein Folding Pathways,"  The classical approach to protein folding inspired by statistical mechanics
avoids the high dimensional structure of the conformation space by using
effective coordinates. Here we introduce a network approach to capture the
statistical properties of the structure of conformation spaces. Conformations
are represented as nodes of the network, while links are transitions via
elementary rotations around a chemical bond. Self-avoidance of a polypeptide
chain introduces degree correlations in the conformation network, which in turn
lead to energy landscape correlations. Folding can be interpreted as a biased
random walk on the conformation network. We show that the folding pathways
along energy gradients organize themselves into scale free networks, thus
explaining previous observations made via molecular dynamics simulations. We
also show that these energy landscape correlations are essential for recovering
the observed connectivity exponent, which belongs to a different universality
class than that of random energy models. In addition, we predict that the
exponent and therefore the structure of the folding network fundamentally
changes at high temperatures, as verified by our simulations on the AK peptide.
",classical approach protein folding inspire statistical mechanic avoid high dimensional structure conformation space effective coordinate introduce network approach capture statistical property structure conformation space conformation represent node network link transition elementary rotation chemical bond self avoidance polypeptide chain introduce degree correlation conformation network turn lead energy landscape correlation folding interpret biased random walk conformation network fold pathway energy gradient organize scale free network explain previous observation molecular dynamic simulation energy landscape correlation essential recover observed connectivity exponent belong different universality class random energy model addition predict exponent structure fold network fundamentally change high temperature verify simulation ak peptide
q-bio,"Self assembly of a model multicellular organism resembling the
  Dictyostelium slime molds","  The evolution of multicellular organisms from monocellular ancestors
represents one of the greatest advances of the history of life. The assembly of
such multicellular organisms requires signalling and response between cells:
over millions of years these signalling processes have become extremely
sophisticated and refined by evolution, such that study of modern organisms may
not be able to shed much light on the original ancient processes . Here we are
interested in determining how simple a signalling method can be, while still
achieving self-assembly. In 2D a coupled cellular automaton/differential
equation approach models organisms and chemotaxic chemicals, producing
spiralling aggregation. In 3D Lennard-Jones-like particles are used to
represent single cells, and their evolution in response to signalling is
followed by molecular dynamics. It is found that if a single cell is able to
emit a signal which induces others to move towards it, then a colony of
single-cell organisms can assemble into shapes as complex as a tower, a ball
atop a stalk, or a fast-moving slug. The similarity with the behaviour of
modern Dictyostelium slime molds signalling with cyclic adenosine monophosphate
(cAMP) is striking.
",evolution multicellular organism monocellular ancestor represent great advance history life assembly multicellular organism require signal response cell million year signal process extremely sophisticated refine evolution study modern organism able shed light original ancient process interested determine simple signal method achieve self assembly 2d couple cellular automaton differential equation approach model organism chemotaxic chemical produce spiralling aggregation 3d lennard jones like particle represent single cell evolution response signal follow molecular dynamic find single cell able emit signal induce colony single cell organism assemble shape complex tower ball atop stalk fast move slug similarity behaviour modern dictyostelium slime mold signal cyclic adenosine monophosphate camp strike
q-bio,Neutral genetic drift can aid functional protein evolution,"  BACKGROUND: Many of the mutations accumulated by naturally evolving proteins
are neutral in the sense that they do not significantly alter a protein's
ability to perform its primary biological function. However, new protein
functions evolve when selection begins to favor other, ""promiscuous"" functions
that are incidental to a protein's biological role. If mutations that are
neutral with respect to a protein's primary biological function cause
substantial changes in promiscuous functions, these mutations could enable
future functional evolution.
  RESULTS: Here we investigate this possibility experimentally by examining how
cytochrome P450 enzymes that have evolved neutrally with respect to activity on
a single substrate have changed in their abilities to catalyze reactions on
five other substrates. We find that the enzymes have sometimes changed as much
as four-fold in the promiscuous activities. The changes in promiscuous
activities tend to increase with the number of mutations, and can be largely
rationalized in terms of the chemical structures of the substrates. The
activities on chemically similar substrates tend to change in a coordinated
fashion, potentially providing a route for systematically predicting the change
in one function based on the measurement of several others.
  CONCLUSIONS: Our work suggests that initially neutral genetic drift can lead
to substantial changes in protein functions that are not currently under
selection, in effect poising the proteins to more readily undergo functional
evolution should selection ""ask new questions"" in the future.
",background mutation accumulate naturally evolve protein neutral sense significantly alter protein ability perform primary biological function new protein function evolve selection begin favor promiscuous function incidental protein biological role mutation neutral respect protein primary biological function cause substantial change promiscuous function mutation enable future functional evolution result investigate possibility experimentally examine cytochrome p450 enzyme evolve neutrally respect activity single substrate change ability catalyze reaction substrate find enzyme change fold promiscuous activity change promiscuous activity tend increase number mutation largely rationalize term chemical structure substrate activity chemically similar substrate tend change coordinated fashion potentially provide route systematically predict change function base measurement conclusion work suggest initially neutral genetic drift lead substantial change protein function currently selection effect poise protein readily undergo functional evolution selection ask new question future
q-bio,Neural networks with transient state dynamics,"  We investigate dynamical systems characterized by a time series of distinct
semi-stable activity patterns, as they are observed in cortical neural activity
patterns. We propose and discuss a general mechanism allowing for an adiabatic
continuation between attractor networks and a specific adjoined transient-state
network, which is strictly dissipative. Dynamical systems with transient states
retain functionality when their working point is autoregulated; avoiding
prolonged periods of stasis or drifting into a regime of rapid fluctuations. We
show, within a continuous-time neural network model, that a single local
updating rule for online learning allows simultaneously (i) for information
storage via unsupervised Hebbian-type learning, (ii) for adaptive regulation of
the working point and (iii) for the suppression of runaway synaptic growth.
Simulation results are presented; the spontaneous breaking of time-reversal
symmetry and link symmetry are discussed.
",investigate dynamical system characterize time series distinct semi stable activity pattern observe cortical neural activity pattern propose discuss general mechanism allow adiabatic continuation attractor network specific adjoined transient state network strictly dissipative dynamical system transient state retain functionality working point autoregulate avoid prolong period stasis drift regime rapid fluctuation continuous time neural network model single local updating rule online learning allow simultaneously information storage unsupervised hebbian type learning ii adaptive regulation working point iii suppression runaway synaptic growth simulation result present spontaneous breaking time reversal symmetry link symmetry discuss
q-bio,"Immunohistochemical pitfalls in the demonstration of insulin-degrading
  enzyme in normal and neoplastic human tissues","  Previously, we have identified the cytoplasmic zinc metalloprotease
insulin-degrading enzyme(IDE) in human tissues by an immunohistochemical method
involving no antigen retrieval (AR) by pressure cooking to avoid artifacts by
endogenous biotin exposure and a detection kit based on the labeled
streptavidin biotin (LSAB) method. Thereby, we also employed 3% hydrogen
peroxide(H2O2) for the inhibition of endogenous peroxidase activity and
incubated the tissue sections with the biotinylated secondary antibody at room
temperature (RT). We now add the immunohistochemical details that had led us to
this optimized procedure as they also bear a more general relevance when
demonstrating intracellular tissue antigens. Our most important result is that
endogenous peroxidase inhibition by 0.3% H2O2 coincided with an apparently
positive IDE staining in an investigated breast cancer specimen whereas
combining a block by 3% H2O2 with an incubation of the biotinylated secondary
antibody at RT, yet not at 37 degrees Celsius, revealed this specimen as almost
entirely IDE-negative. Our present data caution against three different
immunohistochemical pitfalls that might cause falsely positive results and
artifacts when using an LSAB- and peroxidase-based detection method: pressure
cooking for AR, insufficient quenching of endogenous peroxidases and heating of
tissue sections while incubating with biotinylated secondary antibodies.
",previously identify cytoplasmic zinc metalloprotease insulin degrade enzyme(ide human tissue immunohistochemical method involve antigen retrieval ar pressure cook avoid artifact endogenous biotin exposure detection kit base label streptavidin biotin lsab method employ 3 hydrogen peroxide(h2o2 inhibition endogenous peroxidase activity incubate tissue section biotinylated secondary antibody room temperature rt add immunohistochemical detail lead optimize procedure bear general relevance demonstrate intracellular tissue antigen important result endogenous peroxidase inhibition 0.3 h2o2 coincide apparently positive ide stain investigated breast cancer speciman combine block 3 h2o2 incubation biotinylated secondary antibody rt 37 degree celsius reveal speciman entirely ide negative present datum caution different immunohistochemical pitfall cause falsely positive result artifact lsab- peroxidase base detection method pressure cooking ar insufficient quenching endogenous peroxidase heating tissue section incubate biotinylated secondary antibody
q-bio,Information flow and optimization in transcriptional control,"  In the simplest view of transcriptional regulation, the expression of a gene
is turned on or off by changes in the concentration of a transcription factor
(TF). We use recent data on noise levels in gene expression to show that it
should be possible to transmit much more than just one regulatory bit.
Realizing this optimal information capacity would require that the dynamic
range of TF concentrations used by the cell, the input/output relation of the
regulatory module, and the noise levels of binding and transcription satisfy
certain matching relations. This parameter-free prediction is in good agreement
with recent experiments on the Bicoid/Hunchback system in the early Drosophila
embryo, and this system achieves ~90% of its theoretical maximum information
transmission.
",simple view transcriptional regulation expression gene turn change concentration transcription factor tf use recent datum noise level gene expression possible transmit regulatory bit realize optimal information capacity require dynamic range tf concentration cell input output relation regulatory module noise level bind transcription satisfy certain matching relation parameter free prediction good agreement recent experiment bicoid hunchback system early drosophila embryo system achieve ~90 theoretical maximum information transmission
q-bio,Validating module network learning algorithms using simulated data,"  In recent years, several authors have used probabilistic graphical models to
learn expression modules and their regulatory programs from gene expression
data. Here, we demonstrate the use of the synthetic data generator SynTReN for
the purpose of testing and comparing module network learning algorithms. We
introduce a software package for learning module networks, called LeMoNe, which
incorporates a novel strategy for learning regulatory programs. Novelties
include the use of a bottom-up Bayesian hierarchical clustering to construct
the regulatory programs, and the use of a conditional entropy measure to assign
regulators to the regulation program nodes. Using SynTReN data, we test the
performance of LeMoNe in a completely controlled situation and assess the
effect of the methodological changes we made with respect to an existing
software package, namely Genomica. Additionally, we assess the effect of
various parameters, such as the size of the data set and the amount of noise,
on the inference performance. Overall, application of Genomica and LeMoNe to
simulated data sets gave comparable results. However, LeMoNe offers some
advantages, one of them being that the learning process is considerably faster
for larger data sets. Additionally, we show that the location of the regulators
in the LeMoNe regulation programs and their conditional entropy may be used to
prioritize regulators for functional validation, and that the combination of
the bottom-up clustering strategy with the conditional entropy-based assignment
of regulators improves the handling of missing or hidden regulators.
",recent year author probabilistic graphical model learn expression module regulatory program gene expression datum demonstrate use synthetic datum generator syntren purpose testing compare module network learning algorithm introduce software package learn module network call lemone incorporate novel strategy learn regulatory program novelty include use bayesian hierarchical clustering construct regulatory program use conditional entropy measure assign regulator regulation program node syntren datum test performance lemone completely control situation assess effect methodological change respect exist software package genomica additionally assess effect parameter size datum set noise inference performance overall application genomica lemone simulate data set give comparable result lemone offer advantage learning process considerably fast large datum set additionally location regulator lemone regulation program conditional entropy prioritize regulator functional validation combination clustering strategy conditional entropy base assignment regulator improve handling miss hidden regulator
q-bio,"Effects of the DNA state fluctuation on single-cell dynamics of
  self-regulating gene","  A dynamical mean-field theory is developed to analyze stochastic single-cell
dynamics of gene expression. By explicitly taking account of nonequilibrium and
nonadiabatic features of the DNA state fluctuation, two-time correlation
functions and response functions of single-cell dynamics are derived. The
method is applied to a self-regulating gene to predict a rich variety of
dynamical phenomena such as anomalous increase of relaxation time and
oscillatory decay of correlations. Effective ""temperature"" defined as the ratio
of the correlation to the response in the protein number is small when the DNA
state change is frequent, while it grows large when the DNA state change is
infrequent, indicating the strong enhancement of noise in the latter case.
",dynamical mean field theory develop analyze stochastic single cell dynamic gene expression explicitly take account nonequilibrium nonadiabatic feature dna state fluctuation time correlation function response function single cell dynamic derive method apply self regulate gene predict rich variety dynamical phenomenon anomalous increase relaxation time oscillatory decay correlation effective temperature define ratio correlation response protein number small dna state change frequent grow large dna state change infrequent indicate strong enhancement noise case
q-bio,"Cell adhesion and cortex contractility determine cell patterning in the
  Drosophila retina","  Hayashi and Carthew (Nature 431 [2004], 647) have shown that the packing of
cone cells in the Drosophila retina resembles soap bubble packing, and that
changing E- and N-cadherin expression can change this packing, as well as cell
shape.
  The analogy with bubbles suggests that cell packing is driven by surface
minimization. We find that this assumption is insufficient to model the
experimentally observed shapes and packing of the cells based on their cadherin
expression. We then consider a model in which adhesion leads to a surface
increase, balanced by cell cortex contraction. Using the experimentally
observed distributions of E- and N-cadherin, we simulate the packing and cell
shapes in the wildtype eye. Furthermore, by changing only the corresponding
parameters, this model can describe the mutants with different numbers of
cells, or changes in cadherin expression.
",hayashi carthew nature 431 2004 647 show packing cone cell drosophila retina resemble soap bubble packing change e- n cadherin expression change packing cell shape analogy bubble suggest cell packing drive surface minimization find assumption insufficient model experimentally observe shape packing cell base cadherin expression consider model adhesion lead surface increase balance cell cortex contraction experimentally observed distribution e- n cadherin simulate packing cell shape wildtype eye furthermore change corresponding parameter model describe mutant different number cell change cadherin expression
q-bio,Dimensionality and dynamics in the behavior of C. elegans,"  A major challenge in analyzing animal behavior is to discover some underlying
simplicity in complex motor actions. Here we show that the space of shapes
adopted by the nematode C. elegans is surprisingly low dimensional, with just
four dimensions accounting for 95% of the shape variance, and we partially
reconstruct ""equations of motion"" for the dynamics in this space. These
dynamics have multiple attractors, and we find that the worm visits these in a
rapid and almost completely deterministic response to weak thermal stimuli.
Stimulus-dependent correlations among the different modes suggest that one can
generate more reliable behaviors by synchronizing stimuli to the state of the
worm in shape space. We confirm this prediction, effectively ""steering"" the
worm in real time.
",major challenge analyze animal behavior discover underlying simplicity complex motor action space shape adopt nematode c. elegan surprisingly low dimensional dimension account 95 shape variance partially reconstruct equation motion dynamic space dynamic multiple attractor find worm visit rapid completely deterministic response weak thermal stimulus stimulus dependent correlation different mode suggest generate reliable behavior synchronize stimulus state worm shape space confirm prediction effectively steer worm real time
q-bio,Anisotropic probabilistic cellular automaton for a predator-prey system,"  We consider a probabilistic cellular automaton to analyze the stochastic
dynamics of a predator-prey system. The local rules are Markovian and are based
in the Lotka-Volterra model. The individuals of each species reside on the
sites of a lattice and interact with an unsymmetrical neighborhood. We look for
the effect of the space anisotropy in the characterization of the oscillations
of the species population densities. Our study of the probabilistic cellular
automaton is based on simple and pair mean-field approximations and explicitly
takes into account spatial anisotropy.
",consider probabilistic cellular automaton analyze stochastic dynamic predator prey system local rule markovian base lotka volterra model individual specie reside site lattice interact unsymmetrical neighborhood look effect space anisotropy characterization oscillation species population density study probabilistic cellular automaton base simple pair mean field approximation explicitly take account spatial anisotropy
q-bio,Microscale swimming: The molecular dynamics approach,"  The self-propelled motion of microscopic bodies immersed in a fluid medium is
studied using molecular dynamics simulation. The advantage of the atomistic
approach is that the detailed level of description allows complete freedom in
specifying the swimmer design and its coupling with the surrounding fluid. A
series of two-dimensional swimming bodies employing a variety of propulsion
mechanisms -- motivated by biological and microrobotic designs -- is
investigated, including the use of moving limbs, changing body shapes and fluid
jets. The swimming efficiency and the nature of the induced, time-dependent
flow fields are found to differ widely among body designs and propulsion
mechanisms.
",self propel motion microscopic body immerse fluid medium study molecular dynamic simulation advantage atomistic approach detailed level description allow complete freedom specify swimmer design coupling surround fluid series dimensional swimming body employ variety propulsion mechanism motivate biological microrobotic design investigate include use move limb change body shape fluid jet swimming efficiency nature induced time dependent flow field find differ widely body design propulsion mechanism
q-bio,Asymptotic velocity of one dimensional diffusions with periodic drift,"  We consider the asymptotic behaviour of the solution of one dimensional
stochastic differential equations and Langevin equations in periodic
backgrounds with zero average. We prove that in several such models, there is
generically a non vanishing asymptotic velocity, despite of the fact that the
average of the background is zero.
",consider asymptotic behaviour solution dimensional stochastic differential equation langevin equation periodic background zero average prove model generically non vanish asymptotic velocity despite fact average background zero
q-bio,Chromatin Folding in Relation to Human Genome Function,"  Three-dimensional (3D) chromatin structure is closely related to genome
function, in particular transcription. However, the folding path of the
chromatin fiber in the interphase nucleus is unknown. Here, we systematically
measured the 3D physical distance between pairwise labeled genomic positions in
gene-dense, highly transcribed domains and gene-poor less active areas on
chromosomes 1 and 11 in G1 nuclei of human primary fibroblasts, using
fluorescence in situ hybridization. Interpretation of our results and those
published by others, based on polymer physics, shows that the folding of the
chromatin fiber can be described as a polymer in a globular state (GS),
maintained by intra-polymer attractive interactions that counteract
self-avoidance forces. The GS polymer model is able to describe chromatin
folding in as well the highly expressed domains as the lowly expressed ones,
indicating that they differ in Kuhn length and chromatin compaction. Each type
of genomic domain constitutes an ensemble of relatively compact globular
folding states, resulting in a considerable cellto- cell variation between
otherwise identical cells. We present evidence for different polymer folding
regimes of the chromatin fiber on the length scale of a few mega base pairs and
on that of complete chromosome arms (several tens of Mb). Our results present a
novel view on the folding of the chromatin fiber in interphase and open the
possibility to explore the nature of the intra-chromatin fiber interactions.
",dimensional 3d chromatin structure closely relate genome function particular transcription fold path chromatin fiber interphase nucleus unknown systematically measure 3d physical distance pairwise label genomic position gene dense highly transcribe domain gene poor active area chromosome 1 11 g1 nucleus human primary fibroblast fluorescence situ hybridization interpretation result publish base polymer physic show folding chromatin fiber describe polymer globular state gs maintain intra polymer attractive interaction counteract self avoidance force gs polymer model able describe chromatin folding highly express domain lowly express one indicate differ kuhn length chromatin compaction type genomic domain constitute ensemble relatively compact globular folding state result considerable cellto- cell variation identical cell present evidence different polymer folding regime chromatin fiber length scale mega base pair complete chromosome arm ten mb result present novel view folding chromatin fiber interphase open possibility explore nature intra chromatin fiber interaction
q-bio,Nontrivial quantum effects in biology: A skeptical physicists' view,"  Invited contribution to ""Quantum Aspects of Life"", D. Abbott Ed. (World
Scientific, Singapore, 2007).
",invite contribution quantum aspects life d. abbott ed world scientific singapore 2007
q-bio,Detection of Aneuploidy with Digital PCR,"  The widespread use of genetic testing in high risk pregnancies has created
strong interest in rapid and accurate molecular diagnostics for common
chromosomal aneuploidies. We show here that digital polymerase chain reaction
(dPCR) can be used for accurate measurement of trisomy 21 (Down's Syndrome),
the most common human aneuploidy. dPCR is generally applicable to any
aneuploidy, does not depend on allelic distribution or gender, and is able to
detect signals in the presence of mosaics or contaminating maternal DNA.
",widespread use genetic testing high risk pregnancy create strong interest rapid accurate molecular diagnostic common chromosomal aneuploidy digital polymerase chain reaction dpcr accurate measurement trisomy 21 syndrome common human aneuploidy dpcr generally applicable aneuploidy depend allelic distribution gender able detect signal presence mosaic contaminate maternal dna
q-bio,"A three-state prediction of single point mutations on protein stability
  changes","  A basic question of protein structural studies is to which extent mutations
affect the stability. This question may be addressed starting from sequence
and/or from structure. In proteomics and genomics studies prediction of protein
stability free energy change (DDG) upon single point mutation may also help the
annotation process. The experimental SSG values are affected by uncertainty as
measured by standard deviations. Most of the DDG values are nearly zero (about
32% of the DDG data set ranges from -0.5 to 0.5 Kcal/mol) and both the value
and sign of DDG may be either positive or negative for the same mutation
blurring the relationship among mutations and expected DDG value. In order to
overcome this problem we describe a new predictor that discriminates between 3
mutation classes: destabilizing mutations (DDG<-0.5 Kcal/mol), stabilizing
mutations (DDG>0.5 Kcal/mol) and neutral mutations (-0.5<=DDG<=0.5 Kcal/mol).
In this paper a support vector machine starting from the protein sequence or
structure discriminates between stabilizing, destabilizing and neutral
mutations. We rank all the possible substitutions according to a three state
classification system and show that the overall accuracy of our predictor is as
high as 52% when performed starting from sequence information and 58% when the
protein structure is available, with a mean value correlation coefficient of
0.30 and 0.39, respectively. These values are about 20 points per cent higher
than those of a random predictor.
",basic question protein structural study extent mutation affect stability question address start sequence and/or structure proteomic genomic study prediction protein stability free energy change ddg single point mutation help annotation process experimental ssg value affect uncertainty measure standard deviation ddg value nearly zero 32 ddg datum set range -0.5 0.5 kcal mol value sign ddg positive negative mutation blur relationship mutation expect ddg value order overcome problem describe new predictor discriminate 3 mutation class destabilizing mutation ddg<-0.5 kcal mol stabilize mutation ddg>0.5 kcal mol neutral mutation -0.5<=ddg<=0.5 kcal mol paper support vector machine start protein sequence structure discriminate stabilize destabilizing neutral mutation rank possible substitution accord state classification system overall accuracy predictor high 52 perform start sequence information 58 protein structure available mean value correlation coefficient 0.30 0.39 respectively value 20 point cent high random predictor
q-bio,"Theoretical Analysis of Subthreshold Oscillatory Behaviors in Nonlinear
  Autonomous Systems","  We have developed a linearization method to investigate the subthreshold
oscillatory behaviors in nonlinear autonomous systems. By considering firstly
the neuronal system as an example, we show that this theoretical approach can
predict quantitatively the subthreshold oscillatory activities, including the
damping coefficients and the oscillatory frequencies which are in good
agreement with those observed in experiments. Then we generalize the
linearization method to an arbitrary autonomous nonlinear system. The detailed
extension of this theoretical approach is also presented and further discussed.
",develop linearization method investigate subthreshold oscillatory behavior nonlinear autonomous system consider firstly neuronal system example theoretical approach predict quantitatively subthreshold oscillatory activity include damp coefficient oscillatory frequency good agreement observe experiment generalize linearization method arbitrary autonomous nonlinear system detailed extension theoretical approach present discuss
q-bio,Risk perception in epidemic modeling,"  We investigate the effects of risk perception in a simple model of epidemic
spreading. We assume that the perception of the risk of being infected depends
on the fraction of neighbors that are ill. The effect of this factor is to
decrease the infectivity, that therefore becomes a dynamical component of the
model. We study the problem in the mean-field approximation and by numerical
simulations for regular, random and scale-free networks.
  We show that for homogeneous and random networks, there is always a value of
perception that stops the epidemics. In the ``worst-case'' scenario of a
scale-free network with diverging input connectivity, a linear perception
cannot stop the epidemics; however we show that a non-linear increase of the
perception risk may lead to the extinction of the disease. This transition is
discontinuous, and is not predicted by the mean-field analysis.
",investigate effect risk perception simple model epidemic spread assume perception risk infect depend fraction neighbor ill effect factor decrease infectivity dynamical component model study problem mean field approximation numerical simulation regular random scale free network homogeneous random network value perception stop epidemic ` ` bad case scenario scale free network diverge input connectivity linear perception stop epidemic non linear increase perception risk lead extinction disease transition discontinuous predict mean field analysis
q-bio,Introduction to protein folding for physicists,"  The prediction of the three-dimensional native structure of proteins from the
knowledge of their amino acid sequence, known as the protein folding problem,
is one of the most important yet unsolved issues of modern science. Since the
conformational behaviour of flexible molecules is nothing more than a complex
physical problem, increasingly more physicists are moving into the study of
protein systems, bringing with them powerful mathematical and computational
tools, as well as the sharp intuition and deep images inherent to the physics
discipline. This work attempts to facilitate the first steps of such a
transition. In order to achieve this goal, we provide an exhaustive account of
the reasons underlying the protein folding problem enormous relevance and
summarize the present-day status of the methods aimed to solving it. We also
provide an introduction to the particular structure of these biological
heteropolymers, and we physically define the problem stating the assumptions
behind this (commonly implicit) definition. Finally, we review the 'special
flavor' of statistical mechanics that is typically used to study the
astronomically large phase spaces of macromolecules. Throughout the whole work,
much material that is found scattered in the literature has been put together
here to improve comprehension and to serve as a handy reference.
",prediction dimensional native structure protein knowledge amino acid sequence know protein fold problem important unsolved issue modern science conformational behaviour flexible molecule complex physical problem increasingly physicist move study protein system bring powerful mathematical computational tool sharp intuition deep image inherent physics discipline work attempt facilitate step transition order achieve goal provide exhaustive account reason underlie protein fold problem enormous relevance summarize present day status method aim solve provide introduction particular structure biological heteropolymer physically define problem state assumption commonly implicit definition finally review special flavor statistical mechanic typically study astronomically large phase space macromolecule work material find scatter literature improve comprehension serve handy reference
q-bio,Evolving inductive generalization via genetic self-assembly,"  We propose that genetic encoding of self-assembling components greatly
enhances the evolution of complex systems and provides an efficient platform
for inductive generalization, i.e. the inductive derivation of a solution to a
problem with a potentially infinite number of instances from a limited set of
test examples. We exemplify this in simulations by evolving scalable circuitry
for several problems. One of them, digital multiplication, has been intensively
studied in recent years, where hitherto the evolutionary design of only
specific small multipliers was achieved. The fact that this and other problems
can be solved in full generality employing self-assembly sheds light on the
evolutionary role of self-assembly in biology and is of relevance for the
design of complex systems in nano- and bionanotechnology.
",propose genetic encoding self assemble component greatly enhance evolution complex system provide efficient platform inductive generalization i.e. inductive derivation solution problem potentially infinite number instance limited set test example exemplify simulation evolve scalable circuitry problem digital multiplication intensively study recent year hitherto evolutionary design specific small multiplier achieve fact problem solve generality employ self assembly shed light evolutionary role self assembly biology relevance design complex system nano- bionanotechnology
q-bio,"Multipolar Reactive DPD: A Novel Tool for Spatially Resolved Systems
  Biology","  This article reports about a novel extension of dissipative particle dynamics
(DPD) that allows the study of the collective dynamics of complex chemical and
structural systems in a spatially resolved manner with a combinatorially
complex variety of different system constituents. We show that introducing
multipolar interactions between particles leads to extended membrane structures
emerging in a self-organized manner and exhibiting both the necessary
mechanical stability for transport and fluidity so as to provide a
two-dimensional self-organizing dynamic reaction environment for kinetic
studies in the context of cell biology. We further show that the emergent
dynamics of extended membrane bound objects is in accordance with scaling laws
imposed by physics.
",article report novel extension dissipative particle dynamic dpd allow study collective dynamic complex chemical structural system spatially resolve manner combinatorially complex variety different system constituent introduce multipolar interaction particle lead extended membrane structure emerge self organize manner exhibit necessary mechanical stability transport fluidity provide dimensional self organize dynamic reaction environment kinetic study context cell biology emergent dynamic extend membrane bind object accordance scale law impose physic
q-bio,"Visual Data Mining of Genomic Databases by Immersive Graph-Based
  Exploration","  Biologists are leading current research on genome characterization
(sequencing, alignment, transcription), providing a huge quantity of raw data
about many genome organisms. Extracting knowledge from this raw data is an
important process for biologists, using usually data mining approaches.
However, it is difficult to deals with these genomic information using actual
bioinformatics data mining tools, because data are heterogeneous, huge in
quantity and geographically distributed. In this paper, we present a new
approach between data mining and virtual reality visualization, called visual
data mining. Indeed Virtual Reality becomes ripe, with efficient display
devices and intuitive interaction in an immersive context. Moreover, biologists
use to work with 3D representation of their molecules, but in a desktop
context. We present a software solution, Genome3DExplorer, which addresses the
problem of genomic data visualization, of scene management and interaction.
This solution is based on a well-adapted graphical and interaction paradigm,
where local and global topological characteristics of data are easily visible,
on the contrary to traditional genomic database browsers, always focused on the
zoom and details level.
",biologist lead current research genome characterization sequence alignment transcription provide huge quantity raw datum genome organism extract knowledge raw datum important process biologist usually datum mining approach difficult deal genomic information actual bioinformatics datum mining tool datum heterogeneous huge quantity geographically distribute paper present new approach datum mining virtual reality visualization call visual datum mining virtual reality ripe efficient display device intuitive interaction immersive context biologist use work 3d representation molecule desktop context present software solution genome3dexplorer address problem genomic datum visualization scene management interaction solution base adapt graphical interaction paradigm local global topological characteristic datum easily visible contrary traditional genomic database browser focus zoom detail level
q-bio,Statistical mechanics and stability of a model eco-system,"  We study a model ecosystem by means of dynamical techniques from disordered
systems theory. The model describes a set of species subject to competitive
interactions through a background of resources, which they feed upon.
Additionally direct competitive or co-operative interaction between species may
occur through a random coupling matrix. We compute the order parameters of the
system in a fixed point regime, and identify the onset of instability and
compute the phase diagram. We focus on the effects of variability of resources,
direct interaction between species, co-operation pressure and dilution on the
stability and the diversity of the ecosystem. It is shown that resources can be
exploited optimally only in absence of co-operation pressure or direct
interaction between species.
",study model ecosystem mean dynamical technique disorder system theory model describe set specie subject competitive interaction background resource feed additionally direct competitive co operative interaction specie occur random coupling matrix compute order parameter system fix point regime identify onset instability compute phase diagram focus effect variability resource direct interaction specie co operation pressure dilution stability diversity ecosystem show resource exploit optimally absence co operation pressure direct interaction specie
q-bio,"Controlling posture using a plantar pressure-based, tongue-placed
  tactile biofeedback system","  The present paper introduces an original biofeedback system for improving
human balance control, whose underlying principle consists in providing
additional sensory information related to foot sole pressure distribution to
the user through a tongue-placed tactile output device. To assess the effect of
this biofeedback system on postural control during quiet standing, ten young
healthy adults were asked to stand as immobile as possible with their eyes
closed in two conditions of No-biofeedback and Biofeedback. Centre of foot
pressure (CoP) displacements were recorded using a force platform. Results
showed reduced CoP displacements in the Biofeedback relative to the
No-biofeedback condition. The present findings evidenced the ability of the
central nervous system to efficiently integrate an artificial plantar-based,
tongue-placed tactile biofeedback for controlling control posture during quiet
standing.
",present paper introduce original biofeedback system improve human balance control underlie principle consist provide additional sensory information relate foot sole pressure distribution user tongue place tactile output device assess effect biofeedback system postural control quiet standing young healthy adult ask stand immobile possible eye close condition biofeedback biofeedback centre foot pressure cop displacement record force platform result show reduce cop displacement biofeedback relative biofeedback condition present finding evidence ability central nervous system efficiently integrate artificial plantar base tongue place tactile biofeedback control control posture quiet standing
q-bio,"Selection Against Demographic Stochasticity in Age-Structured
  Populations","  It has been shown that differences in fecundity variance can influence the
probability of invasion of a genotype in a population, i.e. a genotype with
lower variance in offspring number can be favored in finite populations even if
it has a somewhat lower mean fitness than a competitor. In this paper,
Gillespie's results are extended to population genetic systems with explicit
age structure, where the demographic variance (variance in growth rate)
calculated in the work of Engen and colleagues is used as a generalization of
""variance in offspring number"" to predict the interaction between deterministic
and random forces driving change in allele frequency. By calculating the
variance from the life history parameters, it is shown that selection against
variance in the growth rate will favor a genotypes with lower stochasticity in
age specific survival and fertility rates. A diffusion approximation for
selection and drift in a population with two genotypes with different life
history matrices (and therefore, different growth rates and demographic
variances) is derived and shown to be consistent with individual based
simulations. It is also argued that for finite populations, perturbation
analyses of both the growth rate and demographic variances may be necessary to
determine the sensitivity of ""fitness"" (broadly defined) to changes in the life
history parameters.
",show difference fecundity variance influence probability invasion genotype population i.e. genotype low variance offspring number favor finite population somewhat low mean fitness competitor paper gillespie result extend population genetic system explicit age structure demographic variance variance growth rate calculate work engen colleague generalization variance offspring number predict interaction deterministic random force drive change allele frequency calculate variance life history parameter show selection variance growth rate favor genotype low stochasticity age specific survival fertility rate diffusion approximation selection drift population genotype different life history matrix different growth rate demographic variance derive show consistent individual base simulation argue finite population perturbation analysis growth rate demographic variance necessary determine sensitivity fitness broadly define change life history parameter
q-bio,Shape instabilities in vesicles: a phase-field model,"  A phase field model for dealing with shape instabilities in fluid membrane
vesicles is presented. This model takes into account the Canham-Helfrich
bending energy with spontaneous curvature. A dynamic equation for the
phase-field is also derived. With this model it is possible to see the vesicle
shape deformation dynamically, when some external agent instabilizes the
membrane, for instance, inducing an inhomogeneous spontaneous curvature. The
numerical scheme used is detailed and some stationary shapes are shown together
with a shape diagram for vesicles of spherical topology and no spontaneous
curvature, in agreement with known results.
",phase field model deal shape instability fluid membrane vesicle present model take account canham helfrich bend energy spontaneous curvature dynamic equation phase field derive model possible vesicle shape deformation dynamically external agent instabilize membrane instance induce inhomogeneous spontaneous curvature numerical scheme detail stationary shape show shape diagram vesicle spherical topology spontaneous curvature agreement known result
q-bio,SIR epidemics in dynamic contact networks,"  Contact patterns in populations fundamentally influence the spread of
infectious diseases. Current mathematical methods for epidemiological
forecasting on networks largely assume that contacts between individuals are
fixed, at least for the duration of an outbreak. In reality, contact patterns
may be quite fluid, with individuals frequently making and breaking social or
sexual relationships. Here we develop a mathematical approach to predicting
disease transmission on dynamic networks in which each individual has a
characteristic behavior (typical contact number), but the identities of their
contacts change in time. We show that dynamic contact patterns shape
epidemiological dynamics in ways that cannot be adequately captured in static
network models or mass-action models. Our new model interpolates smoothly
between static network models and mass-action models using a mixing parameter,
thereby providing a bridge between disparate classes of epidemiological models.
Using epidemiological and sexual contact data from an Atlanta high school, we
then demonstrate the utility of this method for forecasting and controlling
sexually transmitted disease outbreaks.
",contact pattern population fundamentally influence spread infectious disease current mathematical method epidemiological forecasting network largely assume contact individual fix duration outbreak reality contact pattern fluid individual frequently make break social sexual relationship develop mathematical approach predict disease transmission dynamic network individual characteristic behavior typical contact number identity contact change time dynamic contact pattern shape epidemiological dynamic way adequately capture static network model mass action model new model interpolate smoothly static network model mass action model mix parameter provide bridge disparate class epidemiological model epidemiological sexual contact datum atlanta high school demonstrate utility method forecast control sexually transmit disease outbreak
q-fin,Collective behavior of stock price movements in an emerging market,"  To investigate the universality of the structure of interactions in different
markets, we analyze the cross-correlation matrix C of stock price fluctuations
in the National Stock Exchange (NSE) of India. We find that this emerging
market exhibits strong correlations in the movement of stock prices compared to
developed markets, such as the New York Stock Exchange (NYSE). This is shown to
be due to the dominant influence of a common market mode on the stock prices.
By comparison, interactions between related stocks, e.g., those belonging to
the same business sector, are much weaker. This lack of distinct sector
identity in emerging markets is explicitly shown by reconstructing the network
of mutually interacting stocks. Spectral analysis of C for NSE reveals that,
the few largest eigenvalues deviate from the bulk of the spectrum predicted by
random matrix theory, but they are far fewer in number compared to, e.g., NYSE.
We show this to be due to the relative weakness of intra-sector interactions
between stocks, compared to the market mode, by modeling stock price dynamics
with a two-factor model. Our results suggest that the emergence of an internal
structure comprising multiple groups of strongly coupled components is a
signature of market development.
",investigate universality structure interaction different market analyze cross correlation matrix c stock price fluctuation national stock exchange nse india find emerge market exhibit strong correlation movement stock price compare developed market new york stock exchange nyse show dominant influence common market mode stock price comparison interaction related stock e.g. belong business sector weak lack distinct sector identity emerge market explicitly show reconstruct network mutually interact stock spectral analysis c nse reveal large eigenvalue deviate bulk spectrum predict random matrix theory far few number compare e.g. nyse relative weakness intra sector interaction stock compare market mode model stock price dynamic factor model result suggest emergence internal structure comprise multiple group strongly couple component signature market development
q-fin,"Yield Curve Shapes and the Asymptotic Short Rate Distribution in Affine
  One-Factor Models","  We consider a model for interest rates, where the short rate is given by a
time-homogenous, one-dimensional affine process in the sense of Duffie,
Filipovic and Schachermayer. We show that in such a model yield curves can only
be normal, inverse or humped (i.e. endowed with a single local maximum). Each
case can be characterized by simple conditions on the present short rate. We
give conditions under which the short rate process will converge to a limit
distribution and describe the limit distribution in terms of its cumulant
generating function. We apply our results to the Vasicek model, the CIR model,
a CIR model with added jumps and a model of Ornstein-Uhlenbeck type.
",consider model interest rate short rate give time homogenous dimensional affine process sense duffie filipovic schachermayer model yield curve normal inverse hump i.e. endow single local maximum case characterize simple condition present short rate condition short rate process converge limit distribution describe limit distribution term cumulant generating function apply result vasicek model cir model cir model add jump model ornstein uhlenbeck type
q-fin,Average optimality for risk-sensitive control with general state space,"  This paper deals with discrete-time Markov control processes on a general
state space. A long-run risk-sensitive average cost criterion is used as a
performance measure. The one-step cost function is nonnegative and possibly
unbounded. Using the vanishing discount factor approach, the optimality
inequality and an optimal stationary strategy for the decision maker are
established.
",paper deal discrete time markov control process general state space long run risk sensitive average cost criterion performance measure step cost function nonnegative possibly unbounded vanish discount factor approach optimality inequality optimal stationary strategy decision maker establish
q-fin,"Approximation of the distribution of a stationary Markov process with
  application to option pricing","  We build a sequence of empirical measures on the space D(R_+,R^d) of
R^d-valued c\`adl\`ag functions on R_+ in order to approximate the law of a
stationary R^d-valued Markov and Feller process (X_t). We obtain some general
results of convergence of this sequence. Then, we apply them to Brownian
diffusions and solutions to L\'evy driven SDE's under some Lyapunov-type
stability assumptions. As a numerical application of this work, we show that
this procedure gives an efficient way of option pricing in stochastic
volatility models.
","build sequence empirical measure space d(r_+,r^d r^d value c\`adl\`ag function r_+ order approximate law stationary r^d value markov feller process x_t obtain general result convergence sequence apply brownian diffusion solution l\'evy drive sde lyapunov type stability assumption numerical application work procedure give efficient way option pricing stochastic volatility model"
q-fin,Stock market return distributions: from past to present,"  We show that recent stock market fluctuations are characterized by the
cumulative distributions whose tails on short, minute time scales exhibit power
scaling with the scaling index alpha > 3 and this index tends to increase
quickly with decreasing sampling frequency. Our study is based on
high-frequency recordings of the S&P500, DAX and WIG20 indices over the
interval May 2004 - May 2006. Our findings suggest that dynamics of the
contemporary market may differ from the one observed in the past. This effect
indicates a constantly increasing efficiency of world markets.
",recent stock market fluctuation characterize cumulative distribution tail short minute time scale exhibit power scaling scale index alpha > 3 index tend increase quickly decrease sampling frequency study base high frequency recording s&p500 dax wig20 indice interval 2004 2006 finding suggest dynamic contemporary market differ observe past effect indicate constantly increase efficiency world market
q-fin,"Analysis of the real estate market in Las Vegas: Bubble, seasonal
  patterns, and prediction of the CSW indexes","  We analyze 27 house price indexes of Las Vegas from Jun. 1983 to Mar. 2005,
corresponding to 27 different zip codes. These analyses confirm the existence
of a real-estate bubble, defined as a price acceleration faster than
exponential, which is found however to be confined to a rather limited time
interval in the recent past from approximately 2003 to mid-2004 and has
progressively transformed into a more normal growth rate comparable to
pre-bubble levels in 2005. There has been no bubble till 2002 except for a
medium-sized surge in 1990. In addition, we have identified a strong yearly
periodicity which provides a good potential for fine-tuned prediction from
month to month. A monthly monitoring using a model that we have developed could
confirm, by testing the intra-year structure, if indeed the market has returned
to ``normal'' or if more turbulence is expected ahead. We predict the evolution
of the indexes one year ahead, which is validated with new data up to Sep.
2006. The present analysis demonstrates the existence of very significant
variations at the local scale, in the sense that the bubble in Las Vegas seems
to have preceded the more global USA bubble and has ended approximately two
years earlier (mid 2004 for Las Vegas compared with mid-2006 for the whole of
the USA).
",analyze 27 house price index las vegas jun. 1983 mar. 2005 correspond 27 different zip code analysis confirm existence real estate bubble define price acceleration fast exponential find confine limited time interval recent past approximately 2003 mid-2004 progressively transform normal growth rate comparable pre bubble level 2005 bubble till 2002 medium sized surge 1990 addition identify strong yearly periodicity provide good potential fine tune prediction month month monthly monitoring model develop confirm test intra year structure market return ` ` normal turbulence expect ahead predict evolution index year ahead validate new datum sep. 2006 present analysis demonstrate existence significant variation local scale sense bubble las vegas precede global usa bubble end approximately year early mid 2004 las vegas compare mid-2006 usa
q-fin,"Weak and Strong Taylor methods for numerical solutions of stochastic
  differential equations","  We apply results of Malliavin-Thalmaier-Watanabe for strong and weak Taylor
expansions of solutions of perturbed stochastic differential equations (SDEs).
In particular, we work out weight expressions for the Taylor coefficients of
the expansion. The results are applied to LIBOR market models in order to deal
with the typical stochastic drift and with stochastic volatility. In contrast
to other accurate methods like numerical schemes for the full SDE, we obtain
easily tractable expressions for accurate pricing. In particular, we present an
easily tractable alternative to ``freezing the drift'' in LIBOR market models,
which has an accuracy similar to the full numerical scheme. Numerical examples
underline the results.
",apply result malliavin thalmaier watanabe strong weak taylor expansion solution perturb stochastic differential equation sde particular work weight expression taylor coefficient expansion result apply libor market model order deal typical stochastic drift stochastic volatility contrast accurate method like numerical scheme sde obtain easily tractable expression accurate pricing particular present easily tractable alternative ` ` freeze drift libor market model accuracy similar numerical scheme numerical example underline result
q-fin,Information-Based Asset Pricing,"  A new framework for asset price dynamics is introduced in which the concept
of noisy information about future cash flows is used to derive the price
processes. In this framework an asset is defined by its cash-flow structure.
Each cash flow is modelled by a random variable that can be expressed as a
function of a collection of independent random variables called market factors.
With each such ""X-factor"" we associate a market information process, the values
of which are accessible to market agents. Each information process is a sum of
two terms; one contains true information about the value of the market factor;
the other represents ""noise"". The noise term is modelled by an independent
Brownian bridge. The market filtration is assumed to be that generated by the
aggregate of the independent information processes. The price of an asset is
given by the expectation of the discounted cash flows in the risk-neutral
measure, conditional on the information provided by the market filtration. When
the cash flows are the dividend payments associated with equities, an explicit
model is obtained for the share-price, and the prices of options on
dividend-paying assets are derived. Remarkably, the resulting formula for the
price of a European call option is of the Black-Scholes-Merton type. The
information-based framework also generates a natural explanation for the origin
of stochastic volatility.
",new framework asset price dynamic introduce concept noisy information future cash flow derive price process framework asset define cash flow structure cash flow model random variable express function collection independent random variable call market factor x factor associate market information process value accessible market agent information process sum term contain true information value market factor represent noise noise term model independent brownian bridge market filtration assume generate aggregate independent information process price asset give expectation discount cash flow risk neutral measure conditional information provide market filtration cash flow dividend payment associate equity explicit model obtain share price price option dividend pay asset derive remarkably result formula price european option black scholes merton type information base framework generate natural explanation origin stochastic volatility
q-fin,"True and Apparent Scaling: The Proximity of the Markov-Switching
  Multifractal Model to Long-Range Dependence","  In this paper, we consider daily financial data of a collection of different
stock market indices, exchange rates, and interest rates, and we analyze their
multi-scaling properties by estimating a simple specification of the
Markov-switching multifractal model (MSM). In order to see how well the
estimated models capture the temporal dependence of the data, we estimate and
compare the scaling exponents $H(q)$ (for $q = 1, 2$) for both empirical data
and simulated data of the estimated MSM models. In most cases the multifractal
model appears to generate `apparent' long memory in agreement with the
empirical scaling laws.
",paper consider daily financial datum collection different stock market index exchange rate interest rate analyze multi scaling property estimate simple specification markov switch multifractal model msm order estimate model capture temporal dependence datum estimate compare scale exponent $ h(q)$ $ q = 1 2 $ empirical datum simulate datum estimate msm model case multifractal model appear generate ` apparent long memory agreement empirical scale law
q-fin,Patterns of dominant flows in the world trade web,"  The large-scale organization of the world economies is exhibiting
increasingly levels of local heterogeneity and global interdependency.
Understanding the relation between local and global features calls for
analytical tools able to uncover the global emerging organization of the
international trade network. Here we analyze the world network of bilateral
trade imbalances and characterize its overall flux organization, unraveling
local and global high-flux pathways that define the backbone of the trade
system. We develop a general procedure capable to progressively filter out in a
consistent and quantitative way the dominant trade channels. This procedure is
completely general and can be applied to any weighted network to detect the
underlying structure of transport flows. The trade fluxes properties of the
world trade web determines a ranking of trade partnerships that highlights
global interdependencies, providing information not accessible by simple local
analysis. The present work provides new quantitative tools for a dynamical
approach to the propagation of economic crises.
",large scale organization world economy exhibit increasingly level local heterogeneity global interdependency understand relation local global feature call analytical tool able uncover global emerge organization international trade network analyze world network bilateral trade imbalance characterize overall flux organization unravel local global high flux pathway define backbone trade system develop general procedure capable progressively filter consistent quantitative way dominant trade channel procedure completely general apply weighted network detect underlying structure transport flow trade flux property world trade web determine ranking trade partnership highlight global interdependency provide information accessible simple local analysis present work provide new quantitative tool dynamical approach propagation economic crisis
q-fin,The Epps effect revisited,"  We analyse the dependence of stock return cross-correlations on the sampling
frequency of the data known as the Epps effect: For high resolution data the
cross-correlations are significantly smaller than their asymptotic value as
observed on daily data. The former description implies that changing trading
frequency should alter the characteristic time of the phenomenon. This is not
true for the empirical data: The Epps curves do not scale with market activity.
The latter result indicates that the time scale of the phenomenon is connected
to the reaction time of market participants (this we denote as human time
scale), independent of market activity. In this paper we give a new description
of the Epps effect through the decomposition of cross-correlations. After
testing our method on a model of generated random walk price changes we justify
our analytical results by fitting the Epps curves of real world data.
",analyse dependence stock return cross correlation sampling frequency datum know epps effect high resolution datum cross correlation significantly small asymptotic value observe daily datum description imply change trading frequency alter characteristic time phenomenon true empirical datum epps curve scale market activity result indicate time scale phenomenon connect reaction time market participant denote human time scale independent market activity paper new description epps effect decomposition cross correlation test method model generate random walk price change justify analytical result fit epps curve real world datum
q-fin,"Exact retrospective Monte Carlo computation of arithmetic average Asian
  options","  Taking advantage of the recent litterature on exact simulation algorithms
(Beskos, Papaspiliopoulos and Roberts) and unbiased estimation of the
expectation of certain fonctional integrals (Wagner, Beskos et al. and
Fearnhead et al.), we apply an exact simulation based technique for pricing
continuous arithmetic average Asian options in the Black and Scholes framework.
Unlike existing Monte Carlo methods, we are no longer prone to the
discretization bias resulting from the approximation of continuous time
processes through discrete sampling. Numerical results of simulation studies
are presented and variance reduction problems are considered.
",take advantage recent litterature exact simulation algorithms beskos papaspiliopoulos roberts unbiased estimation expectation certain fonctional integral wagner beskos et al fearnhead et al apply exact simulation base technique price continuous arithmetic average asian option black scholes framework unlike exist monte carlo method long prone discretization bias result approximation continuous time process discrete sampling numerical result simulation study present variance reduction problem consider
q-fin,Large portfolio losses: A dynamic contagion model,"  Using particle system methodologies we study the propagation of financial
distress in a network of firms facing credit risk. We investigate the
phenomenon of a credit crisis and quantify the losses that a bank may suffer in
a large credit portfolio. Applying a large deviation principle we compute the
limiting distributions of the system and determine the time evolution of the
credit quality indicators of the firms, deriving moreover the dynamics of a
global financial health indicator. We finally describe a suitable version of
the ""Central Limit Theorem"" useful to study large portfolio losses. Simulation
results are provided as well as applications to portfolio loss distribution
analysis.
",particle system methodology study propagation financial distress network firm face credit risk investigate phenomenon credit crisis quantify loss bank suffer large credit portfolio apply large deviation principle compute limit distribution system determine time evolution credit quality indicator firm derive dynamic global financial health indicator finally describe suitable version central limit theorem useful study large portfolio loss simulation result provide application portfolio loss distribution analysis
q-fin,Financial time-series analysis: A brief overview,"  Prices of commodities or assets produce what is called time-series. Different
kinds of financial time-series have been recorded and studied for decades.
Nowadays, all transactions on a financial market are recorded, leading to a
huge amount of data available, either for free in the Internet or commercially.
Financial time-series analysis is of great interest to practitioners as well as
to theoreticians, for making inferences and predictions. Furthermore, the
stochastic uncertainties inherent in financial time-series and the theory
needed to deal with them make the subject especially interesting not only to
economists, but also to statisticians and physicists. While it would be a
formidable task to make an exhaustive review on the topic, with this review we
try to give a flavor of some of its aspects.
",price commodity asset produce call time series different kind financial time series record study decade nowadays transaction financial market record lead huge datum available free internet commercially financial time series analysis great interest practitioner theoretician make inference prediction furthermore stochastic uncertainty inherent financial time series theory need deal subject especially interesting economist statistician physicist formidable task exhaustive review topic review try flavor aspect
q-fin,Why only few are so successful ?,"  In many professons employees are rewarded according to their relative
performance. Corresponding economy can be modeled by taking $N$ independent
agents who gain from the market with a rate which depends on their current
gain. We argue that this simple realistic rate generates a scale free
distribution even though intrinsic ability of agents are marginally different
from each other. As an evidence we provide distribution of scores for two
different systems (a) the global stock game where players invest in real stock
market and (b) the international cricket.
",professon employee reward accord relative performance correspond economy model take $ n$ independent agent gain market rate depend current gain argue simple realistic rate generate scale free distribution intrinsic ability agent marginally different evidence provide distribution score different system global stock game player invest real stock market b international cricket
q-fin,"Uncovering the Internal Structure of the Indian Financial Market:
  Cross-correlation behavior in the NSE","  The cross-correlations between price fluctuations of 201 frequently traded
stocks in the National Stock Exchange (NSE) of India are analyzed in this
paper. We use daily closing prices for the period 1996-2006, which coincides
with the period of rapid transformation of the market following liberalization.
The eigenvalue distribution of the cross-correlation matrix, $\mathbf{C}$, of
NSE is found to be similar to that of developed markets, such as the New York
Stock Exchange (NYSE): the majority of eigenvalues fall within the bounds
expected for a random matrix constructed from mutually uncorrelated time
series. Of the few largest eigenvalues that deviate from the bulk, the largest
is identified with market-wide movements. The intermediate eigenvalues that
occur between the largest and the bulk have been associated in NYSE with
specific business sectors with strong intra-group interactions. However, in the
Indian market, these deviating eigenvalues are comparatively very few and lie
much closer to the bulk. We propose that this is because of the relative lack
of distinct sector identity in the market, with the movement of stocks
dominantly influenced by the overall market trend. This is shown by explicit
construction of the interaction network in the market, first by generating the
minimum spanning tree from the unfiltered correlation matrix, and later, using
an improved method of generating the graph after filtering out the market mode
and random effects from the data. Both methods show, compared to developed
markets, the relative absence of clusters of co-moving stocks that belong to
the same business sector. This is consistent with the general belief that
emerging markets tend to be more correlated than developed markets.
",cross correlation price fluctuation 201 frequently trade stock national stock exchange nse india analyze paper use daily closing price period 1996 2006 coincide period rapid transformation market follow liberalization eigenvalue distribution cross correlation matrix $ \mathbf{c}$ nse find similar developed market new york stock exchange nyse majority eigenvalue fall bound expect random matrix construct mutually uncorrelated time series large eigenvalue deviate bulk large identify market wide movement intermediate eigenvalue occur large bulk associate nyse specific business sector strong intra group interaction indian market deviate eigenvalue comparatively lie close bulk propose relative lack distinct sector identity market movement stock dominantly influence overall market trend show explicit construction interaction network market generate minimum span tree unfiltered correlation matrix later improved method generate graph filter market mode random effect datum method compare developed market relative absence cluster co move stock belong business sector consistent general belief emerge market tend correlate develop market
q-fin,Classical and quantum randomness and the financial market,"  We analyze complexity of financial (and general economic) processes by
comparing classical and quantum-like models for randomness. Our analysis
implies that it might be that a quantum-like probabilistic description is more
natural for financial market than the classical one. A part of our analysis is
devoted to study the possibility of application of the quantum probabilistic
model to agents of financial market. We show that, although the direct quantum
(physical) reduction (based on using the scales of quantum mechanics) is
meaningless, one may apply so called quantum-like models. In our approach
quantum-like probabilistic behaviour is a consequence of contextualy of
statistical data in finances (and economics in general). However, our
hypothesis on ""quantumness"" of financial data should be tested experimentally
(as opposed to the conventional description based on the noncontextual
classical probabilistic approach). We present a new statistical test based on a
generalization of the well known in quantum physics Bell's inequality.
",analyze complexity financial general economic process compare classical quantum like model randomness analysis imply quantum like probabilistic description natural financial market classical analysis devote study possibility application quantum probabilistic model agent financial market direct quantum physical reduction base scale quantum mechanic meaningless apply call quantum like model approach quantum like probabilistic behaviour consequence contextualy statistical datum finance economic general hypothesis quantumness financial datum test experimentally oppose conventional description base noncontextual classical probabilistic approach present new statistical test base generalization know quantum physics bell inequality
q-fin,"Scaling laws of strategic behaviour and size heterogeneity in agent
  dynamics","  The dynamics of many socioeconomic systems is determined by the decision
making process of agents. The decision process depends on agent's
characteristics, such as preferences, risk aversion, behavioral biases, etc..
In addition, in some systems the size of agents can be highly heterogeneous
leading to very different impacts of agents on the system dynamics. The large
size of some agents poses challenging problems to agents who want to control
their impact, either by forcing the system in a given direction or by hiding
their intentionality. Here we consider the financial market as a model system,
and we study empirically how agents strategically adjust the properties of
large orders in order to meet their preference and minimize their impact. We
quantify this strategic behavior by detecting scaling relations of allometric
nature between the variables characterizing the trading activity of different
institutions. We observe power law distributions in the investment time
horizon, in the number of transactions needed to execute a large order and in
the traded value exchanged by large institutions and we show that heterogeneity
of agents is a key ingredient for the emergence of some aggregate properties
characterizing this complex system.
",dynamic socioeconomic system determine decision making process agent decision process depend agent characteristic preference risk aversion behavioral bias etc addition system size agent highly heterogeneous lead different impact agent system dynamic large size agent pose challenge problem agent want control impact force system give direction hiding intentionality consider financial market model system study empirically agent strategically adjust property large order order meet preference minimize impact quantify strategic behavior detect scale relation allometric nature variable characterize trading activity different institution observe power law distribution investment time horizon number transaction need execute large order trade value exchange large institution heterogeneity agent key ingredient emergence aggregate property characterize complex system
q-fin,"Proving Regularity of the Minimal Probability of Ruin via a Game of
  Stopping and Control","  We reveal an interesting convex duality relationship between two problems:
(a) minimizing the probability of lifetime ruin when the rate of consumption is
stochastic and when the individual can invest in a Black-Scholes financial
market; (b) a controller-and-stopper problem, in which the controller controls
the drift and volatility of a process in order to maximize a running reward
based on that process, and the stopper chooses the time to stop the running
reward and rewards the controller a final amount at that time. Our primary goal
is to show that the minimal probability of ruin, whose stochastic
representation does not have a classical form as does the utility maximization
problem (i.e., the objective's dependence on the initial values of the state
variables is implicit), is the unique classical solution of its
Hamilton-Jacobi-Bellman (HJB) equation, which is a non-linear boundary-value
problem. We establish our goal by exploiting the convex duality relationship
between (a) and (b).
",reveal interesting convex duality relationship problem minimize probability lifetime ruin rate consumption stochastic individual invest black scholes financial market b controller stopper problem controller control drift volatility process order maximize run reward base process stopper choose time stop run reward reward controller final time primary goal minimal probability ruin stochastic representation classical form utility maximization problem i.e. objective dependence initial value state variable implicit unique classical solution hamilton jacobi bellman hjb equation non linear boundary value problem establish goal exploit convex duality relationship b
q-fin,Modeling the Epps effect of cross correlations in asset prices,"  We review the decomposition method of stock return cross-correlations,
presented previously for studying the dependence of the correlation coefficient
on the resolution of data (Epps effect). Through a toy model of random
walk/Brownian motion and memoryless renewal process (i.e. Poisson point
process) of observation times we show that in case of analytical treatability,
by decomposing the correlations we get the exact result for the frequency
dependence. We also demonstrate that our approach produces reasonable fitting
of the dependence of correlations on the data resolution in case of empirical
data. Our results indicate that the Epps phenomenon is a product of the finite
time decay of lagged correlations of high resolution data, which does not scale
with activity. The characteristic time is due to a human time scale, the time
needed to react to news.
",review decomposition method stock return cross correlation present previously study dependence correlation coefficient resolution datum epps effect toy model random walk brownian motion memoryless renewal process i.e. poisson point process observation time case analytical treatability decompose correlation exact result frequency dependence demonstrate approach produce reasonable fitting dependence correlation data resolution case empirical datum result indicate epps phenomenon product finite time decay lag correlation high resolution datum scale activity characteristic time human time scale time need react news
q-fin,"Deterministic Factors of Stock Networks based on Cross-correlation in
  Financial Market","  The stock market has been known to form homogeneous stock groups with a
higher correlation among different stocks according to common economic factors
that influence individual stocks. We investigate the role of common economic
factors in the market in the formation of stock networks, using the arbitrage
pricing model reflecting essential properties of common economic factors. We
find that the degree of consistency between real and model stock networks
increases as additional common economic factors are incorporated into our
model. Furthermore, we find that individual stocks with a large number of links
to other stocks in a network are more highly correlated with common economic
factors than those with a small number of links. This suggests that common
economic factors in the stock market can be understood in terms of
deterministic factors.
",stock market know form homogeneous stock group high correlation different stock accord common economic factor influence individual stock investigate role common economic factor market formation stock network arbitrage pricing model reflect essential property common economic factor find degree consistency real model stock network increase additional common economic factor incorporate model furthermore find individual stock large number link stock network highly correlate common economic factor small number link suggest common economic factor stock market understand term deterministic factor
q-fin,Mutual Fund Theorems when Minimizing the Probability of Lifetime Ruin,"  We show that the mutual fund theorems of Merton (1971) extend to the problem
of optimal investment to minimize the probability of lifetime ruin. We obtain
two such theorems by considering a financial market both with and without a
riskless asset for random consumption. The striking result is that we obtain
two-fund theorems despite the additional source of randomness from consumption.
",mutual fund theorem merton 1971 extend problem optimal investment minimize probability lifetime ruin obtain theorem consider financial market riskless asset random consumption striking result obtain fund theorem despite additional source randomness consumption
q-fin,"Change point estimation for the telegraph process observed at discrete
  times","  The telegraph process models a random motion with finite velocity and it is
usually proposed as an alternative to diffusion models. The process describes
the position of a particle moving on the real line, alternatively with constant
velocity $+ v$ or $-v$. The changes of direction are governed by an homogeneous
Poisson process with rate $\lambda >0.$ In this paper, we consider a change
point estimation problem for the rate of the underlying Poisson process by
means of least squares method. The consistency and the rate of convergence for
the change point estimator are obtained and its asymptotic distribution is
derived. Applications to real data are also presented.
",telegraph process model random motion finite velocity usually propose alternative diffusion model process describe position particle move real line alternatively constant velocity $ + v$ $ -v$. change direction govern homogeneous poisson process rate $ \lambda > 0.$ paper consider change point estimation problem rate underlie poisson process mean square method consistency rate convergence change point estimator obtain asymptotic distribution derive application real datum present
q-fin,EGT through Quantum Mechanics & from Statistical Physics to Economics,"  By analyzing the relationships between a socioeconomical system modeled
through evolutionary game theory and a physical system modeled through quantum
mechanics we show how although both systems are described through two theories
apparently different both are analogous and thus exactly equivalents. The
extensions of quantum mechanics to statistical physics and information theory
let us use some of their definitions for the best understanding of the behavior
of economics and biology. The quantum analogue of the replicator dynamics is
the von Neumann equation. A system in where all its members are in Nash
equilibrium is equivalent to a system in a maximum entropy state. Nature is a
game in where its players compete for a common welfare and the equilibrium of
the system that they are members. They act as a whole besides individuals like
they obey a rule in where they prefer to work for the welfare of the collective
besides the individual welfare.
",analyze relationship socioeconomical system model evolutionary game theory physical system model quantum mechanic system describe theory apparently different analogous exactly equivalent extension quantum mechanic statistical physics information theory let use definition good understanding behavior economic biology quantum analogue replicator dynamic von neumann equation system member nash equilibrium equivalent system maximum entropy state nature game player compete common welfare equilibrium system member act individual like obey rule prefer work welfare collective individual welfare
q-fin,Quantitative relations between corruption and economic factors,"  We report quantitative relations between corruption level and economic
factors, such as country wealth and foreign investment per capita, which are
characterized by a power law spanning multiple scales of wealth and investments
per capita. These relations hold for diverse countries, and also remain stable
over different time periods. We also observe a negative correlation between
level of corruption and long-term economic growth. We find similar results for
two independent indices of corruption, suggesting that the relation between
corruption and wealth does not depend on the specific measure of corruption.
The functional relations we report have implications when assessing the
relative level of corruption for two countries with comparable wealth, and for
quantifying the impact of corruption on economic growth and foreign
investments.
",report quantitative relation corruption level economic factor country wealth foreign investment capita characterize power law span multiple scale wealth investment capita relation hold diverse country remain stable different time period observe negative correlation level corruption long term economic growth find similar result independent index corruption suggest relation corruption wealth depend specific measure corruption functional relation report implication assess relative level corruption country comparable wealth quantify impact corruption economic growth foreign investment
q-fin,Correlated multi-asset portfolio optimisation with transaction cost,"  We employ perturbation analysis technique to study multi-asset portfolio
optimisation with transaction cost. We allow for correlations in risky assets
and obtain optimal trading methods for general utility functions. Our
analytical results are supported by numerical simulations in the context of the
Long Term Growth Model.
",employ perturbation analysis technique study multi asset portfolio optimisation transaction cost allow correlation risky asset obtain optimal trading method general utility function analytical result support numerical simulation context long term growth model
q-fin,"Financial Valuation of Mortality Risk via the Instantaneous Sharpe
  Ratio: Applications to Pricing Pure Endowments","  We develop a theory for pricing non-diversifiable mortality risk in an
incomplete market. We do this by assuming that the company issuing a
mortality-contingent claim requires compensation for this risk in the form of a
pre-specified instantaneous Sharpe ratio. We prove that our ensuing valuation
formula satisfies a number of desirable properties. For example, we show that
it is subadditive in the number of contracts sold. A key result is that if the
hazard rate is stochastic, then the risk-adjusted survival probability is
greater than the physical survival probability, even as the number of contracts
approaches infinity.
",develop theory price non diversifiable mortality risk incomplete market assume company issue mortality contingent claim require compensation risk form pre specified instantaneous sharpe ratio prove ensue valuation formula satisfy number desirable property example subadditive number contract sell key result hazard rate stochastic risk adjust survival probability great physical survival probability number contract approach infinity
q-fin,"The log-normal distribution from Non-Gibrat's law in the middle scale
  region of profits","  Employing profits data of Japanese firms in 2003--2005, we kinematically
exhibit the static log-normal distribution in the middle scale region. In the
derivation, a Non-Gibrat's law under the detailed balance is adopted together
with following two approximations. Firstly, the probability density function of
profits growth rate is described as a tent-shaped exponential function.
Secondly, the value of the origin of the growth rate distribution divided into
bins is constant. The derivation is confirmed in the database consistently.
  This static procedure is applied to a quasi-static system. We dynamically
describe a quasi-static log-normal distribution in the middle scale region. In
the derivation, a Non-Gibrat's law under the detailed quasi-balance is adopted
together with two approximations confirmed in the static system. The resultant
distribution is power-law with varying Pareto index in the large scale region
and the quasi-static log-normal distribution in the middle scale region. In the
distribution, not only the change of Pareto index but also the change of the
variance of the log-normal distribution depends on the parameter of the
detailed quasi-balance. As a result, Pareto index and the variance of the
log-normal distribution are related to each other.
",employ profit datum japanese firm 2003 -2005 kinematically exhibit static log normal distribution middle scale region derivation non gibrat law detailed balance adopt follow approximation firstly probability density function profit growth rate describe tent shape exponential function secondly value origin growth rate distribution divide bin constant derivation confirm database consistently static procedure apply quasi static system dynamically describe quasi static log normal distribution middle scale region derivation non gibrat law detailed quasi balance adopt approximation confirm static system resultant distribution power law vary pareto index large scale region quasi static log normal distribution middle scale region distribution change pareto index change variance log normal distribution depend parameter detailed quasi balance result pareto index variance log normal distribution relate
q-fin,"Pricing Life Insurance under Stochastic Mortality via the Instantaneous
  Sharpe Ratio: Theorems and Proofs","  We develop a pricing rule for life insurance under stochastic mortality in an
incomplete market by assuming that the insurance company requires compensation
for its risk in the form of a pre-specified instantaneous Sharpe ratio. Our
valuation formula satisfies a number of desirable properties, many of which it
shares with the standard deviation premium principle. The major result of the
paper is that the price per contract solves a linear partial differential
equation as the number of contracts approaches infinity. One can interpret the
limiting price as an expectation with respect to an equivalent martingale
measure. Another important result is that if the hazard rate is stochastic,
then the risk-adjusted premium is greater than the net premium, even as the
number of contracts approaches infinity. We present a numerical example to
illustrate our results, along with the corresponding algorithms.
",develop pricing rule life insurance stochastic mortality incomplete market assume insurance company require compensation risk form pre specified instantaneous sharpe ratio valuation formula satisfy number desirable property share standard deviation premium principle major result paper price contract solve linear partial differential equation number contract approach infinity interpret limit price expectation respect equivalent martingale measure important result hazard rate stochastic risk adjust premium great net premium number contract approach infinity present numerical example illustrate result correspond algorithm
q-fin,Optimal quantization for the pricing of swing options,"  In this paper, we investigate a numerical algorithm for the pricing of swing
options, relying on the so-called optimal quantization method. The numerical
procedure is described in details and numerous simulations are provided to
assert its efficiency. In particular, we carry out a comparison with the
Longstaff-Schwartz algorithm.
",paper investigate numerical algorithm pricing swing option rely call optimal quantization method numerical procedure describe detail numerous simulation provide assert efficiency particular carry comparison longstaff schwartz algorithm
q-fin,Kolkata Restaurant Problem as a generalised El Farol Bar Problem,"  Generalisation of the El Farol bar problem to that of many bars here leads to
the Kolkata restaurant problem, where the decision to go to any restaurant or
not is much simpler (depending on the previous experience of course, as in the
El Farol bar problem). This generalised problem can be exactly analysed in some
limiting cases discussed here. The fluctuation in the restaurant service can be
shown to have precisely an inverse cubic behavior, as widely seen in the stock
market fluctuations.
",generalisation el farol bar problem bar lead kolkata restaurant problem decision restaurant simple depend previous experience course el farol bar problem generalised problem exactly analyse limit case discuss fluctuation restaurant service show precisely inverse cubic behavior widely see stock market fluctuation
q-fin,"Entropy Oriented Trading: A Trading Strategy Based on the Second Law of
  Thermodynamics","  The author proposes a finance trading strategy named Entropy Oriented Trading
and apply thermodynamics on the strategy. The state variables are chosen so
that the strategy satisfies the second law of thermodynamics. Using the law,
the author proves that the rate of investment (ROI) of the strategy is equal to
or more than the rate of price change.
",author propose finance trading strategy name entropy oriented trading apply thermodynamic strategy state variable choose strategy satisfy second law thermodynamic law author prove rate investment roi strategy equal rate price change
q-fin,A simple algorithm based on fluctuations to play the market,"  In Biology, all motor enzymes operate on the same principle: they trap
favourable brownian fluctuations in order to generate directed forces and to
move. Whether it is possible or not to copy one such strategy to play the
market was the starting point of our investigations. We found the answer is
yes. In this paper we describe one such strategy and appraise its performance
with historical data from the European Monetary System (EMS), the US Dow Jones,
the german Dax and the french Cac40.
",biology motor enzyme operate principle trap favourable brownian fluctuation order generate direct force possible copy strategy play market starting point investigation find answer yes paper describe strategy appraise performance historical datum european monetary system ems dow jones german dax french cac40
q-fin,Network Topology of an Experimental Futures Exchange,"  Many systems of different nature exhibit scale free behaviors. Economic
systems with power law distribution in the wealth is one of the examples. To
better understand the working behind the complexity, we undertook an empirical
study measuring the interactions between market participants. A Web server was
setup to administer the exchange of futures contracts whose liquidation prices
were coupled to event outcomes. After free registration, participants started
trading to compete for the money prizes upon maturity of the futures contracts
at the end of the experiment. The evolving `cash' flow network was
reconstructed from the transactions between players. We show that the network
topology is hierarchical, disassortative and scale-free with a power law
exponent of 1.02+-0.09 in the degree distribution. The small-world property
emerged early in the experiment while the number of participants was still
small. We also show power law distributions of the net incomes and
inter-transaction time intervals. Big winners and losers are associated with
high degree, high betweenness centrality, low clustering coefficient and low
degree-correlation. We identify communities in the network as groups of the
like-minded. The distribution of the community sizes is shown to be power-law
distributed with an exponent of 1.19+-0.16.
",system different nature exhibit scale free behavior economic system power law distribution wealth example well understand working complexity undertake empirical study measure interaction market participant web server setup administer exchange future contract liquidation price couple event outcome free registration participant start trading compete money prize maturity future contract end experiment evolve ` cash flow network reconstruct transaction player network topology hierarchical disassortative scale free power law exponent 1.02 + -0.09 degree distribution small world property emerge early experiment number participant small power law distribution net income inter transaction time interval big winner loser associate high degree high betweenness centrality low cluster coefficient low degree correlation identify community network group like minded distribution community size show power law distribute exponent 1.19 + -0.16
q-fin,Optimal cross hedging for insurance derivatives,"  We consider insurance derivatives depending on an external physical risk
process, for example a temperature in a low dimensional climate model. We
assume that this process is correlated with a tradable financial asset. We
derive optimal strategies for exponential utility from terminal wealth,
determine the indifference prices of the derivatives, and interpret them in
terms of diversification pressure. Moreover we check the optimal investment
strategies for standard admissibility criteria. Finally we compare the static
risk connected with an insurance derivative to the reduced risk due to a
dynamic investment into the correlated asset. We show that dynamic hedging
reduces the risk aversion in terms of entropic risk measures by a factor
related to the correlation.
",consider insurance derivative depend external physical risk process example temperature low dimensional climate model assume process correlate tradable financial asset derive optimal strategy exponential utility terminal wealth determine indifference price derivative interpret term diversification pressure check optimal investment strategy standard admissibility criterion finally compare static risk connect insurance derivative reduced risk dynamic investment correlate asset dynamic hedging reduce risk aversion term entropic risk measure factor relate correlation
q-fin,"The Macro Model of the Inequality Process and The Surging Relative
  Frequency of Large Wage Incomes","  This paper presents a model of the dynamics of the wage income distribution.
",paper present model dynamic wage income distribution
q-fin,On a generalised model for time-dependent variance with long-term memory,"  The ARCH process (R. F. Engle, 1982) constitutes a paradigmatic generator of
stochastic time series with time-dependent variance like it appears on a wide
broad of systems besides economics in which ARCH was born. Although the ARCH
process captures the so-called ""volatility clustering"" and the asymptotic
power-law probability density distribution of the random variable, it is not
capable to reproduce further statistical properties of many of these time
series such as: the strong persistence of the instantaneous variance
characterised by large values of the Hurst exponent (H > 0.8), and asymptotic
power-law decay of the absolute values self-correlation function. By means of
considering an effective return obtained from a correlation of past returns
that has a q-exponential form we are able to fix the limitations of the
original model. Moreover, this improvement can be obtained through the correct
choice of a sole additional parameter, $q_{m}$. The assessment of its validity
and usefulness is made by mimicking daily fluctuations of SP500 financial
index.
",arch process r. f. engle 1982 constitute paradigmatic generator stochastic time series time dependent variance like appear wide broad system economic arch bear arch process capture call volatility clustering asymptotic power law probability density distribution random variable capable reproduce statistical property time series strong persistence instantaneous variance characterise large value hurst exponent h > 0.8 asymptotic power law decay absolute value self correlation function mean consider effective return obtain correlation past return q exponential form able fix limitation original model improvement obtain correct choice sole additional parameter $ q_{m}$. assessment validity usefulness mimic daily fluctuation sp500 financial index
q-fin,Detecting anchoring in financial markets,"  Anchoring is a term used in psychology to describe the common human tendency
to rely too heavily (anchor) on one piece of information when making decisions.
A trading algorithm inspired by biological motors, introduced by L.
Gil\cite{Gil}, is suggested as a testing ground for anchoring in financial
markets. An exact solution of the algorithm is presented for arbitrary price
distributions. Furthermore the algorithm is extended to cover the case of a
market neutral portfolio, revealing additional evidence that anchoring is
involved in the decision making of market participants. The exposure of
arbitrage possibilities created by anchoring gives yet another illustration on
the difficulty proving market efficiency by only considering lower order
correlations in past price time series
",anchoring term psychology describe common human tendency rely heavily anchor piece information make decision trading algorithm inspire biological motor introduce l. gil\cite{gil suggest testing ground anchor financial market exact solution algorithm present arbitrary price distribution furthermore algorithm extend cover case market neutral portfolio reveal additional evidence anchor involve decision making market participant exposure arbitrage possibility create anchor give illustration difficulty prove market efficiency consider low order correlation past price time series
q-fin,"Kullback-Leibler distance as a measure of the information filtered from
  multivariate data","  We show that the Kullback-Leibler distance is a good measure of the
statistical uncertainty of correlation matrices estimated by using a finite set
of data. For correlation matrices of multivariate Gaussian variables we
analytically determine the expected values of the Kullback-Leibler distance of
a sample correlation matrix from a reference model and we show that the
expected values are known also when the specific model is unknown. We propose
to make use of the Kullback-Leibler distance to estimate the information
extracted from a correlation matrix by correlation filtering procedures. We
also show how to use this distance to measure the stability of filtering
procedures with respect to statistical uncertainty. We explain the
effectiveness of our method by comparing four filtering procedures, two of them
being based on spectral analysis and the other two on hierarchical clustering.
We compare these techniques as applied both to simulations of factor models and
empirical data. We investigate the ability of these filtering procedures in
recovering the correlation matrix of models from simulations. We discuss such
an ability in terms of both the heterogeneity of model parameters and the
length of data series. We also show that the two spectral techniques are
typically more informative about the sample correlation matrix than techniques
based on hierarchical clustering, whereas the latter are more stable with
respect to statistical uncertainty.
",kullback leibler distance good measure statistical uncertainty correlation matrix estimate finite set datum correlation matrix multivariate gaussian variable analytically determine expect value kullback leibler distance sample correlation matrix reference model expect value know specific model unknown propose use kullback leibler distance estimate information extract correlation matrix correlation filtering procedure use distance measure stability filtering procedure respect statistical uncertainty explain effectiveness method compare filtering procedure base spectral analysis hierarchical clustering compare technique apply simulation factor model empirical datum investigate ability filtering procedure recover correlation matrix model simulation discuss ability term heterogeneity model parameter length datum series spectral technique typically informative sample correlation matrix technique base hierarchical clustering stable respect statistical uncertainty
q-fin,Microscopic Origin of Non-Gaussian Distributions of Financial Returns,"  In this paper we study the possible microscopic origin of heavy-tailed
probability density distributions for the price variation of financial
instruments. We extend the standard log-normal process to include another
random component in the so-called stochastic volatility models. We study these
models under an assumption, akin to the Born-Oppenheimer approximation, in
which the volatility has already relaxed to its equilibrium distribution and
acts as a background to the evolution of the price process. In this
approximation, we show that all models of stochastic volatility should exhibit
a scaling relation in the time lag of zero-drift modified log-returns. We
verify that the Dow-Jones Industrial Average index indeed follows this scaling.
We then focus on two popular stochastic volatility models, the Heston and
Hull-White models. In particular, we show that in the Hull-White model the
resulting probability distribution of log-returns in this approximation
corresponds to the Tsallis (t-Student) distribution. The Tsallis parameters are
given in terms of the microscopic stochastic volatility model. Finally, we show
that the log-returns for 30 years Dow Jones index data is well fitted by a
Tsallis distribution, obtaining the relevant parameters.
",paper study possible microscopic origin heavy tail probability density distribution price variation financial instrument extend standard log normal process include random component call stochastic volatility model study model assumption akin born oppenheimer approximation volatility relax equilibrium distribution act background evolution price process approximation model stochastic volatility exhibit scale relation time lag zero drift modify log return verify dow jones industrial average index follow scaling focus popular stochastic volatility model heston hull white model particular hull white model result probability distribution log return approximation correspond tsallis t student distribution tsallis parameter give term microscopic stochastic volatility model finally log return 30 year dow jones index datum fit tsallis distribution obtain relevant parameter
q-fin,Stochastic analysis of an agent-based model,"  We analyze the dynamics of a forecasting game which exhibits the phenomenon
of information cascades. Each agent aims at correctly predicting a binary
variable and he/she can either look for independent information or herd on the
choice of others. We show that dynamics can be analitically described in terms
of a Langevin equation and its collective behavior is described by the solution
of a Kramers' problem. This provides very accurate results in the region where
the vast majority of agents herd, which corresponds to the most interesting one
from a game theoretic point of view.
",analyze dynamic forecasting game exhibit phenomenon information cascade agent aim correctly predict binary variable look independent information herd choice dynamic analitically describe term langevin equation collective behavior describe solution kramers problem provide accurate result region vast majority agent herd correspond interesting game theoretic point view
q-fin,The limit order book on different time scales,"  Financial markets can be described on several time scales. We use data from
the limit order book of the London Stock Exchange (LSE) to compare how the
fluctuation dominated microstructure crosses over to a more systematic global
behavior.
",financial market describe time scale use datum limit order book london stock exchange lse compare fluctuation dominate microstructure crosse systematic global behavior
q-fin,"Utility Maximization with a Stochastic Clock and an Unbounded Random
  Endowment","  We introduce a linear space of finitely additive measures to treat the
problem of optimal expected utility from consumption under a stochastic clock
and an unbounded random endowment process. In this way we establish existence
and uniqueness for a large class of utility maximization problems including the
classical ones of terminal wealth or consumption, as well as the problems
depending on a random time-horizon or multiple consumption instances. As an
example we treat explicitly the problem of maximizing the logarithmic utility
of a consumption stream, where the local time of an Ornstein-Uhlenbeck process
acts as a stochastic clock.
",introduce linear space finitely additive measure treat problem optimal expect utility consumption stochastic clock unbounded random endowment process way establish existence uniqueness large class utility maximization problem include classical one terminal wealth consumption problem depend random time horizon multiple consumption instance example treat explicitly problem maximize logarithmic utility consumption stream local time ornstein uhlenbeck process act stochastic clock
q-fin,Scale-free avalanches in the multifractal random walk,"  Avalanches, or Avalanche-like, events are often observed in the dynamical
behaviour of many complex systems which span from solar flaring to the Earth's
crust dynamics and from traffic flows to financial markets. Self-organized
criticality (SOC) is one of the most popular theories able to explain this
intermittent charge/discharge behaviour. Despite a large amount of theoretical
work, empirical tests for SOC are still in their infancy. In the present paper
we address the common problem of revealing SOC from a simple time series
without having much information about the underlying system. As a working
example we use a modified version of the multifractal random walk originally
proposed as a model for the stock market dynamics. The study reveals, despite
the lack of the typical ingredients of SOC, an avalanche-like dynamics similar
to that of many physical systems. While, on one hand, the results confirm the
relevance of cascade models in representing turbulent-like phenomena, on the
other, they also raise the question about the current state of reliability of
SOC inference from time series analysis.
",avalanches avalanche like event observe dynamical behaviour complex system span solar flaring earth crust dynamic traffic flow financial market self organize criticality soc popular theory able explain intermittent charge discharge behaviour despite large theoretical work empirical test soc infancy present paper address common problem reveal soc simple time series have information underlie system work example use modify version multifractal random walk originally propose model stock market dynamic study reveal despite lack typical ingredient soc avalanche like dynamic similar physical system hand result confirm relevance cascade model represent turbulent like phenomenon raise question current state reliability soc inference time series analysis
q-fin,"Optimal consumption from investment and random endowment in incomplete
  semimartingale markets","  We consider the problem of maximizing expected utility from consumption in a
constrained incomplete semimartingale market with a random endowment process,
and establish a general existence and uniqueness result using techniques from
convex duality. The notion of asymptotic elasticity of Kramkov and
Schachermayer is extended to the time-dependent case. By imposing no smoothness
requirements on the utility function in the temporal argument, we can treat
both pure consumption and combined consumption/terminal wealth problems, in a
common framework. To make the duality approach possible, we provide a detailed
characterization of the enlarged dual domain which is reminiscent of the
enlargement of $L^1$ to its topological bidual $(L^{\infty})^*$, a space of
finitely-additive measures. As an application, we treat the case of a
constrained It\^ o-process market-model.
",consider problem maximize expect utility consumption constrain incomplete semimartingale market random endowment process establish general existence uniqueness result technique convex duality notion asymptotic elasticity kramkov schachermayer extend time dependent case impose smoothness requirement utility function temporal argument treat pure consumption combined consumption terminal wealth problem common framework duality approach possible provide detailed characterization enlarge dual domain reminiscent enlargement $ l^1 $ topological bidual $ l^{\infty})^*$ space finitely additive measure application treat case constrain it\^ o process market model
q-fin,Stability of utility-maximization in incomplete markets,"  The effectiveness of utility-maximization techniques for portfolio management
relies on our ability to estimate correctly the parameters of the dynamics of
the underlying financial assets. In the setting of complete or incomplete
financial markets, we investigate whether small perturbations of the market
coefficient processes lead to small changes in the agent's optimal behavior
derived from the solution of the related utility-maximization problems.
Specifically, we identify the topologies on the parameter process space and the
solution space under which utility-maximization is a continuous operation, and
we provide a counterexample showing that our results are best possible, in a
certain sense. A novel result about the structure of the solution of the
utility-maximization problem where prices are modeled by continuous
semimartingales is established as an offshoot of the proof of our central
theorem.
",effectiveness utility maximization technique portfolio management rely ability estimate correctly parameter dynamic underlie financial asset setting complete incomplete financial market investigate small perturbation market coefficient process lead small change agent optimal behavior derive solution related utility maximization problem specifically identify topology parameter process space solution space utility maximization continuous operation provide counterexample show result well possible certain sense novel result structure solution utility maximization problem price model continuous semimartingale establish offshoot proof central theorem
q-fin,The Quantum Black-Scholes Equation,"  Motivated by the work of Segal and Segal on the Black-Scholes pricing formula
in the quantum context, we study a quantum extension of the Black-Scholes
equation within the context of Hudson-Parthasarathy quantum stochastic
calculus. Our model includes stock markets described by quantum Brownian motion
and Poisson process.
",motivate work segal segal black scholes pricing formula quantum context study quantum extension black scholes equation context hudson parthasarathy quantum stochastic calculus model include stock market describe quantum brownian motion poisson process
q-fin,Rent seeking games with tax evasion,"  We consider the static and dynamic models of Cournot duopoly with tax
evasion. In the dynamic model we introduce the time delay and we analyze the
local stability of the stationary state. There is a critical value of the delay
when the Hopf bifurcation occurs.
",consider static dynamic model cournot duopoly tax evasion dynamic model introduce time delay analyze local stability stationary state critical value delay hopf bifurcation occur
q-fin,Maximizing the Growth Rate under Risk Constraints,"  We investigate the ergodic problem of growth-rate maximization under a class
of risk constraints in the context of incomplete, It\^{o}-process models of
financial markets with random ergodic coefficients. Including {\em
value-at-risk} (VaR), {\em tail-value-at-risk} (TVaR), and {\em limited
expected loss} (LEL), these constraints can be both wealth-dependent(relative)
and wealth-independent (absolute). The optimal policy is shown to exist in an
appropriate admissibility class, and can be obtained explicitly by uniform,
state-dependent scaling down of the unconstrained (Merton) optimal portfolio.
This implies that the risk-constrained wealth-growth optimizer locally behaves
like a CRRA-investor, with the relative risk-aversion coefficient depending on
the current values of the market coefficients.
",investigate ergodic problem growth rate maximization class risk constraint context incomplete it\^{o}-process model financial market random ergodic coefficient include \em value risk var \em tail value risk tvar \em limit expect loss lel constraint wealth dependent(relative wealth independent absolute optimal policy show exist appropriate admissibility class obtain explicitly uniform state dependent scaling unconstrained merton optimal portfolio imply risk constrain wealth growth optimizer locally behave like crra investor relative risk aversion coefficient depend current value market coefficient
q-fin,"Financial equilibria in the semimartingale setting: complete markets and
  markets with withdrawal constraints","  Existence of stochastic financial equilibria giving rise to semimartingale
asset prices is established under a general class of assumptions. These
equilibria are expressed in real terms and span complete markets or markets
with withdrawal constraints.We deal with random endowment density streams which
admit jumps and general time-dependent utility functions on which only
regularity conditions are imposed. As an integral part of the proof of the main
result, we establish a novel characterization of semimartingale functions.
",existence stochastic financial equilibrium give rise semimartingale asset price establish general class assumption equilibrium express real term span complete market market withdrawal constraint deal random endowment density stream admit jump general time dependent utility function regularity condition impose integral proof main result establish novel characterization semimartingale function
q-fin,"Optimal Investment with an Unbounded Random Endowment and Utility-Based
  Pricing","  This paper studies the problem of maximizing the expected utility of terminal
wealth for a financial agent with an unbounded random endowment, and with a
utility function which supports both positive and negative wealth. We prove the
existence of an optimal trading strategy within a class of permissible
strategies -- those strategies whose wealth process is a supermartingale under
all pricing measures with finite relative entropy. We give necessary and
sufficient conditions for the absence of utility-based arbitrage, and for the
existence of a solution to the primal problem.
  We consider two utility-based methods which can be used to price contingent
claims. Firstly we investigate marginal utility-based price processes
(MUBPP's). We show that such processes can be characterized as local
martingales under the normalized optimal dual measure for the utility
maximizing investor. Finally, we present some new results on utility
indifference prices, including continuity properties and volume asymptotics for
the case of a general utility function, unbounded endowment and unbounded
contingent claims.
",paper study problem maximize expect utility terminal wealth financial agent unbounded random endowment utility function support positive negative wealth prove existence optimal trading strategy class permissible strategy strategy wealth process supermartingale pricing measure finite relative entropy necessary sufficient condition absence utility base arbitrage existence solution primal problem consider utility base method price contingent claim firstly investigate marginal utility base price process mubpp process characterize local martingale normalize optimal dual measure utility maximize investor finally present new result utility indifference price include continuity property volume asymptotic case general utility function unbounded endowment unbounded contingent claim
q-fin,On the semimartingale property via bounded logarithmic utility,"  This paper provides a new version of the condition of Di Nunno et al. (2003),
Ankirchner and Imkeller (2005) and Biagini and \{O}ksendal (2005) ensuring the
semimartingale property for a large class of continuous stochastic processes.
Unlike our predecessors, we base our modeling framework on the concept of
portfolio proportions which yields a short self-contained proof of the main
theorem, as well as a counterexample, showing that analogues of our results do
not hold in the discontinuous setting.
",paper provide new version condition di nunno et al 2003 ankirchner imkeller 2005 biagini \{o}ksendal 2005 ensure semimartingale property large class continuous stochastic process unlike predecessor base modeling framework concept portfolio proportion yield short self contain proof main theorem counterexample show analogue result hold discontinuous setting
q-fin,Hiking the hypercube: producers and consumers,"  We study the dynamics of co-evolution of producers and customers described by
bit-strings representing individual traits. Individual ''size-like'' properties
are controlled by binary encounters which outcome depends upon a recognition
process. Depending upon the parameter set-up, mutual selection of producers and
customers results in different types of attractors, either an exclusive niches
regime or a competition regime.
",study dynamic co evolution producer customer describe bit string represent individual trait individual size like property control binary encounter outcome depend recognition process depend parameter set mutual selection producer customer result different type attractor exclusive niche regime competition regime
q-fin,Are all highly liquid securities within the same class?,"  In this manuscript we analyse the leading statistical properties of
fluctuations of (log) 3-month US Treasury bill quotation in the secondary
market, namely: probability density function, autocorrelation, absolute values
autocorrelation, and absolute values persistency. We verify that this financial
instrument, in spite of its high liquidity, shows very peculiar properties.
Particularly, we verify that log-fluctuations belong to the Levy class of
stochastic variables.
",manuscript analyse lead statistical property fluctuation log 3 month treasury bill quotation secondary market probability density function autocorrelation absolute value autocorrelation absolute value persistency verify financial instrument spite high liquidity show peculiar property particularly verify log fluctuation belong levy class stochastic variable
q-fin,"Stability of the utility maximization problem with random endowment in
  incomplete markets","  We perform a stability analysis for the utility maximization problem in a
general semimartingale model where both liquid and illiquid assets (random
endowments) are present. Small misspecifications of preferences (as modeled via
expected utility), as well as views of the world or the market model (as
modeled via subjective probabilities) are considered. Simple sufficient
conditions are given for the problem to be well-posed, in the sense the optimal
wealth and the marginal utility-based prices are continuous functionals of
preferences and probabilistic views.
",perform stability analysis utility maximization problem general semimartingale model liquid illiquid asset random endowment present small misspecification preference model expect utility view world market model model subjective probability consider simple sufficient condition give problem pose sense optimal wealth marginal utility base price continuous functional preference probabilistic view
q-fin,Long Memory in Nonlinear Processes,"  It is generally accepted that many time series of practical interest exhibit
strong dependence, i.e., long memory. For such series, the sample
autocorrelations decay slowly and log-log periodogram plots indicate a
straight-line relationship. This necessitates a class of models for describing
such behavior. A popular class of such models is the autoregressive
fractionally integrated moving average (ARFIMA) which is a linear process.
However, there is also a need for nonlinear long memory models. For example,
series of returns on financial assets typically tend to show zero correlation,
whereas their squares or absolute values exhibit long memory. Furthermore, the
search for a realistic mechanism for generating long memory has led to the
development of other nonlinear long memory models. In this chapter, we will
present several nonlinear long memory models, and discuss the properties of the
models, as well as associated parametric andsemiparametric estimators.
",generally accept time series practical interest exhibit strong dependence i.e. long memory series sample autocorrelation decay slowly log log periodogram plot indicate straight line relationship necessitate class model describe behavior popular class model autoregressive fractionally integrate moving average arfima linear process need nonlinear long memory model example series return financial asset typically tend zero correlation square absolute value exhibit long memory furthermore search realistic mechanism generate long memory lead development nonlinear long memory model chapter present nonlinear long memory model discuss property model associate parametric andsemiparametric estimator
q-fin,Uncertainty in the Fluctuations of the Price of Stocks,"  We report on a study of the Tehran Price Index (TEPIX) from 2001 to 2006 as
an emerging market that has been affected by several political crises during
the recent years, and analyze the non-Gaussian probability density function
(PDF) of the log returns of the stocks' prices. We show that while the average
of the index did not fall very much over the time period of the study, its
day-to-day fluctuations strongly increased due to the crises. Using an approach
based on multiplicative processes with a detrending procedure, we study the
scale-dependence of the non-Gaussian PDFs, and show that the temporal
dependence of their tails indicates a gradual and systematic increase in the
probability of the appearance of large increments in the returns on approaching
distinct critical time scales over which the TEPIX has exhibited maximum
uncertainty.
",report study tehran price index tepix 2001 2006 emerge market affect political crisis recent year analyze non gaussian probability density function pdf log return stock price average index fall time period study day day fluctuation strongly increase crisis approach base multiplicative process detrende procedure study scale dependence non gaussian pdf temporal dependence tail indicate gradual systematic increase probability appearance large increment return approach distinct critical time scale tepix exhibit maximum uncertainty
q-fin,Multifractality in stock indexes: Fact or fiction?,"  Multifractal analysis and extensive statistical tests are performed upon
intraday minutely data within individual trading days for four stock market
indexes (including HSI, SZSC, S&P500, and NASDAQ) to check whether the indexes
(instead of the returns) possess multifractality. We find that the mass
exponent $\tau(q)$ is linear and the singularity $\alpha(q)$ is close to 1 for
all trading days and all indexes. Furthermore, we find strong evidence showing
that the scaling behaviors of the original data sets cannot be distinguished
from those of the shuffled time series. Hence, the so-called multifractality in
the intraday stock market indexes is merely an illusion.
",multifractal analysis extensive statistical test perform intraday minutely datum individual trading day stock market index include hsi szsc s&p500 nasdaq check index instead return possess multifractality find mass exponent $ \tau(q)$ linear singularity $ \alpha(q)$ close 1 trading day index furthermore find strong evidence show scale behavior original data set distinguish shuffled time series call multifractality intraday stock market index merely illusion
q-fin,"Heterogeneity and Increasing Returns May Drive Socio-Economic
  Transitions","  There are clear benefits associated with a particular consumer choice for
many current markets. For example, as we consider here, some products might
carry environmental or `green' benefits. Some consumers might value these
benefits while others do not. However, as evidenced by myriad failed attempts
of environmental products to maintain even a niche market, such benefits do not
necessarily outweigh the extra purchasing cost. The question we pose is, how
can such an initially economically-disadvantaged green product evolve to hold
the greater share of the market? We present a simple mathematical model for the
dynamics of product competition in a heterogeneous consumer population. Our
model preassigns a hierarchy to the products, which designates the consumer
choice when prices are comparable, while prices are dynamically rescaled to
reflect increasing returns to scale. Our approach allows us to model many
scenarios of technology substitution and provides a method for generalizing
market forces. With this model, we begin to forecast irreversible trends
associated with consumer dynamics as well as policies that could be made to
influence transitions
",clear benefit associate particular consumer choice current market example consider product carry environmental ` green benefit consumer value benefit evidence myriad fail attempt environmental product maintain niche market benefit necessarily outweigh extra purchasing cost question pose initially economically disadvantaged green product evolve hold great share market present simple mathematical model dynamic product competition heterogeneous consumer population model preassign hierarchy product designate consumer choice price comparable price dynamically rescale reflect increase return scale approach allow model scenario technology substitution provide method generalize market force model begin forecast irreversible trend associate consumer dynamic policy influence transition
q-fin,Nurturing Breakthroughs: Lessons from Complexity Theory,"  A general theory of innovation and progress in human society is outlined,
based on the combat between two opposite forces (conservatism/inertia and
speculative herding ""bubble"" behavior). We contend that human affairs are
characterized by ubiquitous ``bubbles'', which involve huge risks which would
not otherwise be taken using standard cost/benefit analysis. Bubbles result
from self-reinforcing positive feedbacks. This leads to explore uncharted
territories and niches whose rare successes lead to extraordinary discoveries
and provide the base for the observed accelerating development of technology
and of the economy. But the returns are very heterogeneous, very risky and may
not occur. In other words, bubbles, which are characteristic definitions of
human activity, allow huge risks to get huge returns over large scales. We
outline some underlying mathematical structure and a few results involving
positive feedbacks, emergence, heavy-tailed power laws, outliers/kings/black
swans, the problem of predictability and the illusion of control, as well as
some policy implications.
",general theory innovation progress human society outline base combat opposite force conservatism inertia speculative herding bubble behavior contend human affair characterize ubiquitous ` ` bubble involve huge risk take standard cost benefit analysis bubble result self reinforce positive feedback lead explore uncharted territory niche rare success lead extraordinary discovery provide base observed accelerate development technology economy return heterogeneous risky occur word bubble characteristic definition human activity allow huge risk huge return large scale outline underlie mathematical structure result involve positive feedback emergence heavy tail power law outlier king black swan problem predictability illusion control policy implication
q-fin,"Effects of payoff functions and preference distributions in an adaptive
  population","  Adaptive populations such as those in financial markets and distributed
control can be modeled by the Minority Game. We consider how their dynamics
depends on the agents' initial preferences of strategies, when the agents use
linear or quadratic payoff functions to evaluate their strategies. We find that
the fluctuations of the population making certain decisions (the volatility)
depends on the diversity of the distribution of the initial preferences of
strategies. When the diversity decreases, more agents tend to adapt their
strategies together. In systems with linear payoffs, this results in dynamical
transitions from vanishing volatility to a non-vanishing one. For low signal
dimensions, the dynamical transitions for the different signals do not take
place at the same critical diversity. Rather, a cascade of dynamical
transitions takes place when the diversity is reduced. In contrast, no phase
transitions are found in systems with the quadratic payoffs. Instead, a basin
boundary of attraction separates two groups of samples in the space of the
agents' decisions. Initial states inside this boundary converge to small
volatility, while those outside diverge to a large one. Furthermore, when the
preference distribution becomes more polarized, the dynamics becomes more
erratic. All the above results are supported by good agreement between
simulations and theory.
",adaptive population financial market distribute control model minority game consider dynamic depend agent initial preference strategy agent use linear quadratic payoff function evaluate strategy find fluctuation population make certain decision volatility depend diversity distribution initial preference strategy diversity decrease agent tend adapt strategy system linear payoff result dynamical transition vanish volatility non vanishing low signal dimension dynamical transition different signal place critical diversity cascade dynamical transition take place diversity reduce contrast phase transition find system quadratic payoff instead basin boundary attraction separate group sample space agent decision initial state inside boundary converge small volatility outside diverge large furthermore preference distribution polarized dynamic erratic result support good agreement simulation theory
q-fin,"A Model for Counterparty Risk with Geometric Attenuation Effect and the
  Valuation of CDS","  In this paper, a geometric function is introduced to reflect the attenuation
speed of impact of one firm's default to its partner. If two firms are
competitions (copartners), the default intensity of one firm will decrease
(increase) abruptly when the other firm defaults. As time goes on, the impact
will decrease gradually until extinct. In this model, the joint distribution
and marginal distributions of default times are derived by employing the change
of measure, so can we value the fair swap premium of a CDS.
",paper geometric function introduce reflect attenuation speed impact firm default partner firm competition copartner default intensity firm decrease increase abruptly firm default time go impact decrease gradually extinct model joint distribution marginal distribution default time derive employ change measure value fair swap premium cds
q-fin,The fractional volatility model: An agent-based interpretation,"  Based on criteria of mathematical simplicity and consistency with empirical
market data, a model with volatility driven by fractional noise has been
constructed which provides a fairly accurate mathematical parametrization of
the data. Here, some features of the model are discussed and, using agent-based
models, one tries to find which agent strategies and (or) properties of the
financial institutions might be responsible for the features of the fractional
volatility model.
",base criterion mathematical simplicity consistency empirical market datum model volatility drive fractional noise construct provide fairly accurate mathematical parametrization datum feature model discuss agent base model try find agent strategy property financial institution responsible feature fractional volatility model
q-fin,The minority game: An economics perspective,"  This paper gives a critical account of the minority game literature. The
minority game is a simple congestion game: players need to choose between two
options, and those who have selected the option chosen by the minority win. The
learning model proposed in this literature seems to differ markedly from the
learning models commonly used in economics. We relate the learning model from
the minority game literature to standard game-theoretic learning models, and
show that in fact it shares many features with these models. However, the
predictions of the learning model differ considerably from the predictions of
most other learning models. We discuss the main predictions of the learning
model proposed in the minority game literature, and compare these to
experimental findings on congestion games.
",paper give critical account minority game literature minority game simple congestion game player need choose option select option choose minority win learning model propose literature differ markedly learning model commonly economic relate learn model minority game literature standard game theoretic learning model fact share feature model prediction learn model differ considerably prediction learning model discuss main prediction learning model propose minority game literature compare experimental finding congestion game
q-fin,"Specialization of strategies and herding behavior of trading firms in a
  financial market","  The understanding of complex social or economic systems is an important
scientific challenge. Here we present a comprehensive study of the Spanish
Stock Exchange showing that most financial firms trading in that market are
characterized by a resulting strategy and can be classified in groups of firms
with different specialization. Few large firms overally act as trending firms
whereas many heterogeneous firm act as reversing firms. The herding properties
of these two groups are markedly different and consistently observed over a
four-year period of trading.
",understanding complex social economic system important scientific challenge present comprehensive study spanish stock exchange show financial firm trade market characterize result strategy classify group firm different specialization large firm overally act trend firm heterogeneous firm act reverse firm herding property group markedly different consistently observe year period trading
q-fin,Quantum Nash Equilibria and Quantum Computing,"  In this paper we review our earlier work on quantum computing and the Nash
Equilibrium, in particular, tracing the history of the discovery of new Nash
Equilibria and then reviewing the ways in which quantum computing may be
expected to generate new classes of Nash equilibria. We then extend this work
through a substantive analysis of examples provided by Meyer, Flitney, Iqbal
and Weigert and Cheon and Tsutsui with respect to quantized games, quantum game
strategies and the extension of Nash Equilibrium to solvable games in Hilbert
space. Finally, we review earlier work by Sato, Taiji and Ikegami on non-linear
computation and computational classes by way of reference to coherence,
decoherence and quantum computating systems.
",paper review early work quantum computing nash equilibrium particular trace history discovery new nash equilibria review way quantum computing expect generate new class nash equilibrium extend work substantive analysis example provide meyer flitney iqbal weigert cheon tsutsui respect quantize game quantum game strategy extension nash equilibrium solvable game hilbert space finally review early work sato taiji ikegami non linear computation computational class way reference coherence decoherence quantum computating system
q-fin,Adaptation and Coevolution on an Emergent Global Competitive Landscape,"  Notions of Darwinian selection have been implicit in economic theory for at
least sixty years. Richard Nelson and Sidney Winter have argued that while
evolutionary thinking was prevalent in prewar economics, the postwar
Neoclassical school became almost entirely preoccupied with equilibrium
conditions and their mathematical conditions. One of the problems with the
economic interpretation of firm selection through competition has been a weak
grasp on an incomplete scientific paradigm. As I.F. Price notes, ""The
biological metaphor has long lurked in the background of management theory
largely because the message of 'survival of the fittest' (usually wrongly
attributed to Charles Darwin rather than Herbert Spencer) provides a seemingly
natural model for market competition (e.g. Alchian 1950, Merrell 1984,
Henderson 1989, Moore 1993), without seriously challenging the underlying
paradigms of what an organisation is."" In this paper we examine the application
of dynamic fitness landscape models to economic theory, particularly the theory
of technology substitution, drawing on recent work by Kauffman, Arthur,
McKelvey, Nelson and Winter, and Windrum and Birchenhall. In particular we use
Professor Post's early work with John Holland on the genetic algorithm to
explain some of the key differences between static and dynamic approaches to
economic modeling.
",notions darwinian selection implicit economic theory year richard nelson sidney winter argue evolutionary thinking prevalent prewar economic postwar neoclassical school entirely preoccupied equilibrium condition mathematical condition problem economic interpretation firm selection competition weak grasp incomplete scientific paradigm i.f. price note biological metaphor long lurk background management theory largely message survival fit usually wrongly attribute charles darwin herbert spencer provide seemingly natural model market competition e.g. alchian 1950 merrell 1984 henderson 1989 moore 1993 seriously challenge underlie paradigm organisation paper examine application dynamic fitness landscape model economic theory particularly theory technology substitution draw recent work kauffman arthur mckelvey nelson winter windrum birchenhall particular use professor post early work john holland genetic algorithm explain key difference static dynamic approach economic modeling
q-fin,"Maximum Entropy, the Collective Welfare Principle and the Globalization
  Process","  Although both systems analyzed are described through two theories apparently
different (quantum mechanics and game theory) it is shown that both are
analogous and thus exactly equivalents. The quantum analogue of the replicator
dynamics is the von Neumann equation. Quantum mechanics could be used to
explain more correctly biological and economical processes. It could even
encloses theories like games and evolutionary dynamics. We can take some
concepts and definitions from quantum mechanics and physics for the best
understanding of the behavior of economics and biology. Also, we could maybe
understand nature like a game in where its players compete for a common welfare
and the equilibrium of the system that they are members. All the members of our
system will play a game in which its maximum payoff is the equilibrium of the
system. They act as a whole besides individuals like they obey a rule in where
they prefer to work for the welfare of the collective besides the individual
welfare. A system where its members are in Nash Equilibrium (or ESS) is exactly
equivalent to a system in a maximum entropy state. A system is stable only if
it maximizes the welfare of the collective above the welfare of the individual.
If it is maximized the welfare of the individual above the welfare of the
collective the system gets unstable an eventually collapses. The results of
this work shows that the ""globalization"" process has a behavior exactly
equivalent to a system that is tending to a maximum entropy state and predicts
the apparition of big common markets and strong common currencies that will
find its ""equilibrium"" by decreasing its number until they get a state
characterized by only one common currency and only one common market around the
world.
",system analyze describe theory apparently different quantum mechanic game theory show analogous exactly equivalent quantum analogue replicator dynamic von neumann equation quantum mechanic explain correctly biological economical process enclose theory like game evolutionary dynamic concept definition quantum mechanic physics good understanding behavior economic biology maybe understand nature like game player compete common welfare equilibrium system member member system play game maximum payoff equilibrium system act individual like obey rule prefer work welfare collective individual welfare system member nash equilibrium ess exactly equivalent system maximum entropy state system stable maximize welfare collective welfare individual maximize welfare individual welfare collective system get unstable eventually collapse result work show globalization process behavior exactly equivalent system tend maximum entropy state predict apparition big common market strong common currency find equilibrium decrease number state characterize common currency common market world
q-fin,A Cultural Market Model,"  Social interactions and personal tastes shape our consumption behavior of
cultural products. In this study, we present a computational model of a
cultural market and we aim to analyze the behavior of the consumer population
as an emergent phenomena. Our results suggest that the final market shares of
cultural products dramatically depend on consumer heterogeneity and social
interaction pressure. Furthermore, the relation between the resulting market
shares and social interaction is robust with respect to a wide range of
variation in the parameter values and the type of topology.
",social interaction personal taste shape consumption behavior cultural product study present computational model cultural market aim analyze behavior consumer population emergent phenomena result suggest final market share cultural product dramatically depend consumer heterogeneity social interaction pressure furthermore relation result market share social interaction robust respect wide range variation parameter value type topology
q-fin,"Nonlinear behavior of the Chinese SSEC index with a unit root: Evidence
  from threshold unit root tests","  We investigate the behavior of the Shanghai Stock Exchange Composite (SSEC)
index for the period from 1990:12 to 2007:06 using an unconstrained two-regime
threshold autoregressive (TAR) model with an unit root developed by Caner and
Hansen. The method allows us to simultaneously consider non-stationarity and
nonlinearity in financial time series. Our finding indicates that the Shanghai
stock market exhibits nonlinear behavior with two regimes and has unit roots in
both regimes. The important implications of the threshold effect in stock
markets are also discussed.
",investigate behavior shanghai stock exchange composite ssec index period 1990:12 2007:06 unconstrained regime threshold autoregressive tar model unit root develop caner hansen method allow simultaneously consider non stationarity nonlinearity financial time series finding indicate shanghai stock market exhibit nonlinear behavior regime unit root regime important implication threshold effect stock market discuss
q-fin,Credit risk - A structural model with jumps and correlations,"  We set up a structural model to study credit risk for a portfolio containing
several or many credit contracts. The model is based on a jump--diffusion
process for the risk factors, i.e. for the company assets. We also include
correlations between the companies. We discuss that models of this type have
much in common with other problems in statistical physics and in the theory of
complex systems. We study a simplified version of our model analytically.
Furthermore, we perform extensive numerical simulations for the full model. The
observables are the loss distribution of the credit portfolio, its moments and
other quantities derived thereof. We compile detailed information about the
parameter dependence of these observables. In the course of setting up and
analyzing our model, we also give a review of credit risk modeling for a
physics audience.
",set structural model study credit risk portfolio contain credit contract model base jump diffusion process risk factor i.e. company asset include correlation company discuss model type common problem statistical physics theory complex system study simplified version model analytically furthermore perform extensive numerical simulation model observable loss distribution credit portfolio moment quantity derive thereof compile detailed information parameter dependence observable course set analyze model review credit risk modeling physics audience
q-fin,Multi-scale correlations in different futures markets,"  In the present work we investigate the multiscale nature of the correlations
for high frequency data (1 minute) in different futures markets over a period
of two years, starting on the 1st of January 2003 and ending on the 31st of
December 2004. In particular, by using the concept of ""local"" Hurst exponent,
we point out how the behaviour of this parameter, usually considered as a
benchmark for persistency/antipersistency recognition in time series, is
largely time-scale dependent in the market context. These findings are a direct
consequence of the intrinsic complexity of a system where trading strategies
are scale-adaptive. Moreover, our analysis points out different regimes in the
dynamical behaviour of the market indices under consideration.
",present work investigate multiscale nature correlation high frequency datum 1 minute different future market period year start 1st january 2003 end 31st december 2004 particular concept local hurst exponent point behaviour parameter usually consider benchmark persistency antipersistency recognition time series largely time scale dependent market context finding direct consequence intrinsic complexity system trading strategy scale adaptive analysis point different regime dynamical behaviour market index consideration
q-fin,Economic Amplifier - A New Econophysics Model,"  Most of the econometric and econophysics models have been borrowed from the
statistical physics, and as a cosequence, a new interdisciplinary science
called econophysics has emerged. In this paper we planned to extend the analogy
between different economic processes or phenomena and processes and phenomena
from different fields of physics, other than statistical physics. On the basis
of the economic development process and amplification phenomenon analogy, a new
econophysics model, named economic amplifier, on the electronic amplification
principle from applied physics was proposed und largely analyzed.
",econometric econophysic model borrow statistical physics cosequence new interdisciplinary science call econophysic emerge paper plan extend analogy different economic process phenomenon process phenomenon different field physic statistical physics basis economic development process amplification phenomenon analogy new econophysic model name economic amplifier electronic amplification principle applied physics propose und largely analyze
q-fin,Growth-optimal portfolios under transaction costs,"  This paper studies a portfolio optimization problem in a discrete-time
Markovian model of a financial market, in which asset price dynamics depend on
an external process of economic factors. There are transaction costs with a
structure that covers, in particular, the case of fixed plus proportional
costs. We prove that there exists a self-financing trading strategy maximizing
the average growth rate of the portfolio wealth. We show that this strategy has
a Markovian form. Our result is obtained by large deviations estimates on
empirical measures of the price process and by a generalization of the
vanishing discount method to discontinuous transition operators.
",paper study portfolio optimization problem discrete time markovian model financial market asset price dynamic depend external process economic factor transaction cost structure cover particular case fixed plus proportional cost prove exist self finance trading strategy maximize average growth rate portfolio wealth strategy markovian form result obtain large deviation estimate empirical measure price process generalization vanish discount method discontinuous transition operator
q-fin,A Bayesian Framework for Combining Valuation Estimates,"  Obtaining more accurate equity value estimates is the starting point for
stock selection, value-based indexing in a noisy market, and beating benchmark
indices through tactical style rotation. Unfortunately, discounted cash flow,
method of comparables, and fundamental analysis typically yield discrepant
valuation estimates. Moreover, the valuation estimates typically disagree with
market price. Can one form a superior valuation estimate by averaging over the
individual estimates, including market price? This article suggests a Bayesian
framework for combining two or more estimates into a superior valuation
estimate. The framework justifies the common practice of averaging over several
estimates to arrive at a final point estimate.
",obtain accurate equity value estimate starting point stock selection value base indexing noisy market beat benchmark index tactical style rotation unfortunately discount cash flow method comparable fundamental analysis typically yield discrepant valuation estimate valuation estimate typically disagree market price form superior valuation estimate average individual estimate include market price article suggest bayesian framework combine estimate superior valuation estimate framework justify common practice average estimate arrive final point estimate
q-fin,"Indication of multiscaling in the volatility return intervals of stock
  markets","  The distribution of the return intervals $\tau$ between volatilities above a
threshold $q$ for financial records has been approximated by a scaling
behavior. To explore how accurate is the scaling and therefore understand the
underlined non-linear mechanism, we investigate intraday datasets of 500 stocks
which consist of the Standard & Poor's 500 index. We show that the cumulative
distribution of return intervals has systematic deviations from scaling. We
support this finding by studying the m-th moment $\mu_m \equiv
<(\tau/<\tau>)^m>^{1/m}$, which show a certain trend with the mean interval
$<\tau>$. We generate surrogate records using the Schreiber method, and find
that their cumulative distributions almost collapse to a single curve and
moments are almost constant for most range of $<\tau>$. Those substantial
differences suggest that non-linear correlations in the original volatility
sequence account for the deviations from a single scaling law. We also find
that the original and surrogate records exhibit slight tendencies for short and
long $<\tau>$, due to the discreteness and finite size effects of the records
respectively. To avoid as possible those effects for testing the multiscaling
behavior, we investigate the moments in the range $10<<\tau>\leq100$, and find
the exponent $\alpha$ from the power law fitting $\mu_m\sim<\tau>^\alpha$ has a
narrow distribution around $\alpha\neq0$ which depend on m for the 500 stocks.
The distribution of $\alpha$ for the surrogate records are very narrow and
centered around $\alpha=0$. This suggests that the return interval distribution
exhibit multiscaling behavior due to the non-linear correlations in the
original volatility.
",distribution return interval $ \tau$ volatility threshold $ q$ financial record approximate scaling behavior explore accurate scaling understand underlined non linear mechanism investigate intraday dataset 500 stock consist standard poor 500 index cumulative distribution return interval systematic deviation scale support finding study m th moment $ \mu_m \equiv < \tau/<\tau>)^m>^{1 m}$ certain trend mean interval $ < \tau>$. generate surrogate record schreiber method find cumulative distribution collapse single curve moment constant range $ < \tau>$. substantial difference suggest non linear correlation original volatility sequence account deviation single scaling law find original surrogate record exhibit slight tendency short long $ < \tau>$ discreteness finite size effect record respectively avoid possible effect test multiscaling behavior investigate moment range $ 10<<\tau>\leq100 $ find exponent $ \alpha$ power law fit $ \mu_m\sim<\tau>^\alpha$ narrow distribution $ \alpha\neq0 $ depend m 500 stock distribution $ \alpha$ surrogate record narrow center $ \alpha=0$. suggest return interval distribution exhibit multiscale behavior non linear correlation original volatility
q-fin,"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher
  complexities and oracle inequalities in risk minimization'' by V.
  Koltchinskii","  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]
",discussion ` ` 2004 ims medallion lecture local rademacher complexity oracle inequality risk minimization v. koltchinskii arxiv:0708.0083
q-fin,"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher
  complexities and oracle inequalities in risk minimization'' by V.
  Koltchinskii","  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]
",discussion ` ` 2004 ims medallion lecture local rademacher complexity oracle inequality risk minimization v. koltchinskii arxiv:0708.0083
q-fin,Information flow between composite stock index and individual stocks,"  We investigate the strength and the direction of information transfer in the
U.S. stock market between the composite stock price index of stock market and
prices of individual stocks using the transfer entropy. Through the
directionality of the information transfer, we find that individual stocks are
influenced by the index of the market.
",investigate strength direction information transfer u.s. stock market composite stock price index stock market price individual stock transfer entropy directionality information transfer find individual stock influence index market
q-fin,The International Trade Network: weighted network analysis and modelling,"  Tools of the theory of critical phenomena, namely the scaling analysis and
universality, are argued to be applicable to large complex web-like network
structures. Using a detailed analysis of the real data of the International
Trade Network we argue that the scaled link weight distribution has an
approximate log-normal distribution which remains robust over a period of 53
years. Another universal feature is observed in the power-law growth of the
trade strength with gross domestic product, the exponent being similar for all
countries. Using the 'rich-club' coefficient measure of the weighted networks
it has been shown that the size of the rich-club controlling half of the
world's trade is actually shrinking. While the gravity law is known to describe
well the social interactions in the static networks of population migration,
international trade, etc, here for the first time we studied a non-conservative
dynamical model based on the gravity law which excellently reproduced many
empirical features of the ITN.
",tools theory critical phenomenon scale analysis universality argue applicable large complex web like network structure detailed analysis real datum international trade network argue scale link weight distribution approximate log normal distribution remain robust period 53 year universal feature observe power law growth trade strength gross domestic product exponent similar country rich club coefficient measure weighted network show size rich club control half world trade actually shrink gravity law know describe social interaction static network population migration international trade etc time study non conservative dynamical model base gravity law excellently reproduce empirical feature itn
q-fin,Sparse and stable Markowitz portfolios,"  We consider the problem of portfolio selection within the classical Markowitz
mean-variance framework, reformulated as a constrained least-squares regression
problem. We propose to add to the objective function a penalty proportional to
the sum of the absolute values of the portfolio weights. This penalty
regularizes (stabilizes) the optimization problem, encourages sparse portfolios
(i.e. portfolios with only few active positions), and allows to account for
transaction costs. Our approach recovers as special cases the
no-short-positions portfolios, but does allow for short positions in limited
number. We implement this methodology on two benchmark data sets constructed by
Fama and French. Using only a modest amount of training data, we construct
portfolios whose out-of-sample performance, as measured by Sharpe ratio, is
consistently and significantly better than that of the naive evenly-weighted
portfolio which constitutes, as shown in recent literature, a very tough
benchmark.
",consider problem portfolio selection classical markowitz mean variance framework reformulate constrain square regression problem propose add objective function penalty proportional sum absolute value portfolio weight penalty regularize stabilize optimization problem encourage sparse portfolio i.e. portfolio active position allow account transaction cost approach recover special case short position portfolio allow short position limited number implement methodology benchmark data set construct fama french modest training datum construct portfolio sample performance measure sharpe ratio consistently significantly well naive evenly weight portfolio constitute show recent literature tough benchmark
q-fin,The International Trade Network,"  Bilateral trade relationships in the international level between pairs of
countries in the world give rise to the notion of the International Trade
Network (ITN). This network has attracted the attention of network researchers
as it serves as an excellent example of the weighted networks, the link weight
being defined as a measure of the volume of trade between two countries. In
this paper we analyzed the international trade data for 53 years and studied in
detail the variations of different network related quantities associated with
the ITN. Our observation is that the ITN has also a scale invariant structure
like many other real-world networks.
",bilateral trade relationship international level pair country world rise notion international trade network itn network attract attention network researcher serve excellent example weighted network link weight define measure volume trade country paper analyze international trade datum 53 year study detail variation different network relate quantity associate itn observation itn scale invariant structure like real world network
q-fin,"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher
  complexities and oracle inequalities in risk minimization'' by V.
  Koltchinskii","  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]
",discussion ` ` 2004 ims medallion lecture local rademacher complexity oracle inequality risk minimization v. koltchinskii arxiv:0708.0083
q-fin,"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher
  complexities and oracle inequalities in risk minimization'' by V.
  Koltchinskii","  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]
",discussion ` ` 2004 ims medallion lecture local rademacher complexity oracle inequality risk minimization v. koltchinskii arxiv:0708.0083
q-fin,"Discussion of ""2004 IMS Medallion Lecture: Local Rademacher complexities
  and oracle inequalities in risk minimization"" by V. Koltchinskii","  Discussion of ""2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization"" by V. Koltchinskii [arXiv:0708.0083]
",discussion 2004 ims medallion lecture local rademacher complexity oracle inequality risk minimization v. koltchinskii arxiv:0708.0083
q-fin,"Stochastic Knapsack Problem Revisited: Switch-Over Policies and Dynamic
  Pricing","  The stochastic knapsack has been used as a model in wide ranging applications
from dynamic resource allocation to admission control in telecommunication. In
recent years, a variation of the model has become a basic tool in studying
problems that arise in revenue management and dynamic/flexible pricing; and it
is in this context that our study is undertaken. Based on a dynamic programming
formulation and associated properties of the value function, we study in this
paper a class of control that we call switch-over policies -- start from
accepting only orders of the highest price, and switch to including lower
prices as time goes by, with the switch-over times optimally decided via convex
programming. We establish the asymptotic optimality of the switch-over policy,
and develop pricing models based on this policy to optimize the price
reductions over the decision horizon.
",stochastic knapsack model wide ranging application dynamic resource allocation admission control telecommunication recent year variation model basic tool study problem arise revenue management dynamic flexible pricing context study undertake base dynamic programming formulation associated property value function study paper class control switch policy start accept order high price switch include low price time go switch time optimally decide convex programming establish asymptotic optimality switch policy develop pricing model base policy optimize price reduction decision horizon
q-fin,"The Local Fractal Properties of the Financial Time Series on the Polish
  Stock Exchange Market","  We investigate the local fractal properties of the financial time series
based on the evolution of the Warsaw Stock Exchange Index (WIG) connected with
the largest developing financial market in Europe. Calculating the local Hurst
exponent for the WIG time series we find an interesting dependence between the
behavior of the local fractal properties of the WIG time series and the crashes
appearance on the financial market.
",investigate local fractal property financial time series base evolution warsaw stock exchange index wig connect large develop financial market europe calculate local hurst exponent wig time series find interesting dependence behavior local fractal property wig time series crash appearance financial market
q-fin,"A new formulation of asset trading games in continuous time with
  essential forcing of variation exponent","  We introduce a new formulation of asset trading games in continuous time in
the framework of the game-theoretic probability established by Shafer and Vovk
(Probability and Finance: It's Only a Game! (2001) Wiley). In our formulation,
the market moves continuously, but an investor trades in discrete times, which
can depend on the past path of the market. We prove that an investor can
essentially force that the asset price path behaves with the variation exponent
exactly equal to two. Our proof is based on embedding high-frequency
discrete-time games into the continuous-time game and the use of the Bayesian
strategy of Kumon, Takemura and Takeuchi (Stoch. Anal. Appl. 26 (2008)
1161--1180) for discrete-time coin-tossing games. We also show that the main
growth part of the investor's capital processes is clearly described by the
information quantities, which are derived from the Kullback--Leibler
information with respect to the empirical fluctuation of the asset price.
",introduce new formulation asset trading game continuous time framework game theoretic probability establish shafer vovk probability finance game 2001 wiley formulation market move continuously investor trade discrete time depend past path market prove investor essentially force asset price path behave variation exponent exactly equal proof base embed high frequency discrete time game continuous time game use bayesian strategy kumon takemura takeuchi stoch anal appl 26 2008 1161 -1180 discrete time coin tossing game main growth investor capital process clearly describe information quantity derive kullback leibler information respect empirical fluctuation asset price
q-fin,Fine-tune your smile: Correction to Hagan et al,"  In this small note we use results derived in Berestycki et al. to correct the
celebrated formulae of Hagan et al. We derive explicitly the correct zero order
term in the expansion of the implied volatility in time to maturity. The new
term is consistent as $\beta\to 1$. Furthermore, numerical simulations show
that it reduces or eliminates known pathologies of the earlier formula.
",small note use result derive berestycki et al correct celebrate formulae hagan et al derive explicitly correct zero order term expansion imply volatility time maturity new term consistent $ \beta\to 1$. furthermore numerical simulation reduce eliminate known pathology early formula
q-fin,Models of Financial Markets with Extensive Participation Incentives,"  We consider models of financial markets in which all parties involved find
incentives to participate. Strategies are evaluated directly by their virtual
wealths. By tuning the price sensitivity and market impact, a phase diagram
with several attractor behaviors resembling those of real markets emerge,
reflecting the roles played by the arbitrageurs and trendsetters, and including
a phase with irregular price trends and positive sums. The positive-sumness of
the players' wealths provides participation incentives for them. Evolution and
the bid-ask spread provide mechanisms for the gain in wealth of both the
players and market-makers. New players survive in the market if the
evolutionary rate is sufficiently slow. We test the applicability of the model
on real Hang Seng Index data over 20 years. Comparisons with other models show
that our model has a superior average performance when applied to real
financial data.
",consider model financial market party involve find incentive participate strategy evaluate directly virtual wealth tune price sensitivity market impact phase diagram attractor behavior resemble real market emerge reflect role play arbitrageur trendsetter include phase irregular price trend positive sum positive sumness player wealth provide participation incentive evolution bid ask spread provide mechanism gain wealth player market maker new player survive market evolutionary rate sufficiently slow test applicability model real hang seng index datum 20 year comparison model model superior average performance apply real financial datum
q-fin,Group dynamics of the Japanese market,"  We investigated the network structures of the Japanese stock market through
the minimum spanning tree. We defined grouping coefficient to test the validity
of conventional grouping by industrial categories, and found a decreasing in
trend for the coefficient. This phenomenon supports the increasing external
influences on the market due to the globalization. To reduce this influence, we
used S&P500 index as the international market and removed its correlation with
every stock. We found stronger grouping in this measurement, compared to the
original analysis, which agrees with our assumption that the international
market influences to the Japanese market.
",investigate network structure japanese stock market minimum span tree define group coefficient test validity conventional grouping industrial category find decreasing trend coefficient phenomenon support increase external influence market globalization reduce influence s&p500 index international market remove correlation stock find strong grouping measurement compare original analysis agree assumption international market influence japanese market
q-fin,Perpetual American options within CTRW's,"  Continuous-time random walks are a well suited tool for the description of
market behaviour at the smallest scale: the tick-to-tick evolution. We will
apply this kind of market model to the valuation of perpetual American options:
derivatives with no maturity that can be exercised at any time. Our approach
leads to option prices that fulfil financial formulas when canonical
assumptions on the dynamics governing the process are made, but it is still
suitable for more exotic market conditions.
",continuous time random walk suited tool description market behaviour small scale tick tick evolution apply kind market model valuation perpetual american option derivative maturity exercise time approach lead option price fulfil financial formula canonical assumption dynamic govern process suitable exotic market condition
q-fin,Investment and Consumption without Commitment,"  In this paper, we investigate the Merton portfolio management problem in the
context of non-exponential discounting. This gives rise to time-inconsistency
of the decision-maker. If the decision-maker at time t=0 can commit his/her
successors, he/she can choose the policy that is optimal from his/her point of
view, and constrain the others to abide by it, although they do not see it as
optimal for them. If there is no commitment mechanism, one must seek a
subgame-perfect equilibrium strategy between the successive decision-makers. In
the line of the earlier work by Ekeland and Lazrak we give a precise definition
of equilibrium strategies in the context of the portfolio management problem,
with finite horizon, we characterize it by a system of partial differential
equations, and we show existence in the case when the utility is CRRA and the
terminal time T is small. We also investigate the infinite-horizon case and we
give two different explicit solutions in the case when the utility is CRRA (in
contrast with the case of exponential discount, where there is only one). Some
of our results are proved under the assumption that the discount function h(t)
is a linear combination of two exponentials, or is the product of an
exponential by a linear function.
",paper investigate merton portfolio management problem context non exponential discounting give rise time inconsistency decision maker decision maker time t=0 commit successor choose policy optimal point view constrain abide optimal commitment mechanism seek subgame perfect equilibrium strategy successive decision maker line early work ekeland lazrak precise definition equilibrium strategy context portfolio management problem finite horizon characterize system partial differential equation existence case utility crra terminal time t small investigate infinite horizon case different explicit solution case utility crra contrast case exponential discount result prove assumption discount function h(t linear combination exponential product exponential linear function
q-fin,The Product Space Conditions the Development of Nations,"  Economies grow by upgrading the type of products they produce and export. The
technology, capital, institutions and skills needed to make such new products
are more easily adapted from some products than others. We study the network of
relatedness between products, or product space, finding that most upscale
products are located in a densely connected core while lower income products
occupy a less connected periphery. We show that countries tend to move to goods
close to those they are currently specialized in, allowing nations located in
more connected parts of the product space to upgrade their exports basket more
quickly. Most countries can reach the core only if they jump over empirically
infrequent distances in the product space. This may help explain why poor
countries have trouble developing more competitive exports, failing to converge
to the income levels of rich countries.
",economy grow upgrade type product produce export technology capital institution skill need new product easily adapt product study network relatedness product product space find upscale product locate densely connect core low income product occupy connected periphery country tend good close currently specialize allow nation locate connected part product space upgrade export basket quickly country reach core jump empirically infrequent distance product space help explain poor country trouble develop competitive export fail converge income level rich country
q-fin,"Optimal execution strategies in limit order books with general shape
  functions","  We consider optimal execution strategies for block market orders placed in a
limit order book (LOB). We build on the resilience model proposed by Obizhaeva
and Wang (2005) but allow for a general shape of the LOB defined via a given
density function. Thus, we can allow for empirically observed LOB shapes and
obtain a nonlinear price impact of market orders. We distinguish two
possibilities for modeling the resilience of the LOB after a large market
order: the exponential recovery of the number of limit orders, i.e., of the
volume of the LOB, or the exponential recovery of the bid-ask spread. We
consider both of these resilience modes and, in each case, derive explicit
optimal execution strategies in discrete time. Applying our results to a
block-shaped LOB, we obtain a new closed-form representation for the optimal
strategy, which explicitly solves the recursive scheme given in Obizhaeva and
Wang (2005). We also provide some evidence for the robustness of optimal
strategies with respect to the choice of the shape function and the
resilience-type.
",consider optimal execution strategy block market order place limit order book lob build resilience model propose obizhaeva wang 2005 allow general shape lob define give density function allow empirically observe lob shape obtain nonlinear price impact market order distinguish possibility model resilience lob large market order exponential recovery number limit order i.e. volume lob exponential recovery bid ask spread consider resilience mode case derive explicit optimal execution strategy discrete time apply result block shape lob obtain new closed form representation optimal strategy explicitly solve recursive scheme give obizhaeva wang 2005 provide evidence robustness optimal strategy respect choice shape function resilience type
q-fin,On the Structure of General Mean-Variance Hedging Strategies,"  We provide a new characterization of mean-variance hedging strategies in a
general semimartingale market. The key point is the introduction of a new
probability measure $P^{\star}$ which turns the dynamic asset allocation
problem into a myopic one. The minimal martingale measure relative to
$P^{\star}$ coincides with the variance-optimal martingale measure relative to
the original probability measure $P$.
",provide new characterization mean variance hedging strategy general semimartingale market key point introduction new probability measure $ p^{\star}$ turn dynamic asset allocation problem myopic minimal martingale measure relative $ p^{\star}$ coincide variance optimal martingale measure relative original probability measure $ p$.
q-fin,"Nonlinear option pricing models for illiquid markets: scaling properties
  and explicit solutions","  Several models for the pricing of derivative securities in illiquid markets
are discussed. A typical type of nonlinear partial differential equations
arising from these investigation is studied. The scaling properties of these
equations are discussed. Explicit solutions for one of the models are obtained
and studied.
",model pricing derivative security illiquid market discuss typical type nonlinear partial differential equation arise investigation study scale property equation discuss explicit solution model obtain study
q-fin,"Models with time-dependent parameters using transform methods:
  application to Heston's model","  This paper presents a methodology to introduce time-dependent parameters for
a wide family of models preserving their analytic tractability. This family
includes hybrid models with stochastic volatility, stochastic interest-rates,
jumps and their non-hybrid counterparts. The methodology is applied to Heston's
model. A bootstrapping algorithm is presented for calibration. A case study
works out the calibration of the time-dependent parameters to the volatility
surface of the Eurostoxx 50 index. The methodology is also applied to the
analytic valuation of forward start vanilla options driven by Heston's model.
This result is used to explore the forward skew of the case study.
",paper present methodology introduce time dependent parameter wide family model preserve analytic tractability family include hybrid model stochastic volatility stochastic interest rate jump non hybrid counterpart methodology apply heston model bootstrapping algorithm present calibration case study work calibration time dependent parameter volatility surface eurostoxx 50 index methodology apply analytic valuation forward start vanilla option drive heston model result explore forward skew case study
q-fin,Point estimation with exponentially tilted empirical likelihood,"  Parameters defined via general estimating equations (GEE) can be estimated by
maximizing the empirical likelihood (EL). Newey and Smith [Econometrica 72
(2004) 219--255] have recently shown that this EL estimator exhibits desirable
higher-order asymptotic properties, namely, that its $O(n^{-1})$ bias is small
and that bias-corrected EL is higher-order efficient. Although EL possesses
these properties when the model is correctly specified, this paper shows that,
in the presence of model misspecification, EL may cease to be root n convergent
when the functions defining the moment conditions are unbounded (even when
their expectations are bounded). In contrast, the related exponential tilting
(ET) estimator avoids this problem. This paper shows that the ET and EL
estimators can be naturally combined to yield an estimator called exponentially
tilted empirical likelihood (ETEL) exhibiting the same $O(n^{-1})$ bias and the
same $O(n^{-2})$ variance as EL, while maintaining root n convergence under
model misspecification.
",parameter define general estimating equation gee estimate maximize empirical likelihood el newey smith econometrica 72 2004 219 -255 recently show el estimator exhibit desirable high order asymptotic property $ o(n^{-1})$ bias small bias correct el high order efficient el possess property model correctly specify paper show presence model misspecification el cease root n convergent function define moment condition unbounded expectation bound contrast related exponential tilting et estimator avoid problem paper show et el estimator naturally combine yield estimator call exponentially tilt empirical likelihood etel exhibit $ o(n^{-1})$ bias $ o(n^{-2})$ variance el maintain root n convergence model misspecification
q-fin,Eduction and Economy -- An Analysis of Statistical Data,"  In this paper the correlation between education, research and macroeconomic
strength of countries at a global scale is analyzed on the basis of statistical
data published by the UNIDO and OECD. It uses sets of composite indicators
describing the economical performance and competitiveness as well as those
relevant for human development, education, knowledge and technology achievement
and correlates them. It turns out that for countries with a human development
index (HDI) below 0.7 the basic education and technology achievement indices
are the driving force for further development, whereas for the industrialized
countries the knowledge index as a composite education and communication index
has the strongest effect on the economic strength of a country as measured by
the gross domestic product.
",paper correlation education research macroeconomic strength country global scale analyze basis statistical datum publish unido oecd use set composite indicator describe economical performance competitiveness relevant human development education knowledge technology achievement correlate turn country human development index hdi 0.7 basic education technology achievement index drive force development industrialized country knowledge index composite education communication index strong effect economic strength country measure gross domestic product
stat,Domain wall switching: optimizing the energy landscape,"  It has recently been suggested that exchange spring media offer a way to
increase media density without causing thermal instability
(superparamagnetism), by using a hard and a soft layer coupled by exchange.
Victora has suggested a figure of merit xi = 2 E_b/mu_0 m_s H_sw, the ratio of
the energy barrier to that of a Stoner-Wohlfarth system with the same switching
field, which is 1 for a Stoner-Wohlfarth (coherently switching) particle and 2
for an optimal two-layer composite medium. A number of theoretical approaches
have been used for this problem (e.g., various numbers of coupled
Stoner-Wohlfarth layers and continuum micromagnetics). In this paper we show
that many of these approaches can be regarded as special cases or
approximations to a variational formulation of the problem, in which the energy
is minimized for fixed magnetization. The results can be easily visualized in
terms of a plot of the energy as a function of magnetic moment m_z, in which
both the switching field [the maximum slope of E(m_z)] and the stability
(determined by the energy barrier E_b) are geometrically visible. In this
formulation we can prove a rigorous limit on the figure of merit xi, which can
be no higher than 4. We also show that a quadratic anistropy suggested by Suess
et al comes very close to this limit.
",recently suggest exchange spring medium offer way increase medium density cause thermal instability superparamagnetism hard soft layer couple exchange victora suggest figure merit xi = 2 e_b mu_0 m_s h_sw ratio energy barrier stoner wohlfarth system switching field 1 stoner wohlfarth coherently switch particle 2 optimal layer composite medium number theoretical approach problem e.g. number couple stoner wohlfarth layer continuum micromagnetic paper approach regard special case approximation variational formulation problem energy minimize fix magnetization result easily visualize term plot energy function magnetic moment m_z switch field maximum slope e(m_z stability determine energy barrier e_b geometrically visible formulation prove rigorous limit figure merit xi high 4 quadratic anistropy suggest suess et al come close limit
stat,"Matter-Wave Bright Solitons with a Finite Background in Spinor
  Bose-Einstein Condensates","  We investigate dynamical properties of bright solitons with a finite
background in the F=1 spinor Bose-Einstein condensate (BEC), based on an
integrable spinor model which is equivalent to the matrix nonlinear
Schr\""{o}dinger equation with a self-focusing nonlineality. We apply the
inverse scattering method formulated for nonvanishing boundary conditions. The
resulting soliton solutions can be regarded as a generalization of those under
vanishing boundary conditions. One-soliton solutions are derived in an explicit
manner. According to the behaviors at the infinity, they are classified into
two kinds, domain-wall (DW) type and phase-shift (PS) type. The DW-type implies
the ferromagnetic state with nonzero total spin and the PS-type implies the
polar state, where the total spin amounts to zero. We also discuss two-soliton
collisions. In particular, the spin-mixing phenomenon is confirmed in a
collision involving the DW-type. The results are consistent with those of the
previous studies for bright solitons under vanishing boundary conditions and
dark solitons. As a result, we establish the robustness and the usefulness of
the multiple matter-wave solitons in the spinor BECs.
","investigate dynamical property bright soliton finite background f=1 spinor bose einstein condensate bec base integrable spinor model equivalent matrix nonlinear schr\""{o}dinger equation self focus nonlineality apply inverse scattering method formulate nonvanishe boundary condition result soliton solution regard generalization vanish boundary condition soliton solution derive explicit manner accord behavior infinity classify kind domain wall dw type phase shift ps type dw type imply ferromagnetic state nonzero total spin ps type imply polar state total spin amount zero discuss soliton collision particular spin mix phenomenon confirm collision involve dw type result consistent previous study bright soliton vanish boundary condition dark soliton result establish robustness usefulness multiple matter wave soliton spinor becs"
stat,"Computation of Power Loss in Likelihood Ratio Tests for Probability
  Densities Extended by Lehmann Alternatives","  We compute the loss of power in likelihood ratio tests when we test the
original parameter of a probability density extended by the first Lehmann
alternative.
",compute loss power likelihood ratio test test original parameter probability density extend lehmann alternative
stat,"The density of critical percolation clusters touching the boundaries of
  strips and squares","  We consider the density of two-dimensional critical percolation clusters,
constrained to touch one or both boundaries, in infinite strips, half-infinite
strips, and squares, as well as several related quantities for the infinite
strip. Our theoretical results follow from conformal field theory, and are
compared with high-precision numerical simulation. For example, we show that
the density of clusters touching both boundaries of an infinite strip of unit
width (i.e. crossing clusters) is proportional to $(\sin \pi
y)^{-5/48}\{[\cos(\pi y/2)]^{1/3} +[\sin (\pi y/2)]^{1/3}-1\}$.
  We also determine numerically contours for the density of clusters crossing
squares and long rectangles with open boundaries on the sides, and compare with
theory for the density along an edge.
",consider density dimensional critical percolation cluster constrain touch boundary infinite strip half infinite strip square relate quantity infinite strip theoretical result follow conformal field theory compare high precision numerical simulation example density cluster touch boundary infinite strip unit width i.e. cross cluster proportional $ \sin \pi y)^{-5/48}\{[\cos(\pi y/2)]^{1/3 + \sin \pi y/2)]^{1/3}-1\}$. determine numerically contour density cluster cross square long rectangle open boundary side compare theory density edge
stat,"A density tensor hierarchy for open system dynamics: retrieving the
  noise","  We introduce a density tensor hierarchy for open system dynamics, that
recovers information about fluctuations lost in passing to the reduced density
matrix. For the case of fluctuations arising from a classical probability
distribution, the hierarchy is formed from expectations of products of pure
state density matrix elements, and can be compactly summarized by a simple
generating function. For the case of quantum fluctuations arising when a
quantum system interacts with a quantum environment in an overall pure state,
the corresponding hierarchy is defined as the environmental trace of products
of system matrix elements of the full density matrix. Only the lowest member of
the quantum noise hierarchy is directly experimentally measurable. The unit
trace and idempotence properties of the pure state density matrix imply descent
relations for the tensor hierarchies, that relate the order $n$ tensor, under
contraction of appropriate pairs of tensor indices, to the order $n-1$ tensor.
As examples to illustrate the classical probability distribution formalism, we
consider a quantum system evolving by It\^o stochastic and by jump process
Schr\""odinger equations. As examples to illustrate the corresponding trace
formalism in the quantum fluctuation case, we consider collisional Brownian
motion of an infinite mass Brownian particle, and the weak coupling Born-Markov
master equation. In different specializations, the latter gives the hierarchies
generalizing the quantum optical master equation and the Caldeira--Leggett
master equation. As a further application of the density tensor, we contrast
stochastic Schr\""odinger equations that reduce and that do not reduce the state
vector, and discuss why a quantum system coupled to a quantum environment
behaves like the latter.
","introduce density tensor hierarchy open system dynamic recover information fluctuation lose pass reduce density matrix case fluctuation arise classical probability distribution hierarchy form expectation product pure state density matrix element compactly summarize simple generating function case quantum fluctuation arise quantum system interact quantum environment overall pure state corresponding hierarchy define environmental trace product system matrix element density matrix low member quantum noise hierarchy directly experimentally measurable unit trace idempotence property pure state density matrix imply descent relation tensor hierarchy relate order $ n$ tensor contraction appropriate pair tensor index order $ n-1 $ tensor example illustrate classical probability distribution formalism consider quantum system evolve it\^o stochastic jump process schr\""odinger equation example illustrate corresponding trace formalism quantum fluctuation case consider collisional brownian motion infinite mass brownian particle weak coupling bear markov master equation different specialization give hierarchy generalize quantum optical master equation caldeira leggett master equation application density tensor contrast stochastic schr\""odinger equation reduce reduce state vector discuss quantum system couple quantum environment behave like"
stat,Driven activation versus thermal activation,"  Activated dynamics in a glassy system undergoing steady shear deformation is
studied by numerical simulations. Our results show that the external driving
force has a strong influence on the barrier crossing rate, even though the
reaction coordinate is only weakly coupled to the nonequilibrium system. This
""driven activation"" can be quantified by introducing in the Arrhenius
expression an effective temperature, which is close to the one determined from
the fluctuation-dissipation relation. This conclusion is supported by
analytical results for a simplified model system.
",activate dynamic glassy system undergo steady shear deformation study numerical simulation result external driving force strong influence barrier crossing rate reaction coordinate weakly couple nonequilibrium system drive activation quantify introduce arrhenius expression effective temperature close determine fluctuation dissipation relation conclusion support analytical result simplified model system
stat,"Dependence of the Critical Adsorption Point on Surface and Sequence
  Disorders for Self-Avoiding Walks Interacting with a Planar Surface","  The critical adsorption point (CAP) of self-avoiding walks (SAW) interacting
with a planar surface with surface disorder or sequence disorder has been
studied. We present theoretical equations, based on ones previously developed
by Soteros and Whittington (J. Phys. A.: Math. Gen. 2004, 37, R279-R325), that
describe the dependence of CAP on the disorders along with Monte Carlo
simulation data that are in agreement with the equations. We also show
simulation results that deviate from the equations when the approximations used
in the theory break down. Such knowledge is the first step toward understanding
the correlation of surface disorder and sequence disorder during polymer
adsorption.
",critical adsorption point cap self avoid walk saw interact planar surface surface disorder sequence disorder study present theoretical equation base one previously develop soteros whittington j. phys a. math gen. 2004 37 r279 r325 describe dependence cap disorder monte carlo simulation datum agreement equation simulation result deviate equation approximation theory break knowledge step understand correlation surface disorder sequence disorder polymer adsorption
stat,Application of Ewald summations to long-range dispersion forces,"  We present results illustrating the effects of using explicit summation terms
for the $r^{-6}$ dispersion term on the interfacial properties of a
Lennard-Jones fluid and SPC/E water. For the Lennard-Jones fluid, we find that
the use of long-range summations, even with a short ``crossover radius,''
yields results that are consistent with simulations using large cutoff radii.
Simulations of SPC/E water demonstrate that the long-range dispersion forces
are of secondary importance to the Coulombic forces. In both cases, we find
that the ratio of box size $L_{\parallel}$ to crossover radius $r_{\rm
c}^{\mathbf k}$ plays an important role in determining the magnitude of the
long-range dispersion correction, although its effect is secondary when
Coulombic interactions are also present.
",present result illustrate effect explicit summation term $ r^{-6}$ dispersion term interfacial property lennard jones fluid spc e water lennard jones fluid find use long range summation short ` ` crossover radius yield result consistent simulation large cutoff radius simulation spc e water demonstrate long range dispersion force secondary importance coulombic force case find ratio box size $ l_{\parallel}$ crossover radius $ r_{\rm c}^{\mathbf k}$ play important role determine magnitude long range dispersion correction effect secondary coulombic interaction present
stat,"Stable oscillations of a predator-prey probabilistic cellular automaton:
  a mean-field approach","  We analyze a probabilistic cellular automaton describing the dynamics of
coexistence of a predator-prey system. The individuals of each species are
localized over the sites of a lattice and the local stochastic updating rules
are inspired on the processes of the Lotka-Volterra model. Two levels of
mean-field approximations are set up. The simple approximation is equivalent to
an extended patch model, a simple metapopulation model with patches colonized
by prey, patches colonized by predators and empty patches. This approximation
is capable of describing the limited available space for species occupancy. The
pair approximation is moreover able to describe two types of coexistence of
prey and predators: one where population densities are constant in time and
another displaying self-sustained time-oscillations of the population
densities. The oscillations are associated with limit cycles and arise through
a Hopf bifurcation. They are stable against changes in the initial conditions
and, in this sense, they differ from the Lotka-Volterra cycles which depend on
initial conditions. In this respect, the present model is biologically more
realistic than the Lotka-Volterra model.
",analyze probabilistic cellular automaton describe dynamic coexistence predator prey system individual specie localize site lattice local stochastic updating rule inspire process lotka volterra model level mean field approximation set simple approximation equivalent extended patch model simple metapopulation model patch colonize prey patch colonize predator patch approximation capable describe limited available space specie occupancy pair approximation able describe type coexistence prey predator population density constant time display self sustain time oscillation population density oscillation associate limit cycle arise hopf bifurcation stable change initial condition sense differ lotka volterra cycle depend initial condition respect present model biologically realistic lotka volterra model
stat,Spinor Dynamics in an Antiferromagnetic Spin-1 Condensate,"  We observe coherent spin oscillations in an antiferromagnetic spin-1
Bose-Einstein condensate of sodium. The variation of the spin oscillations with
magnetic field shows a clear signature of nonlinearity, in agreement with
theory, which also predicts anharmonic oscillations near a critical magnetic
field. Measurements of the magnetic phase diagram agree with predictions made
in the approximation of a single spatial mode. The oscillation period yields
the best measurement to date of the sodium spin-dependent interaction
coefficient, determining that the difference between the sodium spin-dependent
s-wave scattering lengths $a_{f=2}-a_{f=0}$ is $2.47\pm0.27$ Bohr radii.
",observe coherent spin oscillation antiferromagnetic spin-1 bose einstein condensate sodium variation spin oscillation magnetic field show clear signature nonlinearity agreement theory predict anharmonic oscillation near critical magnetic field measurement magnetic phase diagram agree prediction approximation single spatial mode oscillation period yield good measurement date sodium spin dependent interaction coefficient determine difference sodium spin dependent s wave scatter length $ a_{f=2}-a_{f=0}$ $ 2.47\pm0.27 $ bohr radius
stat,Stochastic action principle and maximum entropy,"  A stochastic action principle for stochastic dynamics is revisited. We
present first numerical diffusion experiments showing that the diffusion path
probability depend exponentially on average Lagrangian action. This result is
then used to derive an uncertainty measure defined in a way mimicking the heat
or entropy in the first law of thermodynamics. It is shown that the path
uncertainty (or path entropy) can be measured by the Shannon information and
that the maximum entropy principle and the least action principle of classical
mechanics can be unified into a concise form. It is argued that this action
principle, hence the maximum entropy principle, is simply a consequence of the
mechanical equilibrium condition extended to the case of stochastic dynamics.
",stochastic action principle stochastic dynamic revisit present numerical diffusion experiment show diffusion path probability depend exponentially average lagrangian action result derive uncertainty measure define way mimic heat entropy law thermodynamic show path uncertainty path entropy measure shannon information maximum entropy principle action principle classical mechanic unify concise form argue action principle maximum entropy principle simply consequence mechanical equilibrium condition extend case stochastic dynamic
stat,Real Options for Project Schedules (ROPS),"  Real Options for Project Schedules (ROPS) has three recursive
sampling/optimization shells. An outer Adaptive Simulated Annealing (ASA)
optimization shell optimizes parameters of strategic Plans containing multiple
Projects containing ordered Tasks. A middle shell samples probability
distributions of durations of Tasks. An inner shell samples probability
distributions of costs of Tasks. PATHTREE is used to develop options on
schedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to
develop a relative risk analysis among projects.
",real options project schedules rops recursive sampling optimization shell outer adaptive simulated annealing asa optimization shell optimize parameter strategic plan contain multiple project contain order tasks middle shell sample probability distribution duration tasks inner shell sample probability distribution cost tasks pathtree develop option schedule algorithms trading risk dimensions trd apply develop relative risk analysis project
stat,Quantifying social group evolution,"  The rich set of interactions between individuals in the society results in
complex community structure, capturing highly connected circles of friends,
families, or professional cliques in a social network. Thanks to frequent
changes in the activity and communication patterns of individuals, the
associated social and communication network is subject to constant evolution.
Our knowledge of the mechanisms governing the underlying community dynamics is
limited, but is essential for a deeper understanding of the development and
self-optimisation of the society as a whole. We have developed a new algorithm
based on clique percolation, that allows, for the first time, to investigate
the time dependence of overlapping communities on a large scale and as such, to
uncover basic relationships characterising community evolution. Our focus is on
networks capturing the collaboration between scientists and the calls between
mobile phone users. We find that large groups persist longer if they are
capable of dynamically altering their membership, suggesting that an ability to
change the composition results in better adaptability. The behaviour of small
groups displays the opposite tendency, the condition for stability being that
their composition remains unchanged. We also show that the knowledge of the
time commitment of the members to a given community can be used for estimating
the community's lifetime. These findings offer a new view on the fundamental
differences between the dynamics of small groups and large institutions.
",rich set interaction individual society result complex community structure capture highly connect circle friend family professional clique social network thank frequent change activity communication pattern individual associate social communication network subject constant evolution knowledge mechanism govern underlie community dynamic limited essential deep understanding development self optimisation society develop new algorithm base clique percolation allow time investigate time dependence overlap community large scale uncover basic relationship characterise community evolution focus network capture collaboration scientist call mobile phone user find large group persist long capable dynamically alter membership suggest ability change composition result well adaptability behaviour small group display opposite tendency condition stability composition remain unchanged knowledge time commitment member give community estimate community lifetime finding offer new view fundamental difference dynamic small group large institution
stat,Equation-free implementation of statistical moment closures,"  We present a general numerical scheme for the practical implementation of
statistical moment closures suitable for modeling complex, large-scale,
nonlinear systems. Building on recently developed equation-free methods, this
approach numerically integrates the closure dynamics, the equations of which
may not even be available in closed form. Although closure dynamics introduce
statistical assumptions of unknown validity, they can have significant
computational advantages as they typically have fewer degrees of freedom and
may be much less stiff than the original detailed model. The closure method can
in principle be applied to a wide class of nonlinear problems, including
strongly-coupled systems (either deterministic or stochastic) for which there
may be no scale separation. We demonstrate the equation-free approach for
implementing entropy-based Eyink-Levermore closures on a nonlinear stochastic
partial differential equation.
",present general numerical scheme practical implementation statistical moment closure suitable model complex large scale nonlinear system build recently develop equation free method approach numerically integrate closure dynamic equation available closed form closure dynamic introduce statistical assumption unknown validity significant computational advantage typically few degree freedom stiff original detailed model closure method principle apply wide class nonlinear problem include strongly couple system deterministic stochastic scale separation demonstrate equation free approach implement entropy base eyink levermore closure nonlinear stochastic partial differential equation
stat,Crossover behavior in fluids with Coulomb interactions,"  According to extensive experimental findings, the Ginzburg temperature
$t_{G}$ for ionic fluids differs substantially from that of nonionic fluids
[Schr\""oer W., Weig\""{a}rtner H. 2004 {\it Pure Appl. Chem.} {\bf 76} 19]. A
theoretical investigation of this outcome is proposed here by a mean field
analysis of the interplay of short and long range interactions on the value of
$t_{G}$. We consider a quite general continuous charge-asymmetric model made of
charged hard spheres with additional short-range interactions (without
electrostatic interactions the model belongs to the same universality class as
the 3D Ising model). The effective Landau-Ginzburg Hamiltonian of the full
system near its gas-liquid critical point is derived from which the Ginzburg
temperature is calculated as a function of the ionicity. The results obtained
in this way for $t_{G}$ are in good qualitative and sufficient quantitative
agreement with available experimental data.
","accord extensive experimental finding ginzburg temperature $ t_{g}$ ionic fluid differ substantially nonionic fluid schr\""oer w. weig\""{a}rtner h. 2004 \it pure appl chem \bf 76 19 theoretical investigation outcome propose mean field analysis interplay short long range interaction value $ t_{g}$. consider general continuous charge asymmetric model charge hard sphere additional short range interaction electrostatic interaction model belong universality class 3d ising model effective landau ginzburg hamiltonian system near gas liquid critical point derive ginzburg temperature calculate function ionicity result obtain way $ t_{g}$ good qualitative sufficient quantitative agreement available experimental datum"
stat,Probability distributions generated by fractional diffusion equations,"  Fractional calculus allows one to generalize the linear, one-dimensional,
diffusion equation by replacing either the first time derivative or the second
space derivative by a derivative of fractional order. The fundamental solutions
of these equations provide probability density functions, evolving on time or
variable in space, which are related to the class of stable distributions. This
property is a noteworthy generalization of what happens for the standard
diffusion equation and can be relevant in treating financial and economical
problems where the stable probability distributions play a key role.
",fractional calculus allow generalize linear dimensional diffusion equation replace time derivative second space derivative derivative fractional order fundamental solution equation provide probability density function evolve time variable space relate class stable distribution property noteworthy generalization happen standard diffusion equation relevant treat financial economical problem stable probability distribution play key role
stat,When the Cramer-Rao Inequality provides no information,"  We investigate a one-parameter family of probability densities (related to
the Pareto distribution, which describes many natural phenomena) where the
Cramer-Rao inequality provides no information.
",investigate parameter family probability density relate pareto distribution describe natural phenomenon cramer rao inequality provide information
stat,"Ising-like dynamics and frozen states in systems of ultrafine magnetic
  particles","  We use Monte-Carlo simulations to study aging phenomena and the occurence of
spinglass phases in systems of single-domain ferromagnetic nanoparticles under
the combined influence of dipolar interaction and anisotropy energy, for
different combinations of positional and orientational disorder. We find that
the magnetic moments oriente themselves preferably parallel to their anisotropy
axes and changes of the total magnetization are solely achieved by 180 degree
flips of the magnetic moments, as in Ising systems. Since the dipolar
interaction favorizes the formation of antiparallel chain-like structures,
antiparallel chain-like patterns are frozen in at low temperatures, leading to
aging phenomena characteristic for spin-glasses. Contrary to the intuition,
these aging effects are more pronounced in ordered than in disordered
structures.
",use monte carlo simulation study age phenomena occurence spinglass phase system single domain ferromagnetic nanoparticle combined influence dipolar interaction anisotropy energy different combination positional orientational disorder find magnetic moment oriente preferably parallel anisotropy axis change total magnetization solely achieve 180 degree flip magnetic moment ising system dipolar interaction favorize formation antiparallel chain like structure antiparallel chain like pattern freeze low temperature lead age phenomena characteristic spin glass contrary intuition age effect pronounce order disorder structure
stat,"The dissolution of the vacancy gas and grain boundary diffusion in
  crystalline solids","  Based on the formula for the number density of vacancies in a solid under the
stress or tension, the model of grain boundary diffusion in crystalline solids
is developed. We obtain the activation energy of grain boundary diffusion
(dependent on the surface tension or the energy of the grain boundary) and also
the distributions of vacancies and the diffusing species in the vicinity of the
grain boundary.
",base formula number density vacancy solid stress tension model grain boundary diffusion crystalline solid develop obtain activation energy grain boundary diffusion dependent surface tension energy grain boundary distribution vacancy diffuse specie vicinity grain boundary
stat,"Monitoring spatially heterogeneous dynamics in a drying colloidal thin
  film","  We report on a new type of experiment that enables us to monitor spatially
and temporally heterogeneous dynamic properties in complex fluids. Our approach
is based on the analysis of near-field speckles produced by light diffusely
reflected from the superficial volume of a strongly scattering medium. By
periodic modulation of an incident speckle beam we obtain pixel-wise ensemble
averages of the structure function coefficient, a measure of the dynamic
activity. To illustrate the application of our approach we follow the different
stages in the drying process of a colloidal thin film. We show that we can
access ensemble averaged dynamic properties on length scales as small as ten
micrometers over the full field of view.
",report new type experiment enable monitor spatially temporally heterogeneous dynamic property complex fluid approach base analysis near field speckle produce light diffusely reflect superficial volume strongly scatter medium periodic modulation incident speckle beam obtain pixel wise ensemble average structure function coefficient measure dynamic activity illustrate application approach follow different stage dry process colloidal thin film access ensemble average dynamic property length scale small micrometer field view
stat,Growing Perfect Decagonal Quasicrystals by Local Rules,"  A local growth algorithm for a decagonal quasicrystal is presented. We show
that a perfect Penrose tiling (PPT) layer can be grown on a decapod tiling
layer by a three dimensional (3D) local rule growth. Once a PPT layer begins to
form on the upper layer, successive 2D PPT layers can be added on top resulting
in a perfect decagonal quasicrystalline structure in bulk with a point defect
only on the bottom surface layer. Our growth rule shows that an ideal
quasicrystal structure can be constructed by a local growth algorithm in 3D,
contrary to the necessity of non-local information for a 2D PPT growth.
",local growth algorithm decagonal quasicrystal present perfect penrose tile ppt layer grow decapod tiling layer dimensional 3d local rule growth ppt layer begin form upper layer successive 2d ppt layer add result perfect decagonal quasicrystalline structure bulk point defect surface layer growth rule show ideal quasicrystal structure construct local growth algorithm 3d contrary necessity non local information 2d ppt growth
stat,"Metropolis algorithm and equienergy sampling for two mean field spin
  systems","  In this paper we study the Metropolis algorithm in connection with two
mean--field spin systems, the so called mean--field Ising model and the
Blume--Emery--Griffiths model. In both this examples the naive choice of
proposal chain gives rise, for some parameters, to a slowly mixing Metropolis
chain, that is a chain whose spectral gap decreases exponentially fast (in the
dimension $N$ of the problem). Here we show how a slight variant in the
proposal chain can avoid this problem, keeping the mean computational cost
similar to the cost of the usual Metropolis. More precisely we prove that, with
a suitable variant in the proposal, the Metropolis chain has a spectral gap
which decreases polynomially in 1/N. Using some symmetry structure of the
energy, the method rests on allowing appropriate jumps within the energy level
of the starting state.
",paper study metropolis algorithm connection mean field spin system call mean field ising model blume emery griffiths model example naive choice proposal chain give rise parameter slowly mix metropolis chain chain spectral gap decrease exponentially fast dimension $ n$ problem slight variant proposal chain avoid problem keep mean computational cost similar cost usual metropolis precisely prove suitable variant proposal metropolis chain spectral gap decrease polynomially 1 n. symmetry structure energy method rest allow appropriate jump energy level starting state
stat,"Preferential interaction coefficient for nucleic acids and other
  cylindrical poly-ions","  The thermodynamics of nucleic acid processes is heavily affected by the
electric double-layer of micro-ions around the polyions. We focus here on the
Coulombic contribution to the salt-polyelectrolyte preferential interaction
(Donnan) coefficient and we report extremely accurate analytical expressions
valid in the range of low salt concentration (when polyion radius is smaller
than the Debye length). The analysis is performed at Poisson-Boltzmann level,
in cylindrical geometry, with emphasis on highly charged poly-ions (beyond
``counter-ion condensation''). The results hold for any electrolyte of the form
$z_-$:$z_+$. We also obtain a remarkably accurate expression for the electric
potential in the vicinity of the poly-ion.
",thermodynamic nucleic acid process heavily affect electric double layer micro ion polyion focus coulombic contribution salt polyelectrolyte preferential interaction donnan coefficient report extremely accurate analytical expression valid range low salt concentration polyion radius small debye length analysis perform poisson boltzmann level cylindrical geometry emphasis highly charge poly ion ` ` counter ion condensation result hold electrolyte form $ z_-$:$z_+$. obtain remarkably accurate expression electric potential vicinity poly ion
stat,Model C critical dynamics of random anisotropy magnets,"  We study the relaxational critical dynamics of the three-dimensional random
anisotropy magnets with the non-conserved n-component order parameter coupled
to a conserved scalar density. In the random anisotropy magnets the structural
disorder is present in a form of local quenched anisotropy axes of random
orientation. When the anisotropy axes are randomly distributed along the edges
of the n-dimensional hypercube, asymptotical dynamical critical properties
coincide with those of the random-site Ising model. However structural disorder
gives rise to considerable effects for non-asymptotic critical dynamics. We
investigate this phenomenon by a field-theoretical renormalization group
analysis in the two-loop order. We study critical slowing down and obtain
quantitative estimates for the effective and asymptotic critical exponents of
the order parameter and scalar density. The results predict complex scenarios
for the effective critical exponent approaching an asymptotic regime.
",study relaxational critical dynamic dimensional random anisotropy magnet non conserved n component order parameter couple conserved scalar density random anisotropy magnet structural disorder present form local quench anisotropy axis random orientation anisotropy axis randomly distribute edge n dimensional hypercube asymptotical dynamical critical property coincide random site ising model structural disorder give rise considerable effect non asymptotic critical dynamic investigate phenomenon field theoretical renormalization group analysis loop order study critical slow obtain quantitative estimate effective asymptotic critical exponent order parameter scalar density result predict complex scenario effective critical exponent approach asymptotic regime
stat,Recurrence analysis of the Portevin-Le Chatelier effect,"  Tensile tests were carried out by deforming polycrystalline samples of
Al-2.5%Mg alloy at room temperature in a wide range of strain rates where the
Portevin-Le Chatelier (PLC) effect was observed. The experimental stress-time
series data have been analyzed using the recurrence analysis technique based on
the Recurrence Plot (RP) and the Recurrence Quantification Analysis (RQA) to
study the change in the dynamical behavior of the PLC effect with the imposed
strain rate. Our study revealed that the RQA is able to detect the unique
crossover phenomenon in the PLC dynamics.
",tensile test carry deform polycrystalline sample al-2.5%mg alloy room temperature wide range strain rate portevin le chatelier plc effect observe experimental stress time series datum analyze recurrence analysis technique base recurrence plot rp recurrence quantification analysis rqa study change dynamical behavior plc effect impose strain rate study reveal rqa able detect unique crossover phenomenon plc dynamic
stat,Fluctuations in glassy systems,"  We summarize a theoretical framework based on global time-reparametrization
invariance that explains the origin of dynamic fluctuations in glassy systems.
We introduce the main ideas without getting into much technical details. We
describe a number of consequences arising from this scenario that can be tested
numerically and experimentally distinguishing those that can also be explained
by other mechanisms from the ones that we believe, are special to our proposal.
We support our claims by presenting some numerical checks performed on the 3d
Edwards-Anderson spin-glass. Finally, we discuss up to which extent these ideas
apply to super-cooled liquids that have been studied in much more detail up to
present.
",summarize theoretical framework base global time reparametrization invariance explain origin dynamic fluctuation glassy system introduce main idea get technical detail describe number consequence arise scenario test numerically experimentally distinguish explain mechanism one believe special proposal support claim present numerical check perform 3d edwards anderson spin glass finally discuss extent idea apply super cooled liquid study detail present
stat,"Solutions of fractional reaction-diffusion equations in terms of the
  H-function","  This paper deals with the investigation of the solution of an unified
fractional reaction-diffusion equation associated with the Caputo derivative as
the time-derivative and Riesz-Feller fractional derivative as the
space-derivative. The solution is derived by the application of the Laplace and
Fourier transforms in closed form in terms of the H-function. The results
derived are of general nature and include the results investigated earlier by
many authors, notably by Mainardi et al. (2001, 2005) for the fundamental
solution of the space-time fractional diffusion equation, and Saxena et al.
(2006a, b) for fractional reaction- diffusion equations. The advantage of using
Riesz-Feller derivative lies in the fact that the solution of the fractional
reaction-diffusion equation containing this derivative includes the fundamental
solution for space-time fractional diffusion, which itself is a generalization
of neutral fractional diffusion, space-fractional diffusion, and
time-fractional diffusion. These specialized types of diffusion can be
interpreted as spatial probability density functions evolving in time and are
expressible in terms of the H-functions in compact form.
",paper deal investigation solution unified fractional reaction diffusion equation associate caputo derivative time derivative riesz feller fractional derivative space derivative solution derive application laplace fourier transform closed form term h function result derive general nature include result investigate early author notably mainardi et al 2001 2005 fundamental solution space time fractional diffusion equation saxena et al 2006a b fractional reaction- diffusion equation advantage riesz feller derivative lie fact solution fractional reaction diffusion equation contain derivative include fundamental solution space time fractional diffusion generalization neutral fractional diffusion space fractional diffusion time fractional diffusion specialized type diffusion interpret spatial probability density function evolve time expressible term h function compact form
stat,"Using decomposed household food acquisitions as inputs of a Kinetic
  Dietary Exposure Model","  Foods naturally contain a number of contaminants that may have different and
long term toxic effects. This paper introduces a novel approach for the
assessment of such chronic food risk that integrates the pharmacokinetic
properties of a given contaminant. The estimation of such a Kinetic Dietary
Exposure Model (KDEM) should be based on long term consumption data which, for
the moment, can only be provided by Household Budget Surveys such as the
SECODIP panel in France. A semi parametric model is proposed to decompose a
series of household quantities into individual quantities which are then used
as inputs of the KDEM. As an illustration, the risk assessment related to the
presence of methyl mercury in seafood is revisited using this novel approach.
",food naturally contain number contaminant different long term toxic effect paper introduce novel approach assessment chronic food risk integrate pharmacokinetic property give contaminant estimation kinetic dietary exposure model kdem base long term consumption datum moment provide household budget surveys secodip panel france semi parametric model propose decompose series household quantity individual quantity input kdem illustration risk assessment relate presence methyl mercury seafood revisit novel approach
stat,Algebraic geometry of Gaussian Bayesian networks,"  Conditional independence models in the Gaussian case are algebraic varieties
in the cone of positive definite covariance matrices. We study these varieties
in the case of Bayesian networks, with a view towards generalizing the
recursive factorization theorem to situations with hidden variables. In the
case when the underlying graph is a tree, we show that the vanishing ideal of
the model is generated by the conditional independence statements implied by
graph. We also show that the ideal of any Bayesian network is homogeneous with
respect to a multigrading induced by a collection of upstream random variables.
This has a number of important consequences for hidden variable models.
Finally, we relate the ideals of Bayesian networks to a number of classical
constructions in algebraic geometry including toric degenerations of the
Grassmannian, matrix Schubert varieties, and secant varieties.
",conditional independence model gaussian case algebraic variety cone positive definite covariance matrix study variety case bayesian network view generalize recursive factorization theorem situation hidden variable case underlying graph tree vanish ideal model generate conditional independence statement imply graph ideal bayesian network homogeneous respect multigrading induce collection upstream random variable number important consequence hide variable model finally relate ideal bayesian network number classical construction algebraic geometry include toric degeneration grassmannian matrix schubert variety secant variety
stat,"Strong Spherical Asymptotics for Rotor-Router Aggregation and the
  Divisible Sandpile","  The rotor-router model is a deterministic analogue of random walk. It can be
used to define a deterministic growth model analogous to internal DLA. We prove
that the asymptotic shape of this model is a Euclidean ball, in a sense which
is stronger than our earlier work. For the shape consisting of $n=\omega_d r^d$
sites, where $\omega_d$ is the volume of the unit ball in $\R^d$, we show that
the inradius of the set of occupied sites is at least $r-O(\log r)$, while the
outradius is at most $r+O(r^\alpha)$ for any $\alpha > 1-1/d$. For a related
model, the divisible sandpile, we show that the domain of occupied sites is a
Euclidean ball with error in the radius a constant independent of the total
mass. For the classical abelian sandpile model in two dimensions, with $n=\pi
r^2$ particles, we show that the inradius is at least $r/\sqrt{3}$, and the
outradius is at most $(r+o(r))/\sqrt{2}$. This improves on bounds of Le Borgne
and Rossin. Similar bounds apply in higher dimensions.
",rotor router model deterministic analogue random walk define deterministic growth model analogous internal dla prove asymptotic shape model euclidean ball sense strong early work shape consist $ n=\omega_d r^d$ site $ \omega_d$ volume unit ball $ \r^d$ inradius set occupy site $ r o(\log r)$ outradius $ r+o(r^\alpha)$ $ \alpha > 1 1 d$. related model divisible sandpile domain occupy site euclidean ball error radius constant independent total mass classical abelian sandpile model dimension $ n=\pi r^2 $ particle inradius $ r/\sqrt{3}$ outradius $ r+o(r))/\sqrt{2}$. improve bound le borgne rossin similar bound apply high dimension
stat,A General Nonlinear Fokker-Planck Equation and its Associated Entropy,"  A recently introduced nonlinear Fokker-Planck equation, derived directly from
a master equation, comes out as a very general tool to describe
phenomenologically systems presenting complex behavior, like anomalous
diffusion, in the presence of external forces. Such an equation is
characterized by a nonlinear diffusion term that may present, in general, two
distinct powers of the probability distribution. Herein, we calculate the
stationary-state distributions of this equation in some special cases, and
introduce associated classes of generalized entropies in order to satisfy the
H-theorem. Within this approach, the parameters associated with the transition
rates of the original master-equation are related to such generalized
entropies, and are shown to obey some restrictions. Some particular cases are
discussed.
",recently introduce nonlinear fokker planck equation derive directly master equation come general tool describe phenomenologically system present complex behavior like anomalous diffusion presence external force equation characterize nonlinear diffusion term present general distinct power probability distribution calculate stationary state distribution equation special case introduce associated class generalized entropy order satisfy h theorem approach parameter associate transition rate original master equation relate generalized entropy show obey restriction particular case discuss
stat,Gibbs fragmentation trees,"  We study fragmentation trees of Gibbs type. In the binary case, we identify
the most general Gibbs-type fragmentation tree with Aldous' beta-splitting
model, which has an extended parameter range $\beta>-2$ with respect to the
${\rm beta}(\beta+1,\beta+1)$ probability distributions on which it is based.
In the multifurcating case, we show that Gibbs fragmentation trees are
associated with the two-parameter Poisson--Dirichlet models for exchangeable
random partitions of $\mathbb {N}$, with an extended parameter range
$0\le\alpha\le1$, $\theta\ge-2\alpha$ and $\alpha<0$, $\theta =-m\alpha$, $m\in
\mathbb {N}$.
","study fragmentation tree gibbs type binary case identify general gibbs type fragmentation tree aldous beta split model extended parameter range $ \beta>-2 $ respect $ \rm beta}(\beta+1,\beta+1)$ probability distribution base multifurcate case gibbs fragmentation tree associate parameter poisson dirichlet model exchangeable random partition $ \mathbb n}$ extended parameter range $ 0\le\alpha\le1 $ $ \theta\ge-2\alpha$ $ \alpha<0 $ $ \theta = -m\alpha$ $ m\in \mathbb n}$."
stat,Neel order in the two-dimensional S=1/2 Heisenberg Model,"  The existence of Neel order in the S=1/2 Heisenberg model on the square
lattice at T=0 is shown using inequalities set up by Kennedy, Lieb and Shastry
in combination with high precision Quantum Monte Carlo data.
",existence neel order s=1/2 heisenberg model square lattice t=0 show inequality set kennedy lieb shastry combination high precision quantum monte carlo data
stat,"Integral representations for convolutions of non-central multivariate
  gamma distributions","  Three types of integral representations for the cumulative distribution
functions of convolutions of non-central p-variate gamma distributions are
given by integration of elementary complex functions over the p-cube Cp =
(-pi,pi]x...x(-pi,pi]. In particular, the joint distribution of the diagonal
elements of a generalized quadratic form XAX' with n independent normally
distributed column vectors in X is obtained. For a single p-variate gamma
distribution function (p-1)-variate integrals over Cp-1 are derived. The
integrals are numerically more favourable than integrals obtained from the
Fourier or laplace inversion formula.
",type integral representation cumulative distribution function convolution non central p variate gamma distribution give integration elementary complex function p cube cp = -pi pi]x x(-pi pi particular joint distribution diagonal element generalized quadratic form xax n independent normally distribute column vector x obtain single p variate gamma distribution function p-1)-variate integral cp-1 derive integral numerically favourable integral obtain fourier laplace inversion formula
stat,Some aspects of the nonperturbative renormalization of the phi^4 model,"  A nonperturbative renormalization of the phi^4 model is considered. First we
integrate out only a single pair of conjugated modes with wave vectors +/- q.
Then we are looking for the RG equation which would describe the transformation
of the Hamiltonian under the integration over a shell Lambda - d Lambda < k <
Lambda, where d Lambda -> 0. We show that the known Wegner--Houghton equation
is consistent with the assumption of a simple superposition of the integration
results for +/- q. The renormalized action can be expanded in powers of the
phi^4 coupling constant u in the high temperature phase at u -> 0. We compare
the expansion coefficients with those exactly calculated by the diagrammatic
perturbative method, and find some inconsistency. It causes a question in which
sense the Wegner-Houghton equation is really exact.
",nonperturbative renormalization phi^4 model consider integrate single pair conjugate mode wave vector + q. look rg equation describe transformation hamiltonian integration shell lambda d lambda < k < lambda d lambda > 0 known wegner houghton equation consistent assumption simple superposition integration result + q. renormalized action expand power phi^4 coupling constant u high temperature phase u > 0 compare expansion coefficient exactly calculate diagrammatic perturbative method find inconsistency cause question sense wegner houghton equation exact
stat,Capturing knots in polymers,"  This paper visualizes a knot reduction algorithm
",paper visualize knot reduction algorithm
stat,"Optimal stimulus and noise distributions for information transmission
  via suprathreshold stochastic resonance","  Suprathreshold stochastic resonance (SSR) is a form of noise enhanced signal
transmission that occurs in a parallel array of independently noisy identical
threshold nonlinearities, including model neurons. Unlike most forms of
stochastic resonance, the output response to suprathreshold random input
signals of arbitrary magnitude is improved by the presence of even small
amounts of noise. In this paper the information transmission performance of SSR
in the limit of a large array size is considered. Using a relationship between
Shannon's mutual information and Fisher information, a sufficient condition for
optimality, i.e. channel capacity, is derived. It is shown that capacity is
achieved when the signal distribution is Jeffrey's prior, as formed from the
noise distribution, or when the noise distribution depends on the signal
distribution via a cosine relationship. These results provide theoretical
verification and justification for previous work in both computational
neuroscience and electronics.
",suprathreshold stochastic resonance ssr form noise enhance signal transmission occur parallel array independently noisy identical threshold nonlinearitie include model neuron unlike form stochastic resonance output response suprathreshold random input signal arbitrary magnitude improve presence small amount noise paper information transmission performance ssr limit large array size consider relationship shannon mutual information fisher information sufficient condition optimality i.e. channel capacity derive show capacity achieve signal distribution jeffrey prior form noise distribution noise distribution depend signal distribution cosine relationship result provide theoretical verification justification previous work computational neuroscience electronic
stat,A new approach to mutual information,"  A new expression as a certain asymptotic limit via ""discrete micro-states"" of
permutations is provided to the mutual information of both continuous and
discrete random variables.
",new expression certain asymptotic limit discrete micro state permutation provide mutual information continuous discrete random variable
stat,Approaching equilibrium and the distribution of clusters,"  We investigate the approach to stable and metastable equilibrium in Ising
models using a cluster representation. The distribution of nucleation times is
determined using the Metropolis algorithm and the corresponding $\phi^{4}$
model using Langevin dynamics. We find that the nucleation rate is suppressed
at early times even after global variables such as the magnetization and energy
have apparently reached their time independent values. The mean number of
clusters whose size is comparable to the size of the nucleating droplet becomes
time independent at about the same time that the nucleation rate reaches its
constant value. We also find subtle structural differences between the
nucleating droplets formed before and after apparent metastable equilibrium has
been established.
",investigate approach stable metastable equilibrium ising model cluster representation distribution nucleation time determined metropolis algorithm corresponding $ \phi^{4}$ model langevin dynamic find nucleation rate suppress early time global variable magnetization energy apparently reach time independent value mean number cluster size comparable size nucleating droplet time independent time nucleation rate reach constant value find subtle structural difference nucleating droplet form apparent metastable equilibrium establish
stat,"Density matrix elements and entanglement entropy for the spin-1/2 XXZ
  chain at $\Delta$=1/2","  We have analytically obtained all the density matrix elements up to six
lattice sites for the spin-1/2 Heisenberg XXZ chain at $\Delta=1/2$. We use the
multiple integral formula of the correlation function for the massless XXZ
chain derived by Jimbo and Miwa. As for the spin-spin correlation functions, we
have newly obtained the fourth- and fifth-neighbour transverse correlation
functions. We have calculated all the eigenvalues of the density matrix and
analyze the eigenvalue-distribution. Using these results the exact values of
the entanglement entropy for the reduced density matrix up six lattice sites
have been obtained. We observe that our exact results agree quite well with the
asymptotic formula predicted by the conformal field theory.
",analytically obtain density matrix element lattice site spin-1/2 heisenberg xxz chain $ \delta=1/2$. use multiple integral formula correlation function massless xxz chain derive jimbo miwa spin spin correlation function newly obtain fourth- fifth neighbour transverse correlation function calculate eigenvalue density matrix analyze eigenvalue distribution result exact value entanglement entropy reduce density matrix lattice site obtain observe exact result agree asymptotic formula predict conformal field theory
stat,"Competitive nucleation and the Ostwald rule in a generalized Potts model
  with multiple metastable phases","  We introduce a simple nearest-neighbor spin model with multiple metastable
phases, the number and decay pathways of which are explicitly controlled by the
parameters of the system. With this model we can construct, for example, a
system which evolves through an arbitrarily long succession of metastable
phases. We also construct systems in which different phases may nucleate
competitively from a single initial phase. For such a system, we present a
general method to extract from numerical simulations the individual nucleation
rates of the nucleating phases. The results show that the Ostwald rule, which
predicts which phase will nucleate, must be modified probabilistically when the
new phases are almost equally stable. Finally, we show that the nucleation rate
of a phase depends, among other things, on the number of other phases
accessible from it.
",introduce simple near neighbor spin model multiple metastable phase number decay pathway explicitly control parameter system model construct example system evolve arbitrarily long succession metastable phase construct system different phase nucleate competitively single initial phase system present general method extract numerical simulation individual nucleation rate nucleating phase result ostwald rule predict phase nucleate modify probabilistically new phase equally stable finally nucleation rate phase depend thing number phase accessible
stat,A Topological Glass,"  We propose and study a model with glassy behavior. The state space of the
model is given by all triangulations of a sphere with $n$ nodes, half of which
are red and half are blue. Red nodes want to have 5 neighbors while blue ones
want 7. Energies of nodes with different numbers of neighbors are supposed to
be positive. The dynamics is that of flipping the diagonal of two adjacent
triangles, with a temperature dependent probability. We show that this system
has an approach to a steady state which is exponentially slow, and show that
the stationary state is unordered. We also study the local energy landscape and
show that it has the hierarchical structure known from spin glasses. Finally,
we show that the evolution can be described as that of a rarefied gas with
spontaneous generation of particles and annihilating collisions.
",propose study model glassy behavior state space model give triangulation sphere $ n$ node half red half blue red node want 5 neighbor blue one want 7 energy node different number neighbor suppose positive dynamic flip diagonal adjacent triangle temperature dependent probability system approach steady state exponentially slow stationary state unordered study local energy landscape hierarchical structure know spin glass finally evolution describe rarefied gas spontaneous generation particle annihilate collision
stat,"Phase structure of a surface model on dynamically triangulated spheres
  with elastic skeletons","  We find three distinct phases; a tubular phase, a planar phase, and the
spherical phase, in a triangulated fluid surface model. It is also found that
these phases are separated by discontinuous transitions. The fluid surface
model is investigated within the framework of the conventional curvature model
by using the canonical Monte Carlo simulations with dynamical triangulations.
The mechanical strength of the surface is given only by skeletons, and no
two-dimensional bending energy is assumed in the Hamiltonian. The skeletons are
composed of elastic linear-chains and rigid junctions and form a
compartmentalized structure on the surface, and for this reason the vertices of
triangles can diffuse freely only inside the compartments. As a consequence, an
inhomogeneous structure is introduced in the model; the surface strength inside
the compartments is different from the surface strength on the compartments.
However, the rotational symmetry is not influenced by the elastic skeletons;
there is no specific direction on the surface. In addition to the three phases
mentioned above, a collapsed phase is expected to exist in the low bending
rigidity regime that was not studied here. The inhomogeneous structure and the
fluidity of vertices are considered to be the origin of such variety of phases.
",find distinct phase tubular phase planar phase spherical phase triangulate fluid surface model find phase separate discontinuous transition fluid surface model investigate framework conventional curvature model canonical monte carlo simulation dynamical triangulation mechanical strength surface give skeleton dimensional bending energy assume hamiltonian skeleton compose elastic linear chain rigid junction form compartmentalized structure surface reason vertex triangle diffuse freely inside compartment consequence inhomogeneous structure introduce model surface strength inside compartment different surface strength compartment rotational symmetry influence elastic skeleton specific direction surface addition phase mention collapse phase expect exist low bend rigidity regime study inhomogeneous structure fluidity vertex consider origin variety phase
stat,"Confinement into a state with persistent current by thermal quenching of
  loop of Josephson junctions","  We study a loop of Josephson junctions that is quenched through its critical
temperature. For three or more junctions, symmetry breaking states can be
achieved without thermal activation, in spite of the fact that the relaxation
time is practically constant when the critical temperature is approached from
above. The probability for these states decreases with quenching time, but the
dependence is not allometric. For large number of junctions, cooling does not
have to be fast. For this case, we evaluate the standard deviation of the
induced flux. Our results are consistent with the available experimental data.
",study loop josephson junction quench critical temperature junction symmetry break state achieve thermal activation spite fact relaxation time practically constant critical temperature approach probability state decrease quenching time dependence allometric large number junction cool fast case evaluate standard deviation induced flux result consistent available experimental datum
stat,Domain Wall Dynamics near a Quantum Critical Point,"  We study the real-time domain-wall dynamics near a quantum critical point of
the one-dimensional anisotropic ferromagnetic spin 1/2 chain. By numerical
simulation, we find the domain wall is dynamically stable in the
Heisenberg-Ising model. Near the quantum critical point, the width of the
domain wall diverges as $(\Delta -1) ^{-1/2}$.
",study real time domain wall dynamic near quantum critical point dimensional anisotropic ferromagnetic spin 1/2 chain numerical simulation find domain wall dynamically stable heisenberg ising model near quantum critical point width domain wall diverge $ \delta -1 ^{-1/2}$.
stat,"Correlation functions in the Non Perturbative Renormalization Group and
  field expansion","  The usual procedure of including a finite number of vertices in Non
Perturbative Renormalization Group equations in order to obtain $n$-point
correlation functions at finite momenta is analyzed. This is done by exploiting
a general method recently introduced which includes simultaneously all vertices
although approximating their momentum dependence. The study is performed using
the self-energy of the tridimensional scalar model at criticality. At least in
this example, low order truncations miss quantities as the critical exponent
$\eta$ by as much as 60%. However, if one goes to high order truncations the
procedure seems to converge rapidly.
",usual procedure include finite number vertex non perturbative renormalization group equation order obtain $ n$-point correlation function finite momenta analyze exploit general method recently introduce include simultaneously vertex approximate momentum dependence study perform self energy tridimensional scalar model criticality example low order truncation miss quantity critical exponent $ \eta$ 60 go high order truncation procedure converge rapidly
stat,"Vortex-induced topological transition of the bilinear-biquadratic
  Heisenberg antiferromagnet on the triangular lattice","  The ordering of the classical Heisenberg antiferromagnet on the triangular
lattice with the the bilinear-biquadratic interaction is studied by Monte Carlo
simulations. It is shown that the model exhibits a topological phase transition
at a finite-temperature driven by topologically stable vortices, while the spin
correlation length remains finite even at and below the transition point. The
relevant vortices could be of three different types, depending on the value of
the biquadratic coupling. Implications to recent experiments on the triangular
antiferromagnet NiGa$_2$S$_4$ is discussed.
",ordering classical heisenberg antiferromagnet triangular lattice bilinear biquadratic interaction study monte carlo simulation show model exhibit topological phase transition finite temperature drive topologically stable vortex spin correlation length remain finite transition point relevant vortex different type depend value biquadratic coupling implication recent experiment triangular antiferromagnet niga$_2$s$_4 $ discuss
stat,Nonequilibrium entropy limiters in lattice Boltzmann methods,"  We construct a system of nonequilibrium entropy limiters for the lattice
Boltzmann methods (LBM). These limiters erase spurious oscillations without
blurring of shocks, and do not affect smooth solutions. In general, they do the
same work for LBM as flux limiters do for finite differences, finite volumes
and finite elements methods, but for LBM the main idea behind the construction
of nonequilibrium entropy limiter schemes is to transform a field of a scalar
quantity - nonequilibrium entropy. There are two families of limiters: (i)
based on restriction of nonequilibrium entropy (entropy ""trimming"") and (ii)
based on filtering of nonequilibrium entropy (entropy filtering). The physical
properties of LBM provide some additional benefits: the control of entropy
production and accurate estimate of introduced artificial dissipation are
possible. The constructed limiters are tested on classical numerical examples:
1D athermal shock tubes with an initial density ratio 1:2 and the 2D lid-driven
cavity for Reynolds numbers Re between 2000 and 7500 on a coarse 100*100 grid.
All limiter constructions are applicable for both entropic and non-entropic
quasiequilibria.
",construct system nonequilibrium entropy limiter lattice boltzmann method lbm limiter erase spurious oscillation blurring shock affect smooth solution general work lbm flux limiter finite difference finite volume finite element method lbm main idea construction nonequilibrium entropy limiter scheme transform field scalar quantity nonequilibrium entropy family limiter base restriction nonequilibrium entropy entropy trim ii base filtering nonequilibrium entropy entropy filtering physical property lbm provide additional benefit control entropy production accurate estimate introduce artificial dissipation possible construct limiter test classical numerical example 1d athermal shock tube initial density ratio 1:2 2d lid drive cavity reynolds number 2000 7500 coarse 100 100 grid limiter construction applicable entropic non entropic quasiequilibria
stat,The 3D +-J Ising model at the ferromagnetic transition line,"  We study the critical behavior of the three-dimensional $\pm J$ Ising model
[with a random-exchange probability $P(J_{xy}) = p \delta(J_{xy} - J) + (1-p)
\delta(J_{xy} + J)$] at the transition line between the paramagnetic and
ferromagnetic phase, which extends from $p=1$ to a multicritical (Nishimori)
point at $p=p_N\approx 0.767$. By a finite-size scaling analysis of Monte Carlo
simulations at various values of $p$ in the region $p_N<p<1$, we provide strong
numerical evidence that the critical behavior along the ferromagnetic
transition line belongs to the same universality class as the three-dimensional
randomly-dilute Ising model. We obtain the results $\nu=0.682(3)$ and
$\eta=0.036(2)$ for the critical exponents, which are consistent with the
estimates $\nu=0.683(2)$ and $\eta=0.036(1)$ at the transition of
randomly-dilute Ising models.
",study critical behavior dimensional $ \pm j$ ising model random exchange probability $ p(j_{xy = p \delta(j_{xy j + 1 p \delta(j_{xy + j)$ transition line paramagnetic ferromagnetic phase extend $ p=1 $ multicritical nishimori point $ p = p_n\approx 0.767$. finite size scale analysis monte carlo simulation value $ p$ region $ p_n < p<1 $ provide strong numerical evidence critical behavior ferromagnetic transition line belong universality class dimensional randomly dilute ising model obtain result $ \nu=0.682(3)$ $ \eta=0.036(2)$ critical exponent consistent estimate $ \nu=0.683(2)$ $ \eta=0.036(1)$ transition randomly dilute ising model
stat,"Spectroscopic Properties of Polarons in Strongly Correlated Systems by
  Exact Diagrammatic Monte Carlo Method","  We present recent advances in understanding of the ground and excited states
of the electron-phonon coupled systems obtained by novel methods of
Diagrammatic Monte Carlo and Stochastic Optimization, which enable the
approximation-free calculation of Matsubara Green function in imaginary times
and perform unbiased analytic continuation to real frequencies. We present
exact numeric results on the ground state properties, Lehmann spectral function
and optical conductivity of different strongly correlated systems: Frohlich
polaron, Rashba-Pekar exciton-polaron, pseudo Jahn-Teller polaron, exciton, and
interacting with phonons hole in the t-J model.
",present recent advance understanding ground excited state electron phonon couple system obtain novel method diagrammatic monte carlo stochastic optimization enable approximation free calculation matsubara green function imaginary time perform unbiased analytic continuation real frequency present exact numeric result ground state property lehmann spectral function optical conductivity different strongly correlate system frohlich polaron rashba pekar exciton polaron pseudo jahn teller polaron exciton interact phonon hole t j model
stat,The S-Matrix of AdS/CFT and Yangian Symmetry,"  We review the algebraic construction of the S-matrix of AdS/CFT. We also
present its symmetry algebra which turns out to be a Yangian of the centrally
extended su(2|2) superalgebra.
",review algebraic construction s matrix ads cft present symmetry algebra turn yangian centrally extended su(2|2 superalgebra
stat,"Alternative Approaches to the Equilibrium Properties of Hard-Sphere
  Liquids","  An overview of some analytical approaches to the computation of the
structural and thermodynamic properties of single component and multicomponent
hard-sphere fluids is provided. For the structural properties, they yield a
thermodynamically consistent formulation, thus improving and extending the
known analytical results of the Percus-Yevick theory. Approximate expressions
for the contact values of the radial distribution functions and the
corresponding analytical equations of state are also discussed. Extensions of
this methodology to related systems, such as sticky hard spheres and
square-well fluids, as well as its use in connection with the perturbation
theory of fluids are briefly addressed.
",overview analytical approach computation structural thermodynamic property single component multicomponent hard sphere fluid provide structural property yield thermodynamically consistent formulation improve extend know analytical result percus yevick theory approximate expression contact value radial distribution function correspond analytical equation state discuss extension methodology relate system sticky hard sphere square fluid use connection perturbation theory fluid briefly address
stat,"Fluctuation-dissipation relation on a Melde string in a turbulent flow,
  considerations on a ""dynamical temperature""","  We report on measurements of the transverse fluctuations of a string in a
turbulent air jet flow. Harmonic modes are excited by the fluctuating drag
force, at different wave-numbers. This simple mechanical probe makes it
possible to measure excitations of the flow at specific scales, averaged over
space and time: it is a scale-resolved, global measurement. We also measure the
dissipation associated to the string motion, and we consider the ratio of the
fluctuations over dissipation (FDR). In an exploratory approach, we investigate
the concept of {\it effective temperature} defined through the FDR. We compare
our observations with other definitions of temperature in turbulence. From the
theory of Kolmogorov (1941), we derive the exponent -11/3 expected for the
spectrum of the fluctuations. This simple model and our experimental results
are in good agreement, over the range of wave-numbers, and Reynolds number
accessible ($74000 \leq Re \leq 170000$).
",report measurement transverse fluctuation string turbulent air jet flow harmonic mode excite fluctuate drag force different wave number simple mechanical probe make possible measure excitation flow specific scale average space time scale resolve global measurement measure dissipation associate string motion consider ratio fluctuation dissipation fdr exploratory approach investigate concept \it effective temperature define fdr compare observation definition temperature turbulence theory kolmogorov 1941 derive exponent -11/3 expect spectrum fluctuation simple model experimental result good agreement range wave number reynolds number accessible $ 74000 \leq \leq 170000 $
stat,Spline Single-Index Prediction Model,"  For the past two decades, single-index model, a special case of projection
pursuit regression, has proven to be an efficient way of coping with the high
dimensional problem in nonparametric regression. In this paper, based on weakly
dependent sample, we investigate the single-index prediction (SIP) model which
is robust against deviation from the single-index model. The single-index is
identified by the best approximation to the multivariate prediction function of
the response variable, regardless of whether the prediction function is a
genuine single-index function. A polynomial spline estimator is proposed for
the single-index prediction coefficients, and is shown to be root-n consistent
and asymptotically normal. An iterative optimization routine is used which is
sufficiently fast for the user to analyze large data of high dimension within
seconds. Simulation experiments have provided strong evidence that corroborates
with the asymptotic theory. Application of the proposed procedure to the rive
flow data of Iceland has yielded superior out-of-sample rolling forecasts.
",past decade single index model special case projection pursuit regression prove efficient way cope high dimensional problem nonparametric regression paper base weakly dependent sample investigate single index prediction sip model robust deviation single index model single index identify good approximation multivariate prediction function response variable regardless prediction function genuine single index function polynomial spline estimator propose single index prediction coefficient show root n consistent asymptotically normal iterative optimization routine sufficiently fast user analyze large datum high dimension second simulation experiment provide strong evidence corroborate asymptotic theory application propose procedure rive flow datum iceland yield superior sample rolling forecast
stat,"An individual based model with global competition interaction:
  fluctuations effects in pattern formation","  We present some numerical results obtained from a simple individual based
model that describes clustering of organisms caused by competition. Our aim is
to show how, even when a deterministic description developed for continuum
models predicts no pattern formation, an individual based model displays well
defined patterns, as a consequence of fluctuations effects caused by the
discrete nature of the interacting agents.
",present numerical result obtain simple individual base model describe cluster organism cause competition aim deterministic description develop continuum model predict pattern formation individual base model display define pattern consequence fluctuation effect cause discrete nature interact agent
stat,On generalized entropy measures and pathways,"  Product probability property, known in the literature as statistical
independence, is examined first. Then generalized entropies are introduced, all
of which give generalizations to Shannon entropy. It is shown that the nature
of the recursivity postulate automatically determines the logarithmic
functional form for Shannon entropy. Due to the logarithmic nature, Shannon
entropy naturally gives rise to additivity, when applied to situations having
product probability property. It is argued that the natural process is
non-additivity, important, for example, in statistical mechanics, even in
product probability property situations and additivity can hold due to the
involvement of a recursivity postulate leading to a logarithmic function.
Generalizations, including Mathai's generalized entropy are introduced and some
of the properties are examined. Situations are examined where Mathai's entropy
leads to pathway models, exponential and power law behavior and related
differential equations. Connection of Mathai's entropy to Kerridge's measure of
""inaccuracy"" is also explored.
",product probability property know literature statistical independence examine generalize entropy introduce generalization shannon entropy show nature recursivity postulate automatically determine logarithmic functional form shannon entropy logarithmic nature shannon entropy naturally give rise additivity apply situation have product probability property argue natural process non additivity important example statistical mechanic product probability property situation additivity hold involvement recursivity postulate lead logarithmic function generalization include mathai generalized entropy introduce property examine situation examine mathai entropy lead pathway model exponential power law behavior related differential equation connection mathai entropy kerridge measure inaccuracy explore
stat,Connected Operators for the Totally Asymmetric Exclusion Process,"  We fully elucidate the structure of the hierarchy of the connected operators
that commute with the Markov matrix of the Totally Asymmetric Exclusion Process
(TASEP). We prove for the connected operators a combinatorial formula that was
conjectured in a previous work. Our derivation is purely algebraic and relies
on the algebra generated by the local jump operators involved in the TASEP.
  Keywords: Non-Equilibrium Statistical Mechanics, ASEP, Exact Results,
Algebraic Bethe Ansatz.
",fully elucidate structure hierarchy connected operator commute markov matrix totally asymmetric exclusion process tasep prove connected operator combinatorial formula conjecture previous work derivation purely algebraic rely algebra generate local jump operator involve tasep keyword non equilibrium statistical mechanics asep exact result algebraic bethe ansatz
stat,Failure of the work-Hamiltonian connection for free energy calculations,"  Extensions of statistical mechanics are routinely being used to infer free
energies from the work performed over single-molecule nonequilibrium
trajectories. A key element of this approach is the ubiquitous expression
dW/dt=\partial H(x,t)/ \partial t which connects the microscopic work W
performed by a time-dependent force on the coordinate x with the corresponding
Hamiltonian H(x,t) at time t. Here we show that this connection, as pivotal as
it is, cannot be used to estimate free energy changes. We discuss the
implications of this result for single-molecule experiments and atomistic
molecular simulations and point out possible avenues to overcome these
limitations.
",extensions statistical mechanic routinely infer free energy work perform single molecule nonequilibrium trajectory key element approach ubiquitous expression dw dt=\partial h(x t)/ \partial t connect microscopic work w perform time dependent force coordinate x correspond hamiltonian h(x t time t. connection pivotal estimate free energy change discuss implication result single molecule experiment atomistic molecular simulation point possible avenue overcome limitation
stat,Multi-Higgs U(1) Lattice Gauge Theory in Three Dimensions,"  We study the three-dimensional compact U(1) lattice gauge theory with $N$
Higgs fields numerically. This model is relevant to multi-component
superconductors, antiferromagnetic spin systems in easy plane, inflational
cosmology, etc. For N=2, the system has a second-order phase transition line
$\tilde{c}_1(c_2)$ in the $c_2$(gauge coupling)$-c_1$(Higgs coupling) plane,
which separates the confinement phase and the Higgs phase. For N=3, the
critical line is separated into two parts; one for $c_2 \alt 2.25$ with
first-order transitions, and the other for $c_2 \agt 2.25$ with second-order
transitions.
",study dimensional compact u(1 lattice gauge theory $ n$ higgs field numerically model relevant multi component superconductor antiferromagnetic spin system easy plane inflational cosmology etc n=2 system second order phase transition line $ \tilde{c}_1(c_2)$ $ c_2$(gauge coupling)$-c_1$(higgs coupling plane separate confinement phase higgs phase n=3 critical line separate part $ c_2 \alt 2.25 $ order transition $ c_2 \agt 2.25 $ second order transition
stat,"Rounding of first-order phase transitions and optimal cooperation in
  scale-free networks","  We consider the ferromagnetic large-$q$ state Potts model in complex evolving
networks, which is equivalent to an optimal cooperation problem, in which the
agents try to optimize the total sum of pair cooperation benefits and the
supports of independent projects. The agents are found to be typically of two
kinds: a fraction of $m$ (being the magnetization of the Potts model) belongs
to a large cooperating cluster, whereas the others are isolated one man's
projects. It is shown rigorously that the homogeneous model has a strongly
first-order phase transition, which turns to second-order for random
interactions (benefits), the properties of which are studied numerically on the
Barab\'asi-Albert network. The distribution of finite-size transition points is
characterized by a shift exponent, $1/\tilde{\nu}'=.26(1)$, and by a different
width exponent, $1/\nu'=.18(1)$, whereas the magnetization at the transition
point scales with the size of the network, $N$, as: $m\sim N^{-x}$, with
$x=.66(1)$.
",consider ferromagnetic large-$q$ state potts model complex evolving network equivalent optimal cooperation problem agent try optimize total sum pair cooperation benefit support independent project agent find typically kind fraction $ m$ magnetization potts model belong large cooperating cluster isolate man project show rigorously homogeneous model strongly order phase transition turn second order random interaction benefit property study numerically barab\'asi albert network distribution finite size transition point characterize shift exponent $ 1/\tilde{\nu}'=.26(1)$ different width exponent $ 1/\nu'=.18(1)$ magnetization transition point scale size network $ n$ $ m\sim n^{-x}$ $ x=.66(1)$.
stat,Critical Scaling of Shear Viscosity at the Jamming Transition,"  We carry out numerical simulations to study transport behavior about the
jamming transition of a model granular material in two dimensions at zero
temperature. Shear viscosity \eta is computed as a function of particle volume
density \rho and applied shear stress \sigma, for diffusively moving particles
with a soft core interaction. We find an excellent scaling collapse of our data
as a function of the scaling variable \sigma/|\rho_c-\rho|^\Delta, where \rho_c
is the critical density at \sigma=0 (""point J""), and \Delta is the crossover
scaling critical exponent. Our results show that jamming is a true critical
phenomenon, extending to driven steady states along the non-equilibrium \sigma
axis of the \rho-\sigma phase diagram.
",carry numerical simulation study transport behavior jamming transition model granular material dimension zero temperature shear viscosity \eta compute function particle volume density \rho apply shear stress \sigma diffusively move particle soft core interaction find excellent scaling collapse datum function scale variable \sigma/|\rho_c-\rho|^\delta \rho_c critical density \sigma=0 point j \delta crossover scale critical exponent result jamming true critical phenomenon extend drive steady state non equilibrium \sigma axis \rho-\sigma phase diagram
stat,"Exact distribution of the sample variance from a gamma parent
  distribution","  Several representations of the exact cdf of the sum of squares of n
independent gamma-distributed random variables Xi are given, in particular by a
series of gamma distribution functions. Using a characterization of the gamma
distribution by Laha, an expansion of the exact distribution of the sample
variance is derived by a Taylor series approach with the former distribution as
its leading term. In particular for integer orders alpha some further series
are provided, including a convex combination of gamma distributions for alpha =
1 and nearly of this type for alpha > 1. Furthermore, some representations of
the distribution of the angle Phi between (X1,...,Xn) and (1,...,1) are given
by orthogonal series. All these series are based on the same sequence of easily
computed moments of cos(Phi).
","representation exact cdf sum square n independent gamma distribute random variable xi give particular series gamma distribution function characterization gamma distribution laha expansion exact distribution sample variance derive taylor series approach distribution lead term particular integer order alpha series provide include convex combination gamma distribution alpha = 1 nearly type alpha > 1 furthermore representation distribution angle phi x1, ,xn 1, ,1 give orthogonal series series base sequence easily computed moment cos(phi"
stat,Stochastic fluctuations in metabolic pathways,"  Fluctuations in the abundance of molecules in the living cell may affect its
growth and well being. For regulatory molecules (e.g., signaling proteins or
transcription factors), fluctuations in their expression can affect the levels
of downstream targets in a network. Here, we develop an analytic framework to
investigate the phenomenon of noise correlation in molecular networks.
Specifically, we focus on the metabolic network, which is highly inter-linked,
and noise properties may constrain its structure and function. Motivated by the
analogy between the dynamics of a linear metabolic pathway and that of the
exactly soluable linear queueing network or, alternatively, a mass transfer
system, we derive a plethora of results concerning fluctuations in the
abundance of intermediate metabolites in various common motifs of the metabolic
network. For all but one case examined, we find the steady-state fluctuation in
different nodes of the pathways to be effectively uncorrelated. Consequently,
fluctuations in enzyme levels only affect local properties and do not propagate
elsewhere into metabolic networks, and intermediate metabolites can be freely
shared by different reactions. Our approach may be applicable to study
metabolic networks with more complex topologies, or protein signaling networks
which are governed by similar biochemical reactions. Possible implications for
bioinformatic analysis of metabolimic data are discussed.
",fluctuations abundance molecule live cell affect growth regulatory molecule e.g. signal protein transcription factor fluctuation expression affect level downstream target network develop analytic framework investigate phenomenon noise correlation molecular network specifically focus metabolic network highly inter link noise property constrain structure function motivate analogy dynamic linear metabolic pathway exactly soluable linear queue network alternatively mass transfer system derive plethora result concern fluctuation abundance intermediate metabolite common motif metabolic network case examine find steady state fluctuation different node pathway effectively uncorrelated consequently fluctuation enzyme level affect local property propagate metabolic network intermediate metabolite freely share different reaction approach applicable study metabolic network complex topology protein signal network govern similar biochemical reaction possible implication bioinformatic analysis metabolimic datum discuss
stat,"Recent Results on Thermal Casimir Force between Dielectrics and Related
  Problems","  We review recent results obtained in the physics of the thermal Casimir force
acting between two dielectrics, dielectric and metal, and between metal and
semiconductor. The detailed derivation for the low-temperature behavior of the
Casimir free energy, pressure and entropy in the configuration of two real
dielectric plates is presented. For dielectrics with finite static dielectric
permittivity it is shown that the Nernst heat theorem is satisfied. Hence, the
Lifshitz theory of the van der Waals and Casimir forces is demonstrated to be
consistent with thermodynamics. The nonzero dc conductivity of dielectric
plates is proved to lead to a violation of the Nernst heat theorem and, thus,
is not related to the phenomenon of dispersion forces. The low-temperature
asymptotics of the Casimir free energy, pressure and entropy are derived also
in the configuration of one metal and one dielectric plate. The results are
shown to be consistent with thermodynamics if the dielectric plate possesses a
finite static dielectric permittivity. If the dc conductivity of a dielectric
plate is taken into account this results in the violation of the Nernst heat
theorem. We discuss both the experimental and theoretical results related to
the Casimir interaction between metal and semiconductor with different charge
carrier density. Discussions in the literature on the possible influence of
spatial dispersion on the thermal Casimir force are analyzed. In conclusion,
the conventional Lifshitz theory taking into account only the frequency
dispersion remains the reliable foundation for the interpretation of all
present experiments.
",review recent result obtain physics thermal casimir force act dielectric dielectric metal metal semiconductor detailed derivation low temperature behavior casimir free energy pressure entropy configuration real dielectric plate present dielectric finite static dielectric permittivity show nernst heat theorem satisfied lifshitz theory van der waals casimir force demonstrate consistent thermodynamic nonzero dc conductivity dielectric plate prove lead violation nernst heat theorem relate phenomenon dispersion force low temperature asymptotic casimir free energy pressure entropy derive configuration metal dielectric plate result show consistent thermodynamic dielectric plate possess finite static dielectric permittivity dc conductivity dielectric plate take account result violation nernst heat theorem discuss experimental theoretical result relate casimir interaction metal semiconductor different charge carrier density discussion literature possible influence spatial dispersion thermal casimir force analyze conclusion conventional lifshitz theory take account frequency dispersion remain reliable foundation interpretation present experiment
stat,Residual entropy in a model for the unfolding of single polymer chains,"  We study the unfolding of a single polymer chain due to an external force. We
use a simplified model which allows to perform all calculations in closed form
without assuming a Boltzmann-Gibbs form for the equilibrium distribution.
Temperature is then defined by calculating the Legendre transform of the
entropy under certain constraints. The application of the model is limited to
flexible polymers. It exhibits a gradual transition from compact globule to
rod. The boundary line between these two phases shows reentrant behavior. This
behavior is explained by the presence of residual entropy.
",study unfolding single polymer chain external force use simplified model allow perform calculation closed form assume boltzmann gibbs form equilibrium distribution temperature define calculate legendre transform entropy certain constraint application model limit flexible polymer exhibit gradual transition compact globule rod boundary line phase show reentrant behavior behavior explain presence residual entropy
stat,Information-Based Asset Pricing,"  A new framework for asset price dynamics is introduced in which the concept
of noisy information about future cash flows is used to derive the price
processes. In this framework an asset is defined by its cash-flow structure.
Each cash flow is modelled by a random variable that can be expressed as a
function of a collection of independent random variables called market factors.
With each such ""X-factor"" we associate a market information process, the values
of which are accessible to market agents. Each information process is a sum of
two terms; one contains true information about the value of the market factor;
the other represents ""noise"". The noise term is modelled by an independent
Brownian bridge. The market filtration is assumed to be that generated by the
aggregate of the independent information processes. The price of an asset is
given by the expectation of the discounted cash flows in the risk-neutral
measure, conditional on the information provided by the market filtration. When
the cash flows are the dividend payments associated with equities, an explicit
model is obtained for the share-price, and the prices of options on
dividend-paying assets are derived. Remarkably, the resulting formula for the
price of a European call option is of the Black-Scholes-Merton type. The
information-based framework also generates a natural explanation for the origin
of stochastic volatility.
",new framework asset price dynamic introduce concept noisy information future cash flow derive price process framework asset define cash flow structure cash flow model random variable express function collection independent random variable call market factor x factor associate market information process value accessible market agent information process sum term contain true information value market factor represent noise noise term model independent brownian bridge market filtration assume generate aggregate independent information process price asset give expectation discount cash flow risk neutral measure conditional information provide market filtration cash flow dividend payment associate equity explicit model obtain share price price option dividend pay asset derive remarkably result formula price european option black scholes merton type information base framework generate natural explanation origin stochastic volatility
stat,"An information-based traffic control in a public conveyance system:
  reduced clustering and enhanced efficiency","  A new public conveyance model applicable to buses and trains is proposed in
this paper by using stochastic cellular automaton. We have found the optimal
density of vehicles, at which the average velocity becomes maximum,
significantly depends on the number of stops and passengers behavior of getting
on a vehicle at stops. The efficiency of the hail-and-ride system is also
discussed by comparing the different behavior of passengers. Moreover, we have
found that a big cluster of vehicles is divided into small clusters, by
incorporating information of the number of vehicles between successive stops.
",new public conveyance model applicable bus train propose paper stochastic cellular automaton find optimal density vehicle average velocity maximum significantly depend number stop passenger behavior get vehicle stop efficiency hail ride system discuss compare different behavior passenger find big cluster vehicle divide small cluster incorporate information number vehicle successive stop
stat,Spin-spin Correlation in Some Excited States of Transverse Ising Model,"  We consider the transverse Ising model in one dimension with
nearest-neighbour interaction and calculate exactly the longitudinal spin-spin
correlation for a class of excited states. These states are known to play an
important role in the perturbative treatment of one-dimensional transverse
Ising model with frustrated second-neighbour interaction. To calculate the
correlation, we follow the earlier procedure of Wu, use Szego's theorem and
also use Fisher-Hartwig conjecture. The result is that the correlation decays
algebraically with distance ($n$) as $1/\surd n$ and is oscillatory or
non-oscillatory depending on the magnitude of the transverse field.
",consider transverse ising model dimension near neighbour interaction calculate exactly longitudinal spin spin correlation class excited state state know play important role perturbative treatment dimensional transverse ising model frustrated second neighbour interaction calculate correlation follow early procedure wu use szego theorem use fisher hartwig conjecture result correlation decay algebraically distance $ n$ $ 1/\surd n$ oscillatory non oscillatory depend magnitude transverse field
stat,Singular Energy Distributions in Granular Media,"  We study the kinetic theory of driven granular gases, taking into account
both translational and rotational degrees of freedom. We obtain the high-energy
tail of the stationary bivariate energy distribution, depending on the total
energy E and the ratio x=sqrt{E_w/E} of rotational energy E_w to total energy.
Extremely energetic particles have a unique and well-defined distribution f(x)
which has several remarkable features: x is not uniformly distributed as in
molecular gases; f(x) is not smooth but has multiple singularities. The latter
behavior is sensitive to material properties such as the collision parameters,
the moment of inertia and the collision rate. Interestingly, there are
preferred ratios of rotational-to-total energy. In general, f(x) is strongly
correlated with energy and the deviations from a uniform distribution grow with
energy. We also solve for the energy distribution of freely cooling Maxwell
Molecules and find qualitatively similar behavior.
",study kinetic theory drive granular gas take account translational rotational degree freedom obtain high energy tail stationary bivariate energy distribution depend total energy e ratio x = sqrt{e_w e rotational energy e_w total energy extremely energetic particle unique define distribution f(x remarkable feature x uniformly distribute molecular gas f(x smooth multiple singularity behavior sensitive material property collision parameter moment inertia collision rate interestingly preferred ratio rotational total energy general f(x strongly correlate energy deviation uniform distribution grow energy solve energy distribution freely cool maxwell molecules find qualitatively similar behavior
stat,What is the order of 2D polymer escape transition?,"  An end-grafted flexible polymer chain in 3d space between two pistons
undergoes an abrupt transition from a confined coil to a flower-like
conformation when the number of monomers in the chain, N, reaches a critical
value. In 2d geometry, excluded volume interactions between monomers of a chain
confined inside a strip of finite length 2L transform the coil conformation
into a linear string of blobs. However, the blob picture raises questions on
the nature of this escape transition. To check the theoretical predictions
based on the blob picture we study 2d single polymer chains with excluded
volume interactions and with one end grafted in the middle of a strip of length
  2L and width H by simulating self-avoiding walks on a square lattice with the
pruned-enriched-Rosenbluth method (PERM). We estimate the free energy, the
end-to-end distance, the number of imprisoned monomers, the order parameter,
and its distribution. It is shown that in the thermodynamic limit of large N
and L but finite L/N, there is a small but finite jump in several average
characteristics, including the order parameter. We also present a theoretical
description based on the Landau free energy approach, which is in good
agreement with the simulation results. Both simulation results and the
analytical theory indicate that the 2d escape transition is a weak first-order
phase transition.
",end graft flexible polymer chain 3d space piston undergo abrupt transition confine coil flower like conformation number monomer chain n reach critical value 2d geometry exclude volume interaction monomer chain confine inside strip finite length 2l transform coil conformation linear string blob blob picture raise question nature escape transition check theoretical prediction base blob picture study 2d single polymer chain exclude volume interaction end graft middle strip length 2l width h simulate self avoid walk square lattice prune enrich rosenbluth method perm estimate free energy end end distance number imprison monomer order parameter distribution show thermodynamic limit large n l finite l n small finite jump average characteristic include order parameter present theoretical description base landau free energy approach good agreement simulation result simulation result analytical theory indicate 2d escape transition weak order phase transition
stat,Balance of forces in simulated bilayers,"  Two kinds of simulated bilayers are described and the results are reported
for lateral tension and for partial contributions of intermolecular forces to
it.Data for a widest possible range of areas per surfactant head, from tunnel
formation through tensionless state, transition to floppy bilayer,to its
disintegration, are reported and discussed. The significance of the tensionless
state, is discussed. Conclusions: (1) the tensionless state is a
coincidence;(2) the transition from extended to floppy bilayer occurs nearby
and has hallmarks of a phase transition (3) there is no theory of that
transition.(4)The lateral tension of the floppy bilayer scales with size; that
of the extended bilayer does not depend on size. (4) The drumhead model not
appropriate for interfaces as these fluctuate via diffusion.(5) The radius of
gyration also! shows a discontinuity.
",kind simulated bilayer describe result report lateral tension partial contribution intermolecular force datum wide possible range area surfactant head tunnel formation tensionless state transition floppy bilayer disintegration report discuss significance tensionless state discuss conclusion 1 tensionless state coincidence;(2 transition extended floppy bilayer occur nearby hallmark phase transition 3 theory transition.(4)the lateral tension floppy bilayer scale size extended bilayer depend size 4 drumhead model appropriate interface fluctuate diffusion.(5 radius gyration show discontinuity
stat,U-max-Statistics,"  In 1948, W. Hoeffding introduced a large class of unbiased estimators called
U-statistics, defined as the average value of a real-valued k-variate function
h calculated at all possible sets of k points from a random sample. In the
present paper we investigate the corresponding extreme value analogue, which we
shall call U-max-statistics. We are concerned with the behavior of the largest
value of such function h instead of its average. Examples of U-max-statistics
are the diameter or the largest scalar product within a random sample.
U-max-statistics of higher degrees are given by triameters and other metric
invariants.
",1948 w. hoeffding introduce large class unbiased estimator call u statistic define average value real value k variate function h calculate possible set k point random sample present paper investigate corresponding extreme value analogue shall u max statistic concern behavior large value function h instead average example u max statistic diameter large scalar product random sample u max statistic high degree give triameter metric invariant
stat,"Interplay of Anisotropy and Disorder in the Doping-Dependent Melting and
  Glass Transitions of Vortices in Bi$_2$Sr$_2$CaCu$_2$O$_{8+\delta}$","  We study the oxygen doping dependence of the equilibrium first-order melting
and second-order glass transitions of vortices in
Bi$_2$Sr$_2$CaCu$_2$O$_{8+\delta}$. Doping affects both anisotropy and
disorder. Anisotropy scaling is shown to collapse the melting lines only where
thermal fluctuations are dominant. Yet, in the region where disorder breaks
that scaling, the glass lines are still collapsed. A quantitative fit to
melting and replica symmetry breaking lines of a 2D Ginzburg-Landau model
further reveals that disorder amplitude weakens with doping, but to a lesser
degree than thermal fluctuations, enhancing the relative role of disorder.
",study oxygen dope dependence equilibrium order melting second order glass transition vortex bi$_2$sr$_2$cacu$_2$o$_{8+\delta}$. doping affect anisotropy disorder anisotropy scaling show collapse melting line thermal fluctuation dominant region disorder break scale glass line collapse quantitative fit melt replica symmetry break line 2d ginzburg landau model reveal disorder amplitude weaken doping less degree thermal fluctuation enhance relative role disorder
stat,"Dynamical Equilibrium, trajectories study in an economical system. The
  case of the labor market","  The paper deals with the study of labor market dynamics, and aims to
characterize its equilibriums and possible trajectories. The theoretical
background is the theory of the segmented labor market. The main idea is that
this theory is well adapted to interpret the observed trajectories, due to the
heterogeneity of the work situations.
",paper deal study labor market dynamic aim characterize equilibrium possible trajectory theoretical background theory segment labor market main idea theory adapt interpret observed trajectory heterogeneity work situation
stat,"Excitation Spectrum Gap and Spin-Wave Stiffness of XXZ Heisenberg
  Chains: Global Renormalization-Group Calculation","  The anisotropic XXZ spin-1/2 Heisenberg chain is studied using
renormalization-group theory. The specific heats and nearest-neighbor spin-spin
correlations are calculated thoughout the entire temperature and anisotropy
ranges in both ferromagnetic and antiferromagnetic regions, obtaining a global
description and quantitative results. We obtain, for all anisotropies, the
antiferromagnetic spin-liquid spin-wave velocity and the Isinglike
ferromagnetic excitation spectrum gap, exhibiting the spin-wave to spinon
crossover. A number of characteristics of purely quantum nature are found: The
in-plane interaction s_i^x s_j^x + s_i^y s_j^y induces an antiferromagnetic
correlation in the out-of-plane s_i^z component, at higher temperatures in the
antiferromagnetic XXZ chain, dominantly at low temperatures in the
ferromagnetic XXZ chain, and, in-between, at all temperatures in the XY chain.
We find that the converse effect also occurs in the antiferromagnetic XXZ
chain: an antiferromagnetic s_i^z s_j^z interaction induces a correlation in
the s_i^xy component. As another purely quantum effect, (i) in the
antiferromagnet, the value of the specific heat peak is insensitive to
anisotropy and the temperature of the specific heat peak decreases from the
isotropic (Heisenberg) with introduction of either type (Ising or XY)
anisotropy; (ii) in complete contrast, in the ferromagnet, the value and
temperature of the specific heat peak increase with either type of anisotropy.
",anisotropic xxz spin-1/2 heisenberg chain study renormalization group theory specific heat near neighbor spin spin correlation calculate thoughout entire temperature anisotropy range ferromagnetic antiferromagnetic region obtain global description quantitative result obtain anisotropy antiferromagnetic spin liquid spin wave velocity isinglike ferromagnetic excitation spectrum gap exhibit spin wave spinon crossover number characteristic purely quantum nature find plane interaction s_i^x s_j^x + s_i^y s_j^y induce antiferromagnetic correlation plane s_i^z component high temperature antiferromagnetic xxz chain dominantly low temperature ferromagnetic xxz chain temperature xy chain find converse effect occur antiferromagnetic xxz chain antiferromagnetic s_i^z s_j^z interaction induce correlation s_i^xy component purely quantum effect antiferromagnet value specific heat peak insensitive anisotropy temperature specific heat peak decrease isotropic heisenberg introduction type ising xy anisotropy ii complete contrast ferromagnet value temperature specific heat peak increase type anisotropy
stat,"A New Monte Carlo Method and Its Implications for Generalized Cluster
  Algorithms","  We describe a novel switching algorithm based on a ``reverse'' Monte Carlo
method, in which the potential is stochastically modified before the system
configuration is moved. This new algorithm facilitates a generalized
formulation of cluster-type Monte Carlo methods, and the generalization makes
it possible to derive cluster algorithms for systems with both discrete and
continuous degrees of freedom. The roughening transition in the sine-Gordon
model has been studied with this method, and high-accuracy simulations for
system sizes up to $1024^2$ were carried out to examine the logarithmic
divergence of the surface roughness above the transition temperature, revealing
clear evidence for universal scaling of the Kosterlitz-Thouless type.
",describe novel switch algorithm base ` ` reverse monte carlo method potential stochastically modify system configuration move new algorithm facilitate generalize formulation cluster type monte carlo method generalization make possible derive cluster algorithm system discrete continuous degree freedom roughen transition sine gordon model study method high accuracy simulation system size $ 1024 ^ 2 $ carry examine logarithmic divergence surface roughness transition temperature reveal clear evidence universal scaling kosterlitz thouless type
stat,"Connecting microscopic simulations with kinetically constrained models
  of glasses","  Kinetically constrained spin models are known to exhibit dynamical behavior
mimicking that of glass forming systems. They are often understood as
coarse-grained models of glass formers, in terms of some ""mobility"" field. The
identity of this ""mobility"" field has remained elusive due to the lack of
coarse-graining procedures to obtain these models from a more microscopic point
of view. Here we exhibit a scheme to map the dynamics of a two-dimensional soft
disc glass former onto a kinetically constrained spin model, providing an
attempt at bridging these two approaches.
",kinetically constrain spin model know exhibit dynamical behavior mimic glass form system understand coarse grain model glass former term mobility field identity mobility field remain elusive lack coarse graining procedure obtain model microscopic point view exhibit scheme map dynamic dimensional soft disc glass kinetically constrain spin model provide attempt bridge approach
stat,High-dimensional variable selection,"  This paper explores the following question: what kind of statistical
guarantees can be given when doing variable selection in high-dimensional
models? In particular, we look at the error rates and power of some multi-stage
regression methods. In the first stage we fit a set of candidate models. In the
second stage we select one model by cross-validation. In the third stage we use
hypothesis testing to eliminate some variables. We refer to the first two
stages as ""screening"" and the last stage as ""cleaning."" We consider three
screening methods: the lasso, marginal regression, and forward stepwise
regression. Our method gives consistent variable selection under certain
conditions.
",paper explore following question kind statistical guarantee give variable selection high dimensional model particular look error rate power multi stage regression method stage fit set candidate model second stage select model cross validation stage use hypothesis testing eliminate variable refer stage screening stage clean consider screening method lasso marginal regression forward stepwise regression method give consistent variable selection certain condition
stat,The power of choice in network growth,"  The ""power of choice"" has been shown to radically alter the behavior of a
number of randomized algorithms. Here we explore the effects of choice on
models of tree and network growth. In our models each new node has k randomly
chosen contacts, where k > 1 is a constant. It then attaches to whichever one
of these contacts is most desirable in some sense, such as its distance from
the root or its degree. Even when the new node has just two choices, i.e., when
k=2, the resulting network can be very different from a random graph or tree.
For instance, if the new node attaches to the contact which is closest to the
root of the tree, the distribution of depths changes from Poisson to a
traveling wave solution. If the new node attaches to the contact with the
smallest degree, the degree distribution is closer to uniform than in a random
graph, so that with high probability there are no nodes in the network with
degree greater than O(log log N). Finally, if the new node attaches to the
contact with the largest degree, we find that the degree distribution is a
power law with exponent -1 up to degrees roughly equal to k, with an
exponential cutoff beyond that; thus, in this case, we need k >> 1 to see a
power law over a wide range of degrees.
",power choice show radically alter behavior number randomized algorithm explore effect choice model tree network growth model new node k randomly choose contact k > 1 constant attach whichever contact desirable sense distance root degree new node choice i.e. k=2 result network different random graph tree instance new node attache contact close root tree distribution depth change poisson travel wave solution new node attache contact small degree degree distribution close uniform random graph high probability node network degree great o(log log n finally new node attache contact large degree find degree distribution power law exponent -1 degree roughly equal k exponential cutoff case need k > > 1 power law wide range degree
stat,"Finite-size scaling of pseudo-critical point distributions in the random
  transverse-field Ising chain","  We study the distribution of finite size pseudo-critical points in a
one-dimensional random quantum magnet with a quantum phase transition described
by an infinite randomness fixed point. Pseudo-critical points are defined in
three different ways: the position of the maximum of the average entanglement
entropy, the scaling behavior of the surface magnetization, and the energy of a
soft mode. All three lead to a log-normal distribution of the pseudo-critical
transverse fields, where the width scales as $L^{-1/\nu}$ with $\nu=2$ and the
shift of the average value scales as $L^{-1/\nu_{typ}}$ with $\nu_{typ}=1$,
which we related to the scaling of average and typical quantities in the
critical region.
",study distribution finite size pseudo critical point dimensional random quantum magnet quantum phase transition describe infinite randomness fix point pseudo critical point define different way position maximum average entanglement entropy scale behavior surface magnetization energy soft mode lead log normal distribution pseudo critical transverse field width scale $ l^{-1/\nu}$ $ \nu=2 $ shift average value scale $ l^{-1/\nu_{typ}}$ $ \nu_{typ}=1 $ relate scaling average typical quantity critical region
stat,Emergence of U(1) symmetry in the 3D XY model with Zq anisotropy,"  We study the three-dimensional XY model with a Z_q anisotropic term. At
temperatures T < Tc this dangerously irrelevant perturbation is relevant only
above a length scale Lambda, which diverges as a power of the correlation
length; Lambda ~ xi^a_q. Below Lambda the order parameter is U(1) symmetric. We
derive the full scaling function controlling the emergence of U(1) symmetry and
use Monte Carlo results to extract the exponent a_q for q=4,...,8. We find that
a_q = a_4 (q/4)^2, with a_4 only marginally larger than 1. We discuss these
results in the context of U(1) symmetry at ""deconfined"" quantum critical points
separating antiferromagnetic and valence-bond-solid states in quantum spin
systems.
","study dimensional xy model z_q anisotropic term temperature t < tc dangerously irrelevant perturbation relevant length scale lambda diverge power correlation length lambda ~ xi^a_q lambda order parameter u(1 symmetric derive scaling function control emergence u(1 symmetry use monte carlo result extract exponent a_q q=4, ,8 find a_q = a_4 q/4)^2 a_4 marginally large 1 discuss result context u(1 symmetry deconfined quantum critical point separate antiferromagnetic valence bond solid state quantum spin system"
stat,"Ramsey fringes formation during excitation of topological modes in a
  Bose-Einstein condensate","  The Ramsey fringes formation during the excitation of topological coherent
modes of a Bose-Einstein condensate by an external modulating field is
considered. The Ramsey fringes appear when a series of pulses of the excitation
field is applied. In both Rabi and Ramsey interrogations, there is a shift of
the population maximum transfer due to the strong non-linearity present in the
system. It is found that the Ramsey pattern itself retains information about
the accumulated relative phase between both ground and excited coherent modes.
",ramsey fringe formation excitation topological coherent mode bose einstein condensate external modulate field consider ramsey fringe appear series pulse excitation field apply rabi ramsey interrogation shift population maximum transfer strong non linearity present system find ramsey pattern retain information accumulate relative phase ground excited coherent mode
stat,"Highly synchronized noise-driven oscillatory behavior of a
  FitzHugh-Nagumo ring with phase-repulsive coupling","  We investigate a ring of $N$ FitzHugh--Nagumo elements coupled in
\emph{phase-repulsive} fashion and submitted to a (subthreshold) common
oscillatory signal and independent Gaussian white noises. This system can be
regarded as a reduced version of the one studied in [Phys. Rev. E \textbf{64},
041912 (2001)], although externally forced and submitted to noise. The
noise-sustained synchronization of the system with the external signal is
characterized.
",investigate ring $ n$ fitzhugh nagumo element couple \emph{phase repulsive fashion submit subthreshold common oscillatory signal independent gaussian white noise system regard reduce version study phys rev. e \textbf{64 041912 2001 externally force submit noise noise sustain synchronization system external signal characterize
stat,Quantum Quenches in Extended Systems,"  We study in general the time-evolution of correlation functions in a extended
quantum system after the quench of a parameter in the hamiltonian. We show that
correlation functions in d dimensions can be extracted using methods of
boundary critical phenomena in d+1 dimensions. For d=1 this allows to use the
powerful tools of conformal field theory in the case of critical evolution.
Several results are obtained in generic dimension in the gaussian (mean-field)
approximation. These predictions are checked against the real-time evolution of
some solvable models that allows also to understand which features are valid
beyond the critical evolution.
  All our findings may be explained in terms of a picture generally valid,
whereby quasiparticles, entangled over regions of the order of the correlation
length in the initial state, then propagate with a finite speed through the
system. Furthermore we show that the long-time results can be interpreted in
terms of a generalized Gibbs ensemble. We discuss some open questions and
possible future developments.
",study general time evolution correlation function extend quantum system quench parameter hamiltonian correlation function d dimension extract method boundary critical phenomena d+1 dimension d=1 allow use powerful tool conformal field theory case critical evolution result obtain generic dimension gaussian mean field approximation prediction check real time evolution solvable model allow understand feature valid critical evolution finding explain term picture generally valid quasiparticle entangle region order correlation length initial state propagate finite speed system furthermore long time result interpret term generalized gibbs ensemble discuss open question possible future development
stat,Biased Structural Fluctuations due to Electron Wind Force,"  Direct correlation between temporal structural fluctuations and electron wind
force is demonstrated, for the first time, by STM imaging and analysis of
atomically-resolved motion on a thin film surface under large applied current
(10e5 Amp/sqare cm). The magnitude of the momentum transfer between current
carriers and atoms in the fluctuating structure is at least five to fifteen
times (plus or minus one sigma range) larger than for freely diffusing adatoms.
The corresponding changes in surface resistivity will contribute significant
fluctuation signature to nanoscale electronic properties.
",direct correlation temporal structural fluctuation electron wind force demonstrate time stm imaging analysis atomically resolve motion thin film surface large apply current 10e5 amp sqare cm magnitude momentum transfer current carrier atom fluctuating structure time plus minus sigma range large freely diffuse adatom corresponding change surface resistivity contribute significant fluctuation signature nanoscale electronic property
stat,Traitement Des Donnees Manquantes Au Moyen De L'Algorithme De Kohonen,"  Nous montrons comment il est possible d'utiliser l'algorithme d'auto
organisation de Kohonen pour traiter des donn\'ees avec valeurs manquantes et
estimer ces derni\`eres. Apr\`es un rappel m\'ethodologique, nous illustrons
notre propos \`a partir de trois applications \`a des donn\'ees r\'eelles.
  -----
  We show how it is possible to use the Kohonen self-organizing algorithm to
deal with data which contain missing values and to estimate them. After a
methodological recall, we illustrate our purpose from three real databases
applications.
",nous montron comment il est possible d'utiliser l'algorithme d'auto organisation de kohonen pour traiter des donn\'ees avec valeur manquante et estimer ce derni\`ere apr\`es un rappel m\'ethodologique nous illustron notre propos \`a partir de trois application \`a des donn\'ees r\'eelle possible use kohonen self organize algorithm deal datum contain miss value estimate methodological recall illustrate purpose real database application
stat,"Monte Carlo Simulations of Quantum Spin Systems in the Valence Bond
  Basis","  We discuss a projector Monte Carlo method for quantum spin models formulated
in the valence bond basis, using the S=1/2 Heisenberg antiferromagnet as an
example. Its singlet ground state can be projected out of an arbitrary basis
state as the trial state, but a more rapid convergence can be obtained using a
good variational state. As an alternative to first carrying out a time
consuming variational Monte Carlo calculation, we show that a very good trial
state can be generated in an iterative fashion in the course of the simulation
itself. We also show how the properties of the valence bond basis enable
calculations of quantities that are difficult to obtain with the standard basis
of Sz eigenstates. In particular, we discuss quantities involving
finite-momentum states in the triplet sector, such as the dispersion relation
and the spectral weight of the lowest triplet.
",discuss projector monte carlo method quantum spin model formulate valence bond basis s=1/2 heisenberg antiferromagnet example singlet ground state project arbitrary basis state trial state rapid convergence obtain good variational state alternative carry time consume variational monte carlo calculation good trial state generate iterative fashion course simulation property valence bond basis enable calculation quantity difficult obtain standard basis sz eigenstate particular discuss quantity involve finite momentum state triplet sector dispersion relation spectral weight low triplet
stat,"Subdiffusion and weak ergodicity breaking in the presence of a reactive
  boundary","  We derive the boundary condition for a subdiffusive particle interacting with
a reactive boundary with finite reaction rate. Molecular crowding conditions,
that are found to cause subdiffusion of larger molecules in biological cells,
are shown to effect long-tailed distributions with identical exponent for both
the unbinding times from the boundary to the bulk and the rebinding times from
the bulk. This causes a weak ergodicity breaking: typically, an individual
particle either stays bound or remains in the bulk for very long times. We
discuss why this may be beneficial for in vivo gene regulation by DNA-binding
proteins, whose typical concentrations are nanomolar
",derive boundary condition subdiffusive particle interact reactive boundary finite reaction rate molecular crowding condition find cause subdiffusion large molecule biological cell show effect long tail distribution identical exponent unbind time boundary bulk rebind time bulk cause weak ergodicity breaking typically individual particle stay bind remain bulk long time discuss beneficial vivo gene regulation dna bind protein typical concentration nanomolar
stat,"Sparse Estimators and the Oracle Property, or the Return of Hodges'
  Estimator","  We point out some pitfalls related to the concept of an oracle property as
used in Fan and Li (2001, 2002, 2004) which are reminiscent of the well-known
pitfalls related to Hodges' estimator. The oracle property is often a
consequence of sparsity of an estimator. We show that any estimator satisfying
a sparsity property has maximal risk that converges to the supremum of the loss
function; in particular, the maximal risk diverges to infinity whenever the
loss function is unbounded. For ease of presentation the result is set in the
framework of a linear regression model, but generalizes far beyond that
setting. In a Monte Carlo study we also assess the extent of the problem in
finite samples for the smoothly clipped absolute deviation (SCAD) estimator
introduced in Fan and Li (2001). We find that this estimator can perform rather
poorly in finite samples and that its worst-case performance relative to
maximum likelihood deteriorates with increasing sample size when the estimator
is tuned to sparsity.
",point pitfall relate concept oracle property fan li 2001 2002 2004 reminiscent know pitfall relate hodges estimator oracle property consequence sparsity estimator estimator satisfy sparsity property maximal risk converge supremum loss function particular maximal risk diverge infinity loss function unbounded ease presentation result set framework linear regression model generalize far setting monte carlo study assess extent problem finite sample smoothly clip absolute deviation scad estimator introduce fan li 2001 find estimator perform poorly finite sample bad case performance relative maximum likelihood deteriorate increase sample size estimator tune sparsity
stat,"Corner Transfer Matrix Renormalization Group Method Applied to the Ising
  Model on the Hyperbolic Plane","  Critical behavior of the Ising model is investigated at the center of large
scale finite size systems, where the lattice is represented as the tiling of
pentagons. The system is on the hyperbolic plane, and the recursive structure
of the lattice makes it possible to apply the corner transfer matrix
renormalization group method. From the calculated nearest neighbor spin
correlation function and the spontaneous magnetization, it is concluded that
the phase transition of this model is mean-field like. One parameter
deformation of the corner Hamiltonian on the hyperbolic plane is discussed.
",critical behavior ising model investigate center large scale finite size system lattice represent tiling pentagon system hyperbolic plane recursive structure lattice make possible apply corner transfer matrix renormalization group method calculate near neighbor spin correlation function spontaneous magnetization conclude phase transition model mean field like parameter deformation corner hamiltonian hyperbolic plane discuss
stat,"Aspects of stochastic resonance in reaction-diffusion systems: The
  nonequilibrium-potential approach","  We analyze several aspects of the phenomenon of stochastic resonance in
reaction-diffusion systems, exploiting the nonequilibrium potential's
framework. The generalization of this formalism (sketched in the appendix) to
extended systems is first carried out in the context of a simplified scalar
model, for which stationary patterns can be found analytically. We first show
how system-size stochastic resonance arises naturally in this framework, and
then how the phenomenon of array-enhanced stochastic resonance can be further
enhanced by letting the diffusion coefficient depend on the field. A yet less
trivial generalization is exemplified by a stylized version of the
FitzHugh-Nagumo system, a paradigm of the activator-inhibitor class. After
discussing for this system the second aspect enumerated above, we derive from
it -through an adiabatic-like elimination of the inhibitor field- an effective
scalar model that includes a nonlocal contribution. Studying the role played by
the range of the nonlocal kernel and its effect on stochastic resonance, we
find an optimal range that maximizes the system's response.
",analyze aspect phenomenon stochastic resonance reaction diffusion system exploit nonequilibrium potential framework generalization formalism sketch appendix extend system carry context simplify scalar model stationary pattern find analytically system size stochastic resonance arise naturally framework phenomenon array enhance stochastic resonance enhance let diffusion coefficient depend field trivial generalization exemplify stylize version fitzhugh nagumo system paradigm activator inhibitor class discuss system second aspect enumerate derive -through adiabatic like elimination inhibitor field- effective scalar model include nonlocal contribution study role play range nonlocal kernel effect stochastic resonance find optimal range maximize system response
stat,"Fluctuations of the partial filling factors in competitive RSA from
  binary mixtures","  Competitive random sequential adsorption on a line from a binary mix of
incident particles is studied using both an analytic recursive approach and
Monte Carlo simulations. We find a strong correlation between the small and the
large particle distributions so that while both partial contributions to the
fill factor fluctuate widely, the variance of the total fill factor remains
relatively small. The variances of partial contributions themselves are quite
different between the smaller and the larger particles, with the larger
particle distribution being more correlated. The disparity in fluctuations of
partial fill factors increases with the particle size ratio. The additional
variance in the partial contribution of smaller particle originates from the
fluctuations in the size of gaps between larger particles. We discuss the
implications of our results to semiconductor high-energy gamma detectors where
the detector energy resolution is controlled by correlations in the cascade
energy branching process.
",competitive random sequential adsorption line binary mix incident particle study analytic recursive approach monte carlo simulation find strong correlation small large particle distribution partial contribution fill factor fluctuate widely variance total fill factor remain relatively small variance partial contribution different small large particle large particle distribution correlate disparity fluctuation partial fill factor increase particle size ratio additional variance partial contribution small particle originate fluctuation size gap large particle discuss implication result semiconductor high energy gamma detector detector energy resolution control correlation cascade energy branching process
stat,"Capillary ordering and layering transitions in two-dimensional hard-rod
  fluids","  In this article we calculate the surface phase diagram of a two-dimensional
hard-rod fluid confined between two hard lines. In a first stage we study the
semi-infinite system consisting of an isotropic fluid in contact with a single
hard line. We have found complete wetting by the columnar phase at the
wall-isotropic fluid interface. When the fluid is confined between two hard
walls, capillary columnar ordering occurs via a first-order phase transition.
For higher chemical potentials the system exhibits layering transitions even
for very narrow slits (near the one-dimensional limit). The theoretical model
used was a density-functional theory based on the Fundamental-Measure
Functional applied to a fluid of hard rectangles in the restricted-orientation
approximation (Zwanzig model). The results presented here can be checked
experimentally in two-dimensional granular media made of rods, where vertical
motions induced by an external source and excluded volume interactions between
the grains allow the system to explore those stationary states which
entropically maximize packing configurations. We claim that some of the surface
phenomena found here can be present in two-dimensional granular-media fluids.
",article calculate surface phase diagram dimensional hard rod fluid confine hard line stage study semi infinite system consist isotropic fluid contact single hard line find complete wetting columnar phase wall isotropic fluid interface fluid confine hard wall capillary columnar ordering occur order phase transition high chemical potential system exhibit layering transition narrow slit near dimensional limit theoretical model density functional theory base fundamental measure functional apply fluid hard rectangle restrict orientation approximation zwanzig model result present check experimentally dimensional granular medium rod vertical motion induce external source exclude volume interaction grain allow system explore stationary state entropically maximize packing configuration claim surface phenomenon find present dimensional granular medium fluid
stat,"Heisenberg antiferromagnet with anisotropic exchange on the Kagome
  lattice: Description of the magnetic properties of volborthite","  We study the properties of the Heisenberg antiferromagnet with spatially
anisotropic nearest-neighbour exchange couplings on the kagome net, i.e. with
coupling J in one lattice direction and couplings J' along the other two
directions. For J/J' > 1, this model is believed to describe the magnetic
properties of the mineral volborthite. In the classical limit, it exhibits two
kinds of ground states: a ferrimagnetic state for J/J' < 1/2 and a large
manifold of canted spin states for J/J' > 1/2. To include quantum effects
self-consistently, we investigate the Sp(N) symmetric generalisation of the
original SU(2) symmetric model in the large-N limit. In addition to the
dependence on the anisotropy, the Sp(N) symmetric model depends on a parameter
kappa that measures the importance of quantum effects. Our numerical
calculations reveal that in the kappa-J/J' plane, the system shows a rich phase
diagram containing a ferrimagnetic phase, an incommensurate phase, and a
decoupled chain phase, the latter two with short- and long-range order. We
corroborate these results by showing that the boundaries between the various
phases and several other features of the Sp(N) phase diagram can be determined
by analytical calculations. Finally, the application of a block-spin
perturbation expansion to the trimerised version of the original spin-1/2 model
leads us to suggest that in the limit of strong anisotropy, J/J' >> 1, the
ground state of the original model is a collinearly ordered antiferromagnet,
which is separated from the incommensurate state by a quantum phase transition.
",study property heisenberg antiferromagnet spatially anisotropic near neighbour exchange coupling kagome net i.e. couple j lattice direction coupling j direction j j > 1 model believe describe magnetic property mineral volborthite classical limit exhibit kind ground state ferrimagnetic state j j < 1/2 large manifold cant spin state j j > 1/2 include quantum effect self consistently investigate sp(n symmetric generalisation original su(2 symmetric model large n limit addition dependence anisotropy sp(n symmetric model depend parameter kappa measure importance quantum effect numerical calculation reveal kappa j j plane system show rich phase diagram contain ferrimagnetic phase incommensurate phase decouple chain phase short- long range order corroborate result show boundary phase feature sp(n phase diagram determine analytical calculation finally application block spin perturbation expansion trimerise version original spin-1/2 model lead suggest limit strong anisotropy j j > > 1 ground state original model collinearly order antiferromagnet separate incommensurate state quantum phase transition
stat,Optimal fluctuation approach to a directed polymer in a random medium,"  A modification of the optimal fluctuation approach is applied to study the
tails of the free energy distribution function P(F) for an elastic string in
quenched disorder both in the regions of the universal behavior of P(F) and in
the regions of large fluctuations, where the behavior of P(F) is non-universal.
The difference between the two regimes is shown to consist in whether it is
necessary or not to take into account the renormalization of parameters by the
fluctuations of disorder in the vicinity of the optimal fluctuation.
",modification optimal fluctuation approach apply study tail free energy distribution function p(f elastic string quench disorder region universal behavior p(f region large fluctuation behavior p(f non universal difference regime show consist necessary account renormalization parameter fluctuation disorder vicinity optimal fluctuation
stat,Phase Transitions in the Coloring of Random Graphs,"  We consider the problem of coloring the vertices of a large sparse random
graph with a given number of colors so that no adjacent vertices have the same
color. Using the cavity method, we present a detailed and systematic analytical
study of the space of proper colorings (solutions).
  We show that for a fixed number of colors and as the average vertex degree
(number of constraints) increases, the set of solutions undergoes several phase
transitions similar to those observed in the mean field theory of glasses.
First, at the clustering transition, the entropically dominant part of the
phase space decomposes into an exponential number of pure states so that beyond
this transition a uniform sampling of solutions becomes hard. Afterward, the
space of solutions condenses over a finite number of the largest states and
consequently the total entropy of solutions becomes smaller than the annealed
one. Another transition takes place when in all the entropically dominant
states a finite fraction of nodes freezes so that each of these nodes is
allowed a single color in all the solutions inside the state. Eventually, above
the coloring threshold, no more solutions are available. We compute all the
critical connectivities for Erdos-Renyi and regular random graphs and determine
their asymptotic values for large number of colors.
  Finally, we discuss the algorithmic consequences of our findings. We argue
that the onset of computational hardness is not associated with the clustering
transition and we suggest instead that the freezing transition might be the
relevant phenomenon. We also discuss the performance of a simple local Walk-COL
algorithm and of the belief propagation algorithm in the light of our results.
",consider problem color vertex large sparse random graph give number color adjacent vertex color cavity method present detailed systematic analytical study space proper coloring solution fix number color average vertex degree number constraint increase set solution undergo phase transition similar observe mean field theory glass cluster transition entropically dominant phase space decompose exponential number pure state transition uniform sampling solution hard afterward space solution condense finite number large state consequently total entropy solution small anneal transition take place entropically dominant state finite fraction node freeze node allow single color solution inside state eventually color threshold solution available compute critical connectivity erdos renyi regular random graph determine asymptotic value large number color finally discuss algorithmic consequence finding argue onset computational hardness associate cluster transition suggest instead freeze transition relevant phenomenon discuss performance simple local walk col algorithm belief propagation algorithm light result
stat,"Markov basis and Groebner basis of Segre-Veronese configuration for
  testing independence in group-wise selections","  We consider testing independence in group-wise selections with some
restrictions on combinations of choices. We present models for frequency data
of selections for which it is easy to perform conditional tests by Markov chain
Monte Carlo (MCMC) methods. When the restrictions on the combinations can be
described in terms of a Segre-Veronese configuration, an explicit form of a
Gr\""obner basis consisting of moves of degree two is readily available for
performing a Markov chain. We illustrate our setting with the National Center
Test for university entrance examinations in Japan. We also apply our method to
testing independence hypotheses involving genotypes at more than one locus or
haplotypes of alleles on the same chromosome.
","consider test independence group wise selection restriction combination choice present model frequency datum selection easy perform conditional test markov chain monte carlo mcmc method restriction combination describe term segre veronese configuration explicit form gr\""obner basis consist move degree readily available perform markov chain illustrate setting national center test university entrance examination japan apply method test independence hypothesis involve genotype locus haplotype allele chromosome"
stat,"Noise-induced phase transitions: Effects of the noises' statistics and
  spectrum","  The local, uncorrelated multiplicative noises driving a second-order, purely
noise-induced, ordering phase transition (NIPT) were assumed to be Gaussian and
white in the model of [Phys. Rev. Lett. \textbf{73}, 3395 (1994)]. The
potential scientific and technological interest of this phenomenon calls for a
study of the effects of the noises' statistics and spectrum. This task is
facilitated if these noises are dynamically generated by means of stochastic
differential equations (SDE) driven by white noises. One such case is that of
Ornstein--Uhlenbeck noises which are stationary, with Gaussian pdf and a
variance reduced by the self-correlation time (\tau), and whose effect on the
NIPT phase diagram has been studied some time ago. Another such case is when
the stationary pdf is a (colored) Tsallis' (q)--\emph{Gaussian} which, being a
\emph{fat-tail} distribution for (q>1) and a \emph{compact-support} one for
(q<1), allows for a controlled exploration of the effects of the departure from
Gaussian statistics. As done before with stochastic resonance and other
phenomena, we now exploit this tool to study--within a simple mean-field
approximation and with an emphasis on the \emph{order parameter} and the
``\emph{susceptibility}''--the combined effect on NIPT of the noises'
statistics and spectrum. Even for relatively small (\tau), it is shown that
whereas fat-tail noise distributions ((q>1)) counteract the effect of
self-correlation, compact-support ones ((q<1)) enhance it. Also, an interesting
effect on the susceptibility is seen in the last case.
",local uncorrelated multiplicative noise drive second order purely noise induce order phase transition nipt assume gaussian white model phys rev. lett \textbf{73 3395 1994 potential scientific technological interest phenomenon call study effect noise statistic spectrum task facilitate noise dynamically generate mean stochastic differential equation sde drive white noise case ornstein uhlenbeck noise stationary gaussian pdf variance reduce self correlation time \tau effect nipt phase diagram study time ago case stationary pdf color tsallis q)--\emph{gaussian \emph{fat tail distribution q>1 \emph{compact support q<1 allow control exploration effect departure gaussian statistic stochastic resonance phenomenon exploit tool study simple mean field approximation emphasis \emph{ord parameter ` ` \emph{susceptibility}''--the combined effect nipt noise statistic spectrum relatively small \tau show fat tail noise distribution q>1 counteract effect self correlation compact support one q<1 enhance interesting effect susceptibility see case
stat,Critical Behavior of a Trapped Interacting Bose Gas,"  The phase transition of Bose-Einstein condensation is studied in the critical
regime, when fluctuations extend far beyond the length scale of thermal de
Broglie waves. Using matter-wave interference we measure the correlation length
of these critical fluctuations as a function of temperature. The diverging
behavior of the correlation length above the critical temperature is observed,
from which we determine the critical exponent of the correlation length for a
trapped, weakly interacting Bose gas to be $\nu=0.67\pm 0.13$. This measurement
has direct implications for the understanding of second order phase
transitions.
",phase transition bose einstein condensation study critical regime fluctuation extend far length scale thermal de broglie wave matter wave interference measure correlation length critical fluctuation function temperature diverging behavior correlation length critical temperature observe determine critical exponent correlation length trap weakly interact bose gas $ \nu=0.67\pm 0.13$. measurement direct implication understanding second order phase transition
stat,"The property of kappa-deformed statistics for a relativistic gas in an
  electromagnetic field: kappa parameter and kappa-distribution","  We investigate the physical property of the kappa parameter and the
kappa-distribution in the kappa-deformed statistics, based on Kaniadakis
entropy, for a relativistic gas in an electromagnetic field. We derive two
relations for the relativistic gas in the framework of kappa-deformed
statistics, which describe the physical situation represented by the
relativistic kappa-distribution function, provide a reasonable connection
between the parameter kappa, the temperature four-gradient and the four-vector
potential gradient, and thus present for the case kappa different from zero a
clearly physical meaning. It is shown that such a physical situation is a
meta-equilibrium state of the system, but has a new physical characteristic.
",investigate physical property kappa parameter kappa distribution kappa deform statistic base kaniadakis entropy relativistic gas electromagnetic field derive relation relativistic gas framework kappa deform statistic describe physical situation represent relativistic kappa distribution function provide reasonable connection parameter kappa temperature gradient vector potential gradient present case kappa different zero clearly physical meaning show physical situation meta equilibrium state system new physical characteristic
eess,Optimization of Synthesis Oversampled Complex Filter Banks,"  An important issue with oversampled FIR analysis filter banks (FBs) is to
determine inverse synthesis FBs, when they exist. Given any complex oversampled
FIR analysis FB, we first provide an algorithm to determine whether there
exists an inverse FIR synthesis system. We also provide a method to ensure the
Hermitian symmetry property on the synthesis side, which is serviceable to
processing real-valued signals. As an invertible analysis scheme corresponds to
a redundant decomposition, there is no unique inverse FB. Given a particular
solution, we parameterize the whole family of inverses through a null space
projection. The resulting reduced parameter set simplifies design procedures,
since the perfect reconstruction constrained optimization problem is recast as
an unconstrained optimization problem. The design of optimized synthesis FBs
based on time or frequency localization criteria is then investigated, using a
simple yet efficient gradient algorithm.
",important issue oversample fir analysis filter bank fb determine inverse synthesis fb exist give complex oversample fir analysis fb provide algorithm determine exist inverse fir synthesis system provide method ensure hermitian symmetry property synthesis serviceable process real value signal invertible analysis scheme correspond redundant decomposition unique inverse fb give particular solution parameterize family inverse null space projection result reduce parameter set simplifie design procedure perfect reconstruction constrain optimization problem recast unconstrained optimization problem design optimize synthesis fb base time frequency localization criterion investigate simple efficient gradient algorithm
eess,Dichotic harmony for the musical practice,"  The dichotic method of hearing sound adapts in the region of musical harmony.
The algorithm of the separation of the being dissonant voices into several
separate groups is proposed. For an increase in the pleasantness of chords the
different groups of voices are heard out through the different channels of
headphones. Is created two demonstration program for PC. Keywords: music,
harmony, chord, dichotic listening, dissonance, consonance, headphones,
pleasantness, midi.
",dichotic method hear sound adapt region musical harmony algorithm separation dissonant voice separate group propose increase pleasantness chord different group voice hear different channel headphone create demonstration program pc keyword music harmony chord dichotic listening dissonance consonance headphone pleasantness midi
eess,"Multi-Sensor Fuzzy Data Fusion Using Sensors with Different
  Characteristics","  This paper proposes a new approach to multi-sensor data fusion. It suggests
that aggregation of data from multiple sensors can be done more efficiently
when we consider information about sensors' different characteristics. Similar
to most research on effective sensors' characteristics, especially in control
systems, our focus is on sensors' accuracy and frequency response. A rule-based
fuzzy system is presented for fusion of raw data obtained from the sensors that
have complement characteristics in accuracy and bandwidth. Furthermore, a fuzzy
predictor system is suggested aiming for extreme accuracy which is a common
need in highly sensitive applications. Advantages of our proposed sensor fusion
system are shown by simulation of a control system utilizing the fusion system
for output estimation.
",paper propose new approach multi sensor datum fusion suggest aggregation datum multiple sensor efficiently consider information sensor different characteristic similar research effective sensor characteristic especially control system focus sensor accuracy frequency response rule base fuzzy system present fusion raw datum obtain sensor complement characteristic accuracy bandwidth furthermore fuzzy predictor system suggest aim extreme accuracy common need highly sensitive application advantage propose sensor fusion system show simulation control system utilize fusion system output estimation
eess,Warping Peirce Quincuncial Panoramas,"  The Peirce quincuncial projection is a mapping of the surface of a sphere to
the interior of a square. It is a conformal map except for four points on the
equator. These points of non-conformality cause significant artifacts in
photographic applications. In this paper, we propose an algorithm and
user-interface to mitigate these artifacts. Moreover, in order to facilitate an
interactive user-interface, we present a fast algorithm for calculating the
Peirce quincuncial projection of spherical imagery. We then promote the Peirce
quincuncial projection as a viable alternative to the more popular
stereographic projection in some scenarios.
",peirce quincuncial projection mapping surface sphere interior square conformal map point equator point non conformality cause significant artifact photographic application paper propose algorithm user interface mitigate artifact order facilitate interactive user interface present fast algorithm calculate peirce quincuncial projection spherical imagery promote peirce quincuncial projection viable alternative popular stereographic projection scenario
eess,"Rotation, Scaling and Translation Analysis of Biometric Signature
  Templates","  Biometric authentication systems that make use of signature verification
methods often render optimum performance only under limited and restricted
conditions. Such methods utilize several training samples so as to achieve high
accuracy. Moreover, several constraints are imposed on the end-user so that the
system may work optimally, and as expected. For example, the user is made to
sign within a small box, in order to limit their signature to a predefined set
of dimensions, thus eliminating scaling. Moreover, the angular rotation with
respect to the referenced signature that will be inadvertently introduced as
human error, hampers performance of biometric signature verification systems.
To eliminate this, traditionally, a user is asked to sign exactly on top of a
reference line. In this paper, we propose a robust system that optimizes the
signature obtained from the user for a large range of variation in
Rotation-Scaling-Translation (RST) and resolves these error parameters in the
user signature according to the reference signature stored in the database.
",biometric authentication system use signature verification method render optimum performance limited restrict condition method utilize training sample achieve high accuracy constraint impose end user system work optimally expect example user sign small box order limit signature predefine set dimension eliminate scaling angular rotation respect reference signature inadvertently introduce human error hamper performance biometric signature verification system eliminate traditionally user ask sign exactly reference line paper propose robust system optimize signature obtain user large range variation rotation scale translation rst resolve error parameter user signature accord reference signature store database
eess,Audio Watermarking with Error Correction,"  In recent times, communication through the internet has tremendously
facilitated the distribution of multimedia data. Although this is indubitably a
boon, one of its repercussions is that it has also given impetus to the
notorious issue of online music piracy. Unethical attempts can also be made to
deliberately alter such copyrighted data and thus, misuse it. Copyright
violation by means of unauthorized distribution, as well as unauthorized
tampering of copyrighted audio data is an important technological and research
issue. Audio watermarking has been proposed as a solution to tackle this issue.
The main purpose of audio watermarking is to protect against possible threats
to the audio data and in case of copyright violation or unauthorized tampering,
authenticity of such data can be disputed by virtue of audio watermarking.
",recent time communication internet tremendously facilitate distribution multimedia datum indubitably boon repercussion give impetus notorious issue online music piracy unethical attempt deliberately alter copyright datum misuse copyright violation mean unauthorized distribution unauthorized tampering copyright audio data important technological research issue audio watermarking propose solution tackle issue main purpose audio watermarking protect possible threat audio datum case copyright violation unauthorized tampering authenticity datum dispute virtue audio watermarking
eess,"Alternatives with stronger convergence than coordinate-descent iterative
  LMI algorithms","  In this note we aim at putting more emphasis on the fact that trying to solve
non-convex optimization problems with coordinate-descent iterative linear
matrix inequality algorithms leads to suboptimal solutions, and put forward
other optimization methods better equipped to deal with such problems (having
theoretical convergence guarantees and/or being more efficient in practice).
This fact, already outlined at several places in the literature, still appears
to be disregarded by a sizable part of the systems and control community. Thus,
main elements on this issue and better optimization alternatives are presented
and illustrated by means of an example.
",note aim put emphasis fact try solve non convex optimization problem coordinate descent iterative linear matrix inequality algorithms lead suboptimal solution forward optimization method well equip deal problem have theoretical convergence guarantee and/or efficient practice fact outline place literature appear disregard sizable system control community main element issue well optimization alternative present illustrate mean example
eess,"Text-Independent Speaker Recognition for Low SNR Environments with
  Encryption","  Recognition systems are commonly designed to authenticate users at the access
control levels of a system. A number of voice recognition methods have been
developed using a pitch estimation process which are very vulnerable in low
Signal to Noise Ratio (SNR) environments thus, these programs fail to provide
the desired level of accuracy and robustness. Also, most text independent
speaker recognition programs are incapable of coping with unauthorized attempts
to gain access by tampering with the samples or reference database. The
proposed text-independent voice recognition system makes use of multilevel
cryptography to preserve data integrity while in transit or storage. Encryption
and decryption follow a transform based approach layered with pseudorandom
noise addition whereas for pitch detection, a modified version of the
autocorrelation pitch extraction algorithm is used. The experimental results
show that the proposed algorithm can decrypt the signal under test with
exponentially reducing Mean Square Error over an increasing range of SNR.
Further, it outperforms the conventional algorithms in actual identification
tasks even in noisy environments. The recognition rate thus obtained using the
proposed method is compared with other conventional methods used for speaker
identification.
",recognition system commonly design authenticate user access control level system number voice recognition method develop pitch estimation process vulnerable low signal noise ratio snr environment program fail provide desire level accuracy robustness text independent speaker recognition program incapable cope unauthorized attempt gain access tamper sample reference database propose text independent voice recognition system make use multilevel cryptography preserve datum integrity transit storage encryption decryption follow transform base approach layer pseudorandom noise addition pitch detection modify version autocorrelation pitch extraction algorithm experimental result propose algorithm decrypt signal test exponentially reduce mean square error increase range snr outperform conventional algorithm actual identification task noisy environment recognition rate obtain propose method compare conventional method speaker identification
eess,"A robust, low-cost approach to Face Detection and Face Recognition","  In the domain of Biometrics, recognition systems based on iris, fingerprint
or palm print scans etc. are often considered more dependable due to extremely
low variance in the properties of these entities with respect to time. However,
over the last decade data processing capability of computers has increased
manifold, which has made real-time video content analysis possible. This shows
that the need of the hour is a robust and highly automated Face Detection and
Recognition algorithm with credible accuracy rate. The proposed Face Detection
and Recognition system using Discrete Wavelet Transform (DWT) accepts face
frames as input from a database containing images from low cost devices such as
VGA cameras, webcams or even CCTV's, where image quality is inferior. Face
region is then detected using properties of L*a*b* color space and only Frontal
Face is extracted such that all additional background is eliminated. Further,
this extracted image is converted to grayscale and its dimensions are resized
to 128 x 128 pixels. DWT is then applied to entire image to obtain the
coefficients. Recognition is carried out by comparison of the DWT coefficients
belonging to the test image with those of the registered reference image. On
comparison, Euclidean distance classifier is deployed to validate the test
image from the database. Accuracy for various levels of DWT Decomposition is
obtained and hence, compared.
",domain biometrics recognition system base iris fingerprint palm print scan etc consider dependable extremely low variance property entity respect time decade datum processing capability computer increase manifold real time video content analysis possible show need hour robust highly automate face detection recognition algorithm credible accuracy rate propose face detection recognition system discrete wavelet transform dwt accept face frame input database contain image low cost device vga camera webcam cctv image quality inferior face region detect property l*a*b color space frontal face extract additional background eliminate extract image convert grayscale dimension resize 128 x 128 pixel dwt apply entire image obtain coefficient recognition carry comparison dwt coefficient belong test image register reference image comparison euclidean distance classifier deploy validate test image database accuracy level dwt decomposition obtain compare
eess,Linear Consensus Algorithms Based on Balanced Asymmetric Chains,"  Multi agent consensus algorithms with update steps based on so-called
balanced asymmetric chains, are analyzed. For such algorithms it is shown that
(i) the set of accumulation points of states is finite, (ii) the asymptotic
unconditional occurrence of single consensus or multiple consensuses is
directly related to the property of absolute infinite flow for the underlying
update chain. The results are applied to well known consensus models.
",multi agent consensus algorithm update step base call balanced asymmetric chain analyze algorithm show set accumulation point state finite ii asymptotic unconditional occurrence single consensus multiple consensus directly relate property absolute infinite flow underlie update chain result apply know consensus model
eess,"Theorems about Ergodicity and Class-Ergodicity of Chains with
  Applications in Known Consensus Models","  In a multi-agent system, unconditional (multiple) consensus is the property
of reaching to (multiple) consensus irrespective of the instant and values at
which states are initialized. For linear algorithms, occurrence of
unconditional (multiple) consensus turns out to be equivalent to (class-)
ergodicity of the transition chain (A_n). For a wide class of chains, chains
with so-called balanced asymmetry property, necessary and sufficient conditions
for ergodicity and class-ergodicity are derived. The results are employed to
analyze the limiting behavior of agents' states in the JLM model, the Krause
model, and the Cucker-Smale model. In particular, unconditional single or
multiple consensus occurs in all three models. Moreover, a necessary and
sufficient condition for unconditional consensus in the JLM model and a
sufficient condition for consensus in the Cucker-Smale model are obtained.
",multi agent system unconditional multiple consensus property reach multiple consensus irrespective instant value state initialize linear algorithm occurrence unconditional multiple consensus turn equivalent class- ergodicity transition chain a_n wide class chain chain call balanced asymmetry property necessary sufficient condition ergodicity class ergodicity derive result employ analyze limit behavior agent state jlm model krause model cucker smale model particular unconditional single multiple consensus occur model necessary sufficient condition unconditional consensus jlm model sufficient condition consensus cucker smale model obtain
eess,Tracking Tetrahymena Pyriformis Cells using Decision Trees,"  Matching cells over time has long been the most difficult step in cell
tracking. In this paper, we approach this problem by recasting it as a
classification problem. We construct a feature set for each cell, and compute a
feature difference vector between a cell in the current frame and a cell in a
previous frame. Then we determine whether the two cells represent the same cell
over time by training decision trees as our binary classifiers. With the output
of decision trees, we are able to formulate an assignment problem for our cell
association task and solve it using a modified version of the Hungarian
algorithm.
",matching cell time long difficult step cell tracking paper approach problem recast classification problem construct feature set cell compute feature difference vector cell current frame cell previous frame determine cell represent cell time training decision tree binary classifier output decision tree able formulate assignment problem cell association task solve modify version hungarian algorithm
eess,Signal processing with Levy information,"  Levy processes, which have stationary independent increments, are ideal for
modelling the various types of noise that can arise in communication channels.
If a Levy process admits exponential moments, then there exists a parametric
family of measure changes called Esscher transformations. If the parameter is
replaced with an independent random variable, the true value of which
represents a ""message"", then under the transformed measure the original Levy
process takes on the character of an ""information process"". In this paper we
develop a theory of such Levy information processes. The underlying Levy
process, which we call the fiducial process, represents the ""noise type"". Each
such noise type is capable of carrying a message of a certain specification. A
number of examples are worked out in detail, including information processes of
the Brownian, Poisson, gamma, variance gamma, negative binomial, inverse
Gaussian, and normal inverse Gaussian type. Although in general there is no
additive decomposition of information into signal and noise, one is led
nevertheless for each noise type to a well-defined scheme for signal detection
and enhancement relevant to a variety of practical situations.
",levy process stationary independent increment ideal model type noise arise communication channel levy process admit exponential moment exist parametric family measure change call esscher transformation parameter replace independent random variable true value represent message transformed measure original levy process take character information process paper develop theory levy information process underlying levy process fiducial process represent noise type noise type capable carry message certain specification number example work detail include information process brownian poisson gamma variance gamma negative binomial inverse gaussian normal inverse gaussian type general additive decomposition information signal noise lead noise type define scheme signal detection enhancement relevant variety practical situation
eess,"Analysis of a Modern Voice Morphing Approach using Gaussian Mixture
  Models for Laryngectomees","  This paper proposes a voice morphing system for people suffering from
Laryngectomy, which is the surgical removal of all or part of the larynx or the
voice box, particularly performed in cases of laryngeal cancer. A primitive
method of achieving voice morphing is by extracting the source's vocal
coefficients and then converting them into the target speaker's vocal
parameters. In this paper, we deploy Gaussian Mixture Models (GMM) for mapping
the coefficients from source to destination. However, the use of the
traditional/conventional GMM-based mapping approach results in the problem of
over-smoothening of the converted voice. Thus, we hereby propose a unique
method to perform efficient voice morphing and conversion based on GMM,which
overcomes the traditional-method effects of over-smoothening. It uses a
technique of glottal waveform separation and prediction of excitations and
hence the result shows that not only over-smoothening is eliminated but also
the transformed vocal tract parameters match with the target. Moreover, the
synthesized speech thus obtained is found to be of a sufficiently high quality.
Thus, voice morphing based on a unique GMM approach has been proposed and also
critically evaluated based on various subjective and objective evaluation
parameters. Further, an application of voice morphing for Laryngectomees which
deploys this unique approach has been recommended by this paper.
",paper propose voice morph system people suffer laryngectomy surgical removal larynx voice box particularly perform case laryngeal cancer primitive method achieve voice morph extract source vocal coefficient convert target speaker vocal parameter paper deploy gaussian mixture models gmm mapping coefficient source destination use traditional conventional gmm base mapping approach result problem smoothening converted voice propose unique method perform efficient voice morphing conversion base gmm overcome traditional method effect smoothening use technique glottal waveform separation prediction excitation result show smoothen eliminate transformed vocal tract parameter match target synthesized speech obtain find sufficiently high quality voice morph base unique gmm approach propose critically evaluate base subjective objective evaluation parameter application voice morph laryngectomees deploy unique approach recommend paper
eess,"Modeling and performance evaluation of computer systems security
  operation","  A model of computer system security operation is developed based on the
fork-join queueing network formalism. We introduce a security operation
performance measure, and show how it may be used to performance evaluation of
actual systems.
",model computer system security operation develop base fork join queue network formalism introduce security operation performance measure performance evaluation actual system
eess,"Feature Learning in Deep Neural Networks - Studies on Speech Recognition
  Tasks","  Recent studies have shown that deep neural networks (DNNs) perform
significantly better than shallow networks and Gaussian mixture models (GMMs)
on large vocabulary speech recognition tasks. In this paper, we argue that the
improved accuracy achieved by the DNNs is the result of their ability to
extract discriminative internal representations that are robust to the many
sources of variability in speech signals. We show that these representations
become increasingly insensitive to small perturbations in the input with
increasing network depth, which leads to better speech recognition performance
with deeper networks. We also show that DNNs cannot extrapolate to test samples
that are substantially different from the training examples. If the training
data are sufficiently representative, however, internal features learned by the
DNN are relatively stable with respect to speaker differences, bandwidth
differences, and environment distortion. This enables DNN-based recognizers to
perform as well or better than state-of-the-art systems based on GMMs or
shallow networks without the need for explicit model adaptation or feature
normalization.
",recent study show deep neural network dnn perform significantly well shallow network gaussian mixture model gmm large vocabulary speech recognition task paper argue improve accuracy achieve dnn result ability extract discriminative internal representation robust source variability speech signal representation increasingly insensitive small perturbation input increase network depth lead well speech recognition performance deep network dnn extrapolate test sample substantially different training example training datum sufficiently representative internal feature learn dnn relatively stable respect speaker difference bandwidth difference environment distortion enable dnn base recognizer perform well state art system base gmm shallow network need explicit model adaptation feature normalization
eess,Consensus Algorithms and the Decomposition-Separation Theorem,"  Convergence properties of time inhomogeneous Markov chain based discrete and
continuous time linear consensus algorithms are analyzed. Provided that a
so-called infinite jet flow property is satisfied by the underlying chains,
necessary conditions for both consensus and multiple consensus are established.
A recenet extension by Sonin of the classical Kolmogorov-Doeblin
decomposition-separation for homogeneous Markov chains to the inhomogeneous
case is then employed to show that the obtained necessary conditions are also
sufficient when the chain is of Class P*, as defined by Touri and Nedic. It is
also shown that Sonin's theorem leads to a rediscovery and generalization of
most of the existing related consensus results in the literature.
",convergence property time inhomogeneous markov chain base discrete continuous time linear consensus algorithm analyze provide call infinite jet flow property satisfied underlie chain necessary condition consensus multiple consensus establish recenet extension sonin classical kolmogorov doeblin decomposition separation homogeneous markov chain inhomogeneous case employ obtain necessary condition sufficient chain class p define touri nedic show sonin theorem lead rediscovery generalization exist relate consensus result literature
eess,"Comparing Edge Detection Methods based on Stochastic Entropies and
  Distances for PolSAR Imagery","  Polarimetric synthetic aperture radar (PolSAR) has achieved a prominent
position as a remote imaging method. However, PolSAR images are contaminated by
speckle noise due to the coherent illumination employed during the data
acquisition. This noise provides a granular aspect to the image, making its
processing and analysis (such as in edge detection) hard tasks. This paper
discusses seven methods for edge detection in multilook PolSAR images. In all
methods, the basic idea consists in detecting transition points in the finest
possible strip of data which spans two regions. The edge is contoured using the
transitions points and a B-spline curve. Four stochastic distances, two
differences of entropies, and the maximum likelihood criterion were used under
the scaled complex Wishart distribution; the first six stem from the h-phi
class of measures. The performance of the discussed detection methods was
quantified and analyzed by the computational time and probability of correct
edge detection, with respect to the number of looks, the backscatter matrix as
a whole, the SPAN, the covariance an the spatial resolution. The detection
procedures were applied to three real PolSAR images. Results provide evidence
that the methods based on the Bhattacharyya distance and the difference of
Shannon entropies outperform the other techniques.
",polarimetric synthetic aperture radar polsar achieve prominent position remote imaging method polsar image contaminate speckle noise coherent illumination employ datum acquisition noise provide granular aspect image make processing analysis edge detection hard task paper discuss seven method edge detection multilook polsar image method basic idea consist detect transition point fine possible strip datum span region edge contour transition point b spline curve stochastic distance difference entropy maximum likelihood criterion scale complex wishart distribution stem h phi class measure performance discuss detection method quantify analyze computational time probability correct edge detection respect number look backscatter matrix span covariance spatial resolution detection procedure apply real polsar image result provide evidence method base bhattacharyya distance difference shannon entropy outperform technique
eess,Sparse Representation-based Image Quality Assessment,"  A successful approach to image quality assessment involves comparing the
structural information between a distorted and its reference image. However,
extracting structural information that is perceptually important to our visual
system is a challenging task. This paper addresses this issue by employing a
sparse representation-based approach and proposes a new metric called the
\emph{sparse representation-based quality} (SPARQ) \emph{index}. The proposed
method learns the inherent structures of the reference image as a set of basis
vectors, such that any structure in the image can be represented by a linear
combination of only a few of those basis vectors. This sparse strategy is
employed because it is known to generate basis vectors that are qualitatively
similar to the receptive field of the simple cells present in the mammalian
primary visual cortex. The visual quality of the distorted image is estimated
by comparing the structures of the reference and the distorted images in terms
of the learnt basis vectors resembling cortical cells. Our approach is
evaluated on six publicly available subject-rated image quality assessment
datasets. The proposed SPARQ index consistently exhibits high correlation with
the subjective ratings on all datasets and performs better or at par with the
state-of-the-art.
",successful approach image quality assessment involve compare structural information distorted reference image extract structural information perceptually important visual system challenging task paper address issue employ sparse representation base approach propose new metric call \emph{sparse representation base quality sparq \emph{index propose method learn inherent structure reference image set basis vector structure image represent linear combination basis vector sparse strategy employ know generate basis vector qualitatively similar receptive field simple cell present mammalian primary visual cortex visual quality distorted image estimate compare structure reference distorted image term learn basis vector resemble cortical cell approach evaluate publicly available subject rate image quality assessment dataset propose sparq index consistently exhibit high correlation subjective rating dataset perform well par state art
eess,Cognitive Random Stepped Frequency Radar with Sparse Recovery,"  Random stepped frequency (RSF) radar, which transmits random-frequency
pulses, can suppress the range ambiguity, improve convert detection, and
possess excellent electronic counter-countermeasures (ECCM) ability [1]. In
this paper, we apply a sparse recovery method to estimate the range and Doppler
of targets. We also propose a cognitive mechanism for RSF radar to further
enhance the performance of the sparse recovery method. The carrier frequencies
of transmitted pulses are adaptively designed in response to the observed
circumstance. We investigate the criterion to design carrier frequencies, and
efficient methods are then devised. Simulation results demonstrate that the
adaptive frequency-design mechanism significantly improves the performance of
target reconstruction in comparison with the non-adaptive mechanism.
",random step frequency rsf radar transmit random frequency pulse suppress range ambiguity improve convert detection possess excellent electronic counter countermeasure eccm ability 1 paper apply sparse recovery method estimate range doppler target propose cognitive mechanism rsf radar enhance performance sparse recovery method carrier frequency transmit pulse adaptively design response observed circumstance investigate criterion design carrier frequency efficient method devised simulation result demonstrate adaptive frequency design mechanism significantly improve performance target reconstruction comparison non adaptive mechanism
eess,Adaptive matching pursuit for off-grid compressed sensing,"  Compressive sensing (CS) can effectively recover a signal when it is sparse
in some discrete atoms. However, in some applications, signals are sparse in a
continuous parameter space, e.g., frequency space, rather than discrete atoms.
Usually, we divide the continuous parameter into finite discrete grid points
and build a dictionary from these grid points. However, the actual targets may
not exactly lie on the grid points no matter how densely the parameter is
grided, which introduces mismatch between the predefined dictionary and the
actual one. In this article, a novel method, namely adaptive matching pursuit
with constrained total least squares (AMP-CTLS), is proposed to find actual
atoms even if they are not included in the initial dictionary. In AMP-CTLS, the
grid and the dictionary are adaptively updated to better agree with
measurements. The convergence of the algorithm is discussed, and numerical
experiments demonstrate the advantages of AMP-CTLS.
",compressive sensing cs effectively recover signal sparse discrete atom application signal sparse continuous parameter space e.g. frequency space discrete atom usually divide continuous parameter finite discrete grid point build dictionary grid point actual target exactly lie grid point matter densely parameter gride introduce mismatch predefine dictionary actual article novel method adaptive matching pursuit constrain total square amp ctls propose find actual atom include initial dictionary amp ctls grid dictionary adaptively update well agree measurement convergence algorithm discuss numerical experiment demonstrate advantage amp ctls
eess,Sparsity-Promoting Sensor Selection for Non-linear Measurement Models,"  Sensor selection is an important design problem in large-scale sensor
networks. Sensor selection can be interpreted as the problem of selecting the
best subset of sensors that guarantees a certain estimation performance. We
focus on observations that are related to a general non-linear model. The
proposed framework is valid as long as the observations are independent, and
its likelihood satisfies the regularity conditions. We use several functions of
the Cram\'er-Rao bound (CRB) as a performance measure. We formulate the sensor
selection problem as the design of a selection vector, which in its original
form is a nonconvex l0-(quasi) norm optimization problem. We present relaxed
sensor selection solvers that can be efficiently solved in polynomial time. We
also propose a projected subgradient algorithm that is attractive for
large-scale problems and also show how the algorithm can be easily distributed.
The proposed framework is illustrated with a number of examples related to
sensor placement design for localization.
",sensor selection important design problem large scale sensor network sensor selection interpret problem select good subset sensor guarantee certain estimation performance focus observation relate general non linear model propose framework valid long observation independent likelihood satisfy regularity condition use function cram\'er rao bind crb performance measure formulate sensor selection problem design selection vector original form nonconvex l0-(quasi norm optimization problem present relaxed sensor selection solver efficiently solve polynomial time propose project subgradient algorithm attractive large scale problem algorithm easily distribute propose framework illustrate number example relate sensor placement design localization
eess,Eminence Grise Coalitions: On the Shaping of Public Opinion,"  We consider a network of evolving opinions. It includes multiple individuals
with first-order opinion dynamics defined in continuous time and evolving based
on a general exogenously defined time-varying underlying graph. In such a
network, for an arbitrary fixed initial time, a subset of individuals forms an
eminence grise coalition, abbreviated as EGC, if the individuals in that subset
are capable of leading the entire network to agreeing on any desired opinion,
through a cooperative choice of their own initial opinions. In this endeavor,
the coalition members are assumed to have access to full profile of the
underlying graph of the network as well as the initial opinions of all other
individuals. While the complete coalition of individuals always qualifies as an
EGC, we establish the existence of a minimum size EGC for an arbitrary
time-varying network; also, we develop a non-trivial set of upper and lower
bounds on that size. As a result, we show that, even when the underlying graph
does not guarantee convergence to a global or multiple consensus, a generally
restricted coalition of agents can steer public opinion towards a desired
global consensus without affecting any of the predefined graph interactions,
provided they can cooperatively adjust their own initial opinions. Geometric
insights into the structure of EGC's are given. The results are also extended
to the discrete time case where the relation with Decomposition-Separation
Theorem is also made explicit.
",consider network evolve opinion include multiple individual order opinion dynamic define continuous time evolve base general exogenously define time vary underlying graph network arbitrary fix initial time subset individual form eminence grise coalition abbreviate egc individual subset capable lead entire network agree desire opinion cooperative choice initial opinion endeavor coalition member assume access profile underlying graph network initial opinion individual complete coalition individual qualify egc establish existence minimum size egc arbitrary time vary network develop non trivial set upper low bound size result underlie graph guarantee convergence global multiple consensus generally restricted coalition agent steer public opinion desire global consensus affect predefine graph interaction provide cooperatively adjust initial opinion geometric insight structure egc give result extend discrete time case relation decomposition separation theorem explicit
eess,"Cramer-Rao Lower Bounds of Joint Delay-Doppler Estimation for an
  Extended Target","  The problem on the Cramer-Rao Lower Bounds (CRLBs) for the joint time delay
and Doppler stretch estimation of an extended target is considered in this
paper. The integral representations of the CRLBs for both the time delay and
the Doppler stretch are derived. To facilitate computation and analysis, series
representations and approximations of the CRLBs are introduced. According to
these series representations, the impact of several waveform parameters on the
estimation accuracy is investigated, which reveals that the CRLB of the Doppler
stretch is inversely proportional to the effective time-bandwidth product of
the waveform. This conclusion generalizes a previous result in the narrowband
case. The popular wideband ambiguity function (WBAF) based delay-Doppler
estimator is evaluated and compared with the CRLBs through numerical
experiments. Our results indicate that the WBAF estimator, originally derived
from a single scatterer model, is not suitable for the parameter estimation of
an extended target.
",problem cramer rao lower bounds crlb joint time delay doppler stretch estimation extended target consider paper integral representation crlb time delay doppler stretch derive facilitate computation analysis series representation approximation crlb introduce accord series representation impact waveform parameter estimation accuracy investigate reveal crlb doppler stretch inversely proportional effective time bandwidth product waveform conclusion generalize previous result narrowband case popular wideband ambiguity function wbaf base delay doppler estimator evaluate compare crlb numerical experiment result indicate wbaf estimator originally derive single scatterer model suitable parameter estimation extended target
eess,"Dynamic Optimal Power Flow in Microgrids using the Alternating Direction
  Method of Multipliers","  Smart devices, storage and other distributed technologies have the potential
to greatly improve the utilisation of network infrastructure and renewable
generation. Decentralised control of these technologies overcomes many
scalability and privacy concerns, but in general still requires the underlying
problem to be convex in order to guarantee convergence to a global optimum.
Considering that AC power flows are non-convex in nature, and the operation of
household devices often requires discrete decisions, there has been uncertainty
surrounding the use of distributed methods in a realistic setting. This paper
extends prior work on the alternating direction method of multipliers (ADMM)
for solving the dynamic optimal power flow (D-OPF) problem. We utilise more
realistic line and load models, and introduce a two-stage approach to managing
discrete decisions and uncertainty. Our experiments on a suburb-sized microgrid
show that this approach provides near optimal results, in a time that is fast
enough for receding horizon control. This work brings distributed control of
smart-grid technologies closer to reality.
",smart device storage distribute technology potential greatly improve utilisation network infrastructure renewable generation decentralise control technology overcome scalability privacy concern general require underlie problem convex order guarantee convergence global optimum consider ac power flow non convex nature operation household device require discrete decision uncertainty surround use distribute method realistic setting paper extend prior work alternate direction method multiplier admm solve dynamic optimal power flow d opf problem utilise realistic line load model introduce stage approach manage discrete decision uncertainty experiment suburb sized microgrid approach provide near optimal result time fast recede horizon control work bring distribute control smart grid technology close reality
eess,Automatic Photo Adjustment Using Deep Neural Networks,"  Photo retouching enables photographers to invoke dramatic visual impressions
by artistically enhancing their photos through stylistic color and tone
adjustments. However, it is also a time-consuming and challenging task that
requires advanced skills beyond the abilities of casual photographers. Using an
automated algorithm is an appealing alternative to manual work but such an
algorithm faces many hurdles. Many photographic styles rely on subtle
adjustments that depend on the image content and even its semantics. Further,
these adjustments are often spatially varying. Because of these
characteristics, existing automatic algorithms are still limited and cover only
a subset of these challenges. Recently, deep machine learning has shown unique
abilities to address hard problems that resisted machine algorithms for long.
This motivated us to explore the use of deep learning in the context of photo
editing. In this paper, we explain how to formulate the automatic photo
adjustment problem in a way suitable for this approach. We also introduce an
image descriptor that accounts for the local semantics of an image. Our
experiments demonstrate that our deep learning formulation applied using these
descriptors successfully capture sophisticated photographic styles. In
particular and unlike previous techniques, it can model local adjustments that
depend on the image semantics. We show on several examples that this yields
results that are qualitatively and quantitatively better than previous work.
",photo retouching enable photographer invoke dramatic visual impression artistically enhance photo stylistic color tone adjustment time consume challenging task require advanced skill ability casual photographer automate algorithm appealing alternative manual work algorithm face hurdle photographic style rely subtle adjustment depend image content semantic adjustment spatially varying characteristic exist automatic algorithm limited cover subset challenge recently deep machine learning show unique ability address hard problem resist machine algorithm long motivate explore use deep learning context photo editing paper explain formulate automatic photo adjustment problem way suitable approach introduce image descriptor account local semantic image experiment demonstrate deep learning formulation apply descriptor successfully capture sophisticated photographic style particular unlike previous technique model local adjustment depend image semantic example yield result qualitatively quantitatively well previous work
eess,"Implementation of an Automatic Syllabic Division Algorithm from Speech
  Files in Portuguese Language","  A new algorithm for voice automatic syllabic splitting in the Portuguese
language is proposed, which is based on the envelope of the speech signal of
the input audio file. A computational implementation in MatlabTM is presented
and made available at the URL
http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its
straightforwardness, the proposed method is very attractive for embedded
systems (e.g. i-phones). It can also be used as a screen to assist more
sophisticated methods. Voice excerpts containing more than one syllable and
identified by the same envelope are named as super-syllables and they are
subsequently separated. The results indicate which samples corresponds to the
beginning and end of each detected syllable. Preliminary tests were performed
to fifty words at an identification rate circa 70% (further improvements may be
incorporated to treat particular phonemes). This algorithm is also useful in
voice command systems, as a tool in the teaching of Portuguese language or even
for patients with speech pathology.
",new algorithm voice automatic syllabic splitting portuguese language propose base envelope speech signal input audio file computational implementation matlabtm present available url http://www2.ee.ufpe.br/codec/divisao_silabica.html straightforwardness propose method attractive embed system e.g. phone screen assist sophisticated method voice excerpt contain syllable identify envelope name super syllable subsequently separate result indicate sample correspond beginning end detect syllable preliminary test perform word identification rate circa 70 improvement incorporate treat particular phoneme algorithm useful voice command system tool teaching portuguese language patient speech pathology
eess,"A Matrix Laurent Series-based Fast Fourier Transform for Blocklengths
  N=4 (mod 8)","  General guidelines for a new fast computation of blocklength 8m+4 DFTs are
presented, which is based on a Laurent series involving matrices. Results of
non-trivial real multiplicative complexity are presented for blocklengths N=64,
achieving lower multiplication counts than previously published FFTs. A
detailed description for the cases m=1 and m=2 is presented.
",general guideline new fast computation blocklength 8m+4 dft present base laurent series involve matrix result non trivial real multiplicative complexity present blocklength n=64 achieve low multiplication count previously publish fft detailed description case m=1 m=2 present
eess,The Z Transform over Finite Fields,"  Finite field transforms have many applications and, in many cases, can be
implemented with a low computational complexity. In this paper, the Z Transform
over a finite field is introduced and some of its properties are presented.
",finite field transform application case implement low computational complexity paper z transform finite field introduce property present
eess,"A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation
  Recording","  This paper presents a new approach for a vocoder design based on full
frequency masking by octaves in addition to a technique for spectral filling
via beta probability distribution. Some psycho-acoustic characteristics of
human hearing - inaudibility masking in frequency and phase - are used as a
basis for the proposed algorithm. The results confirm that this technique may
be useful to save bandwidth in applications requiring intelligibility. It is
recommended for the legal eavesdropping of long voice conversations.
",paper present new approach vocoder design base frequency mask octave addition technique spectral filling beta probability distribution psycho acoustic characteristic human hearing inaudibility masking frequency phase basis propose algorithm result confirm technique useful save bandwidth application require intelligibility recommend legal eavesdropping long voice conversation
eess,Real-Time Stochastic Optimal Control for Multi-agent Quadrotor Systems,"  This paper presents a novel method for controlling teams of unmanned aerial
vehicles using Stochastic Optimal Control (SOC) theory. The approach consists
of a centralized high-level planner that computes optimal state trajectories as
velocity sequences, and a platform-specific low-level controller which ensures
that these velocity sequences are met. The planning task is expressed as a
centralized path-integral control problem, for which optimal control
computation corresponds to a probabilistic inference problem that can be solved
by efficient sampling methods. Through simulation we show that our SOC approach
(a) has significant benefits compared to deterministic control and other SOC
methods in multimodal problems with noise-dependent optimal solutions, (b) is
capable of controlling a large number of platforms in real-time, and (c) yields
collective emergent behaviour in the form of flight formations. Finally, we
show that our approach works for real platforms, by controlling a team of three
quadrotors in outdoor conditions.
",paper present novel method control team unmanned aerial vehicle stochastic optimal control soc theory approach consist centralized high level planner compute optimal state trajectory velocity sequence platform specific low level controller ensure velocity sequence meet planning task express centralize path integral control problem optimal control computation correspond probabilistic inference problem solve efficient sampling method simulation soc approach significant benefit compare deterministic control soc method multimodal problem noise dependent optimal solution b capable control large number platform real time c yield collective emergent behaviour form flight formation finally approach work real platform control team quadrotor outdoor condition
eess,"A Flexible Implementation of a Matrix Laurent Series-Based 16-Point Fast
  Fourier and Hartley Transforms","  This paper describes a flexible architecture for implementing a new fast
computation of the discrete Fourier and Hartley transforms, which is based on a
matrix Laurent series. The device calculates the transforms based on a single
bit selection operator. The hardware structure and synthesis are presented,
which handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E
device.
",paper describe flexible architecture implement new fast computation discrete fourier hartley transform base matrix laurent series device calculate transform base single bit selection operator hardware structure synthesis present handle 16 point fast transform 65 nsec xilinx spartan 3e device
eess,"Cluster Synchronization of Coupled Systems with Nonidentical Linear
  Dynamics","  This paper considers the cluster synchronization problem of generic linear
dynamical systems whose system models are distinct in different clusters. These
nonidentical linear models render control design and coupling conditions highly
correlated if static couplings are used for all individual systems. In this
paper, a dynamic coupling structure, which incorporates a global weighting
factor and a vanishing auxiliary control variable, is proposed for each agent
and is shown to be a feasible solution. Lower bounds on the global and local
weighting factors are derived under the condition that every interaction
subgraph associated with each cluster admits a directed spanning tree. The
spanning tree requirement is further shown to be a necessary condition when the
clusters connect acyclically with each other. Simulations for two applications,
cluster heading alignment of nonidentical ships and cluster phase
synchronization of nonidentical harmonic oscillators, illustrate essential
parts of the derived theoretical results.
",paper consider cluster synchronization problem generic linear dynamical system system model distinct different cluster nonidentical linear model render control design couple condition highly correlate static coupling individual system paper dynamic coupling structure incorporate global weighting factor vanish auxiliary control variable propose agent show feasible solution low bound global local weighting factor derive condition interaction subgraph associate cluster admit direct span tree span tree requirement show necessary condition cluster connect acyclically simulation application cluster heading alignment nonidentical ship cluster phase synchronization nonidentical harmonic oscillator illustrate essential part derive theoretical result
eess,"New Algorithms for Computing a Single Component of the Discrete Fourier
  Transform","  This paper introduces the theory and hardware implementation of two new
algorithms for computing a single component of the discrete Fourier transform.
In terms of multiplicative complexity, both algorithms are more efficient, in
general, than the well known Goertzel Algorithm.
",paper introduce theory hardware implementation new algorithm compute single component discrete fourier transform term multiplicative complexity algorithm efficient general know goertzel algorithm
eess,The Discrete Cosine Transform over Prime Finite Fields,"  This paper examines finite field trigonometry as a tool to construct
trigonometric digital transforms. In particular, by using properties of the
k-cosine function over GF(p), the Finite Field Discrete Cosine Transform
(FFDCT) is introduced. The FFDCT pair in GF(p) is defined, having blocklengths
that are divisors of (p+1)/2. A special case is the Mersenne FFDCT, defined
when p is a Mersenne prime. In this instance blocklengths that are powers of
two are possible and radix-2 fast algorithms can be used to compute the
transform.
",paper examine finite field trigonometry tool construct trigonometric digital transform particular property k cosine function gf(p finite field discrete cosine transform ffdct introduce ffdct pair gf(p define have blocklength divisor p+1)/2 special case mersenne ffdct define p mersenne prime instance blocklength power possible radix-2 fast algorithm compute transform
eess,Video Inpainting of Complex Scenes,"  We propose an automatic video inpainting algorithm which relies on the
optimisation of a global, patch-based functional. Our algorithm is able to deal
with a variety of challenging situations which naturally arise in video
inpainting, such as the correct reconstruction of dynamic textures, multiple
moving objects and moving background. Furthermore, we achieve this in an order
of magnitude less execution time with respect to the state-of-the-art. We are
also able to achieve good quality results on high definition videos. Finally,
we provide specific algorithmic details to make implementation of our algorithm
as easy as possible. The resulting algorithm requires no segmentation or manual
input other than the definition of the inpainting mask, and can deal with a
wider variety of situations than is handled by previous work. 1. Introduction.
Advanced image and video editing techniques are increasingly common in the
image processing and computer vision world, and are also starting to be used in
media entertainment. One common and difficult task closely linked to the world
of video editing is image and video "" inpainting "". Generally speaking, this is
the task of replacing the content of an image or video with some other content
which is visually pleasing. This subject has been extensively studied in the
case of images, to such an extent that commercial image inpainting products
destined for the general public are available, such as Photoshop's "" Content
Aware fill "" [1]. However, while some impressive results have been obtained in
the case of videos, the subject has been studied far less extensively than
image inpainting. This relative lack of research can largely be attributed to
high time complexity due to the added temporal dimension. Indeed, it has only
very recently become possible to produce good quality inpainting results on
high definition videos, and this only in a semi-automatic manner. Nevertheless,
high-quality video inpainting has many important and useful applications such
as film restoration, professional post-production in cinema and video editing
for personal use. For this reason, we believe that an automatic, generic video
inpainting algorithm would be extremely useful for both academic and
professional communities.
",propose automatic video inpainte algorithm rely optimisation global patch base functional algorithm able deal variety challenge situation naturally arise video inpainting correct reconstruction dynamic texture multiple moving object move background furthermore achieve order magnitude execution time respect state art able achieve good quality result high definition video finally provide specific algorithmic detail implementation algorithm easy possible result algorithm require segmentation manual input definition inpainte mask deal wide variety situation handle previous work 1 introduction advanced image video editing technique increasingly common image processing computer vision world start medium entertainment common difficult task closely link world video editing image video inpainte generally speak task replace content image video content visually pleasing subject extensively study case image extent commercial image inpainte product destine general public available photoshop content aware fill 1 impressive result obtain case video subject study far extensively image inpainte relative lack research largely attribute high time complexity add temporal dimension recently possible produce good quality inpainte result high definition video semi automatic manner high quality video inpainting important useful application film restoration professional post production cinema video editing personal use reason believe automatic generic video inpainte algorithm extremely useful academic professional community
eess,"Global stabilization of multiple integrators by a bounded feedback with
  constraints on its successive derivatives","  In this paper, we address the global stabilization of chains of integrators
by means of a bounded static feedback law whose p first time derivatives are
bounded. Our construction is based on the technique of nested saturations
introduced by Teel. We show that the control amplitude and the maximum value of
its p first derivatives can be imposed below any prescribed values. Our results
are illustrated by the stabilization of the third order integrator on the
feedback and its first two derivatives.
",paper address global stabilization chain integrator mean bound static feedback law p time derivative bound construction base technique nested saturation introduce teel control amplitude maximum value p derivative impose prescribed value result illustrate stabilization order integrator feedback derivative
eess,"Energy-efficient hybrid spintronic-straintronic reconfigurable bit
  comparator","  We propose a reconfigurable bit comparator implemented with a nanowire spin
valve whose two contacts are magnetostrictive with bistable magnetization.
Reference and input bits are ""written"" into the magnetization states of the two
contacts with electrically generated strain and the spin-valve's resistance is
lowered if they match. Multiple comparators can be interfaced in parallel with
a magneto-tunneling junction to determine if an N-bit input stream matches an
N-bit reference stream bit by bit. The system is robust against thermal noise
at room temperature and a 16-bit comparator can operate at roughly 416 MHz
while dissipating at most 420 aJ per cycle.
",propose reconfigurable bit comparator implement nanowire spin valve contact magnetostrictive bistable magnetization reference input bit write magnetization state contact electrically generate strain spin valve resistance lower match multiple comparator interface parallel magneto tunnel junction determine n bit input stream match n bit reference stream bit bit system robust thermal noise room temperature 16 bit comparator operate roughly 416 mhz dissipate 420 aj cycle
eess,"Multiscale edge detection and parametric shape modeling for boundary
  delineation in optoacoustic images","  In this article, we present a novel scheme for segmenting the image boundary
(with the background) in optoacoustic small animal in vivo imaging systems. The
method utilizes a multiscale edge detection algorithm to generate a binary edge
map. A scale dependent morphological operation is employed to clean spurious
edges. Thereafter, an ellipse is fitted to the edge map through constrained
parametric transformations and iterative goodness of fit calculations. The
method delimits the tissue edges through the curve fitting model, which has
shown high levels of accuracy. Thus, this method enables segmentation of
optoacoutic images with minimal human intervention, by eliminating need of
scale selection for multiscale processing and seed point determination for
contour mapping.
",article present novel scheme segment image boundary background optoacoustic small animal vivo imaging system method utilize multiscale edge detection algorithm generate binary edge map scale dependent morphological operation employ clean spurious edge ellipse fit edge map constrain parametric transformation iterative goodness fit calculation method delimit tissue edge curve fitting model show high level accuracy method enable segmentation optoacoutic image minimal human intervention eliminate need scale selection multiscale processing seed point determination contour mapping
eess,"Low PMEPR OFDM radar waveform design using the iterative least squares
  algorithm","  This letter considers waveform design of orthogonal frequency division
multiplexing (OFDM) signal for radar applications, and aims at mitigating the
envelope fluctuation in OFDM. A novel method is proposed to reduce the
peak-to-mean envelope power ratio (PMEPR), which is commonly used to evaluate
the fluctuation. The proposed method is based on the tone reservation approach,
in which some bits or subcarriers of OFDM are allocated for decreasing PMEPR.
We introduce the coefficient of variation of envelopes (CVE) as the cost
function for waveform optimization, and develop an iterative least squares
algorithm. Minimizing CVE leads to distinct PMEPR reduction, and it is
guaranteed that the cost function monotonically decreases by applying the
iterative algorithm. Simulations demonstrate that the envelope is significantly
smoothed by the proposed method.
",letter consider waveform design orthogonal frequency division multiplexing ofdm signal radar application aim mitigate envelope fluctuation ofdm novel method propose reduce peak mean envelope power ratio pmepr commonly evaluate fluctuation propose method base tone reservation approach bit subcarrier ofdm allocate decrease pmepr introduce coefficient variation envelope cve cost function waveform optimization develop iterative square algorithm minimize cve lead distinct pmepr reduction guarantee cost function monotonically decrease apply iterative algorithm simulation demonstrate envelope significantly smooth propose method
eess,"An Iterative Receiver for OFDM With Sparsity-Based Parametric Channel
  Estimation","  In this work we design a receiver that iteratively passes soft information
between the channel estimation and data decoding stages. The receiver
incorporates sparsity-based parametric channel estimation. State-of-the-art
sparsity-based iterative receivers simplify the channel estimation problem by
restricting the multipath delays to a grid. Our receiver does not impose such a
restriction. As a result it does not suffer from the leakage effect, which
destroys sparsity. Communication at near capacity rates in high SNR requires a
large modulation order. Due to the close proximity of modulation symbols in
such systems, the grid-based approximation is of insufficient accuracy. We show
numerically that a state-of-the-art iterative receiver with grid-based sparse
channel estimation exhibits a bit-error-rate floor in the high SNR regime. On
the contrary, our receiver performs very close to the perfect channel state
information bound for all SNR values. We also demonstrate both theoretically
and numerically that parametric channel estimation works well in dense
channels, i.e., when the number of multipath components is large and each
individual component cannot be resolved.
",work design receiver iteratively pass soft information channel estimation datum decode stage receiver incorporate sparsity base parametric channel estimation state art sparsity base iterative receiver simplify channel estimation problem restrict multipath delay grid receiver impose restriction result suffer leakage effect destroy sparsity communication near capacity rate high snr require large modulation order close proximity modulation symbol system grid base approximation insufficient accuracy numerically state art iterative receiver grid base sparse channel estimation exhibit bit error rate floor high snr regime contrary receiver perform close perfect channel state information bind snr value demonstrate theoretically numerically parametric channel estimation work dense channel i.e. number multipath component large individual component resolve
eess,A Pessimistic Approximation for the Fisher Information Measure,"  The problem of determining the intrinsic quality of a signal processing
system with respect to the inference of an unknown deterministic parameter
$\theta$ is considered. While the Fisher information measure $F(\theta)$ forms
a classical tool for such a problem, direct computation of the information
measure can become difficult in various situations. For the estimation
theoretic performance analysis of nonlinear measurement systems, the form of
the likelihood function can make the calculation of the information measure
$F(\theta)$ challenging. In situations where no closed-form expression of the
statistical system model is available, the analytical derivation of $F(\theta)$
is not possible at all. Based on the Cauchy-Schwarz inequality, we derive an
alternative information measure $S(\theta)$. It provides a lower bound on the
Fisher information $F(\theta)$ and has the property of being evaluated with the
mean, the variance, the skewness and the kurtosis of the system model at hand.
These entities usually exhibit good mathematical tractability or can be
determined at low-complexity by real-world measurements in a calibrated setup.
With various examples, we show that $S(\theta)$ provides a good conservative
approximation for $F(\theta)$ and outline different estimation theoretic
problems where the presented information bound turns out to be useful.
",problem determine intrinsic quality signal processing system respect inference unknown deterministic parameter $ \theta$ consider fisher information measure $ f(\theta)$ form classical tool problem direct computation information measure difficult situation estimation theoretic performance analysis nonlinear measurement system form likelihood function calculation information measure $ f(\theta)$ challenging situation closed form expression statistical system model available analytical derivation $ f(\theta)$ possible base cauchy schwarz inequality derive alternative information measure $ s(\theta)$. provide low bind fisher information $ f(\theta)$ property evaluate mean variance skewness kurtosis system model hand entity usually exhibit good mathematical tractability determine low complexity real world measurement calibrated setup example $ s(\theta)$ provide good conservative approximation $ f(\theta)$ outline different estimation theoretic problem present information bind turn useful
eess,Dual-Layer Video Encryption using RSA Algorithm,"  This paper proposes a video encryption algorithm using RSA and Pseudo Noise
(PN) sequence, aimed at applications requiring sensitive video information
transfers. The system is primarily designed to work with files encoded using
the Audio Video Interleaved (AVI) codec, although it can be easily ported for
use with Moving Picture Experts Group (MPEG) encoded files. The audio and video
components of the source separately undergo two layers of encryption to ensure
a reasonable level of security. Encryption of the video component involves
applying the RSA algorithm followed by the PN-based encryption. Similarly, the
audio component is first encrypted using PN and further subjected to encryption
using the Discrete Cosine Transform. Combining these techniques, an efficient
system, invulnerable to security breaches and attacks with favorable values of
parameters such as encryption/decryption speed, encryption/decryption ratio and
visual degradation; has been put forth. For applications requiring encryption
of sensitive data wherein stringent security requirements are of prime concern,
the system is found to yield negligible similarities in visual perception
between the original and the encrypted video sequence. For applications wherein
visual similarity is not of major concern, we limit the encryption task to a
single level of encryption which is accomplished by using RSA, thereby
quickening the encryption process. Although some similarity between the
original and encrypted video is observed in this case, it is not enough to
comprehend the happenings in the video.
",paper propose video encryption algorithm rsa pseudo noise pn sequence aim application require sensitive video information transfer system primarily design work file encode audio video interleaved avi codec easily port use moving picture experts group mpeg encode file audio video component source separately undergo layer encryption ensure reasonable level security encryption video component involve apply rsa algorithm follow pn base encryption similarly audio component encrypt pn subject encryption discrete cosine transform combine technique efficient system invulnerable security breach attack favorable value parameter encryption decryption speed encryption decryption ratio visual degradation forth application require encryption sensitive datum stringent security requirement prime concern system find yield negligible similarity visual perception original encrypt video sequence application visual similarity major concern limit encryption task single level encryption accomplish rsa quicken encryption process similarity original encrypted video observe case comprehend happening video
eess,"Visual Quality Enhancement in Optoacoustic Tomography using Active
  Contour Segmentation Priors","  Segmentation of biomedical images is essential for studying and
characterizing anatomical structures, detection and evaluation of pathological
tissues. Segmentation has been further shown to enhance the reconstruction
performance in many tomographic imaging modalities by accounting for
heterogeneities of the excitation field and tissue properties in the imaged
region. This is particularly relevant in optoacoustic tomography, where
discontinuities in the optical and acoustic tissue properties, if not properly
accounted for, may result in deterioration of the imaging performance.
Efficient segmentation of optoacoustic images is often hampered by the
relatively low intrinsic contrast of large anatomical structures, which is
further impaired by the limited angular coverage of some commonly employed
tomographic imaging configurations. Herein, we analyze the performance of
active contour models for boundary segmentation in cross-sectional optoacoustic
tomography. The segmented mask is employed to construct a two compartment model
for the acoustic and optical parameters of the imaged tissues, which is
subsequently used to improve accuracy of the image reconstruction routines. The
performance of the suggested segmentation and modeling approach are showcased
in tissue-mimicking phantoms and small animal imaging experiments.
",segmentation biomedical image essential study characterize anatomical structure detection evaluation pathological tissue segmentation show enhance reconstruction performance tomographic imaging modality account heterogeneity excitation field tissue property image region particularly relevant optoacoustic tomography discontinuity optical acoustic tissue property properly account result deterioration imaging performance efficient segmentation optoacoustic image hamper relatively low intrinsic contrast large anatomical structure impair limited angular coverage commonly employ tomographic imaging configuration analyze performance active contour model boundary segmentation cross sectional optoacoustic tomography segmented mask employ construct compartment model acoustic optical parameter imaged tissue subsequently improve accuracy image reconstruction routine performance suggest segmentation modeling approach showcase tissue mimic phantom small animal imaging experiment
eess,"Prediction-Adaptation-Correction Recurrent Neural Networks for
  Low-Resource Language Speech Recognition","  In this paper, we investigate the use of prediction-adaptation-correction
recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A
PAC-RNN is comprised of a pair of neural networks in which a {\it correction}
network uses auxiliary information given by a {\it prediction} network to help
estimate the state probability. The information from the correction network is
also used by the prediction network in a recurrent loop. Our model outperforms
other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.
Moreover, transfer learning from a language that is similar to the target
language can help improve performance further.
",paper investigate use prediction adaptation correction recurrent neural network pac rnns low resource speech recognition pac rnn comprise pair neural network \it correction network use auxiliary information give \it prediction network help estimate state probability information correction network prediction network recurrent loop model outperform state art neural network dnn lstm iarpa babel task transfer learning language similar target language help improve performance
eess,Highway Long Short-Term Memory RNNs for Distant Speech Recognition,"  In this paper, we extend the deep long short-term memory (DLSTM) recurrent
neural networks by introducing gated direct connections between memory cells in
adjacent layers. These direct links, called highway connections, enable
unimpeded information flow across different layers and thus alleviate the
gradient vanishing problem when building deeper LSTMs. We further introduce the
latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole
history while keeping the latency under control. Efficient algorithms are
proposed to train these novel networks using both frame and sequence
discriminative criteria. Experiments on the AMI distant speech recognition
(DSR) task indicate that we can train deeper LSTMs and achieve better
improvement from sequence training with highway LSTMs (HLSTMs). Our novel model
obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all
previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and
$5.3\%$ relative improvement respectively.
",paper extend deep long short term memory dlstm recurrent neural network introduce gate direct connection memory cell adjacent layer direct link call highway connection enable unimpeded information flow different layer alleviate gradient vanishing problem build deep lstm introduce latency control bidirectional lstm blstm exploit history keep latency control efficient algorithm propose train novel network frame sequence discriminative criterion experiment ami distant speech recognition dsr task indicate train deep lstm achieve well improvement sequence training highway lstm hlstm novel model obtain $ 43.9/47.7\%$ wer ami sdm dev eval set outperform previous work beat strong dnn dlstm baseline $ 15.7\%$ $ 5.3\%$ relative improvement respectively
eess,"On Computational Complexity Reduction Methods for Kalman Filter
  Extensions","  The Kalman filter and its extensions are used in a vast number of aerospace
and navigation applications for nonlinear state estimation of time series. In
the literature, different approaches have been proposed to exploit the
structure of the state and measurement models to reduce the computational
demand of the algorithms. In this tutorial, we survey existing code
optimization methods and present them using unified notation that allows them
to be used with various Kalman filter extensions. We develop the optimization
methods to cover a wider range of models, show how different structural
optimizations can be combined, and present new applications for the existing
optimizations. Furthermore, we present an example that shows that the
exploitation of the structure of the problem can lead to improved estimation
accuracy while reducing the computational load. This tutorial is intended for
persons who are familiar with Kalman filtering and want to get insights for
reducing the computational demand of different Kalman filter extensions.
",kalman filter extension vast number aerospace navigation application nonlinear state estimation time series literature different approach propose exploit structure state measurement model reduce computational demand algorithm tutorial survey exist code optimization method present unified notation allow kalman filter extension develop optimization method cover wide range model different structural optimization combine present new application exist optimization furthermore present example show exploitation structure problem lead improved estimation accuracy reduce computational load tutorial intend person familiar kalman filtering want insight reduce computational demand different kalman filter extension
eess,"Sensitivity Analysis for Binary Sampling Systems via Quantitative Fisher
  Information Lower Bounds","  The problem of determining the achievable sensitivity with digitization
exhibiting minimal complexity is addressed. In this case, measurements are
exclusively available in hard-limited form. Assessing the achievable
sensitivity via the Cram\'{e}r-Rao lower bound requires characterization of the
likelihood function, which is intractable for multivariate binary
distributions. In this context, the Fisher matrix of the exponential family and
a lower bound for arbitrary probabilistic models are discussed. The
conservative approximation for Fisher's information matrix rests on a surrogate
exponential family distribution connected to the actual data-generating system
by two compact equivalences. Without characterizing the likelihood and its
support, this probabilistic notion enables designing estimators that
consistently achieve the sensitivity as defined by the inverse of the
conservative information matrix. For parameter estimation with multivariate
binary samples, a quadratic exponential surrogate distribution tames
statistical complexity such that a quantitative assessment of an achievable
sensitivity level becomes tractable. This fact is exploited for the performance
analysis concerning parameter estimation with an array of low-complexity binary
sensors in comparison to an ideal system featuring infinite amplitude
resolution. Additionally, data-driven assessment by estimating a conservative
approximation for the Fisher matrix under recursive binary sampling as
implemented in $\Sigma\Delta$-modulating analog-to-digital converters is
demonstrated.
",problem determine achievable sensitivity digitization exhibit minimal complexity address case measurement exclusively available hard limited form assess achievable sensitivity cram\'{e}r rao lower bind require characterization likelihood function intractable multivariate binary distribution context fisher matrix exponential family lower bind arbitrary probabilistic model discuss conservative approximation fisher information matrix rest surrogate exponential family distribution connect actual data generate system compact equivalence characterize likelihood support probabilistic notion enable design estimator consistently achieve sensitivity define inverse conservative information matrix parameter estimation multivariate binary sample quadratic exponential surrogate distribution tame statistical complexity quantitative assessment achievable sensitivity level tractable fact exploit performance analysis concern parameter estimation array low complexity binary sensor comparison ideal system feature infinite amplitude resolution additionally data drive assessment estimate conservative approximation fisher matrix recursive binary sampling implement $ \sigma\delta$-modulate analog digital converter demonstrate
eess,"Plug-and-Play Priors for Bright Field Electron Tomography and Sparse
  Interpolation","  Many material and biological samples in scientific imaging are characterized
by non-local repeating structures. These are studied using scanning electron
microscopy and electron tomography. Sparse sampling of individual pixels in a
2D image acquisition geometry, or sparse sampling of projection images with
large tilt increments in a tomography experiment, can enable high speed data
acquisition and minimize sample damage caused by the electron beam.
  In this paper, we present an algorithm for electron tomographic
reconstruction and sparse image interpolation that exploits the non-local
redundancy in images. We adapt a framework, termed plug-and-play (P&P) priors,
to solve these imaging problems in a regularized inversion setting. The power
of the P&P approach is that it allows a wide array of modern denoising
algorithms to be used as a ""prior model"" for tomography and image
interpolation. We also present sufficient mathematical conditions that ensure
convergence of the P&P approach, and we use these insights to design a new
non-local means denoising algorithm. Finally, we demonstrate that the algorithm
produces higher quality reconstructions on both simulated and real electron
microscope data, along with improved convergence properties compared to other
methods.
",material biological sample scientific imaging characterize non local repeat structure study scan electron microscopy electron tomography sparse sample individual pixel 2d image acquisition geometry sparse sampling projection image large tilt increment tomography experiment enable high speed datum acquisition minimize sample damage cause electron beam paper present algorithm electron tomographic reconstruction sparse image interpolation exploit non local redundancy image adapt framework term plug play p&p prior solve imaging problem regularized inversion setting power p&p approach allow wide array modern denoising algorithm prior model tomography image interpolation present sufficient mathematical condition ensure convergence p&p approach use insight design new non local mean denoise algorithm finally demonstrate algorithm produce high quality reconstruction simulated real electron microscope datum improved convergence property compare method
eess,"Privacy, Secrecy, and Storage with Multiple Noisy Measurements of
  Identifiers","  The key-leakage-storage region is derived for a generalization of a classic
two-terminal key agreement model. The additions to the model are that the
encoder observes a hidden, or noisy, version of the identifier, and that the
encoder and decoder can perform multiple measurements. To illustrate the
behavior of the region, the theory is applied to binary identifiers and noise
modeled via binary symmetric channels. In particular, the key-leakage-storage
region is simplified by applying Mrs. Gerber's lemma twice in different
directions to a Markov chain. The growth in the region as the number of
measurements increases is quantified. The amount by which the privacy-leakage
rate reduces for a hidden identifier as compared to a noise-free (visible)
identifier at the encoder is also given. If the encoder incorrectly models the
source as visible, it is shown that substantial secrecy leakage may occur and
the reliability of the reconstructed key might decrease.
",key leakage storage region derive generalization classic terminal key agreement model addition model encoder observe hide noisy version identifi encoder decoder perform multiple measurement illustrate behavior region theory apply binary identifier noise model binary symmetric channel particular key leakage storage region simplify apply mrs. gerber lemma twice different direction markov chain growth region number measurement increase quantify privacy leakage rate reduce hidden identifier compare noise free visible identifi encoder give encoder incorrectly model source visible show substantial secrecy leakage occur reliability reconstructed key decrease
eess,Analysis of Random Pulse Repetition Interval Radar,"  Random pulse repetition interval (PRI) waveform arouses great interests in
the field of modern radars due to its ability to alleviate range and Doppler
ambiguities as well as enhance electronic counter-countermeasures (ECCM)
capabilities. Theoretical results pertaining to the statistical characteristics
of ambiguity function (AF) are derived in this work, indicating that the range
and Doppler ambiguities can be effectively suppressed by increasing the number
of pulses and the range of PRI jitters. This provides an important guidance in
terms of waveform design. As is well known, the significantly lifted sidelobe
pedestal induced by PRI randomization will degrade the performance of weak
target detection. Proceeding from that, we propose to employ orthogonal
matching pursuit (OMP) to overcome this issue. Simulation results demonstrate
that the OMP method can effectively lower the sidelobe pedestal of strong
target and improve the performance of weak target estimation.
",random pulse repetition interval pri waveform arouse great interest field modern radar ability alleviate range doppler ambiguity enhance electronic counter countermeasure eccm capability theoretical result pertain statistical characteristic ambiguity function af derive work indicate range doppler ambiguity effectively suppress increase number pulse range pri jitter provide important guidance term waveform design know significantly lift sidelobe pedestal induce pri randomization degrade performance weak target detection proceed propose employ orthogonal matching pursuit omp overcome issue simulation result demonstrate omp method effectively lower sidelobe pedestal strong target improve performance weak target estimation
eess,"Stochastic Battery Model for Aggregation of Thermostatically Controlled
  Loads","  The potential of demand side as a frequency reserve proposes interesting
opportunity in handling imbalances due to intermittent renewable energy
sources. This paper proposes a novel approach for computing the parameters of a
stochastic battery model representing the aggregation of Thermostatically
Controlled Loads (TCLs). A hysteresis based non-disruptive control is used
using priority stack algorithm to track the reference regulation signal. The
parameters of admissible ramp-rate and the charge limits of the battery are
dynamically calculated using the information from TCLs that is the status
(on/off), availability and relative temperature distance till the switching
boundary. The approach builds on and improves on the existing research work by
providing a straight-forward mechanism for calculation of stochastic parameters
of equivalent battery model. The effectiveness of proposed approach is
demonstrated by a test case having a large number of residential TCLs tracking
a scaled down real frequency regulation signal.
",potential demand frequency reserve propose interesting opportunity handle imbalance intermittent renewable energy source paper propose novel approach compute parameter stochastic battery model represent aggregation thermostatically controlled loads tcls hysteresis base non disruptive control priority stack algorithm track reference regulation signal parameter admissible ramp rate charge limit battery dynamically calculate information tcl status availability relative temperature distance till switching boundary approach build improve exist research work provide straight forward mechanism calculation stochastic parameter equivalent battery model effectiveness propose approach demonstrate test case have large number residential tcl tracking scale real frequency regulation signal
eess,Voltage stress minimization by optimal reactive power control,"  A standard operational requirement in power systems is that the voltage
magnitudes lie within prespecified bounds. Conventional engineering wisdom
suggests that such a tightly-regulated profile, imposed for system design
purposes and good operation of the network, should also guarantee a secure
system, operating far from static bifurcation instabilities such as voltage
collapse. In general however, these two objectives are distinct and must be
separately enforced. We formulate an optimization problem which maximizes the
distance to voltage collapse through injections of reactive power, subject to
power flow and operational voltage constraints. By exploiting a linear
approximation of the power flow equations we arrive at a convex reformulation
which can be efficiently solved for the optimal injections. We also address the
planning problem of allocating the resources by recasting our problem in a
sparsity-promoting framework that allows us to choose a desired trade-off
between optimality of injections and the number of required actuators. Finally,
we present a distributed algorithm to solve the optimization problem, showing
that it can be implemented on-line as a feedback controller. We illustrate the
performance of our results with the IEEE30 bus network.
",standard operational requirement power system voltage magnitude lie prespecified bound conventional engineering wisdom suggest tightly regulate profile impose system design purpose good operation network guarantee secure system operate far static bifurcation instability voltage collapse general objective distinct separately enforce formulate optimization problem maximize distance voltage collapse injection reactive power subject power flow operational voltage constraint exploit linear approximation power flow equation arrive convex reformulation efficiently solve optimal injection address planning problem allocate resource recast problem sparsity promote framework allow choose desire trade optimality injection number require actuator finally present distribute algorithm solve optimization problem show implement line feedback controller illustrate performance result ieee30 bus network
eess,"Grading of Mammalian Cumulus Oocyte Complexes using Machine Learning for
  in Vitro Embryo Culture","  Visual observation of Cumulus Oocyte Complexes provides only limited
information about its functional competence, whereas the molecular evaluations
methods are cumbersome or costly. Image analysis of mammalian oocytes can
provide attractive alternative to address this challenge. However, it is
complex, given the huge number of oocytes under inspection and the subjective
nature of the features inspected for identification. Supervised machine
learning methods like random forest with annotations from expert biologists can
make the analysis task standardized and reduces inter-subject variability. We
present a semi-automatic framework for predicting the class an oocyte belongs
to, based on multi-object parametric segmentation on the acquired microscopic
image followed by a feature based classification using random forests.
",visual observation cumulus oocyte complexes provide limited information functional competence molecular evaluation method cumbersome costly image analysis mammalian oocyte provide attractive alternative address challenge complex give huge number oocyte inspection subjective nature feature inspect identification supervise machine learn method like random forest annotation expert biologist analysis task standardize reduce inter subject variability present semi automatic framework predict class oocyte belong base multi object parametric segmentation acquire microscopic image follow feature base classification random forest
eess,Subsampling for Graph Power Spectrum Estimation,"  In this paper we focus on subsampling stationary random processes that reside
on the vertices of undirected graphs. Second-order stationary graph signals are
obtained by filtering white noise and they admit a well-defined power spectrum.
Estimating the graph power spectrum forms a central component of stationary
graph signal processing and related inference tasks. We show that by sampling a
significantly smaller subset of vertices and using simple least squares, we can
reconstruct the power spectrum of the graph signal from the subsampled
observations, without any spectral priors. In addition, a near-optimal greedy
algorithm is developed to design the subsampling scheme.
",paper focus subsample stationary random process reside vertex undirected graph second order stationary graph signal obtain filter white noise admit define power spectrum estimate graph power spectrum form central component stationary graph signal processing related inference task sample significantly small subset vertex simple square reconstruct power spectrum graph signal subsample observation spectral prior addition near optimal greedy algorithm develop design subsample scheme
eess,"A pairwise approach to simultaneous onset/offset detection for singing
  voice using correntropy","  In this paper, we propose a novelmethod to search for precise locations of
paired note onset and offset in a singing voice signal. In comparison with the
existing onset detection algorithms,our approach differs in two key respects.
First, we employ Correntropy, a generalized correlation function inspired from
Reyni's entropy, as a detection function to capture the instantaneous flux
while preserving insensitiveness to outliers. Next, a novel peak picking
algorithm is specially designed for this detection function. By calculating the
fitness of a pre-defined inverse hyperbolic kernel to a detection function, it
is possible to find an onset and its corresponding offset simultaneously.
Experimental results show that the proposed method achieves performance
significantly better than or comparable to other state-of-the-art techniques
for onset detection in singing voice.
",paper propose novelmethod search precise location pair note onset offset singing voice signal comparison exist onset detection algorithm approach differ key respect employ correntropy generalized correlation function inspire reyni entropy detection function capture instantaneous flux preserve insensitiveness outlier novel peak pick algorithm specially design detection function calculate fitness pre define inverse hyperbolic kernel detection function possible find onset corresponding offset simultaneously experimental result propose method achieve performance significantly well comparable state art technique onset detection singe voice
eess,"Extended Object Tracking: Introduction, Overview and Applications","  This article provides an elaborate overview of current research in extended
object tracking. We provide a clear definition of the extended object tracking
problem and discuss its delimitation to other types of object tracking. Next,
different aspects of extended object modelling are extensively discussed.
Subsequently, we give a tutorial introduction to two basic and well used
extended object tracking approaches - the random matrix approach and the Kalman
filter-based approach for star-convex shapes. The next part treats the tracking
of multiple extended objects and elaborates how the large number of feasible
association hypotheses can be tackled using both Random Finite Set (RFS) and
Non-RFS multi-object trackers. The article concludes with a summary of current
applications, where four example applications involving camera, X-band radar,
light detection and ranging (lidar), red-green-blue-depth (RGB-D) sensors are
highlighted.
",article provide elaborate overview current research extend object tracking provide clear definition extend object tracking problem discuss delimitation type object tracking different aspect extend object modelling extensively discuss subsequently tutorial introduction basic extend object tracking approach random matrix approach kalman filter base approach star convex shape treat tracking multiple extended object elaborate large number feasible association hypothesis tackle random finite set rfs non rfs multi object tracker article conclude summary current application example application involve camera x band radar light detection range lidar red green blue depth rgb d sensor highlight
eess,"Brain Emotional Learning-Based Prediction Model (For Long-Term Chaotic
  Prediction Applications)","  This study suggests a new prediction model for chaotic time series inspired
by the brain emotional learning of mammals. We describe the structure and
function of this model, which is referred to as BELPM (Brain Emotional
Learning-Based Prediction Model). Structurally, the model mimics the connection
between the regions of the limbic system, and functionally it uses weighted k
nearest neighbors to imitate the roles of those regions. The learning algorithm
of BELPM is defined using steepest descent (SD) and the least square estimator
(LSE). Two benchmark chaotic time series, Lorenz and Henon, have been used to
evaluate the performance of BELPM. The obtained results have been compared with
those of other prediction methods. The results show that BELPM has the
capability to achieve a reasonable accuracy for long-term prediction of chaotic
time series, using a limited amount of training data and a reasonably low
computational time.
",study suggest new prediction model chaotic time series inspire brain emotional learning mammal describe structure function model refer belpm brain emotional learning based prediction model structurally model mimic connection region limbic system functionally uses weight k near neighbor imitate role region learning algorithm belpm define steep descent sd square estimator lse benchmark chaotic time series lorenz henon evaluate performance belpm obtain result compare prediction method result belpm capability achieve reasonable accuracy long term prediction chaotic time series limited training datum reasonably low computational time
eess,On the Performance of Mismatched Data Detection in Large MIMO Systems,"  We investigate the performance of mismatched data detection in large
multiple-input multiple-output (MIMO) systems, where the prior distribution of
the transmit signal used in the data detector differs from the true prior. To
minimize the performance loss caused by this prior mismatch, we include a
tuning stage into our recently-proposed large MIMO approximate message passing
(LAMA) algorithm, which allows us to develop mismatched LAMA algorithms with
optimal as well as sub-optimal tuning. We show that carefully-selected priors
often enable simpler and computationally more efficient algorithms compared to
LAMA with the true prior while achieving near-optimal performance. A
performance analysis of our algorithms for a Gaussian prior and a uniform prior
within a hypercube covering the QAM constellation recovers classical and recent
results on linear and non-linear MIMO data detection, respectively.
",investigate performance mismatch datum detection large multiple input multiple output mimo system prior distribution transmit signal data detector differ true prior minimize performance loss cause prior mismatch include tuning stage recently propose large mimo approximate message pass lama algorithm allow develop mismatch lama algorithm optimal sub optimal tuning carefully select prior enable simple computationally efficient algorithm compare lama true prior achieve near optimal performance performance analysis algorithm gaussian prior uniform prior hypercube cover qam constellation recover classical recent result linear non linear mimo datum detection respectively
eess,An Efficient and Flexible Spike Train Model via Empirical Bayes,"  Accurate statistical models of neural spike responses can characterize the
information carried by neural populations. But the limited samples of spike
counts during recording usually result in model overfitting. Besides, current
models assume spike counts to be Poisson-distributed, which ignores the fact
that many neurons demonstrate over-dispersed spiking behaviour. Although the
Negative Binomial Generalized Linear Model (NB-GLM) provides a powerful tool
for modeling over-dispersed spike counts, the maximum likelihood-based standard
NB-GLM leads to highly variable and inaccurate parameter estimates. Thus, we
propose a hierarchical parametric empirical Bayes method to estimate the neural
spike responses among neuronal population. Our method integrates both
Generalized Linear Models (GLMs) and empirical Bayes theory, which aims to (1)
improve the accuracy and reliability of parameter estimation, compared to the
maximum likelihood-based method for NB-GLM and Poisson-GLM; (2) effectively
capture the over-dispersion nature of spike counts from both simulated data and
experimental data; and (3) provide insight into both neural interactions and
spiking behaviours of the neuronal populations. We apply our approach to study
both simulated data and experimental neural data. The estimation of simulation
data indicates that the new framework can accurately predict mean spike counts
simulated from different models and recover the connectivity weights among
neural populations. The estimation based on retinal neurons demonstrate the
proposed method outperforms both NB-GLM and Poisson-GLM in terms of the
predictive log-likelihood of held-out data. Codes are available in
https://doi.org/10.5281/zenodo.4704423
",accurate statistical model neural spike response characterize information carry neural population limited sample spike count recording usually result model overfitting current model assume spike count poisson distribute ignore fact neuron demonstrate disperse spike behaviour negative binomial generalized linear model nb glm provide powerful tool model disperse spike count maximum likelihood base standard nb glm lead highly variable inaccurate parameter estimate propose hierarchical parametric empirical bayes method estimate neural spike response neuronal population method integrate generalized linear models glms empirical bayes theory aim 1 improve accuracy reliability parameter estimation compare maximum likelihood base method nb glm poisson glm 2 effectively capture dispersion nature spike count simulate datum experimental datum 3 provide insight neural interaction spike behaviour neuronal population apply approach study simulate datum experimental neural datum estimation simulation datum indicate new framework accurately predict mean spike count simulate different model recover connectivity weight neural population estimation base retinal neuron demonstrate propose method outperform nb glm poisson glm term predictive log likelihood hold datum code available https://doi.org/10.5281/zenodo.4704423
eess,Optimal Number of Choices in Rating Contexts,"  In many settings people must give numerical scores to entities from a small
discrete set. For instance, rating physical attractiveness from 1--5 on dating
sites, or papers from 1--10 for conference reviewing. We study the problem of
understanding when using a different number of options is optimal. We consider
the case when scores are uniform random and Gaussian. We study computationally
when using 2, 3, 4, 5, and 10 options out of a total of 100 is optimal in these
models (though our theoretical analysis is for a more general setting with $k$
choices from $n$ total options as well as a continuous underlying space). One
may expect that using more options would always improve performance in this
model, but we show that this is not necessarily the case, and that using fewer
choices---even just two---can surprisingly be optimal in certain situations.
While in theory for this setting it would be optimal to use all 100 options, in
practice this is prohibitive, and it is preferable to utilize a smaller number
of options due to humans' limited computational resources. Our results could
have many potential applications, as settings requiring entities to be ranked
by humans are ubiquitous. There could also be applications to other fields such
as signal or image processing where input values from a large set must be
mapped to output values in a smaller set.
",setting people numerical score entity small discrete set instance rate physical attractiveness 1 -5 date site paper 1 -10 conference reviewing study problem understanding different number option optimal consider case score uniform random gaussian study computationally 2 3 4 5 10 option total 100 optimal model theoretical analysis general setting $ k$ choice $ n$ total option continuous underlie space expect option improve performance model necessarily case few choice surprisingly optimal certain situation theory setting optimal use 100 option practice prohibitive preferable utilize small number option human limited computational resource result potential application setting require entity rank human ubiquitous application field signal image processing input value large set map output value small set
eess,"Using instantaneous frequency and aperiodicity detection to estimate F0
  for high-quality speech synthesis","  This paper introduces a general and flexible framework for F0 and
aperiodicity (additive non periodic component) analysis, specifically intended
for high-quality speech synthesis and modification applications. The proposed
framework consists of three subsystems: instantaneous frequency estimator and
initial aperiodicity detector, F0 trajectory tracker, and F0 refinement and
aperiodicity extractor. A preliminary implementation of the proposed framework
substantially outperformed (by a factor of 10 in terms of RMS F0 estimation
error) existing F0 extractors in tracking ability of temporally varying F0
trajectories. The front end aperiodicity detector consists of a complex-valued
wavelet analysis filter with a highly selective temporal and spectral envelope.
This front end aperiodicity detector uses a new measure that quantifies the
deviation from periodicity. The measure is less sensitive to slow FM and AM and
closely correlates with the signal to noise ratio.
",paper introduce general flexible framework f0 aperiodicity additive non periodic component analysis specifically intend high quality speech synthesis modification application propose framework consist subsystem instantaneous frequency estimator initial aperiodicity detector f0 trajectory tracker f0 refinement aperiodicity extractor preliminary implementation propose framework substantially outperform factor 10 term rms f0 estimation error exist f0 extractor track ability temporally vary f0 trajectory end aperiodicity detector consist complex value wavelet analysis filter highly selective temporal spectral envelope end aperiodicity detector use new measure quantify deviation periodicity measure sensitive slow fm closely correlate signal noise ratio
eess,Multiple target tracking based on sets of trajectories,"  We propose a solution of the multiple target tracking (MTT) problem based on
sets of trajectories and the random finite set framework. A full Bayesian
approach to MTT should characterise the distribution of the trajectories given
the measurements, as it contains all information about the trajectories. We
attain this by considering multi-object density functions in which objects are
trajectories. For the standard tracking models, we also describe a conjugate
family of multitrajectory density functions.
",propose solution multiple target tracking mtt problem base set trajectory random finite set framework bayesian approach mtt characterise distribution trajectory give measurement contain information trajectory attain consider multi object density function object trajectory standard tracking model describe conjugate family multitrajectory density function
eess,"Permutation Invariant Training of Deep Models for Speaker-Independent
  Multi-talker Speech Separation","  We propose a novel deep learning model, which supports permutation invariant
training (PIT), for speaker independent multi-talker speech separation,
commonly known as the cocktail-party problem. Different from most of the prior
arts that treat speech separation as a multi-class regression problem and the
deep clustering technique that considers it a segmentation (or clustering)
problem, our model optimizes for the separation regression error, ignoring the
order of mixing sources. This strategy cleverly solves the long-lasting label
permutation problem that has prevented progress on deep learning based
techniques for speech separation. Experiments on the equal-energy mixing setup
of a Danish corpus confirms the effectiveness of PIT. We believe improvements
built upon PIT can eventually solve the cocktail-party problem and enable
real-world adoption of, e.g., automatic meeting transcription and multi-party
human-computer interaction, where overlapping speech is common.
",propose novel deep learning model support permutation invariant training pit speaker independent multi talker speech separation commonly know cocktail party problem different prior art treat speech separation multi class regression problem deep clustering technique consider segmentation clustering problem model optimize separation regression error ignore order mix source strategy cleverly solve long last label permutation problem prevent progress deep learning base technique speech separation experiment equal energy mixing setup danish corpus confirm effectiveness pit believe improvement build pit eventually solve cocktail party problem enable real world adoption e.g. automatic meeting transcription multi party human computer interaction overlap speech common
eess,How Much Do Downlink Pilots Improve Cell-Free Massive MIMO?,"  In this paper, we analyze the benefits of including downlink pilots in a
cell-free massive MIMO system. We derive an approximate per-user achievable
downlink rate for conjugate beamforming processing, which takes into account
both uplink and downlink channel estimation errors, and power control. A
performance comparison is carried out, in terms of per-user net throughput,
considering cell-free massive MIMO operation with and without downlink
training, for different network densities. We take also into account the
performance improvement provided by max-min fairness power control in the
downlink. Numerical results show that, exploiting downlink pilots, the
performance can be considerably improved in low density networks over the
conventional scheme where the users rely on statistical channel knowledge only.
In high density networks, performance improvements are moderate.
",paper analyze benefit include downlink pilot cell free massive mimo system derive approximate user achievable downlink rate conjugate beamforming processing take account uplink downlink channel estimation error power control performance comparison carry term user net throughput consider cell free massive mimo operation downlink training different network density account performance improvement provide max min fairness power control downlink numerical result exploit downlink pilot performance considerably improve low density network conventional scheme user rely statistical channel knowledge high density network performance improvement moderate
eess,"On the Performance of Cell-Free Massive MIMO with Short-Term Power
  Constraints","  In this paper we consider a time-division duplex cell-free massive
multiple-input multiple-output (MIMO) system where many distributed access
points (APs) simultaneously serve many users. A normalized conjugate
beamforming scheme, which satisfies short-term average power constraints at the
APs, is proposed and analyzed taking into account the effect of imperfect
channel information. We derive an approximate closed-form expression for the
per-user achievable downlink rate of this scheme. We also provide, analytically
and numerically, a performance comparison between the normalized conjugate
beamforming and the conventional conjugate beamforming scheme in [1] (which
satisfies long-term average power constraints). Normalized conjugate
beamforming scheme reduces the beamforming uncertainty gain, which comes from
the users' lack of the channel state information knowledge, and hence, it
improves the achievable downlink rate compared to the conventional conjugate
beamforming scheme.
",paper consider time division duplex cell free massive multiple input multiple output mimo system distribute access point ap simultaneously serve user normalized conjugate beamforming scheme satisfy short term average power constraint ap propose analyze take account effect imperfect channel information derive approximate closed form expression user achievable downlink rate scheme provide analytically numerically performance comparison normalize conjugate beamforming conventional conjugate beamforming scheme 1 satisfy long term average power constraint normalized conjugate beamforming scheme reduce beamforming uncertainty gain come user lack channel state information knowledge improve achievable downlink rate compare conventional conjugate beamforming scheme
eess,Learning Sparse Graphs Under Smoothness Prior,"  In this paper, we are interested in learning the underlying graph structure
behind training data. Solving this basic problem is essential to carry out any
graph signal processing or machine learning task. To realize this, we assume
that the data is smooth with respect to the graph topology, and we parameterize
the graph topology using an edge sampling function. That is, the graph
Laplacian is expressed in terms of a sparse edge selection vector, which
provides an explicit handle to control the sparsity level of the graph. We
solve the sparse graph learning problem given some training data in both the
noiseless and noisy settings. Given the true smooth data, the posed sparse
graph learning problem can be solved optimally and is based on simple rank
ordering. Given the noisy data, we show that the joint sparse graph learning
and denoising problem can be simplified to designing only the sparse edge
selection vector, which can be solved using convex optimization.
",paper interested learn underlie graph structure training datum solve basic problem essential carry graph signal processing machine learning task realize assume data smooth respect graph topology parameterize graph topology edge sampling function graph laplacian express term sparse edge selection vector provide explicit handle control sparsity level graph solve sparse graph learn problem give training datum noiseless noisy setting give true smooth datum pose sparse graph learning problem solve optimally base simple rank ordering give noisy datum joint sparse graph learning denoise problem simplify design sparse edge selection vector solve convex optimization
eess,The Microsoft 2016 Conversational Speech Recognition System,"  We describe Microsoft's conversational speech recognition system, in which we
combine recent developments in neural-network-based acoustic and language
modeling to advance the state of the art on the Switchboard recognition task.
Inspired by machine learning ensemble techniques, the system uses a range of
convolutional and recurrent neural networks. I-vector modeling and lattice-free
MMI training provide significant gains for all acoustic model architectures.
Language model rescoring with multiple forward and backward running RNNLMs, and
word posterior-based system combination provide a 20% boost. The best single
system uses a ResNet architecture acoustic model with RNNLM rescoring, and
achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The
combined system has an error rate of 6.2%, representing an improvement over
previously reported results on this benchmark task.
",describe microsoft conversational speech recognition system combine recent development neural network base acoustic language modeling advance state art switchboard recognition task inspire machine learn ensemble technique system use range convolutional recurrent neural network vector modeling lattice free mmi training provide significant gain acoustic model architecture language model rescore multiple forward backward run rnnlm word posterior base system combination provide 20 boost good single system use resnet architecture acoustic model rnnlm rescoring achieve word error rate 6.9 nist 2000 switchboard task combine system error rate 6.2 represent improvement previously report result benchmark task
eess,Complex Laplacian based Distributed Control for Multi-Agent Network,"  The work done in this paper, proposes a complex Laplacian-based distributed
control scheme for convergence in the multi-agent network. The proposed scheme
has been designated as cascade formulation. The proposed technique exploits the
traditional method of organizing large scattered networks into smaller
interconnected clusters to optimize information flow within the network. The
complex Laplacian-based approach results in a hierarchical structure, with
formation of a meta-cluster leading other clusters in the network. The proposed
formulation enables flexibility to constrain the eigen spectra of the overall
closed-loop dynamics, ensuring desired convergence rate and control input
intensity. The sufficient conditions ensuring globally stable formation for
proposed formulation are also asserted. Robustness of the proposed formulation
to uncertainties like loss in communication links and actuator failure has also
been discussed. The effectiveness of the proposed approach is illustrated by
simulating a finitely large network of thirty vehicles.
",work paper propose complex laplacian base distribute control scheme convergence multi agent network propose scheme designate cascade formulation propose technique exploit traditional method organize large scatter network small interconnect cluster optimize information flow network complex laplacian base approach result hierarchical structure formation meta cluster lead cluster network propose formulation enable flexibility constrain eigen spectra overall closed loop dynamic ensure desire convergence rate control input intensity sufficient condition ensure globally stable formation propose formulation assert robustness propose formulation uncertainty like loss communication link actuator failure discuss effectiveness propose approach illustrate simulate finitely large network thirty vehicle
eess,"Macroscopic Modeling, Calibration, and Simulation of Managed
  Lane-Freeway Networks, Part I: Topological and Phenomenological Modeling","  To help mitigate road congestion caused by the unrelenting growth of traffic
demand, many transit authorities have implemented managed lane policies.
Managed lanes typically run parallel to a freeway's standard, general-purpose
(GP) lanes, but are restricted to certain types of vehicles. It was originally
thought that managed lanes would improve the use of existing infrastructure
through incentivization of demand-management behaviors like carpooling, but
implementations have often been characterized by unpredicted phenomena that is
often to detrimental system performance.
  This paper presents several macroscopic traffic modeling tools we have used
for study of freeways equipped with managed lanes, or ""managed lane-freeway
networks."" The proposed framework is based on the widely-used first-order
kinematic wave theory. In this model, the GP and the managed lanes are modeled
as parallel links connected by nodes, where certain type of traffic may switch
between GP and managed lane links. Two types of managed lane topologies are
considered: full-access, where vehicles can switch between the GP and the
managed lanes anywhere; and separated, where such switching is allowed only at
certain locations called gates.
  We also describe methods to incorporate in three phenomena into our model
that are particular to managed lane-freeway networks. The inertia effect
reflects drivers' inclination to stay in their lane as long as possible and
switch only if this would obviously improve their travel condition. The
friction effect reflects the empirically-observed driver fear of moving fast in
a managed lane while traffic in the adjacent GP lanes moves slowly due to
congestion. The smoothing effect describes how managed lanes can increase
throughput at bottlenecks by reducing lane changes. We present simple models
for each of these phenomena that fit within the general macroscopic theory.
",help mitigate road congestion cause unrelenting growth traffic demand transit authority implement manage lane policy manage lane typically run parallel freeway standard general purpose gp lane restrict certain type vehicle originally think manage lane improve use exist infrastructure incentivization demand management behavior like carpooling implementation characterize unpredicted phenomenon detrimental system performance paper present macroscopic traffic modeling tool study freeway equip manage lane manage lane freeway network propose framework base widely order kinematic wave theory model gp manage lane model parallel link connect node certain type traffic switch gp manage lane link type manage lane topology consider access vehicle switch gp manage lane separate switching allow certain location call gate describe method incorporate phenomenon model particular manage lane freeway network inertia effect reflect driver inclination stay lane long possible switch obviously improve travel condition friction effect reflect empirically observe driver fear move fast manage lane traffic adjacent gp lane move slowly congestion smoothing effect describe manage lane increase throughput bottleneck reduce lane change present simple model phenomenon fit general macroscopic theory
eess,"Approximate Gram-Matrix Interpolation for Wideband Massive MU-MIMO
  Systems","  Numerous linear and non-linear data-detection and precoding algorithms for
wideband massive multi-user (MU) multiple-input multiple-output (MIMO) wireless
systems that rely on orthogonal frequency-division multiplexing (OFDM) or
single-carrier frequency-division multiple access (SC-FDMA) require the
computation of the Gram matrix for each active subcarrier. Computing the Gram
matrix for each active subcarrier, however, results in excessively high
computational complexity. In this paper, we propose novel, approximate
algorithms that significantly reduce the complexity of Gram-matrix computation
by simultaneously exploiting correlation across subcarriers and channel
hardening. We show analytically that a small fraction of Gram-matrix
computations in combination with approximate interpolation schemes are
sufficient to achieve near-optimal error-rate performance at low computational
complexity in massive MU-MIMO systems. We also demonstrate that the proposed
methods exhibit improved robustness against channel-estimation errors compared
to exact Gram-matrix interpolation algorithms that typically require high
computational complexity.
",numerous linear non linear data detection precode algorithm wideband massive multi user mu multiple input multiple output mimo wireless system rely orthogonal frequency division multiplexing ofdm single carrier frequency division multiple access sc fdma require computation gram matrix active subcarrier compute gram matrix active subcarrier result excessively high computational complexity paper propose novel approximate algorithm significantly reduce complexity gram matrix computation simultaneously exploit correlation subcarrier channel hardening analytically small fraction gram matrix computation combination approximate interpolation scheme sufficient achieve near optimal error rate performance low computational complexity massive mu mimo system demonstrate propose method exhibit improve robustness channel estimation error compare exact gram matrix interpolation algorithm typically require high computational complexity
eess,"Covert Single-hop Communication in a Wireless Network with Distributed
  Artificial Noise Generation","  Covert communication, also known as low probability of detection (LPD)
communication, prevents the adversary from knowing that a communication is
taking place. Recent work has demonstrated that, in a three-party scenario with
a transmitter (Alice), intended recipient (Bob), and adversary (Warden Willie),
the maximum number of bits that can be transmitted reliably from Alice to Bob
without detection by Willie, when additive white Gaussian noise (AWGN) channels
exist between all parties, is on the order of the square root of the number of
channel uses. In this paper, we begin consideration of network scenarios by
studying the case where there are additional ""friendly"" nodes present in the
environment that can produce artificial noise to aid in hiding the
communication. We establish achievability results by considering constructions
where the system node closest to the warden produces artificial noise and
demonstrate a significant improvement in the throughput achieved covertly,
without requiring close coordination between Alice and the noise-generating
node. Conversely, under mild restrictions on the communication strategy, we
demonstrate no higher covert throughput is possible. Extensions to the
consideration of the achievable covert throughput when multiple wardens
randomly located in the environment collaborate to attempt detection of the
transmitter are also considered.
",covert communication know low probability detection lpd communication prevent adversary know communication take place recent work demonstrate party scenario transmitter alice intend recipient bob adversary warden willie maximum number bit transmit reliably alice bob detection willie additive white gaussian noise awgn channel exist party order square root number channel use paper begin consideration network scenario study case additional friendly node present environment produce artificial noise aid hide communication establish achievability result consider construction system node close warden produce artificial noise demonstrate significant improvement throughput achieve covertly require close coordination alice noise generate node conversely mild restriction communication strategy demonstrate high covert throughput possible extension consideration achievable covert throughput multiple warden randomly locate environment collaborate attempt detection transmitter consider
eess,A System Level Approach to Controller Synthesis,"  Biological and advanced cyberphysical control systems often have limited,
sparse, uncertain, and distributed communication and computing in addition to
sensing and actuation. Fortunately, the corresponding plants and performance
requirements are also sparse and structured, and this must be exploited to make
constrained controller design feasible and tractable. We introduce a new
""system level"" (SL) approach involving three complementary SL elements. System
Level Parameterizations (SLPs) generalize state space and Youla
parameterizations of all stabilizing controllers and the responses they
achieve, and combine with System Level Constraints (SLCs) to parameterize the
largest known class of constrained stabilizing controllers that admit a convex
characterization, generalizing quadratic invariance (QI). SLPs also lead to a
generalization of detectability and stabilizability, suggesting the existence
of a rich separation structure, that when combined with SLCs, is naturally
applicable to structurally constrained controllers and systems. We further
provide a catalog of useful SLCs, most importantly including sparsity, delay,
and locality constraints on both communication and computing internal to the
controller, and external system performance. The resulting System Level
Synthesis (SLS) problems that arise define the broadest known class of
constrained optimal control problems that can be solved using convex
programming. An example illustrates how this system level approach can
systematically explore tradeoffs in controller performance, robustness, and
synthesis/implementation complexity.
",biological advanced cyberphysical control system limit sparse uncertain distribute communication computing addition sensing actuation fortunately correspond plant performance requirement sparse structure exploit constrain controller design feasible tractable introduce new system level sl approach involve complementary sl element system level parameterizations slps generalize state space youla parameterization stabilize controller response achieve combine system level constraints slcs parameterize large know class constrain stabilizing controller admit convex characterization generalize quadratic invariance qi slp lead generalization detectability stabilizability suggest existence rich separation structure combine slcs naturally applicable structurally constrain controller system provide catalog useful slc importantly include sparsity delay locality constraint communication compute internal controller external system performance result system level synthesis sls problem arise define broadest know class constrain optimal control problem solve convex programming example illustrate system level approach systematically explore tradeoff controller performance robustness synthesis implementation complexity
eess,Achieving Human Parity in Conversational Speech Recognition,"  Conversational speech recognition has served as a flagship speech recognition
task since the release of the Switchboard corpus in the 1990s. In this paper,
we measure the human error rate on the widely used NIST 2000 test set, and find
that our latest automated system has reached human parity. The error rate of
professional transcribers is 5.9% for the Switchboard portion of the data, in
which newly acquainted pairs of people discuss an assigned topic, and 11.3% for
the CallHome portion where friends and family members have open-ended
conversations. In both cases, our automated system establishes a new state of
the art, and edges past the human benchmark, achieving error rates of 5.8% and
11.0%, respectively. The key to our system's performance is the use of various
convolutional and LSTM acoustic model architectures, combined with a novel
spatial smoothing method and lattice-free MMI acoustic training, multiple
recurrent neural network language modeling approaches, and a systematic use of
system combination.
",conversational speech recognition serve flagship speech recognition task release switchboard corpus 1990s paper measure human error rate widely nist 2000 test set find late automate system reach human parity error rate professional transcriber 5.9 switchboard portion datum newly acquaint pair people discuss assign topic 11.3 callhome portion friend family member open end conversation case automate system establish new state art edge past human benchmark achieve error rate 5.8 11.0 respectively key system performance use convolutional lstm acoustic model architecture combine novel spatial smoothing method lattice free mmi acoustic training multiple recurrent neural network language modeling approach systematic use system combination
eess,Inverse Power Flow Problem,"  This paper formulates an inverse power flow problem which is to infer a nodal
admittance matrix (hence the network structure of a power system) from voltage
and current phasors measured at a number of buses. We show that the admittance
matrix can be uniquely identified from a sequence of measurements corresponding
to different steady states when every node in the system is equipped with a
measurement device, and a Kron-reduced admittance matrix can be determined even
if some nodes in the system are not monitored (hidden nodes). Furthermore, we
propose effective algorithms based on graph theory to uncover the actual
admittance matrix of radial systems with hidden nodes. We provide theoretical
guarantees for the recovered admittance matrix and demonstrate that the actual
admittance matrix can be fully recovered even from the Kron-reduced admittance
matrix under some mild assumptions. Simulations on standard test systems
confirm that these algorithms are capable of providing accurate estimates of
the admittance matrix from noisy sensor data.
",paper formulate inverse power flow problem infer nodal admittance matrix network structure power system voltage current phasor measure number bus admittance matrix uniquely identify sequence measurement correspond different steady state node system equip measurement device kron reduce admittance matrix determine node system monitor hide node furthermore propose effective algorithm base graph theory uncover actual admittance matrix radial system hide node provide theoretical guarantee recovered admittance matrix demonstrate actual admittance matrix fully recover kron reduce admittance matrix mild assumption simulation standard test system confirm algorithm capable provide accurate estimate admittance matrix noisy sensor datum
eess,"Approximate eigenvalue distribution of a cylindrically isotropic noise
  sample covariance matrix","  The statistical behavior of the eigenvalues of the sample covariance matrix
(SCM) plays a key role in determining the performance of adaptive beamformers
(ABF) in presence of noise. This paper presents a method to compute the
approximate eigenvalue density function (EDF) for the SCM of a \cin{} field
when only a finite number of shapshots are available. The EDF of the ensemble
covariance matrix (ECM) is modeled as an atomic density with many fewer atoms
than the SCM size. The model results in substantial computational savings over
more direct methods of computing the EDF. The approximate EDF obtained from
this method agrees closely with histograms of eigenvalues obtained from
simulation.
",statistical behavior eigenvalue sample covariance matrix scm play key role determine performance adaptive beamformer abf presence noise paper present method compute approximate eigenvalue density function edf scm \cin field finite number shapshot available edf ensemble covariance matrix ecm model atomic density few atom scm size model result substantial computational saving direct method compute edf approximate edf obtain method agree closely histogram eigenvalue obtain simulation
eess,Unit circle MVDR beamformer,"  The array polynomial is the z-transform of the array weights for a narrowband
planewave beamformer using a uniform linear array (ULA). Evaluating the array
polynomial on the unit circle in the complex plane yields the beampattern. The
locations of the polynomial zeros on the unit circle indicate the nulls of the
beampattern. For planewave signals measured with a ULA, the locations of the
ensemble MVDR polynomial zeros are constrained on the unit circle. However,
sample matrix inversion (SMI) MVDR polynomial zeros generally do not fall on
the unit circle. The proposed unit circle MVDR (UC MVDR) projects the zeros of
the SMI MVDR polynomial radially on the unit circle. This satisfies the
constraint on the zeros of ensemble MVDR polynomial. Numerical simulations show
that the UC MVDR beamformer suppresses interferers better than the SMI MVDR and
the diagonal loaded MVDR beamformer and also improves the white noise gain
(WNG).
",array polynomial z transform array weight narrowband planewave beamformer uniform linear array ula evaluate array polynomial unit circle complex plane yield beampattern location polynomial zero unit circle indicate null beampattern planewave signal measure ula location ensemble mvdr polynomial zero constrain unit circle sample matrix inversion smi mvdr polynomial zero generally fall unit circle propose unit circle mvdr uc mvdr project zero smi mvdr polynomial radially unit circle satisfy constraint zero ensemble mvdr polynomial numerical simulation uc mvdr beamformer suppresse interferer well smi mvdr diagonal load mvdr beamformer improve white noise gain wng
eess,"Computationally Efficient Target Classification in Multispectral Image
  Data with Deep Neural Networks","  Detecting and classifying targets in video streams from surveillance cameras
is a cumbersome, error-prone and expensive task. Often, the incurred costs are
prohibitive for real-time monitoring. This leads to data being stored locally
or transmitted to a central storage site for post-incident examination. The
required communication links and archiving of the video data are still
expensive and this setup excludes preemptive actions to respond to imminent
threats. An effective way to overcome these limitations is to build a smart
camera that transmits alerts when relevant video sequences are detected. Deep
neural networks (DNNs) have come to outperform humans in visual classifications
tasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be
extended to make use of higher-dimensional input data such as multispectral
data. We explore this opportunity in terms of achievable accuracy and required
computational effort. To analyze the precision of DNNs for scene labeling in an
urban surveillance scenario we have created a dataset with 8 classes obtained
in a field experiment. We combine an RGB camera with a 25-channel VIS-NIR
snapshot sensor to assess the potential of multispectral image data for target
classification. We evaluate several new DNNs, showing that the spectral
information fused together with the RGB frames can be used to improve the
accuracy of the system or to achieve similar accuracy with a 3x smaller
computation effort. We achieve a very high per-pixel accuracy of 99.1%. Even
for scarcely occurring, but particularly interesting classes, such as cars, 75%
of the pixels are labeled correctly with errors occurring only around the
border of the objects. This high accuracy was obtained with a training set of
only 30 labeled images, paving the way for fast adaptation to various
application scenarios.
",detect classify target video stream surveillance camera cumbersome error prone expensive task incur cost prohibitive real time monitoring lead datum store locally transmit central storage site post incident examination require communication link archiving video datum expensive setup exclude preemptive action respond imminent threat effective way overcome limitation build smart camera transmit alert relevant video sequence detect deep neural network dnn come outperform human visual classification task concept dnn convolutional networks convnets easily extend use high dimensional input datum multispectral datum explore opportunity term achievable accuracy require computational effort analyze precision dnn scene labeling urban surveillance scenario create dataset 8 class obtain field experiment combine rgb camera 25 channel vis nir snapshot sensor assess potential multispectral image datum target classification evaluate new dnn show spectral information fuse rgb frame improve accuracy system achieve similar accuracy 3x small computation effort achieve high pixel accuracy 99.1 scarcely occurring particularly interesting class car 75 pixel label correctly error occur border object high accuracy obtain training set 30 label image pave way fast adaptation application scenario
eess,Stall Pattern Avoidance in Polynomial Product Codes,"  Product codes are a concatenated error-correction scheme that has been often
considered for applications requiring very low bit-error rates, which demand
that the error floor be decreased as much as possible. In this work, we
consider product codes constructed from polynomial algebraic codes, and propose
a novel low-complexity post-processing technique that is able to improve the
error-correction performance by orders of magnitude. We provide lower bounds
for the error rate achievable under post processing, and present simulation
results indicating that these bounds are tight.
",product code concatenate error correction scheme consider application require low bit error rate demand error floor decrease possible work consider product code construct polynomial algebraic code propose novel low complexity post processing technique able improve error correction performance order magnitude provide low bound error rate achievable post processing present simulation result indicate bound tight
eess,"Image Processing with Dipole-Coupled Nanomagnets: Noise Suppression and
  Edge Enhancement Detection","  Hardware based image processing offers speed and convenience not found in
software-centric approaches. Here, we show theoretically that a two-dimensional
periodic array of dipole-coupled elliptical nanomagnets, delineated on a
piezoelectric substrate, can act as a dynamical system for specific image
processing functions. Each nanomagnet has two stable magnetization states that
encode pixel color (black or white). An image containing black and white pixels
is first converted to voltage states and then mapped into the magnetization
states of a nanomagnet array with magneto-tunneling junctions (MTJs). The same
MTJs are employed to read out the processed pixel colors later. Dipole
interaction between the nanomagnets implements specific image processing tasks
such as noise reduction and edge enhancement detection. These functions are
triggered by applying a global strain to the nanomagnets with a voltage dropped
across the piezoelectric substrate. An image containing an arbitrary number of
black and white pixels can be processed in few nanoseconds with very low energy
cost.
",hardware base image processing offer speed convenience find software centric approach theoretically dimensional periodic array dipole couple elliptical nanomagnet delineate piezoelectric substrate act dynamical system specific image processing function nanomagnet stable magnetization state encode pixel color black white image contain black white pixel convert voltage state map magnetization state nanomagnet array magneto tunneling junction mtj mtj employ read process pixel color later dipole interaction nanomagnet implement specific image processing task noise reduction edge enhancement detection function trigger apply global strain nanomagnet voltage drop piezoelectric substrate image contain arbitrary number black white pixel process nanosecond low energy cost
eess,"Microseismic events enhancement and detection in sensor arrays using
  autocorrelation based filtering","  Passive microseismic data are commonly buried in noise, which presents a
significant challenge for signal detection and recovery. For recordings from a
surface sensor array where each trace contains a time-delayed arrival from the
event, we propose an autocorrelation-based stacking method that designs a
denoising filter from all the traces, as well as a multi-channel detection
scheme. This approach circumvents the issue of time aligning the traces prior
to stacking because every trace's autocorrelation is centered at zero in the
lag domain. The effect of white noise is concentrated near zero lag, so the
filter design requires a predictable adjustment of the zero-lag value.
Truncation of the autocorrelation is employed to smooth the impulse response of
the denoising filter. In order to extend the applicability of the algorithm, we
also propose a noise prewhitening scheme that addresses cases with colored
noise. The simplicity and robustness of this method are validated with
synthetic and real seismic traces.
",passive microseismic datum commonly bury noise present significant challenge signal detection recovery recording surface sensor array trace contain time delay arrival event propose autocorrelation base stacking method design denoising filter trace multi channel detection scheme approach circumvent issue time align trace prior stack trace autocorrelation center zero lag domain effect white noise concentrate near zero lag filter design require predictable adjustment zero lag value truncation autocorrelation employ smooth impulse response denoise filter order extend applicability algorithm propose noise prewhitening scheme address case color noise simplicity robustness method validate synthetic real seismic trace
eess,"Safety Verification and Control for Collision Avoidance at Road
  Intersections","  This paper presents the design of a supervisory algorithm that monitors
safety at road intersections and overrides drivers with a safe input when
necessary. The design of the supervisor consists of two parts: safety
verification and control design. Safety verification is the problem to
determine if vehicles will be able to cross the intersection without colliding
with current drivers' inputs. We translate this safety verification problem
into a jobshop scheduling problem, which minimizes the maximum lateness and
evaluates if the optimal cost is zero. The zero optimal cost corresponds to the
case in which all vehicles can cross each conflict area without collisions.
Computing the optimal cost requires solving a Mixed Integer Nonlinear
Programming (MINLP) problem due to the nonlinear second-order dynamics of the
vehicles. We therefore estimate this optimal cost by formulating two related
Mixed Integer Linear Programming (MILP) problems that assume simpler vehicle
dynamics. We prove that these two MILP problems yield lower and upper bounds of
the optimal cost. We also quantify the worst case approximation errors of these
MILP problems. We design the supervisor to override the vehicles with a safe
control input if the MILP problem that computes the upper bound yields a
positive optimal cost. We theoretically demonstrate that the supervisor keeps
the intersection safe and is non-blocking. Computer simulations further
validate that the algorithms can run in real time for problems of realistic
size.
",paper present design supervisory algorithm monitor safety road intersection override driver safe input necessary design supervisor consist part safety verification control design safety verification problem determine vehicle able cross intersection collide current driver input translate safety verification problem jobshop scheduling problem minimize maximum lateness evaluate optimal cost zero zero optimal cost correspond case vehicle cross conflict area collision compute optimal cost require solve mixed integer nonlinear programming minlp problem nonlinear second order dynamic vehicle estimate optimal cost formulate related mixed integer linear programming milp problem assume simple vehicle dynamic prove milp problem yield low upper bound optimal cost quantify bad case approximation error milp problem design supervisor override vehicle safe control input milp problem compute upper bind yield positive optimal cost theoretically demonstrate supervisor keep intersection safe non blocking computer simulation validate algorithm run real time problem realistic size
eess,Kernel-based Reconstruction of Space-time Functions on Dynamic Graphs,"  Graph-based methods pervade the inference toolkits of numerous disciplines
including sociology, biology, neuroscience, physics, chemistry, and
engineering. A challenging problem encountered in this context pertains to
determining the attributes of a set of vertices given those of another subset
at possibly different time instants. Leveraging spatiotemporal dynamics can
drastically reduce the number of observed vertices, and hence the cost of
sampling. Alleviating the limited flexibility of existing approaches, the
present paper broadens the existing kernel-based graph function reconstruction
framework to accommodate time-evolving functions over possibly time-evolving
topologies. This approach inherits the versatility and generality of
kernel-based methods, for which no knowledge on distributions or second-order
statistics is required. Systematic guidelines are provided to construct two
families of space-time kernels with complementary strengths. The first
facilitates judicious control of regularization on a space-time frequency
plane, whereas the second can afford time-varying topologies. Batch and online
estimators are also put forth, and a novel kernel Kalman filter is developed to
obtain these estimates at affordable computational cost. Numerical tests with
real data sets corroborate the merits of the proposed methods relative to
competing alternatives.
",graph base method pervade inference toolkit numerous discipline include sociology biology neuroscience physics chemistry engineering challenging problem encounter context pertain determine attribute set vertex give subset possibly different time instant leverage spatiotemporal dynamic drastically reduce number observed vertex cost sampling alleviate limited flexibility exist approach present paper broaden exist kernel base graph function reconstruction framework accommodate time evolve function possibly time evolve topology approach inherit versatility generality kernel base method knowledge distribution second order statistic require systematic guideline provide construct family space time kernel complementary strength facilitate judicious control regularization space time frequency plane second afford time vary topology batch online estimator forth novel kernel kalman filter develop obtain estimate affordable computational cost numerical test real datum set corroborate merit propose method relative compete alternative
eess,"Local Sparse Approximation for Image Restoration with Adaptive Block
  Size Selection","  In this paper the problem of image restoration (denoising and inpainting) is
approached using sparse approximation of local image blocks. The local image
blocks are extracted by sliding square windows over the image. An adaptive
block size selection procedure for local sparse approximation is proposed,
which affects the global recovery of underlying image. Ideally the adaptive
local block selection yields the minimum mean square error (MMSE) in recovered
image. This framework gives us a clustered image based on the selected block
size, then each cluster is restored separately using sparse approximation. The
results obtained using the proposed framework are very much comparable with the
recently proposed image restoration techniques.
",paper problem image restoration denoising inpainting approach sparse approximation local image block local image block extract slide square window image adaptive block size selection procedure local sparse approximation propose affect global recovery underlie image ideally adaptive local block selection yield minimum mean square error mmse recovered image framework give cluster image base select block size cluster restore separately sparse approximation result obtain propose framework comparable recently propose image restoration technique
eess,Image biomarker standardisation initiative,"  The image biomarker standardisation initiative (IBSI) is an independent
international collaboration which works towards standardising the extraction of
image biomarkers from acquired imaging for the purpose of high-throughput
quantitative image analysis (radiomics). Lack of reproducibility and validation
of high-throughput quantitative image analysis studies is considered to be a
major challenge for the field. Part of this challenge lies in the scantiness of
consensus-based guidelines and definitions for the process of translating
acquired imaging into high-throughput image biomarkers. The IBSI therefore
seeks to provide image biomarker nomenclature and definitions, benchmark data
sets, and benchmark values to verify image processing and image biomarker
calculations, as well as reporting guidelines, for high-throughput image
analysis.
",image biomarker standardisation initiative ibsi independent international collaboration work standardise extraction image biomarker acquire imaging purpose high throughput quantitative image analysis radiomic lack reproducibility validation high throughput quantitative image analysis study consider major challenge field challenge lie scantiness consensus base guideline definition process translate acquire imaging high throughput image biomarker ibsi seek provide image biomarker nomenclature definition benchmark datum set benchmark value verify image processing image biomarker calculation report guideline high throughput image analysis
eess,"Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive
  Patterns in Vowel Acoustics","  Most of the previous approaches to lyrics-to-audio alignment used a
pre-developed automatic speech recognition (ASR) system that innately suffered
from several difficulties to adapt the speech model to individual singers. A
significant aspect missing in previous works is the self-learnability of
repetitive vowel patterns in the singing voice, where the vowel part used is
more consistent than the consonant part. Based on this, our system first learns
a discriminative subspace of vowel sequences, based on weighted symmetric
non-negative matrix factorization (WS-NMF), by taking the self-similarity of a
standard acoustic feature as an input. Then, we make use of canonical time
warping (CTW), derived from a recent computer vision technique, to find an
optimal spatiotemporal transformation between the text and the acoustic
sequences. Experiments with Korean and English data sets showed that deploying
this method after a pre-developed, unsupervised, singing source separation
achieved more promising results than other state-of-the-art unsupervised
approaches and an existing ASR-based system.
",previous approach lyric audio alignment pre developed automatic speech recognition asr system innately suffer difficulty adapt speech model individual singer significant aspect miss previous work self learnability repetitive vowel pattern singing voice vowel consistent consonant base system learn discriminative subspace vowel sequence base weighted symmetric non negative matrix factorization ws nmf take self similarity standard acoustic feature input use canonical time warping ctw derive recent computer vision technique find optimal spatiotemporal transformation text acoustic sequence experiment korean english data set show deploy method pre developed unsupervised singe source separation achieve promising result state art unsupervised approach exist asr base system
eess,"PI(D) tuning for Flight Control Systems via Incremental Nonlinear
  Dynamic Inversion","  Previous results reported in the robotics literature show the relationship
between time-delay control (TDC) and proportional-integral-derivative control
(PID). In this paper, we show that incremental nonlinear dynamic inversion
(INDI) - more familiar in the aerospace community - are in fact equivalent to
TDC. This leads to a meaningful and systematic method for PI(D)-control tuning
of robust nonlinear flight control systems via INDI. We considered a
reformulation of the plant dynamics inversion which removes effector blending
models from the resulting control law, resulting in robust model-free control
laws like PI(D)-control.
",previous result report robotic literature relationship time delay control tdc proportional integral derivative control pid paper incremental nonlinear dynamic inversion indi familiar aerospace community fact equivalent tdc lead meaningful systematic method pi(d)-control tuning robust nonlinear flight control system indi consider reformulation plant dynamic inversion remove effector blending model result control law result robust model free control law like pi(d)-control
eess,"A New View of Multi-User Hybrid Massive MIMO: Non-Orthogonal Angle
  Division Multiple Access","  This paper presents a new view of multi-user (MU) hybrid massive
multiple-input and multiple-output (MIMO) systems from array signal processing
perspective. We first show that the instantaneous channel vectors corresponding
to different users are asymptotically orthogonal if the angles of arrival
(AOAs) of users are different. We then decompose the channel matrix into an
angle domain basis matrix and a gain matrix. The former can be formulated by
steering vectors and the latter has the same size as the number of RF chains,
which perfectly matches the structure of hybrid precoding. A novel hybrid
channel estimation is proposed by separately estimating the angle information
and the gain matrix, which could significantly save the training overhead and
substantially improve the channel estimation accuracy compared to the
conventional beamspace approach. Moreover, with the aid of the angle domain
matrix, the MU massive MIMO system can be viewed as a type of non-orthogonal
angle division multiple access (ADMA) to simultaneously serve multiple users at
the same frequency band. Finally, the performance of the proposed scheme is
validated by computer simulation results.
",paper present new view multi user mu hybrid massive multiple input multiple output mimo system array signal processing perspective instantaneous channel vector correspond different user asymptotically orthogonal angle arrival aoas user different decompose channel matrix angle domain basis matrix gain matrix formulate steering vector size number rf chain perfectly match structure hybrid precoding novel hybrid channel estimation propose separately estimate angle information gain matrix significantly save training overhead substantially improve channel estimation accuracy compare conventional beamspace approach aid angle domain matrix mu massive mimo system view type non orthogonal angle division multiple access adma simultaneously serve multiple user frequency band finally performance propose scheme validate computer simulation result
eess,"A new cosine series antialiasing function and its application to
  aliasing-free glottal source models for speech and singing synthesis","  We formulated and implemented a procedure to generate aliasing-free
excitation source signals. It uses a new antialiasing filter in the continuous
time domain followed by an IIR digital filter for response equalization. We
introduced a cosine-series-based general design procedure for the new
antialiasing function. We applied this new procedure to implement the
antialiased Fujisaki-Ljungqvist model. We also applied it to revise our
previous implementation of the antialiased Fant-Liljencrants model. A
combination of these signals and a lattice implementation of the time varying
vocal tract model provides a reliable and flexible basis to test fo extractors
and source aperiodicity analysis methods. MATLAB implementations of these
antialiased excitation source models are available as part of our open source
tools for speech science.
",formulate implement procedure generate aliasing free excitation source signal use new antialiasing filter continuous time domain follow iir digital filter response equalization introduce cosine series base general design procedure new antialiasing function apply new procedure implement antialiase fujisaki ljungqvist model apply revise previous implementation antialiased fant liljencrants model combination signal lattice implementation time vary vocal tract model provide reliable flexible basis test fo extractor source aperiodicity analysis method matlab implementation antialiase excitation source model available open source tool speech science
eess,"Modification of Social Dominance in Social Networks by Selective
  Adjustment of Interpersonal Weights","  According to the DeGroot-Friedkin model of a social network, an individual's
social power evolves as the network discusses individual opinions over a
sequence of issues. Under mild assumptions on the connectivity of the network,
the social power of every individual converges to a constant strictly positive
value as the number of issues discussed increases. If the network has a special
topology, termed ""star topology"", then all social power accumulates with the
individual at the centre of the star. This paper studies the strategic
introduction of new individuals and/or interpersonal relationships into a
social network with star topology to reduce the social power of the centre
individual. In fact, several strategies are proposed. For each strategy, we
derive necessary and sufficient conditions on the strength of the new
interpersonal relationships, based on local information, which ensures that the
centre individual no longer has the greatest social power within the social
network. Interpretations of these conditions show that the strategies are
remarkably intuitive and that certain strategies are favourable compared to
others, all of which is sociologically expected.
",accord degroot friedkin model social network individual social power evolve network discuss individual opinion sequence issue mild assumption connectivity network social power individual converge constant strictly positive value number issue discuss increase network special topology term star topology social power accumulate individual centre star paper study strategic introduction new individual and/or interpersonal relationship social network star topology reduce social power centre individual fact strategy propose strategy derive necessary sufficient condition strength new interpersonal relationship base local information ensure centre individual long great social power social network interpretation condition strategy remarkably intuitive certain strategy favourable compare sociologically expect
eess,"Numerical Integration and Dynamic Discretization in Heuristic Search
  Planning over Hybrid Domains","  In this paper we look into the problem of planning over hybrid domains, where
change can be both discrete and instantaneous, or continuous over time. In
addition, it is required that each state on the trajectory induced by the
execution of plans complies with a given set of global constraints. We approach
the computation of plans for such domains as the problem of searching over a
deterministic state model. In this model, some of the successor states are
obtained by solving numerically the so-called initial value problem over a set
of ordinary differential equations (ODE) given by the current plan prefix.
These equations hold over time intervals whose duration is determined
dynamically, according to whether zero crossing events take place for a set of
invariant conditions. The resulting planner, FS+, incorporates these features
together with effective heuristic guidance. FS+ does not impose any of the
syntactic restrictions on process effects often found on the existing
literature on Hybrid Planning. A key concept of our approach is that a clear
separation is struck between planning and simulation time steps. The former is
the time allowed to observe the evolution of a given dynamical system before
committing to a future course of action, whilst the later is part of the model
of the environment. FS+ is shown to be a robust planner over a diverse set of
hybrid domains, taken from the existing literature on hybrid planning and
systems.
",paper look problem planning hybrid domain change discrete instantaneous continuous time addition require state trajectory induce execution plan complie give set global constraint approach computation plan domain problem search deterministic state model model successor state obtain solve numerically call initial value problem set ordinary differential equation ode give current plan prefix equation hold time interval duration determine dynamically accord zero crossing event place set invariant condition result planner fs+ incorporate feature effective heuristic guidance fs+ impose syntactic restriction process effect find exist literature hybrid planning key concept approach clear separation strike planning simulation time step time allow observe evolution give dynamical system commit future course action whilst later model environment fs+ show robust planner diverse set hybrid domain take exist literature hybrid planning system
eess,"Robust Power System Dynamic State Estimator with Non-Gaussian
  Measurement Noise: Part I--Theory","  This paper develops the theoretical framework and the equations of a new
robust Generalized Maximum-likelihood-type Unscented Kalman Filter (GM-UKF)
that is able to suppress observation and innovation outliers while filtering
out non-Gaussian measurement noise. Because the errors of the real and reactive
power measurements calculated using Phasor Measurement Units (PMUs) follow
long-tailed probability distributions, the conventional UKF provides strongly
biased state estimates since it relies on the weighted least squares estimator.
By contrast, the state estimates and residuals of our GM-UKF are proved to be
roughly Gaussian, allowing the sigma points to reliably approximate the mean
and the covariance matrices of the predicted and corrected state vectors. To
develop our GM-UKF, we first derive a batch-mode regression form by processing
the predictions and observations simultaneously, where the statistical
linearization approach is used. We show that the set of equations so derived
are equivalent to those of the unscented transformation. Then, a robust
GM-estimator that minimizes a convex Huber cost function while using weights
calculated via Projection Statistics (PS's) is proposed. The PS's are applied
to a two-dimensional matrix that consists of serially correlated predicted
state and innovation vectors to detect observation and innovation outliers.
These outliers are suppressed by the GM-estimator using the iteratively
reweighted least squares algorithm. Finally, the asymptotic error covariance
matrix of the GM-UKF state estimates is derived from the total influence
function. In the companion paper, extensive simulation results will be shown to
verify the effectiveness and robustness of the proposed method.
",paper develop theoretical framework equation new robust generalized maximum likelihood type unscented kalman filter gm ukf able suppress observation innovation outlier filter non gaussian measurement noise error real reactive power measurement calculate phasor measurement units pmu follow long tail probability distribution conventional ukf provide strongly biased state estimate rely weight square estimator contrast state estimate residual gm ukf prove roughly gaussian allow sigma point reliably approximate mean covariance matrix predict correct state vector develop gm ukf derive batch mode regression form process prediction observation simultaneously statistical linearization approach set equation derive equivalent unscented transformation robust gm estimator minimize convex huber cost function weight calculate projection statistics ps propose ps apply dimensional matrix consist serially correlate predict state innovation vector detect observation innovation outlier outlier suppress gm estimator iteratively reweighte square algorithm finally asymptotic error covariance matrix gm ukf state estimate derive total influence function companion paper extensive simulation result show verify effectiveness robustness propose method
eess,"On the Analysis of the DeGroot-Friedkin Model with Dynamic Relative
  Interaction Matrices","  This paper analyses the DeGroot-Friedkin model for evolution of the
individuals' social powers in a social network when the network topology varies
dynamically (described by dynamic relative interaction matrices). The
DeGroot-Friedkin model describes how individual social power (self-appraisal,
self-weight) evolves as a network of individuals discuss a sequence of issues.
We seek to study dynamically changing relative interactions because
interactions may change depending on the issue being discussed. In order to
explore the problem in detail, two different cases of issue-dependent network
topologies are studied. First, if the topology varies between issues in a
periodic manner, it is shown that the individuals' self-appraisals admit a
periodic solution. Second, if the topology changes arbitrarily, under the
assumption that each relative interaction matrix is doubly stochastic and
irreducible, the individuals' self-appraisals asymptotically converge to a
unique non-trivial equilibrium.
",paper analyse degroot friedkin model evolution individual social power social network network topology vary dynamically describe dynamic relative interaction matrix degroot friedkin model describe individual social power self appraisal self weight evolve network individual discuss sequence issue seek study dynamically change relative interaction interaction change depend issue discuss order explore problem detail different case issue dependent network topology study topology vary issue periodic manner show individual self appraisal admit periodic solution second topology change arbitrarily assumption relative interaction matrix doubly stochastic irreducible individual self appraisal asymptotically converge unique non trivial equilibrium
eess,"Robust Power System Dynamic State Estimator with Non-Gaussian
  Measurement Noise: Part II--Implementation and Results","  This paper is the second of a two-part series that discusses the
implementation issues and test results of a robust Unscented Kalman Filter
(UKF) for power system dynamic state estimation with non-Gaussian synchrophasor
measurement noise. The tuning of the parameters of our Generalized
Maximum-Likelihood-type robust UKF (GM-UKF) is presented and discussed in a
systematic way. Using simulations carried out on the IEEE 39-bus system, its
performance is evaluated under different scenarios, including i) the occurrence
of two different types of noises following thick-tailed distributions, namely
the Laplace or Cauchy probability distributions for real and reactive power
measurements; ii) the occurrence of observation and innovation outliers; iii)
the occurrence of PMU measurement losses due to communication failures; iv)
cyber attacks; and v) strong system nonlinearities. It is also compared to the
UKF and the Generalized Maximum-Likelihood-type robust iterated EKF (GM-IEKF).
Simulation results reveal that the GM-UKF outperforms the GM-IEKF and the UKF
in all scenarios considered. In particular, when the system is operating under
stressed conditions, inducing system nonlinearities, the GM-IEKF and the UKF
diverge while our GM-UKF does converge. In addition, when the power measurement
noises obey a Cauchy distribution, our GM-UKF converges to a state estimate
vector that exhibits a much higher statistical efficiency than that of the
GM-IEKF; by contrast, the UKF fails to converge. Finally, potential
applications and future work of the proposed GM-UKF are discussed in concluding
remarks section.
",paper second series discuss implementation issue test result robust unscented kalman filter ukf power system dynamic state estimation non gaussian synchrophasor measurement noise tuning parameter generalized maximum likelihood type robust ukf gm ukf present discuss systematic way simulation carry ieee 39 bus system performance evaluate different scenario include occurrence different type noise follow thick tail distribution laplace cauchy probability distribution real reactive power measurement ii occurrence observation innovation outlier iii occurrence pmu measurement loss communication failure iv cyber attack v strong system nonlinearitie compare ukf generalized maximum likelihood type robust iterate ekf gm iekf simulation result reveal gm ukf outperform gm iekf ukf scenario consider particular system operate stressed condition induce system nonlinearitie gm iekf ukf diverge gm ukf converge addition power measurement noise obey cauchy distribution gm ukf converge state estimate vector exhibit high statistical efficiency gm iekf contrast ukf fail converge finally potential application future work propose gm ukf discuss conclude remark section
eess,"Empirical Evaluation of Parallel Training Algorithms on Acoustic
  Modeling","  Deep learning models (DLMs) are state-of-the-art techniques in speech
recognition. However, training good DLMs can be time consuming especially for
production-size models and corpora. Although several parallel training
algorithms have been proposed to improve training efficiency, there is no clear
guidance on which one to choose for the task in hand due to lack of systematic
and fair comparison among them. In this paper we aim at filling this gap by
comparing four popular parallel training algorithms in speech recognition,
namely asynchronous stochastic gradient descent (ASGD), blockwise model-update
filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging
stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using
feed-forward deep neural networks (DNNs) and convolutional, long short-term
memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the
top choice to train acoustic models since it is most stable, scales well with
number of GPUs, can achieve reproducible results, and in many cases even
outperforms single-GPU SGD. ASGD can be used as a substitute in some cases.
",deep learning model dlm state art technique speech recognition train good dlm time consume especially production size model corpora parallel training algorithm propose improve training efficiency clear guidance choose task hand lack systematic fair comparison paper aim fill gap compare popular parallel training algorithm speech recognition asynchronous stochastic gradient descent asgd blockwise model update filtering bmuf bulk synchronous parallel bsp elastic averaging stochastic gradient descent easgd 1000 hour librispeech corpora feed forward deep neural network dnn convolutional long short term memory dnn cldnn base experiment recommend bmuf choice train acoustic model stable scale number gpu achieve reproducible result case outperform single gpu sgd asgd substitute case
eess,"Multi-talker Speech Separation with Utterance-level Permutation
  Invariant Training of Deep Recurrent Neural Networks","  In this paper we propose the utterance-level Permutation Invariant Training
(uPIT) technique. uPIT is a practically applicable, end-to-end, deep learning
based solution for speaker independent multi-talker speech separation.
Specifically, uPIT extends the recently proposed Permutation Invariant Training
(PIT) technique with an utterance-level cost function, hence eliminating the
need for solving an additional permutation problem during inference, which is
otherwise required by frame-level PIT. We achieve this using Recurrent Neural
Networks (RNNs) that, during training, minimize the utterance-level separation
error, hence forcing separated frames belonging to the same speaker to be
aligned to the same output stream. In practice, this allows RNNs, trained with
uPIT, to separate multi-talker mixed speech without any prior knowledge of
signal duration, number of speakers, speaker identity or gender. We evaluated
uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks
and found that uPIT outperforms techniques based on Non-negative Matrix
Factorization (NMF) and Computational Auditory Scene Analysis (CASA), and
compares favorably with Deep Clustering (DPCL) and the Deep Attractor Network
(DANet). Furthermore, we found that models trained with uPIT generalize well to
unseen speakers and languages. Finally, we found that a single model, trained
with uPIT, can handle both two-speaker, and three-speaker speech mixtures.
",paper propose utterance level permutation invariant training upit technique upit practically applicable end end deep learning base solution speaker independent multi talker speech separation specifically upit extend recently propose permutation invariant training pit technique utterance level cost function eliminate need solve additional permutation problem inference require frame level pit achieve recurrent neural networks rnns training minimize utterance level separation error force separated frame belong speaker align output stream practice allow rnn train upit separate multi talker mixed speech prior knowledge signal duration number speaker speaker identity gender evaluate upit wsj0 danish two- talker mixed speech separation task find upit outperform technique base non negative matrix factorization nmf computational auditory scene analysis casa compare favorably deep clustering dpcl deep attractor network danet furthermore find model train upit generalize unseen speaker language finally find single model train upit handle speaker speaker speech mixture
eess,"Flatness-based control of a two-degree-of-freedom platform with
  pneumatic artificial muscles","  Pneumatic artificial muscles are a quite interesting type of actuators which
have a very high power-to-weight and power-to-volume ratio. However, their
efficient use requires very accurate control methods which can take into
account their complex dynamic, which is highly nonlinear. This paper consider a
model of two-degree-of-freedom platform whose attitude is determined by three
pneumatic muscles controlled by servovalves, which mimics a simplified version
of a Stewart platform. For this testbed, a model-based control approach is
proposed, based on accurate first principle modeling of the muscles and the
platform and on a static model for the servovalve. The employed control method
is the so-called flatness-based control introduced by Fliess. The paper first
recalls the basics of this control technique and then it shows how it can be
applied to the proposed experimental platform; being flatness-based control an
open-loop kind of control, a proportional-integral controller is added on top
of it in order to add robustness with respect to modelling errors and external
perturbations. At the end of the paper, the effectiveness of the proposed
approach is shown by means of experimental results. A clear improvement of the
tracking performance is visible compared to a simple proportional-integral
controller.
",pneumatic artificial muscle interesting type actuator high power weight power volume ratio efficient use require accurate control method account complex dynamic highly nonlinear paper consider model degree freedom platform attitude determine pneumatic muscle control servovalve mimic simplified version stewart platform testbe model base control approach propose base accurate principle modeling muscle platform static model servovalve employ control method call flatness base control introduce fliess paper recall basic control technique show apply propose experimental platform flatness base control open loop kind control proportional integral controller add order add robustness respect modelling error external perturbation end paper effectiveness propose approach show mean experimental result clear improvement tracking performance visible compare simple proportional integral controller
eess,"Cooperative Localisation of a GPS-Denied UAV in 3-Dimensional Space
  Using Direction of Arrival Measurements","  This paper presents a novel approach for localising a GPS (Global Positioning
System)-denied Unmanned Aerial Vehicle (UAV) with the aid of a GPS-equipped UAV
in three-dimensional space. The GPS-equipped UAV makes discrete-time broadcasts
of its global coordinates. The GPS-denied UAV simultaneously receives the
broadcast and takes direction of arrival (DOA) measurements towards the origin
of the broadcast in its local coordinate frame (obtained via an inertial
navigation system (INS)). The aim is to determine the difference between the
local and global frames, described by a rotation and a translation. In the
noiseless case, global coordinates were recovered exactly by solving a system
of linear equations. When DOA measurements are contaminated with noise, rank
relaxed semidefinite programming (SDP) and the Orthogonal Procrustes algorithm
are employed. Simulations are provided and factors affecting accuracy, such as
noise levels and number of measurements, are explored.
",paper present novel approach localise gps global positioning system)-denie unmanned aerial vehicle uav aid gps equip uav dimensional space gps equip uav make discrete time broadcast global coordinate gps deny uav simultaneously receive broadcast take direction arrival doa measurement origin broadcast local coordinate frame obtain inertial navigation system ins aim determine difference local global frame describe rotation translation noiseless case global coordinate recover exactly solve system linear equation doa measurement contaminate noise rank relax semidefinite programming sdp orthogonal procrustes algorithm employ simulation provide factor affect accuracy noise level number measurement explore
eess,"Online Simultaneous State and Parameter Estimation for Second-order
  Nonlinear Systems","  In this paper, a concurrent learning based adaptive observer is developed for
a class of second-order nonlinear time-invariant systems with uncertain
dynamics. The developed technique results in simultaneous online state and
parameter estimation. A Lyapunov-based analysis is used to show that the state
and parameter estimation errors are uniformly ultimately bounded. As opposed to
persistent excitation which is required for parameter estimation in traditional
adaptive control methods, the developed technique only requires excitation over
a finite time interval.
",paper concurrent learning base adaptive observer develop class second order nonlinear time invariant system uncertain dynamic developed technique result simultaneous online state parameter estimation lyapunov base analysis state parameter estimation error uniformly ultimately bound oppose persistent excitation require parameter estimation traditional adaptive control method develop technique require excitation finite time interval
eess,Optimal Precoders for Tracking the AoD and AoA of a mm-Wave Path,"  In millimeter-wave channels, most of the received energy is carried by a few
paths. Traditional precoders sweep the angle-of-departure (AoD) and
angle-of-arrival (AoA) space with directional precoders to identify directions
with largest power. Such precoders are heuristic and lead to sub-optimal
AoD/AoA estimation. We derive optimal precoders, minimizing the Cram\'{e}r-Rao
bound (CRB) of the AoD/AoA, assuming a fully digital architecture at the
transmitter and spatial filtering of a single path. The precoders are found by
solving a suitable convex optimization problem. We demonstrate that the
accuracy can be improved by at least a factor of two over traditional
precoders, and show that there is an optimal number of distinct precoders
beyond which the CRB does not improve.
",millimeter wave channel received energy carry path traditional precoder sweep angle departure aod angle arrival aoa space directional precoder identify direction large power precoder heuristic lead sub optimal aod aoa estimation derive optimal precoder minimize cram\'{e}r rao bind crb aod aoa assume fully digital architecture transmitter spatial filtering single path precoder find solve suitable convex optimization problem demonstrate accuracy improve factor traditional precoder optimal number distinct precoder crb improve
econ,Quantile and Probability Curves Without Crossing,"  This paper proposes a method to address the longstanding problem of lack of
monotonicity in estimation of conditional and structural quantile functions,
also known as the quantile crossing problem. The method consists in sorting or
monotone rearranging the original estimated non-monotone curve into a monotone
rearranged curve. We show that the rearranged curve is closer to the true
quantile curve in finite samples than the original curve, establish a
functional delta method for rearrangement-related operators, and derive
functional limit theory for the entire rearranged curve and its functionals. We
also establish validity of the bootstrap for estimating the limit law of the
the entire rearranged curve and its functionals. Our limit results are generic
in that they apply to every estimator of a monotone econometric function,
provided that the estimator satisfies a functional central limit theorem and
the function satisfies some smoothness conditions. Consequently, our results
apply to estimation of other econometric functions with monotonicity
restrictions, such as demand, production, distribution, and structural
distribution functions. We illustrate the results with an application to
estimation of structural quantile functions using data on Vietnam veteran
status and earnings.
",paper propose method address longstanding problem lack monotonicity estimation conditional structural quantile function know quantile crossing problem method consist sort monotone rearrange original estimate non monotone curve monotone rearrange curve rearrange curve close true quantile curve finite sample original curve establish functional delta method rearrangement relate operator derive functional limit theory entire rearrange curve functional establish validity bootstrap estimate limit law entire rearrange curve functional limit result generic apply estimator monotone econometric function provide estimator satisfy functional central limit theorem function satisfy smoothness condition consequently result apply estimation econometric function monotonicity restriction demand production distribution structural distribution function illustrate result application estimation structural quantile function datum vietnam veteran status earning
econ,Improving Estimates of Monotone Functions by Rearrangement,"  Suppose that a target function is monotonic, namely, weakly increasing, and
an original estimate of the target function is available, which is not weakly
increasing. Many common estimation methods used in statistics produce such
estimates. We show that these estimates can always be improved with no harm
using rearrangement techniques: The rearrangement methods, univariate and
multivariate, transform the original estimate to a monotonic estimate, and the
resulting estimate is closer to the true curve in common metrics than the
original estimate. We illustrate the results with a computational example and
an empirical example dealing with age-height growth charts.
",suppose target function monotonic weakly increase original estimate target function available weakly increase common estimation method statistic produce estimate estimate improve harm rearrangement technique rearrangement method univariate multivariate transform original estimate monotonic estimate result estimate close true curve common metric original estimate illustrate result computational example empirical example deal age height growth chart
econ,Rearranging Edgeworth-Cornish-Fisher Expansions,"  This paper applies a regularization procedure called increasing rearrangement
to monotonize Edgeworth and Cornish-Fisher expansions and any other related
approximations of distribution and quantile functions of sample statistics.
Besides satisfying the logical monotonicity, required of distribution and
quantile functions, the procedure often delivers strikingly better
approximations to the distribution and quantile functions of the sample mean
than the original Edgeworth-Cornish-Fisher expansions.
",paper apply regularization procedure call increase rearrangement monotonize edgeworth cornish fisher expansion related approximation distribution quantile function sample statistic satisfy logical monotonicity require distribution quantile function procedure deliver strikingly well approximation distribution quantile function sample mean original edgeworth cornish fisher expansion
econ,"Evolutionarily stable strategies of random games, and the vertices of
  random polygons","  An evolutionarily stable strategy (ESS) is an equilibrium strategy that is
immune to invasions by rare alternative (``mutant'') strategies. Unlike Nash
equilibria, ESS do not always exist in finite games. In this paper we address
the question of what happens when the size of the game increases: does an ESS
exist for ``almost every large'' game? Letting the entries in the $n\times n$
game matrix be independently randomly chosen according to a distribution $F$,
we study the number of ESS with support of size $2.$ In particular, we show
that, as $n\to \infty$, the probability of having such an ESS: (i) converges to
1 for distributions $F$ with ``exponential and faster decreasing tails'' (e.g.,
uniform, normal, exponential); and (ii) converges to $1-1/\sqrt{e}$ for
distributions $F$ with ``slower than exponential decreasing tails'' (e.g.,
lognormal, Pareto, Cauchy). Our results also imply that the expected number of
vertices of the convex hull of $n$ random points in the plane converges to
infinity for the distributions in (i), and to 4 for the distributions in (ii).
",evolutionarily stable strategy ess equilibrium strategy immune invasion rare alternative ` ` mutant strategy unlike nash equilibrium ess exist finite game paper address question happen size game increase ess exist ` ` large game let entry $ n\time n$ game matrix independently randomly choose accord distribution $ f$ study number ess support size $ 2.$ particular $ n\to \infty$ probability have ess converge 1 distribution $ f$ ` ` exponential fast decrease tail e.g. uniform normal exponential ii converge $ 1 1/\sqrt{e}$ distribution $ f$ ` ` slow exponential decrease tail e.g. lognormal pareto cauchy result imply expect number vertex convex hull $ n$ random point plane converge infinity distribution 4 distribution ii
econ,Projective Expected Utility,"  Motivated by several classic decision-theoretic paradoxes, and by analogies
with the paradoxes which in physics motivated the development of quantum
mechanics, we introduce a projective generalization of expected utility along
the lines of the quantum-mechanical generalization of probability theory. The
resulting decision theory accommodates the dominant paradoxes, while retaining
significant simplicity and tractability. In particular, every finite game
within this larger class of preferences still has an equilibrium.
",motivate classic decision theoretic paradox analogy paradox physics motivate development quantum mechanic introduce projective generalization expected utility line quantum mechanical generalization probability theory result decision theory accommodate dominant paradox retain significant simplicity tractability particular finite game large class preference equilibrium
econ,"Improving Point and Interval Estimates of Monotone Functions by
  Rearrangement","  Suppose that a target function is monotonic, namely, weakly increasing, and
an available original estimate of this target function is not weakly
increasing. Rearrangements, univariate and multivariate, transform the original
estimate to a monotonic estimate that always lies closer in common metrics to
the target function. Furthermore, suppose an original simultaneous confidence
interval, which covers the target function with probability at least
$1-\alpha$, is defined by an upper and lower end-point functions that are not
weakly increasing. Then the rearranged confidence interval, defined by the
rearranged upper and lower end-point functions, is shorter in length in common
norms than the original interval and also covers the target function with
probability at least $1-\alpha$. We demonstrate the utility of the improved
point and interval estimates with an age-height growth chart example.
",suppose target function monotonic weakly increase available original estimate target function weakly increase rearrangement univariate multivariate transform original estimate monotonic estimate lie close common metric target function furthermore suppose original simultaneous confidence interval cover target function probability $ 1-\alpha$ define upper low end point function weakly increase rearrange confidence interval define rearrange upper low end point function short length common norm original interval cover target function probability $ 1-\alpha$. demonstrate utility improved point interval estimate age height growth chart example
econ,Inference on Counterfactual Distributions,"  Counterfactual distributions are important ingredients for policy analysis
and decomposition analysis in empirical economics. In this article we develop
modeling and inference tools for counterfactual distributions based on
regression methods. The counterfactual scenarios that we consider consist of
ceteris paribus changes in either the distribution of covariates related to the
outcome of interest or the conditional distribution of the outcome given
covariates. For either of these scenarios we derive joint functional central
limit theorems and bootstrap validity results for regression-based estimators
of the status quo and counterfactual outcome distributions. These results allow
us to construct simultaneous confidence sets for function-valued effects of the
counterfactual changes, including the effects on the entire distribution and
quantile functions of the outcome as well as on related functionals. These
confidence sets can be used to test functional hypotheses such as no-effect,
positive effect, or stochastic dominance. Our theory applies to general
counterfactual changes and covers the main regression methods including
classical, quantile, duration, and distribution regressions. We illustrate the
results with an empirical application to wage decompositions using data for the
United States.
  As a part of developing the main results, we introduce distribution
regression as a comprehensive and flexible tool for modeling and estimating the
\textit{entire} conditional distribution. We show that distribution regression
encompasses the Cox duration regression and represents a useful alternative to
quantile regression. We establish functional central limit theorems and
bootstrap validity results for the empirical distribution regression process
and various related functionals.
",counterfactual distribution important ingredient policy analysis decomposition analysis empirical economic article develop modeling inference tool counterfactual distribution base regression method counterfactual scenario consider consist ceteris paribus change distribution covariate relate outcome interest conditional distribution outcome give covariate scenario derive joint functional central limit theorem bootstrap validity result regression base estimator status quo counterfactual outcome distribution result allow construct simultaneous confidence set function value effect counterfactual change include effect entire distribution quantile function outcome relate functional confidence set test functional hypothesis effect positive effect stochastic dominance theory apply general counterfactual change cover main regression method include classical quantile duration distribution regression illustrate result empirical application wage decomposition datum united states develop main result introduce distribution regression comprehensive flexible tool modeling estimate \textit{entire conditional distribution distribution regression encompass cox duration regression represent useful alternative quantile regression establish functional central limit theorem bootstrap validity result empirical distribution regression process related functional
econ,Average and Quantile Effects in Nonseparable Panel Models,"  Nonseparable panel models are important in a variety of economic settings,
including discrete choice. This paper gives identification and estimation
results for nonseparable models under time homogeneity conditions that are like
""time is randomly assigned"" or ""time is an instrument."" Partial identification
results for average and quantile effects are given for discrete regressors,
under static or dynamic conditions, in fully nonparametric and in
semiparametric models, with time effects. It is shown that the usual, linear,
fixed-effects estimator is not a consistent estimator of the identified average
effect, and a consistent estimator is given. A simple estimator of identified
quantile treatment effects is given, providing a solution to the important
problem of estimating quantile treatment effects from panel data. Bounds for
overall effects in static and dynamic models are given. The dynamic bounds
provide a partial identification solution to the important problem of
estimating the effect of state dependence in the presence of unobserved
heterogeneity. The impact of $T$, the number of time periods, is shown by
deriving shrinkage rates for the identified set as $T$ grows. We also consider
semiparametric, discrete-choice models and find that semiparametric panel
bounds can be much tighter than nonparametric bounds.
Computationally-convenient methods for semiparametric models are presented. We
propose a novel inference method that applies in panel data and other settings
and show that it produces uniformly valid confidence regions in large samples.
We give empirical illustrations.
",nonseparable panel model important variety economic setting include discrete choice paper give identification estimation result nonseparable model time homogeneity condition like time randomly assign time instrument partial identification result average quantile effect give discrete regressor static dynamic condition fully nonparametric semiparametric model time effect show usual linear fix effect estimator consistent estimator identify average effect consistent estimator give simple estimator identify quantile treatment effect give provide solution important problem estimate quantile treatment effect panel datum bound overall effect static dynamic model give dynamic bound provide partial identification solution important problem estimate effect state dependence presence unobserved heterogeneity impact $ t$ number time period show deriving shrinkage rate identify set $ t$ grow consider semiparametric discrete choice model find semiparametric panel bound tight nonparametric bound computationally convenient method semiparametric model present propose novel inference method apply panel datum setting produce uniformly valid confidence region large sample empirical illustration
econ,L1-Penalized Quantile Regression in High-Dimensional Sparse Models,"  We consider median regression and, more generally, a possibly infinite
collection of quantile regressions in high-dimensional sparse models. In these
models the overall number of regressors $p$ is very large, possibly larger than
the sample size $n$, but only $s$ of these regressors have non-zero impact on
the conditional quantile of the response variable, where $s$ grows slower than
$n$. We consider quantile regression penalized by the $\ell_1$-norm of
coefficients ($\ell_1$-QR). First, we show that $\ell_1$-QR is consistent at
the rate $\sqrt{s/n} \sqrt{\log p}$. The overall number of regressors $p$
affects the rate only through the $\log p$ factor, thus allowing nearly
exponential growth in the number of zero-impact regressors. The rate result
holds under relatively weak conditions, requiring that $s/n$ converges to zero
at a super-logarithmic speed and that regularization parameter satisfies
certain theoretical constraints. Second, we propose a pivotal, data-driven
choice of the regularization parameter and show that it satisfies these
theoretical constraints. Third, we show that $\ell_1$-QR correctly selects the
true minimal model as a valid submodel, when the non-zero coefficients of the
true model are well separated from zero. We also show that the number of
non-zero coefficients in $\ell_1$-QR is of same stochastic order as $s$.
Fourth, we analyze the rate of convergence of a two-step estimator that applies
ordinary quantile regression to the selected model. Fifth, we evaluate the
performance of $\ell_1$-QR in a Monte-Carlo experiment, and illustrate its use
on an international economic growth application.
",consider median regression generally possibly infinite collection quantile regression high dimensional sparse model model overall number regressor $ p$ large possibly large sample size $ n$ $ s$ regressor non zero impact conditional quantile response variable $ s$ grow slow $ n$. consider quantile regression penalize $ \ell_1$-norm coefficient $ \ell_1$-qr $ \ell_1$-qr consistent rate $ \sqrt{s n \sqrt{\log p}$. overall number regressor $ p$ affect rate $ \log p$ factor allow nearly exponential growth number zero impact regressor rate result hold relatively weak condition require $ s n$ converge zero super logarithmic speed regularization parameter satisfie certain theoretical constraint second propose pivotal data drive choice regularization parameter satisfy theoretical constraint $ \ell_1$-qr correctly select true minimal model valid submodel non zero coefficient true model separate zero number non zero coefficient $ \ell_1$-qr stochastic order $ s$. fourth analyze rate convergence step estimator apply ordinary quantile regression select model fifth evaluate performance $ \ell_1$-qr monte carlo experiment illustrate use international economic growth application
econ,"Posterior Inference in Curved Exponential Families under Increasing
  Dimensions","  This work studies the large sample properties of the posterior-based
inference in the curved exponential family under increasing dimension. The
curved structure arises from the imposition of various restrictions on the
model, such as moment restrictions, and plays a fundamental role in
econometrics and others branches of data analysis. We establish conditions
under which the posterior distribution is approximately normal, which in turn
implies various good properties of estimation and inference procedures based on
the posterior. In the process we also revisit and improve upon previous results
for the exponential family under increasing dimension by making use of
concentration of measure. We also discuss a variety of applications to
high-dimensional versions of the classical econometric models including the
multinomial model with moment restrictions, seemingly unrelated regression
equations, and single structural equation models. In our analysis, both the
parameter dimension and the number of moments are increasing with the sample
size.
",work study large sample property posterior base inference curved exponential family increase dimension curved structure arise imposition restriction model moment restriction play fundamental role econometric branch datum analysis establish condition posterior distribution approximately normal turn imply good property estimation inference procedure base posterior process revisit improve previous result exponential family increase dimension make use concentration measure discuss variety application high dimensional version classical econometric model include multinomial model moment restriction seemingly unrelated regression equation single structural equation model analysis parameter dimension number moment increase sample size
econ,Constructive Decision Theory,"  In most contemporary approaches to decision making, a decision problem is
described by a sets of states and set of outcomes, and a rich set of acts,
which are functions from states to outcomes over which the decision maker (DM)
has preferences. Most interesting decision problems, however, do not come with
a state space and an outcome space. Indeed, in complex problems it is often far
from clear what the state and outcome spaces would be. We present an
alternative foundation for decision making, in which the primitive objects of
choice are syntactic programs. A representation theorem is proved in the spirit
of standard representation theorems, showing that if the DM's preference
relation on objects of choice satisfies appropriate axioms, then there exist a
set S of states, a set O of outcomes, a way of interpreting the objects of
choice as functions from S to O, a probability on S, and a utility function on
O, such that the DM prefers choice a to choice b if and only if the expected
utility of a is higher than that of b. Thus, the state space and outcome space
are subjective, just like the probability and utility; they are not part of the
description of the problem. In principle, a modeler can test for SEU behavior
without having access to states or outcomes. We illustrate the power of our
approach by showing that it can capture decision makers who are subject to
framing effects.
",contemporary approach decision making decision problem describe set state set outcome rich set act function state outcome decision maker dm preference interesting decision problem come state space outcome space complex problem far clear state outcome space present alternative foundation decision making primitive object choice syntactic program representation theorem prove spirit standard representation theorem show dm preference relation object choice satisfie appropriate axiom exist set s state set o outcome way interpret object choice function s o probability s utility function o dm prefers choice choice b expect utility high b. state space outcome space subjective like probability utility description problem principle modeler test seu behavior have access state outcome illustrate power approach show capture decision maker subject frame effect
econ,"Complete Characterization of Functions Satisfying the Conditions of
  Arrow's Theorem","  Arrow's theorem implies that a social choice function satisfying
Transitivity, the Pareto Principle (Unanimity) and Independence of Irrelevant
Alternatives (IIA) must be dictatorial. When non-strict preferences are
allowed, a dictatorial social choice function is defined as a function for
which there exists a single voter whose strict preferences are followed. This
definition allows for many different dictatorial functions. In particular, we
construct examples of dictatorial functions which do not satisfy Transitivity
and IIA. Thus Arrow's theorem, in the case of non-strict preferences, does not
provide a complete characterization of all social choice functions satisfying
Transitivity, the Pareto Principle, and IIA.
  The main results of this article provide such a characterization for Arrow's
theorem, as well as for follow up results by Wilson. In particular, we
strengthen Arrow's and Wilson's result by giving an exact if and only if
condition for a function to satisfy Transitivity and IIA (and the Pareto
Principle). Additionally, we derive formulas for the number of functions
satisfying these conditions.
",arrow theorem imply social choice function satisfy transitivity pareto principle unanimity independence irrelevant alternatives iia dictatorial non strict preference allow dictatorial social choice function define function exist single voter strict preference follow definition allow different dictatorial function particular construct example dictatorial function satisfy transitivity iia arrow theorem case non strict preference provide complete characterization social choice function satisfy transitivity pareto principle iia main result article provide characterization arrow theorem follow result wilson particular strengthen arrow wilson result give exact condition function satisfy transitivity iia pareto principle additionally derive formula number function satisfy condition
econ,"Inference for Extremal Conditional Quantile Models, with an Application
  to Market and Birthweight Risks","  Quantile regression is an increasingly important empirical tool in economics
and other sciences for analyzing the impact of a set of regressors on the
conditional distribution of an outcome. Extremal quantile regression, or
quantile regression applied to the tails, is of interest in many economic and
financial applications, such as conditional value-at-risk, production
efficiency, and adjustment bands in (S,s) models. In this paper we provide
feasible inference tools for extremal conditional quantile models that rely
upon extreme value approximations to the distribution of self-normalized
quantile regression statistics. The methods are simple to implement and can be
of independent interest even in the non-regression case. We illustrate the
results with two empirical examples analyzing extreme fluctuations of a stock
return and extremely low percentiles of live infants' birthweights in the range
between 250 and 1500 grams.
",quantile regression increasingly important empirical tool economic science analyze impact set regressor conditional distribution outcome extremal quantile regression quantile regression apply tail interest economic financial application conditional value risk production efficiency adjustment band s s model paper provide feasible inference tool extremal conditional quantile model rely extreme value approximation distribution self normalize quantile regression statistic method simple implement independent interest non regression case illustrate result empirical example analyze extreme fluctuation stock return extremely low percentile live infant birthweight range 250 1500 gram
econ,Toy Model for Large Non-Symmetric Random Matrices,"  Non-symmetric rectangular correlation matrices occur in many problems in
economics. We test the method of extracting statistically meaningful
correlations between input and output variables of large dimensionality and
build a toy model for artificially included correlations in large random time
series.The results are then applied to analysis of polish macroeconomic data
and can be used as an alternative to classical cointegration approach.
",non symmetric rectangular correlation matrix occur problem economic test method extract statistically meaningful correlation input output variable large dimensionality build toy model artificially include correlation large random time series result apply analysis polish macroeconomic datum alternative classical cointegration approach
econ,"Sparse Models and Methods for Optimal Instruments with an Application to
  Eminent Domain","  We develop results for the use of Lasso and Post-Lasso methods to form
first-stage predictions and estimate optimal instruments in linear instrumental
variables (IV) models with many instruments, $p$. Our results apply even when
$p$ is much larger than the sample size, $n$. We show that the IV estimator
based on using Lasso or Post-Lasso in the first stage is root-n consistent and
asymptotically normal when the first-stage is approximately sparse; i.e. when
the conditional expectation of the endogenous variables given the instruments
can be well-approximated by a relatively small set of variables whose
identities may be unknown. We also show the estimator is semi-parametrically
efficient when the structural error is homoscedastic. Notably our results allow
for imperfect model selection, and do not rely upon the unrealistic ""beta-min""
conditions that are widely used to establish validity of inference following
model selection. In simulation experiments, the Lasso-based IV estimator with a
data-driven penalty performs well compared to recently advocated
many-instrument-robust procedures. In an empirical example dealing with the
effect of judicial eminent domain decisions on economic outcomes, the
Lasso-based IV estimator outperforms an intuitive benchmark.
  In developing the IV results, we establish a series of new results for Lasso
and Post-Lasso estimators of nonparametric conditional expectation functions
which are of independent theoretical and practical interest. We construct a
modification of Lasso designed to deal with non-Gaussian, heteroscedastic
disturbances which uses a data-weighted $\ell_1$-penalty function. Using
moderate deviation theory for self-normalized sums, we provide convergence
rates for the resulting Lasso and Post-Lasso estimators that are as sharp as
the corresponding rates in the homoscedastic Gaussian case under the condition
that $\log p = o(n^{1/3})$.
",develop result use lasso post lasso method form stage prediction estimate optimal instrument linear instrumental variable iv model instrument $ p$. result apply $ p$ large sample size $ n$. iv estimator base lasso post lasso stage root n consistent asymptotically normal stage approximately sparse i.e. conditional expectation endogenous variable give instrument approximate relatively small set variable identity unknown estimator semi parametrically efficient structural error homoscedastic notably result allow imperfect model selection rely unrealistic beta min condition widely establish validity inference follow model selection simulation experiment lasso base iv estimator data drive penalty perform compare recently advocate instrument robust procedure empirical example deal effect judicial eminent domain decision economic outcome lasso base iv estimator outperform intuitive benchmark develop iv result establish series new result lasso post lasso estimator nonparametric conditional expectation function independent theoretical practical interest construct modification lasso design deal non gaussian heteroscedastic disturbance use data weight $ \ell_1$-penalty function moderate deviation theory self normalize sum provide convergence rate result lasso post lasso estimator sharp correspond rate homoscedastic gaussian case condition $ \log p = o(n^{1/3})$.
econ,LASSO Methods for Gaussian Instrumental Variables Models,"  In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO,
sqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate
optimal instruments in linear instrumental variables (IV) models with many
instruments in the canonical Gaussian case. The methods apply even when the
number of instruments is much larger than the sample size. We derive asymptotic
distributions for the resulting IV estimators and provide conditions under
which these sparsity-based IV estimators are asymptotically oracle-efficient.
In simulation experiments, a sparsity-based IV estimator with a data-driven
penalty performs well compared to recently advocated many-instrument-robust
procedures. We illustrate the procedure in an empirical example using the
Angrist and Krueger (1991) schooling data.
",note propose use sparse method e.g. lasso post lasso sqrt lasso post sqrt lasso form stage prediction estimate optimal instrument linear instrumental variable iv model instrument canonical gaussian case method apply number instrument large sample size derive asymptotic distribution result iv estimator provide condition sparsity base iv estimator asymptotically oracle efficient simulation experiment sparsity base iv estimator data drive penalty perform compare recently advocate instrument robust procedure illustrate procedure empirical example angrist krueger 1991 school datum
econ,Quantile Regression with Censoring and Endogeneity,"  In this paper, we develop a new censored quantile instrumental variable
(CQIV) estimator and describe its properties and computation. The CQIV
estimator combines Powell (1986) censored quantile regression (CQR) to deal
with censoring, with a control variable approach to incorporate endogenous
regressors. The CQIV estimator is obtained in two stages that are non-additive
in the unobservables. The first stage estimates a non-additive model with
infinite dimensional parameters for the control variable, such as a quantile or
distribution regression model. The second stage estimates a non-additive
censored quantile regression model for the response variable of interest,
including the estimated control variable to deal with endogeneity. For
computation, we extend the algorithm for CQR developed by Chernozhukov and Hong
(2002) to incorporate the estimation of the control variable. We give generic
regularity conditions for asymptotic normality of the CQIV estimator and for
the validity of resampling methods to approximate its asymptotic distribution.
We verify these conditions for quantile and distribution regression estimation
of the control variable. Our analysis covers two-stage (uncensored) quantile
regression with non-additive first stage as an important special case. We
illustrate the computation and applicability of the CQIV estimator with a
Monte-Carlo numerical example and an empirical application on estimation of
Engel curves for alcohol.
",paper develop new censor quantile instrumental variable cqiv estimator describe property computation cqiv estimator combine powell 1986 censor quantile regression cqr deal censoring control variable approach incorporate endogenous regressor cqiv estimator obtain stage non additive unobservable stage estimate non additive model infinite dimensional parameter control variable quantile distribution regression model second stage estimate non additive censor quantile regression model response variable interest include estimate control variable deal endogeneity computation extend algorithm cqr develop chernozhukov hong 2002 incorporate estimation control variable generic regularity condition asymptotic normality cqiv estimator validity resample method approximate asymptotic distribution verify condition quantile distribution regression estimation control variable analysis cover stage uncensored quantile regression non additive stage important special case illustrate computation applicability cqiv estimator monte carlo numerical example empirical application estimation engel curve alcohol
econ,Conditional Quantile Processes based on Series or Many Regressors,"  Quantile regression (QR) is a principal regression method for analyzing the
impact of covariates on outcomes. The impact is described by the conditional
quantile function and its functionals. In this paper we develop the
nonparametric QR-series framework, covering many regressors as a special case,
for performing inference on the entire conditional quantile function and its
linear functionals. In this framework, we approximate the entire conditional
quantile function by a linear combination of series terms with
quantile-specific coefficients and estimate the function-valued coefficients
from the data. We develop large sample theory for the QR-series coefficient
process, namely we obtain uniform strong approximations to the QR-series
coefficient process by conditionally pivotal and Gaussian processes. Based on
these strong approximations, or couplings, we develop four resampling methods
(pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be
used for inference on the entire QR-series coefficient function.
  We apply these results to obtain estimation and inference methods for linear
functionals of the conditional quantile function, such as the conditional
quantile function itself, its partial derivatives, average partial derivatives,
and conditional average partial derivatives. Specifically, we obtain uniform
rates of convergence and show how to use the four resampling methods mentioned
above for inference on the functionals. All of the above results are for
function-valued parameters, holding uniformly in both the quantile index and
the covariate value, and covering the pointwise case as a by-product. We
demonstrate the practical utility of these results with an example, where we
estimate the price elasticity function and test the Slutsky condition of the
individual demand for gasoline, as indexed by the individual unobserved
propensity for gasoline consumption.
",quantile regression qr principal regression method analyze impact covariate outcome impact describe conditional quantile function functional paper develop nonparametric qr series framework cover regressor special case perform inference entire conditional quantile function linear functional framework approximate entire conditional quantile function linear combination series term quantile specific coefficient estimate function value coefficient datum develop large sample theory qr series coefficient process obtain uniform strong approximation qr series coefficient process conditionally pivotal gaussian process base strong approximation coupling develop resample method pivotal gradient bootstrap gaussian weight bootstrap inference entire qr series coefficient function apply result obtain estimation inference method linear functional conditional quantile function conditional quantile function partial derivative average partial derivative conditional average partial derivative specifically obtain uniform rate convergence use resample method mention inference functional result function value parameter hold uniformly quantile index covariate value cover pointwise case product demonstrate practical utility result example estimate price elasticity function test slutsky condition individual demand gasoline index individual unobserved propensity gasoline consumption
econ,High Dimensional Sparse Econometric Models: An Introduction,"  In this chapter we discuss conceptually high dimensional sparse econometric
models as well as estimation of these models using L1-penalization and
post-L1-penalization methods. Focusing on linear and nonparametric regression
frameworks, we discuss various econometric examples, present basic theoretical
results, and illustrate the concepts and methods with Monte Carlo simulations
and an empirical application. In the application, we examine and confirm the
empirical validity of the Solow-Swan model for international economic growth.
",chapter discuss conceptually high dimensional sparse econometric model estimation model l1 penalization post l1 penalization method focus linear nonparametric regression framework discuss econometric example present basic theoretical result illustrate concept method monte carlo simulation empirical application application examine confirm empirical validity solow swan model international economic growth
econ,Team Decision Problems with Classical and Quantum Signals,"  We study team decision problems where communication is not possible, but
coordination among team members can be realized via signals in a shared
environment. We consider a variety of decision problems that differ in what
team members know about one another's actions and knowledge. For each type of
decision problem, we investigate how different assumptions on the available
signals affect team performance. Specifically, we consider the cases of
perfectly correlated, i.i.d., and exchangeable classical signals, as well as
the case of quantum signals. We find that, whereas in perfect-recall trees
(Kuhn [1950], [1953]) no type of signal improves performance, in
imperfect-recall trees quantum signals may bring an improvement. Isbell [1957]
proved that in non-Kuhn trees, classical i.i.d. signals may improve
performance. We show that further improvement may be possible by use of
classical exchangeable or quantum signals. We include an example of the effect
of quantum signals in the context of high-frequency trading.
",study team decision problem communication possible coordination team member realize signal shared environment consider variety decision problem differ team member know action knowledge type decision problem investigate different assumption available signal affect team performance specifically consider case perfectly correlate i.i.d exchangeable classical signal case quantum signal find perfect recall tree kuhn 1950 1953 type signal improve performance imperfect recall tree quantum signal bring improvement isbell 1957 prove non kuhn tree classical i.i.d signal improve performance improvement possible use classical exchangeable quantum signal include example effect quantum signal context high frequency trading
econ,Inference for High-Dimensional Sparse Econometric Models,"  This article is about estimation and inference methods for high dimensional
sparse (HDS) regression models in econometrics. High dimensional sparse models
arise in situations where many regressors (or series terms) are available and
the regression function is well-approximated by a parsimonious, yet unknown set
of regressors. The latter condition makes it possible to estimate the entire
regression function effectively by searching for approximately the right set of
regressors. We discuss methods for identifying this set of regressors and
estimating their coefficients based on $\ell_1$-penalization and describe key
theoretical results. In order to capture realistic practical situations, we
expressly allow for imperfect selection of regressors and study the impact of
this imperfect selection on estimation and inference results. We focus the main
part of the article on the use of HDS models and methods in the instrumental
variables model and the partially linear model. We present a set of novel
inference results for these models and illustrate their use with applications
to returns to schooling and growth regression.
",article estimation inference method high dimensional sparse hds regression model econometric high dimensional sparse model arise situation regressor series term available regression function approximate parsimonious unknown set regressor condition make possible estimate entire regression function effectively search approximately right set regressor discuss method identify set regressor estimate coefficient base $ \ell_1$-penalization describe key theoretical result order capture realistic practical situation expressly allow imperfect selection regressor study impact imperfect selection estimation inference result focus main article use hds model method instrumental variable model partially linear model present set novel inference result model illustrate use application return schooling growth regression
econ,"Inference on Treatment Effects After Selection Amongst High-Dimensional
  Controls","  We propose robust methods for inference on the effect of a treatment variable
on a scalar outcome in the presence of very many controls. Our setting is a
partially linear model with possibly non-Gaussian and heteroscedastic
disturbances. Our analysis allows the number of controls to be much larger than
the sample size. To make informative inference feasible, we require the model
to be approximately sparse; that is, we require that the effect of confounding
factors can be controlled for up to a small approximation error by conditioning
on a relatively small number of controls whose identities are unknown. The
latter condition makes it possible to estimate the treatment effect by
selecting approximately the right set of controls. We develop a novel
estimation and uniformly valid inference method for the treatment effect in
this setting, called the ""post-double-selection"" method. Our results apply to
Lasso-type methods used for covariate selection as well as to any other model
selection method that is able to find a sparse model with good approximation
properties.
  The main attractive feature of our method is that it allows for imperfect
selection of the controls and provides confidence intervals that are valid
uniformly across a large class of models. In contrast, standard post-model
selection estimators fail to provide uniform inference even in simple cases
with a small, fixed number of controls. Thus our method resolves the problem of
uniform inference after model selection for a large, interesting class of
models. We illustrate the use of the developed methods with numerical
simulations and an application to the effect of abortion on crime rates.
",propose robust method inference effect treatment variable scalar outcome presence control setting partially linear model possibly non gaussian heteroscedastic disturbance analysis allow number control large sample size informative inference feasible require model approximately sparse require effect confound factor control small approximation error condition relatively small number control identity unknown condition make possible estimate treatment effect select approximately right set control develop novel estimation uniformly valid inference method treatment effect setting call post double selection method result apply lasso type method covariate selection model selection method able find sparse model good approximation property main attractive feature method allow imperfect selection control provide confidence interval valid uniformly large class model contrast standard post model selection estimator fail provide uniform inference simple case small fix number control method resolve problem uniform inference model selection large interesting class model illustrate use develop method numerical simulation application effect abortion crime rate
econ,Approximate Revenue Maximization with Multiple Items,"  Maximizing the revenue from selling _more than one_ good (or item) to a
single buyer is a notoriously difficult problem, in stark contrast to the
one-good case. For two goods, we show that simple ""one-dimensional"" mechanisms,
such as selling the goods separately, _guarantee_ at least 73% of the optimal
revenue when the valuations of the two goods are independent and identically
distributed, and at least $50\%$ when they are independent. For the case of
$k>2$ independent goods, we show that selling them separately guarantees at
least a $c/\log^2 k$ fraction of the optimal revenue; and, for independent and
identically distributed goods, we show that selling them as one bundle
guarantees at least a $c/\log k$ fraction of the optimal revenue. Additional
results compare the revenues from the two simple mechanisms of selling the
goods separately and bundled, identify situations where bundling is optimal,
and extend the analysis to multiple buyers.
",maximize revenue sell good item single buyer notoriously difficult problem stark contrast good case good simple dimensional mechanism sell good separately guarantee 73 optimal revenue valuation good independent identically distribute $ 50\%$ independent case $ k>2 $ independent good sell separately guarantee $ c/\log^2 k$ fraction optimal revenue independent identically distribute good sell bundle guarantee $ c/\log k$ fraction optimal revenue additional result compare revenue simple mechanism sell good separately bundle identify situation bundle optimal extend analysis multiple buyer
econ,"Panel Data Models with Nonadditive Unobserved Heterogeneity: Estimation
  and Inference","  This paper considers fixed effects estimation and inference in linear and
nonlinear panel data models with random coefficients and endogenous regressors.
The quantities of interest -- means, variances, and other moments of the random
coefficients -- are estimated by cross sectional sample moments of GMM
estimators applied separately to the time series of each individual. To deal
with the incidental parameter problem introduced by the noise of the
within-individual estimators in short panels, we develop bias corrections.
These corrections are based on higher-order asymptotic expansions of the GMM
estimators and produce improved point and interval estimates in moderately long
panels. Under asymptotic sequences where the cross sectional and time series
dimensions of the panel pass to infinity at the same rate, the uncorrected
estimator has an asymptotic bias of the same order as the asymptotic variance.
The bias corrections remove the bias without increasing variance. An empirical
example on cigarette demand based on Becker, Grossman and Murphy (1994) shows
significant heterogeneity in the price effect across U.S. states.
",paper consider fix effect estimation inference linear nonlinear panel data model random coefficient endogenous regressor quantity interest mean variance moment random coefficient estimate cross sectional sample moment gmm estimator apply separately time series individual deal incidental parameter problem introduce noise individual estimator short panel develop bias correction correction base high order asymptotic expansion gmm estimator produce improve point interval estimate moderately long panel asymptotic sequence cross sectional time series dimension panel pass infinity rate uncorrected estimator asymptotic bias order asymptotic variance bias correction remove bias increase variance empirical example cigarette demand base becker grossman murphy 1994 show significant heterogeneity price effect u.s. state
econ,Social learning equilibria,"  We consider a large class of social learning models in which a group of
agents face uncertainty regarding a state of the world, share the same utility
function, observe private signals, and interact in a general dynamic setting.
We introduce Social Learning Equilibria, a static equilibrium concept that
abstracts away from the details of the given extensive form, but nevertheless
captures the corresponding asymptotic equilibrium behavior. We establish
general conditions for agreement, herding, and information aggregation in
equilibrium, highlighting a connection between agreement and information
aggregation.
",consider large class social learning model group agent face uncertainty state world share utility function observe private signal interact general dynamic setting introduce social learning equilibria static equilibrium concept abstract away detail give extensive form capture corresponding asymptotic equilibrium behavior establish general condition agreement herding information aggregation equilibrium highlight connection agreement information aggregation
econ,Strategic Learning and the Topology of Social Networks,"  We consider a group of strategic agents who must each repeatedly take one of
two possible actions. They learn which of the two actions is preferable from
initial private signals, and by observing the actions of their neighbors in a
social network.
  We show that the question of whether or not the agents learn efficiently
depends on the topology of the social network. In particular, we identify a
geometric ""egalitarianism"" condition on the social network that guarantees
learning in infinite networks, or learning with high probability in large
finite networks, in any equilibrium. We also give examples of non-egalitarian
networks with equilibria in which learning fails.
",consider group strategic agent repeatedly possible action learn action preferable initial private signal observe action neighbor social network question agent learn efficiently depend topology social network particular identify geometric egalitarianism condition social network guarantee learning infinite network learn high probability large finite network equilibrium example non egalitarian network equilibrium learn fail
econ,Dual Regression,"  We propose dual regression as an alternative to the quantile regression
process for the global estimation of conditional distribution functions under
minimal assumptions. Dual regression provides all the interpretational power of
the quantile regression process while avoiding the need for repairing the
intersecting conditional quantile surfaces that quantile regression often
produces in practice. Our approach introduces a mathematical programming
characterization of conditional distribution functions which, in its simplest
form, is the dual program of a simultaneous estimator for linear location-scale
models. We apply our general characterization to the specification and
estimation of a flexible class of conditional distribution functions, and
present asymptotic theory for the corresponding empirical dual regression
process.
",propose dual regression alternative quantile regression process global estimation conditional distribution function minimal assumption dual regression provide interpretational power quantile regression process avoid need repair intersecting conditional quantile surface quantile regression produce practice approach introduce mathematical programming characterization conditional distribution function simple form dual program simultaneous estimator linear location scale model apply general characterization specification estimation flexible class conditional distribution function present asymptotic theory corresponding empirical dual regression process
econ,"Some New Asymptotic Theory for Least Squares Series: Pointwise and
  Uniform Results","  In applications it is common that the exact form of a conditional expectation
is unknown and having flexible functional forms can lead to improvements.
Series method offers that by approximating the unknown function based on $k$
basis functions, where $k$ is allowed to grow with the sample size $n$. We
consider series estimators for the conditional mean in light of: (i) sharp LLNs
for matrices derived from the noncommutative Khinchin inequalities, (ii) bounds
on the Lebesgue factor that controls the ratio between the $L^\infty$ and
$L_2$-norms of approximation errors, (iii) maximal inequalities for processes
whose entropy integrals diverge, and (iv) strong approximations to series-type
processes.
  These technical tools allow us to contribute to the series literature,
specifically the seminal work of Newey (1997), as follows. First, we weaken the
condition on the number $k$ of approximating functions used in series
estimation from the typical $k^2/n \to 0$ to $k/n \to 0$, up to log factors,
which was available only for spline series before. Second, we derive $L_2$
rates and pointwise central limit theorems results when the approximation error
vanishes. Under an incorrectly specified model, i.e. when the approximation
error does not vanish, analogous results are also shown. Third, under stronger
conditions we derive uniform rates and functional central limit theorems that
hold if the approximation error vanishes or not. That is, we derive the strong
approximation for the entire estimate of the nonparametric function.
  We derive uniform rates, Gaussian approximations, and uniform confidence
bands for a wide collection of linear functionals of the conditional
expectation function.
",application common exact form conditional expectation unknown have flexible functional form lead improvement series method offer approximate unknown function base $ k$ basis function $ k$ allow grow sample size $ n$. consider series estimator conditional mean light sharp lln matrix derive noncommutative khinchin inequality ii bound lebesgue factor control ratio $ l^\infty$ $ l_2$-norm approximation error iii maximal inequality process entropy integral diverge iv strong approximation series type process technical tool allow contribute series literature specifically seminal work newey 1997 follow weaken condition number $ k$ approximating function series estimation typical $ k^2 n \to 0 $ $ k n \to 0 $ log factor available spline series second derive $ l_2 $ rate pointwise central limit theorem result approximation error vanishe incorrectly specify model i.e. approximation error vanish analogous result show strong condition derive uniform rate functional central limit theorem hold approximation error vanishe derive strong approximation entire estimate nonparametric function derive uniform rate gaussian approximation uniform confidence band wide collection linear functional conditional expectation function
econ,"Semi-parametric Bayesian Partially Identified Models based on Support
  Function","  We provide a comprehensive semi-parametric study of Bayesian partially
identified econometric models. While the existing literature on Bayesian
partial identification has mostly focused on the structural parameter, our
primary focus is on Bayesian credible sets (BCS's) of the unknown identified
set and the posterior distribution of its support function. We construct a
(two-sided) BCS based on the support function of the identified set. We prove
the Bernstein-von Mises theorem for the posterior distribution of the support
function. This powerful result in turn infers that, while the BCS and the
frequentist confidence set for the partially identified parameter are
asymptotically different, our constructed BCS for the identified set has an
asymptotically correct frequentist coverage probability. Importantly, we
illustrate that the constructed BCS for the identified set does not require a
prior on the structural parameter. It can be computed efficiently for subset
inference, especially when the target of interest is a sub-vector of the
partially identified parameter, where projecting to a low-dimensional subset is
often required. Hence, the proposed methods are useful in many applications.
  The Bayesian partial identification literature has been assuming a known
parametric likelihood function. However, econometric models usually only
identify a set of moment inequalities, and therefore using an incorrect
likelihood function may result in misleading inferences. In contrast, with a
nonparametric prior on the unknown likelihood function, our proposed Bayesian
procedure only requires a set of moment conditions, and can efficiently make
inference about both the partially identified parameter and its identified set.
This makes it widely applicable in general moment inequality models. Finally,
the proposed method is illustrated in a financial asset pricing problem.
",provide comprehensive semi parametric study bayesian partially identify econometric model exist literature bayesian partial identification focus structural parameter primary focus bayesian credible set bcs unknown identify set posterior distribution support function construct side bcs base support function identify set prove bernstein von mises theorem posterior distribution support function powerful result turn infer bcs frequentist confidence set partially identify parameter asymptotically different construct bcs identify set asymptotically correct frequentist coverage probability importantly illustrate construct bcs identify set require prior structural parameter compute efficiently subset inference especially target interest sub vector partially identify parameter project low dimensional subset require propose method useful application bayesian partial identification literature assume know parametric likelihood function econometric model usually identify set moment inequality incorrect likelihood function result mislead inference contrast nonparametric prior unknown likelihood function propose bayesian procedure require set moment condition efficiently inference partially identify parameter identified set make widely applicable general moment inequality model finally propose method illustrate financial asset pricing problem
econ,"Gaussian approximations and multiplier bootstrap for maxima of sums of
  high-dimensional random vectors","  We derive a Gaussian approximation result for the maximum of a sum of
high-dimensional random vectors. Specifically, we establish conditions under
which the distribution of the maximum is approximated by that of the maximum of
a sum of the Gaussian random vectors with the same covariance matrices as the
original vectors. This result applies when the dimension of random vectors
($p$) is large compared to the sample size ($n$); in fact, $p$ can be much
larger than $n$, without restricting correlations of the coordinates of these
vectors. We also show that the distribution of the maximum of a sum of the
random vectors with unknown covariance matrices can be consistently estimated
by the distribution of the maximum of a sum of the conditional Gaussian random
vectors obtained by multiplying the original vectors with i.i.d. Gaussian
multipliers. This is the Gaussian multiplier (or wild) bootstrap procedure.
Here too, $p$ can be large or even much larger than $n$. These distributional
approximations, either Gaussian or conditional Gaussian, yield a high-quality
approximation to the distribution of the original maximum, often with
approximation error decreasing polynomially in the sample size, and hence are
of interest in many applications. We demonstrate how our Gaussian
approximations and the multiplier bootstrap can be used for modern
high-dimensional estimation, multiple hypothesis testing, and adaptive
specification testing. All these results contain nonasymptotic bounds on
approximation errors.
",derive gaussian approximation result maximum sum high dimensional random vector specifically establish condition distribution maximum approximate maximum sum gaussian random vector covariance matrix original vector result apply dimension random vector $ p$ large compare sample size $ n$ fact $ p$ large $ n$ restrict correlation coordinate vector distribution maximum sum random vector unknown covariance matrix consistently estimate distribution maximum sum conditional gaussian random vector obtain multiply original vector i.i.d gaussian multiplier gaussian multipli wild bootstrap procedure $ p$ large large $ n$. distributional approximation gaussian conditional gaussian yield high quality approximation distribution original maximum approximation error decrease polynomially sample size interest application demonstrate gaussian approximation multipli bootstrap modern high dimensional estimation multiple hypothesis testing adaptive specification testing result contain nonasymptotic bound approximation error
econ,Quantile Models with Endogeneity,"  In this article, we review quantile models with endogeneity. We focus on
models that achieve identification through the use of instrumental variables
and discuss conditions under which partial and point identification are
obtained. We discuss key conditions, which include monotonicity and
full-rank-type conditions, in detail. In providing this review, we update the
identification results of Chernozhukov and Hansen (2005, Econometrica). We
illustrate the modeling assumptions through economically motivated examples. We
also briefly review the literature on estimation and inference.
  Key Words: identification, treatment effects, structural models, instrumental
variables
",article review quantile model endogeneity focus model achieve identification use instrumental variable discuss condition partial point identification obtain discuss key condition include monotonicity rank type condition detail provide review update identification result chernozhukov hansen 2005 econometrica illustrate modeling assumption economically motivated example briefly review literature estimation inference key word identification treatment effect structural model instrumental variable
econ,"Uniform Post Selection Inference for LAD Regression and Other
  Z-estimation problems","  We develop uniformly valid confidence regions for regression coefficients in
a high-dimensional sparse median regression model with homoscedastic errors.
Our methods are based on a moment equation that is immunized against
non-regular estimation of the nuisance part of the median regression function
by using Neyman's orthogonalization. We establish that the resulting
instrumental median regression estimator of a target regression coefficient is
asymptotically normally distributed uniformly with respect to the underlying
sparse model and is semi-parametrically efficient. We also generalize our
method to a general non-smooth Z-estimation framework with the number of target
parameters $p_1$ being possibly much larger than the sample size $n$. We extend
Huber's results on asymptotic normality to this setting, demonstrating uniform
asymptotic normality of the proposed estimators over $p_1$-dimensional
rectangles, constructing simultaneous confidence bands on all of the $p_1$
target parameters, and establishing asymptotic validity of the bands uniformly
over underlying approximately sparse models.
  Keywords: Instrument; Post-selection inference; Sparsity; Neyman's Orthogonal
Score test; Uniformly valid inference; Z-estimation.
",develop uniformly valid confidence region regression coefficient high dimensional sparse median regression model homoscedastic error method base moment equation immunize non regular estimation nuisance median regression function neyman orthogonalization establish result instrumental median regression estimator target regression coefficient asymptotically normally distribute uniformly respect underlie sparse model semi parametrically efficient generalize method general non smooth z estimation framework number target parameter $ p_1 $ possibly large sample size $ n$. extend huber result asymptotic normality setting demonstrate uniform asymptotic normality propose estimator $ p_1$-dimensional rectangle construct simultaneous confidence band $ p_1 $ target parameter establish asymptotic validity band uniformly underlie approximately sparse model keyword instrument post selection inference sparsity neyman orthogonal score test uniformly valid inference z estimation
econ,"Post-Selection Inference for Generalized Linear Models with Many
  Controls","  This paper considers generalized linear models in the presence of many
controls. We lay out a general methodology to estimate an effect of interest
based on the construction of an instrument that immunize against model
selection mistakes and apply it to the case of logistic binary choice model.
More specifically we propose new methods for estimating and constructing
confidence regions for a regression parameter of primary interest $\alpha_0$, a
parameter in front of the regressor of interest, such as the treatment variable
or a policy variable. These methods allow to estimate $\alpha_0$ at the
root-$n$ rate when the total number $p$ of other regressors, called controls,
potentially exceed the sample size $n$ using sparsity assumptions. The sparsity
assumption means that there is a subset of $s<n$ controls which suffices to
accurately approximate the nuisance part of the regression function.
Importantly, the estimators and these resulting confidence regions are valid
uniformly over $s$-sparse models satisfying $s^2\log^2 p = o(n)$ and other
technical conditions. These procedures do not rely on traditional consistent
model selection arguments for their validity. In fact, they are robust with
respect to moderate model selection mistakes in variable selection. Under
suitable conditions, the estimators are semi-parametrically efficient in the
sense of attaining the semi-parametric efficiency bounds for the class of
models in this paper.
",paper consider generalize linear model presence control lay general methodology estimate effect interest base construction instrument immunize model selection mistake apply case logistic binary choice model specifically propose new method estimate construct confidence region regression parameter primary interest $ \alpha_0 $ parameter regressor interest treatment variable policy variable method allow estimate $ \alpha_0 $ root-$n$ rate total number $ p$ regressor call control potentially exceed sample size $ n$ sparsity assumption sparsity assumption mean subset $ s < n$ control suffice accurately approximate nuisance regression function importantly estimator result confidence region valid uniformly $ s$-sparse model satisfy $ s^2\log^2 p = o(n)$ technical condition procedure rely traditional consistent model selection argument validity fact robust respect moderate model selection mistake variable selection suitable condition estimator semi parametrically efficient sense attain semi parametric efficiency bound class model paper
econ,"Selling Multiple Correlated Goods: Revenue Maximization and Menu-Size
  Complexity (old title: ""The Menu-Size Complexity of Auctions"")","  We consider the well known, and notoriously difficult, problem of a single
revenue-maximizing seller selling two or more heterogeneous goods to a single
buyer whose private values for the goods are drawn from a (possibly correlated)
known distribution, and whose valuation is additive over the goods. We show
that when there are two (or more) goods, _simple mechanisms_ -- such as selling
the goods separately or as a bundle -- _may yield only a negligible fraction of
the optimal revenue_. This resolves the open problem of Briest, Chawla,
Kleinberg, and Weinberg (JET 2015) who prove the result for at least three
goods in the related setup of a unit-demand buyer. We also introduce the menu
size as a simple measure of the complexity of mechanisms, and show that the
revenue may increase polynomially with _menu size_ and that no bounded menu
size can ensure any positive fraction of the optimal revenue. The menu size
also turns out to ""pin down"" the revenue properties of deterministic
mechanisms.
",consider know notoriously difficult problem single revenue maximize seller sell heterogeneous good single buyer private value good draw possibly correlate know distribution valuation additive good good simple mechanism sell good separately bundle yield negligible fraction optimal revenue resolve open problem briest chawla kleinberg weinberg jet 2015 prove result good related setup unit demand buyer introduce menu size simple measure complexity mechanism revenue increase polynomially menu size bounded menu size ensure positive fraction optimal revenue menu size turn pin revenue property deterministic mechanism
econ,The Query Complexity of Correlated Equilibria,"  We consider the complexity of finding a correlated equilibrium of an
$n$-player game in a model that allows the algorithm to make queries on
players' payoffs at pure strategy profiles. Randomized regret-based dynamics
are known to yield an approximate correlated equilibrium efficiently, namely,
in time that is polynomial in the number of players $n$. Here we show that both
randomization and approximation are necessary: no efficient deterministic
algorithm can reach even an approximate correlated equilibrium, and no
efficient randomized algorithm can reach an exact correlated equilibrium. The
results are obtained by bounding from below the number of payoff queries that
are needed.
",consider complexity find correlated equilibrium $ n$-player game model allow algorithm query player payoff pure strategy profile randomized regret base dynamic know yield approximate correlate equilibrium efficiently time polynomial number player $ n$. randomization approximation necessary efficient deterministic algorithm reach approximate correlate equilibrium efficient randomized algorithm reach exact correlate equilibrium result obtain bound number payoff query need
econ,"Supplementary Appendix for ""Inference on Treatment Effects After
  Selection Amongst High-Dimensional Controls""","  In this supplementary appendix we provide additional results, omitted proofs
and extensive simulations that complement the analysis of the main text
(arXiv:1201.0224).
",supplementary appendix provide additional result omit proof extensive simulation complement analysis main text arxiv:1201.0224
econ,"Periodic Strategies: A New Solution Concept and an Algorithm for
  NonTrivial Strategic Form Games","  We introduce a new solution concept, called periodicity, for selecting
optimal strategies in strategic form games. This periodicity solution concept
yields new insight into non-trivial games. In mixed strategy strategic form
games, periodic solutions yield values for the utility function of each player
that are equal to the Nash equilibrium ones. In contrast to the Nash
strategies, here the payoffs of each player are robust against what the
opponent plays. Sometimes, periodicity strategies yield higher utilities, and
sometimes the Nash strategies do, but often the utilities of these two
strategies coincide. We formally define and study periodic strategies in two
player perfect information strategic form games with pure strategies and we
prove that every non-trivial finite game has at least one periodic strategy,
with non-trivial meaning non-degenerate payoffs. In some classes of games where
mixed strategies are used, we identify quantitative features. Particularly
interesting are the implications for collective action games, since there the
collective action strategy can be incorporated in a purely non-cooperative
context. Moreover, we address the periodicity issue when the players have a
continuum set of strategies available.
",introduce new solution concept call periodicity select optimal strategy strategic form game periodicity solution concept yield new insight non trivial game mixed strategy strategic form game periodic solution yield value utility function player equal nash equilibrium one contrast nash strategy payoff player robust opponent play periodicity strategy yield high utility nash strategy utility strategy coincide formally define study periodic strategy player perfect information strategic form game pure strategy prove non trivial finite game periodic strategy non trivial meaning non degenerate payoff class game mixed strategy identify quantitative feature particularly interesting implication collective action game collective action strategy incorporate purely non cooperative context address periodicity issue player continuum set strategy available
econ,"Robust Inference on Average Treatment Effects with Possibly More
  Covariates than Observations","  This paper concerns robust inference on average treatment effects following
model selection. In the selection on observables framework, we show how to
construct confidence intervals based on a doubly-robust estimator that are
robust to model selection errors and prove that they are valid uniformly over a
large class of treatment effect models. The class allows for multivalued
treatments with heterogeneous effects (in observables), general
heteroskedasticity, and selection amongst (possibly) more covariates than
observations. Our estimator attains the semiparametric efficiency bound under
appropriate conditions. Precise conditions are given for any model selector to
yield these results, and we show how to combine data-driven selection with
economic theory. For implementation, we give a specific proposal for selection
based on the group lasso, which is particularly well-suited to treatment
effects data, and derive new results for high-dimensional, sparse multinomial
logistic regression. A simulation study shows our estimator performs very well
in finite samples over a wide range of models. Revisiting the National
Supported Work demonstration data, our method yields accurate estimates and
tight confidence intervals.
",paper concern robust inference average treatment effect follow model selection selection observable framework construct confidence interval base doubly robust estimator robust model selection error prove valid uniformly large class treatment effect model class allow multivalued treatment heterogeneous effect observable general heteroskedasticity selection possibly covariate observation estimator attain semiparametric efficiency bind appropriate condition precise condition give model selector yield result combine data drive selection economic theory implementation specific proposal selection base group lasso particularly suit treatment effect datum derive new result high dimensional sparse multinomial logistic regression simulation study show estimator perform finite sample wide range model revisit national supported work demonstration datum method yield accurate estimate tight confidence interval
econ,"Optimal Uniform Convergence Rates for Sieve Nonparametric Instrumental
  Variables Regression","  We study the problem of nonparametric regression when the regressor is
endogenous, which is an important nonparametric instrumental variables (NPIV)
regression in econometrics and a difficult ill-posed inverse problem with
unknown operator in statistics. We first establish a general upper bound on the
sup-norm (uniform) convergence rate of a sieve estimator, allowing for
endogenous regressors and weakly dependent data. This result leads to the
optimal sup-norm convergence rates for spline and wavelet least squares
regression estimators under weakly dependent data and heavy-tailed error terms.
This upper bound also yields the sup-norm convergence rates for sieve NPIV
estimators under i.i.d. data: the rates coincide with the known optimal
$L^2$-norm rates for severely ill-posed problems, and are power of $\log(n)$
slower than the optimal $L^2$-norm rates for mildly ill-posed problems. We then
establish the minimax risk lower bound in sup-norm loss, which coincides with
our upper bounds on sup-norm rates for the spline and wavelet sieve NPIV
estimators. This sup-norm rate optimality provides another justification for
the wide application of sieve NPIV estimators. Useful results on
weakly-dependent random matrices are also provided.
",study problem nonparametric regression regressor endogenous important nonparametric instrumental variable npiv regression econometric difficult ill pose inverse problem unknown operator statistic establish general upper bind sup norm uniform convergence rate sieve estimator allow endogenous regressor weakly dependent datum result lead optimal sup norm convergence rate spline wavelet square regression estimator weakly dependent datum heavy tail error term upper bind yield sup norm convergence rate sieve npiv estimator i.i.d datum rate coincide know optimal $ l^2$-norm rate severely ill pose problem power $ \log(n)$ slow optimal $ l^2$-norm rate mildly ill pose problem establish minimax risk lower bind sup norm loss coincide upper bound sup norm rate spline wavelet sieve npiv estimator sup norm rate optimality provide justification wide application sieve npiv estimator useful result weakly dependent random matrix provide
econ,Program Evaluation and Causal Inference with High-Dimensional Data,"  In this paper, we provide efficient estimators and honest confidence bands
for a variety of treatment effects including local average (LATE) and local
quantile treatment effects (LQTE) in data-rich environments. We can handle very
many control variables, endogenous receipt of treatment, heterogeneous
treatment effects, and function-valued outcomes. Our framework covers the
special case of exogenous receipt of treatment, either conditional on controls
or unconditionally as in randomized control trials. In the latter case, our
approach produces efficient estimators and honest bands for (functional)
average treatment effects (ATE) and quantile treatment effects (QTE). To make
informative inference possible, we assume that key reduced form predictive
relationships are approximately sparse. This assumption allows the use of
regularization and selection methods to estimate those relations, and we
provide methods for post-regularization and post-selection inference that are
uniformly valid (honest) across a wide-range of models. We show that a key
ingredient enabling honest inference is the use of orthogonal or doubly robust
moment conditions in estimating certain reduced form functional parameters. We
illustrate the use of the proposed methods with an application to estimating
the effect of 401(k) eligibility and participation on accumulated assets.
",paper provide efficient estimator honest confidence band variety treatment effect include local average late local quantile treatment effect lqte data rich environment handle control variable endogenous receipt treatment heterogeneous treatment effect function value outcome framework cover special case exogenous receipt treatment conditional control unconditionally randomized control trial case approach produce efficient estimator honest band functional average treatment effect ate quantile treatment effect qte informative inference possible assume key reduce form predictive relationship approximately sparse assumption allow use regularization selection method estimate relation provide method post regularization post selection inference uniformly valid honest wide range model key ingredient enable honest inference use orthogonal doubly robust moment condition estimate certain reduce form functional parameter illustrate use propose method application estimate effect 401(k eligibility participation accumulate asset
econ,"Individual and Time Effects in Nonlinear Panel Models with Large N, T","  We derive fixed effects estimators of parameters and average partial effects
in (possibly dynamic) nonlinear panel data models with individual and time
effects. They cover logit, probit, ordered probit, Poisson and Tobit models
that are important for many empirical applications in micro and macroeconomics.
Our estimators use analytical and jackknife bias corrections to deal with the
incidental parameter problem, and are asymptotically unbiased under asymptotic
sequences where $N/T$ converges to a constant. We develop inference methods and
show that they perform well in numerical examples.
",derive fix effect estimator parameter average partial effect possibly dynamic nonlinear panel data model individual time effect cover logit probit order probit poisson tobit model important empirical application micro macroeconomic estimator use analytical jackknife bias correction deal incidental parameter problem asymptotically unbiased asymptotic sequence $ n t$ converge constant develop inference method perform numerical example
econ,Nonparametric Identification in Panels using Quantiles,"  This paper considers identification and estimation of ceteris paribus effects
of continuous regressors in nonseparable panel models with time homogeneity.
The effects of interest are derivatives of the average and quantile structural
functions of the model. We find that these derivatives are identified with two
time periods for ""stayers"", i.e. for individuals with the same regressor values
in two time periods. We show that the identification results carry over to
models that allow location and scale time effects. We propose nonparametric
series methods and a weighted bootstrap scheme to estimate and make inference
on the identified effects. The bootstrap proposed allows uniform inference for
function-valued parameters such as quantile effects uniformly over a region of
quantile indices and/or regressor values. An empirical application to Engel
curve estimation with panel data illustrates the results.
",paper consider identification estimation ceteris paribus effect continuous regressor nonseparable panel model time homogeneity effect interest derivative average quantile structural function model find derivative identify time period stayer i.e. individual regressor value time period identification result carry model allow location scale time effect propose nonparametric series method weighted bootstrap scheme estimate inference identify effect bootstrap propose allow uniform inference function value parameter quantile effect uniformly region quantile indices and/or regressor value empirical application engel curve estimation panel datum illustrate result
econ,"Valid Post-Selection Inference in High-Dimensional Approximately Sparse
  Quantile Regression Models","  This work proposes new inference methods for a regression coefficient of
interest in a (heterogeneous) quantile regression model. We consider a
high-dimensional model where the number of regressors potentially exceeds the
sample size but a subset of them suffice to construct a reasonable
approximation to the conditional quantile function. The proposed methods are
(explicitly or implicitly) based on orthogonal score functions that protect
against moderate model selection mistakes, which are often inevitable in the
approximately sparse model considered in the present paper. We establish the
uniform validity of the proposed confidence regions for the quantile regression
coefficient. Importantly, these methods directly apply to more than one
variable and a continuum of quantile indices. In addition, the performance of
the proposed methods is illustrated through Monte-Carlo experiments and an
empirical example, dealing with risk factors in childhood malnutrition.
",work propose new inference method regression coefficient interest heterogeneous quantile regression model consider high dimensional model number regressor potentially exceed sample size subset suffice construct reasonable approximation conditional quantile function propose method explicitly implicitly base orthogonal score function protect moderate model selection mistake inevitable approximately sparse model consider present paper establish uniform validity propose confidence region quantile regression coefficient importantly method directly apply variable continuum quantile index addition performance propose method illustrate monte carlo experiment empirical example deal risk factor childhood malnutrition
econ,"Inference on causal and structural parameters using many moment
  inequalities","  This paper considers the problem of testing many moment inequalities where
the number of moment inequalities, denoted by $p$, is possibly much larger than
the sample size $n$. There is a variety of economic applications where solving
this problem allows to carry out inference on causal and structural parameters,
a notable example is the market structure model of Ciliberto and Tamer (2009)
where $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter
the market. We consider the test statistic given by the maximum of $p$
Studentized (or $t$-type) inequality-specific statistics, and analyze various
ways to compute critical values for the test statistic. Specifically, we
consider critical values based upon (i) the union bound combined with a
moderate deviation inequality for self-normalized sums, (ii) the multiplier and
empirical bootstraps, and (iii) two-step and three-step variants of (i) and
(ii) by incorporating the selection of uninformative inequalities that are far
from being binding and a novel selection of weakly informative inequalities
that are potentially binding but do not provide first order information. We
prove validity of these methods, showing that under mild conditions, they lead
to tests with the error in size decreasing polynomially in $n$ while allowing
for $p$ being much larger than $n$, indeed $p$ can be of order $\exp (n^{c})$
for some $c > 0$. Importantly, all these results hold without any restriction
on the correlation structure between $p$ Studentized statistics, and also hold
uniformly with respect to suitably large classes of underlying distributions.
Moreover, in the online supplement, we show validity of a test based on the
block multiplier bootstrap in the case of dependent data under some general
mixing conditions.
",paper consider problem test moment inequality number moment inequality denote $ p$ possibly large sample size $ n$. variety economic application solve problem allow carry inference causal structural parameter notable example market structure model ciliberto tamer 2009 $ p=2^{m+1}$ $ m$ number firm possibly enter market consider test statistic give maximum $ p$ studentized $ t$-type inequality specific statistic analyze way compute critical value test statistic specifically consider critical value base union bind combine moderate deviation inequality self normalize sum ii multipli empirical bootstrap iii step step variant ii incorporate selection uninformative inequality far bind novel selection weakly informative inequality potentially bind provide order information prove validity method show mild condition lead test error size decrease polynomially $ n$ allow $ p$ large $ n$ $ p$ order $ \exp n^{c})$ $ c > 0$. importantly result hold restriction correlation structure $ p$ studentized statistic hold uniformly respect suitably large class underlie distribution online supplement validity test base block multipli bootstrap case dependent datum general mixing condition
econ,"What does the financial market pricing do? A simulation analysis with a
  view to systemic volatility, exuberance and vagary","  Biondi et al. (2012) develop an analytical model to examine the emergent
dynamic properties of share market price formation over time, capable to
capture important stylized facts. These latter properties prove to be sensitive
to regulatory regimes for fundamental information provision, as well as to
market confidence conditions among actual and potential investors. Regimes
based upon mark-to-market (fair value) measurement of traded security, while
generating higher linear correlation between market prices and fundamental
signals, also involve higher market instability and volatility. These regimes
also incur more relevant episodes of market exuberance and vagary in some
regions of the market confidence space, where lower market liquidity further
occurs.
",biondi et al 2012 develop analytical model examine emergent dynamic property share market price formation time capable capture important stylize fact property prove sensitive regulatory regime fundamental information provision market confidence condition actual potential investor regimes base mark market fair value measurement trade security generate high linear correlation market price fundamental signal involve high market instability volatility regime incur relevant episode market exuberance vagary region market confidence space low market liquidity occur
econ,Efficient Modeling and Forecasting of the Electricity Spot Price,"  The increasing importance of renewable energy, especially solar and wind
power, has led to new forces in the formation of electricity prices. Hence,
this paper introduces an econometric model for the hourly time series of
electricity prices of the European Power Exchange (EPEX) which incorporates
specific features like renewable energy. The model consists of several
sophisticated and established approaches and can be regarded as a periodic
VAR-TARCH with wind power, solar power, and load as influences on the time
series. It is able to map the distinct and well-known features of electricity
prices in Germany. An efficient iteratively reweighted lasso approach is used
for the estimation. Moreover, it is shown that several existing models are
outperformed by the procedure developed in this paper.
",increase importance renewable energy especially solar wind power lead new force formation electricity price paper introduce econometric model hourly time series electricity price european power exchange epex incorporate specific feature like renewable energy model consist sophisticated establish approach regard periodic var tarch wind power solar power load influence time series able map distinct know feature electricity price germany efficient iteratively reweighte lasso approach estimation show exist model outperform procedure develop paper
econ,Graphical potential games,"  We study the class of potential games that are also graphical games with
respect to a given graph $G$ of connections between the players. We show that,
up to strategic equivalence, this class of games can be identified with the set
of Markov random fields on $G$.
  From this characterization, and from the Hammersley-Clifford theorem, it
follows that the potentials of such games can be decomposed to local
potentials. We use this decomposition to strongly bound the number of strategy
changes of a single player along a better response path. This result extends to
generalized graphical potential games, which are played on infinite graphs.
",study class potential game graphical game respect give graph $ g$ connection player strategic equivalence class game identify set markov random field $ g$. characterization hammersley clifford theorem follow potential game decompose local potential use decomposition strongly bind number strategy change single player well response path result extend generalize graphical potential game play infinite graph
econ,Zero-determinant strategies in iterated multi-strategy games,"  Self-serving, rational agents sometimes cooperate to their mutual benefit.
The two-player iterated prisoner's dilemma game is a model for including the
emergence of cooperation. It is generally believed that there is no simple
ultimatum strategy which a player can control the return of the other
participants. The recent discovery of the powerful class of zero-determinant
strategies in the iterated prisoner's dilemma dramatically expands our
understanding of the classic game by uncovering strategies that provide a
unilateral advantage to sentient players pitted against unwitting opponents.
However, strategies in the prisoner's dilemma game are only two strategies. Are
there these results for general multi-strategy games? To address this question,
the paper develops a theory for zero-determinant strategies for multi-strategy
games, with any number of strategies. The analytical results exhibit a similar
yet different scenario to the case of two-strategy games. Zero-determinant
strategies in iterated prisoner's dilemma can be seen as degenerate case of our
results. The results are also applied to the snowdrift game, the hawk-dove game
and the chicken game.
",self serve rational agent cooperate mutual benefit player iterate prisoner dilemma game model include emergence cooperation generally believe simple ultimatum strategy player control return participant recent discovery powerful class zero determinant strategy iterated prisoner dilemma dramatically expand understanding classic game uncover strategy provide unilateral advantage sentient player pit unwitte opponent strategy prisoner dilemma game strategy result general multi strategy game address question paper develop theory zero determinant strategy multi strategy game number strategy analytical result exhibit similar different scenario case strategy game zero determinant strategy iterated prisoner dilemma see degenerate case result result apply snowdrift game hawk dove game chicken game
econ,Fiscal stimulus as an optimal control problem,"  During the Great Recession, Democrats in the United States argued that
government spending could be utilized to ""grease the wheels"" of the economy in
order to create wealth and to increase employment; Republicans, on the other
hand, contended that government spending is wasteful and discouraged
investment, thereby increasing unemployment. Today, in 2020, we find ourselves
in the midst of another crisis where government spending and fiscal stimulus is
again being considered as a solution. In the present paper, we address this
question by formulating an optimal control problem generalizing the model of
Radner & Shepp (1996). The model allows for the company to borrow continuously
from the government. We prove that there exists an optimal strategy; rigorous
verification proofs for its optimality are provided. We proceed to prove that
government loans increase the expected net value of a company. We also examine
the consequences of different profit-taking behaviors among firms who receive
fiscal stimulus.
",great recession democrats united states argue government spending utilize grease wheel economy order create wealth increase employment republicans hand contend government spending wasteful discourage investment increase unemployment today 2020 find midst crisis government spending fiscal stimulus consider solution present paper address question formulate optimal control problem generalize model radner shepp 1996 model allow company borrow continuously government prove exist optimal strategy rigorous verification proof optimality provide proceed prove government loan increase expected net value company examine consequence different profit take behavior firm receive fiscal stimulus
econ,"Sieve Wald and QLR Inferences on Semi/nonparametric Conditional Moment
  Models","  This paper considers inference on functionals of semi/nonparametric
conditional moment restrictions with possibly nonsmooth generalized residuals,
which include all of the (nonlinear) nonparametric instrumental variables (IV)
as special cases. These models are often ill-posed and hence it is difficult to
verify whether a (possibly nonlinear) functional is root-$n$ estimable or not.
We provide computationally simple, unified inference procedures that are
asymptotically valid regardless of whether a functional is root-$n$ estimable
or not. We establish the following new useful results: (1) the asymptotic
normality of a plug-in penalized sieve minimum distance (PSMD) estimator of a
(possibly nonlinear) functional; (2) the consistency of simple sieve variance
estimators for the plug-in PSMD estimator, and hence the asymptotic chi-square
distribution of the sieve Wald statistic; (3) the asymptotic chi-square
distribution of an optimally weighted sieve quasi likelihood ratio (QLR) test
under the null hypothesis; (4) the asymptotic tight distribution of a
non-optimally weighted sieve QLR statistic under the null; (5) the consistency
of generalized residual bootstrap sieve Wald and QLR tests; (6) local power
properties of sieve Wald and QLR tests and of their bootstrap versions; (7)
asymptotic properties of sieve Wald and SQLR for functionals of increasing
dimension. Simulation studies and an empirical illustration of a nonparametric
quantile IV regression are presented.
",paper consider inference functional semi nonparametric conditional moment restriction possibly nonsmooth generalized residual include nonlinear nonparametric instrumental variable iv special case model ill pose difficult verify possibly nonlinear functional root-$n$ estimable provide computationally simple unify inference procedure asymptotically valid regardless functional root-$n$ estimable establish follow new useful result 1 asymptotic normality plug penalize sieve minimum distance psmd estimator possibly nonlinear functional 2 consistency simple sieve variance estimator plug psmd estimator asymptotic chi square distribution sieve wald statistic 3 asymptotic chi square distribution optimally weight sieve quasi likelihood ratio qlr test null hypothesis 4 asymptotic tight distribution non optimally weight sieve qlr statistic null 5 consistency generalized residual bootstrap sieve wald qlr test 6 local power property sieve wald qlr test bootstrap version 7 asymptotic property sieve wald sqlr functional increase dimension simulation study empirical illustration nonparametric quantile iv regression present
econ,"Bootstrap Consistency for Quadratic Forms of Sample Averages with
  Increasing Dimension","  This paper establishes consistency of the weighted bootstrap for quadratic
forms $\left( n^{-1/2} \sum_{i=1}^{n} Z_{i,n} \right)^{T}\left( n^{-1/2}
\sum_{i=1}^{n} Z_{i,n} \right)$ where $(Z_{i,n})_{i=1}^{n}$ are mean zero,
independent $\mathbb{R}^{d}$-valued random variables and $d=d(n)$ is allowed to
grow with the sample size $n$, slower than $n^{1/4}$. The proof relies on an
adaptation of Lindeberg interpolation technique whereby we simplify the
original problem to a Gaussian approximation problem. We apply our bootstrap
results to model-specification testing problems when the number of moments is
allowed to grow with the sample size.
",paper establish consistency weighted bootstrap quadratic form $ \left n^{-1/2 \sum_{i=1}^{n z_{i n \right)^{t}\left n^{-1/2 \sum_{i=1}^{n z_{i n \right)$ $ z_{i n})_{i=1}^{n}$ mean zero independent $ \mathbb{r}^{d}$-value random variable $ d = d(n)$ allow grow sample size $ n$ slow $ n^{1/4}$. proof rely adaptation lindeberg interpolation technique simplify original problem gaussian approximation problem apply bootstrap result model specification testing problem number moment allow grow sample size
econ,Identification and Estimation of Multidimensional Screening,"  We study the identification and estimation of a multidimensional screening
model, where a monopolist sells a multi-attribute product to consumers with
private information about their multidimensional preferences. Under optimal
screening, the seller designs product and payment rules that exclude ""low-type""
consumers, bunches the ""medium types"" at ""medium-quality"" products, and
perfectly screens the ""high types."" Under the assumption that the cost function
is quadratic and additively separable in products, we determine sufficient
conditions to identify the joint distribution of preferences and the marginal
costs from data on optimal individual choices and payments. Then, we propose
estimators for these objects, establish their asymptotic properties, and assess
their small-sample performance using Monte Carlo experiments.
",study identification estimation multidimensional screening model monopolist sell multi attribute product consumer private information multidimensional preference optimal screening seller design product payment rule exclude low type consumer bunch medium type medium quality product perfectly screen high type assumption cost function quadratic additively separable product determine sufficient condition identify joint distribution preference marginal cost datum optimal individual choice payment propose estimator object establish asymptotic property assess small sample performance monte carlo experiment
econ,"Inference in High Dimensional Panel Models with an Application to Gun
  Control","  We consider estimation and inference in panel data models with additive
unobserved individual specific heterogeneity in a high dimensional setting. The
setting allows the number of time varying regressors to be larger than the
sample size. To make informative estimation and inference feasible, we require
that the overall contribution of the time varying variables after eliminating
the individual specific heterogeneity can be captured by a relatively small
number of the available variables whose identities are unknown. This
restriction allows the problem of estimation to proceed as a variable selection
problem. Importantly, we treat the individual specific heterogeneity as fixed
effects which allows this heterogeneity to be related to the observed time
varying variables in an unspecified way and allows that this heterogeneity may
be non-zero for all individuals. Within this framework, we provide procedures
that give uniformly valid inference over a fixed subset of parameters in the
canonical linear fixed effects model and over coefficients on a fixed vector of
endogenous variables in panel data instrumental variables models with fixed
effects and many instruments. An input to developing the properties of our
proposed procedures is the use of a variant of the Lasso estimator that allows
for a grouped data structure where data across groups are independent and
dependence within groups is unrestricted. We provide formal conditions within
this structure under which the proposed Lasso variant selects a sparse model
with good approximation properties. We present simulation results in support of
the theoretical developments and illustrate the use of the methods in an
application aimed at estimating the effect of gun prevalence on crime rates.
",consider estimation inference panel datum model additive unobserved individual specific heterogeneity high dimensional setting setting allow number time vary regressor large sample size informative estimation inference feasible require overall contribution time vary variable eliminate individual specific heterogeneity capture relatively small number available variable identity unknown restriction allow problem estimation proceed variable selection problem importantly treat individual specific heterogeneity fix effect allow heterogeneity relate observed time vary variable unspecified way allow heterogeneity non zero individual framework provide procedure uniformly valid inference fix subset parameter canonical linear fix effect model coefficient fix vector endogenous variable panel datum instrumental variable model fix effect instrument input develop property propose procedure use variant lasso estimator allow group data structure datum group independent dependence group unrestricted provide formal condition structure propose lasso variant select sparse model good approximation property present simulation result support theoretical development illustrate use method application aim estimate effect gun prevalence crime rate
econ,"Comprehensive Time-Series Regression Models Using GRETL -- U.S. GDP and
  Government Consumption Expenditures & Gross Investment from 1980 to 2013","  Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman
filter, transfer-function and intervention models, unit root tests,
cointegration test, volatility models (ARCH, GARCH, ARCH-M, GARCH-M,
Taylor-Schwert GARCH, GJR, TARCH, NARCH, APARCH, EGARCH) to analyze quarterly
time series of GDP and Government Consumption Expenditures & Gross Investment
(GCEGI) from 1980 to 2013. The article is organized as: (I) Definition; (II)
Regression Models; (III) Discussion. Additionally, I discovered a unique
interaction between GDP and GCEGI in both the short-run and the long-run and
provided policy makers with some suggestions. For example in the short run, GDP
responded positively and very significantly (0.00248) to GCEGI, while GCEGI
reacted positively but not too significantly (0.08051) to GDP. In the long run,
current GDP responded negatively and permanently (0.09229) to a shock in past
GCEGI, while current GCEGI reacted negatively yet temporarily (0.29821) to a
shock in past GDP. Therefore, policy makers should not adjust current GCEGI
based merely on the condition of current and past GDP. Although increasing
GCEGI does help GDP in the short-term, significantly abrupt increase in GCEGI
might not be good to the long-term health of GDP. Instead, a balanced,
sustainable, and economically viable solution is recommended, so that the
short-term benefits to the current economy from increasing GCEGI often largely
secured by the long-term loan outweigh or at least equal to the negative effect
to the future economy from the long-term debt incurred by the loan. Finally, I
found that non-normally distributed volatility models generally perform better
than normally distributed ones. More specifically, TARCH-GED performs the best
in the group of non-normally distributed, while GARCH-M does the best in the
group of normally distributed.
",gretl apply arma vector arma var state space model kalman filter transfer function intervention model unit root test cointegration test volatility model arch garch arch m garch m taylor schwert garch gjr tarch narch aparch egarch analyze quarterly time series gdp government consumption expenditures gross investment gcegi 1980 2013 article organize definition ii regression model iii discussion additionally discover unique interaction gdp gcegi short run long run provide policy maker suggestion example short run gdp respond positively significantly 0.00248 gcegi gcegi react positively significantly 0.08051 gdp long run current gdp respond negatively permanently 0.09229 shock past gcegi current gcegi react negatively temporarily 0.29821 shock past gdp policy maker adjust current gcegi base merely condition current past gdp increase gcegi help gdp short term significantly abrupt increase gcegi good long term health gdp instead balanced sustainable economically viable solution recommend short term benefit current economy increase gcegi largely secure long term loan outweigh equal negative effect future economy long term debt incur loan finally find non normally distribute volatility model generally perform well normally distribute one specifically tarch ged perform good group non normally distribute garch m good group normally distribute
econ,Nonlinear Factor Models for Network and Panel Data,"  Factor structures or interactive effects are convenient devices to
incorporate latent variables in panel data models. We consider fixed effect
estimation of nonlinear panel single-index models with factor structures in the
unobservables, which include logit, probit, ordered probit and Poisson
specifications. We establish that fixed effect estimators of model parameters
and average partial effects have normal distributions when the two dimensions
of the panel grow large, but might suffer of incidental parameter bias. We show
how models with factor structures can also be applied to capture important
features of network data such as reciprocity, degree heterogeneity, homophily
in latent variables and clustering. We illustrate this applicability with an
empirical example to the estimation of a gravity equation of international
trade between countries using a Poisson model with multiple factors.
",factor structure interactive effect convenient device incorporate latent variable panel datum model consider fix effect estimation nonlinear panel single index model factor structure unobservable include logit probit order probit poisson specification establish fix effect estimator model parameter average partial effect normal distribution dimension panel grow large suffer incidental parameter bias model factor structure apply capture important feature network datum reciprocity degree heterogeneity homophily latent variable cluster illustrate applicability empirical example estimation gravity equation international trade country poisson model multiple factor
econ,Rational Groupthink,"  We study how long-lived rational agents learn from repeatedly observing a
private signal and each others' actions. With normal signals, a group of any
size learns more slowly than just four agents who directly observe each others'
private signals in each period. Similar results apply to general signal
structures. We identify rational groupthink---in which agents ignore their
private signals and choose the same action for long periods of time---as the
cause of this failure of information aggregation.
",study long live rational agent learn repeatedly observe private signal action normal signal group size learn slowly agent directly observe private signal period similar result apply general signal structure identify rational groupthink agent ignore private signal choose action long period time cause failure information aggregation
econ,"Monge-Kantorovich Depth, Quantiles, Ranks, and Signs","  We propose new concepts of statistical depth, multivariate quantiles, ranks
and signs, based on canonical transportation maps between a distribution of
interest on $R^d$ and a reference distribution on the $d$-dimensional unit
ball. The new depth concept, called Monge-Kantorovich depth, specializes to
halfspace depth in the case of spherical distributions, but, for more general
distributions, differs from the latter in the ability for its contours to
account for non convex features of the distribution of interest. We propose
empirical counterparts to the population versions of those Monge-Kantorovich
depth contours, quantiles, ranks and signs, and show their consistency by
establishing a uniform convergence property for empirical transport maps, which
is of independent interest.
",propose new concept statistical depth multivariate quantile rank sign base canonical transportation map distribution interest $ r^d$ reference distribution $ d$-dimensional unit ball new depth concept call monge kantorovich depth specialize halfspace depth case spherical distribution general distribution differ ability contours account non convex feature distribution interest propose empirical counterpart population version monge kantorovich depth contour quantile rank sign consistency establish uniform convergence property empirical transport map independent interest
econ,"Forecasting day ahead electricity spot prices: The impact of the EXAA to
  other European electricity markets","  In our paper we analyze the relationship between the day-ahead electricity
price of the Energy Exchange Austria (EXAA) and other day-ahead electricity
prices in Europe. We focus on markets, which settle their prices after the
EXAA, which enables traders to include the EXAA price into their calculations.
For each market we employ econometric models to incorporate the EXAA price and
compare them with their counterparts without the price of the Austrian
exchange. By employing a forecasting study, we find that electricity price
models can be improved when EXAA prices are considered.
",paper analyze relationship day ahead electricity price energy exchange austria exaa day ahead electricity price europe focus market settle price exaa enable trader include exaa price calculation market employ econometric model incorporate exaa price compare counterpart price austrian exchange employ forecasting study find electricity price model improve exaa price consider
econ,The ABC of Simulation Estimation with Auxiliary Statistics,"  The frequentist method of simulated minimum distance (SMD) is widely used in
economics to estimate complex models with an intractable likelihood. In other
disciplines, a Bayesian approach known as Approximate Bayesian Computation
(ABC) is far more popular. This paper connects these two seemingly related
approaches to likelihood-free estimation by means of a Reverse Sampler that
uses both optimization and importance weighting to target the posterior
distribution. Its hybrid features enable us to analyze an ABC estimate from the
perspective of SMD. We show that an ideal ABC estimate can be obtained as a
weighted average of a sequence of SMD modes, each being the minimizer of the
deviations between the data and the model. This contrasts with the SMD, which
is the mode of the average deviations. Using stochastic expansions, we provide
a general characterization of frequentist estimators and those based on
Bayesian computations including Laplace-type estimators. Their differences are
illustrated using analytical examples and a simulation study of the dynamic
panel model.
",frequentist method simulated minimum distance smd widely economic estimate complex model intractable likelihood discipline bayesian approach know approximate bayesian computation abc far popular paper connect seemingly related approach likelihood free estimation mean reverse sampler use optimization importance weighting target posterior distribution hybrid feature enable analyze abc estimate perspective smd ideal abc estimate obtain weight average sequence smd mode minimizer deviation datum model contrast smd mode average deviation stochastic expansion provide general characterization frequentist estimator base bayesian computation include laplace type estimator difference illustrate analytical example simulation study dynamic panel model
econ,"Post-Selection and Post-Regularization Inference in Linear Models with
  Many Controls and Instruments","  In this note, we offer an approach to estimating causal/structural parameters
in the presence of many instruments and controls based on methods for
estimating sparse high-dimensional models. We use these high-dimensional
methods to select both which instruments and which control variables to use.
The approach we take extends BCCH2012, which covers selection of instruments
for IV models with a small number of controls, and extends BCH2014, which
covers selection of controls in models where the variable of interest is
exogenous conditional on observables, to accommodate both a large number of
controls and a large number of instruments. We illustrate the approach with a
simulation and an empirical example. Technical supporting material is available
in a supplementary online appendix.
",note offer approach estimate causal structural parameter presence instrument control base method estimate sparse high dimensional model use high dimensional method select instrument control variable use approach extend bcch2012 cover selection instrument iv model small number control extend bch2014 cover selection control model variable interest exogenous conditional observable accommodate large number control large number instrument illustrate approach simulation empirical example technical support material available supplementary online appendix
econ,"Valid Post-Selection and Post-Regularization Inference: An Elementary,
  General Approach","  Here we present an expository, general analysis of valid post-selection or
post-regularization inference about a low-dimensional target parameter,
$\alpha$, in the presence of a very high-dimensional nuisance parameter,
$\eta$, which is estimated using modern selection or regularization methods.
Our analysis relies on high-level, easy-to-interpret conditions that allow one
to clearly see the structures needed for achieving valid post-regularization
inference. Simple, readily verifiable sufficient conditions are provided for a
class of affine-quadratic models. We focus our discussion on estimation and
inference procedures based on using the empirical analog of theoretical
equations $$M(\alpha, \eta)=0$$ which identify $\alpha$. Within this structure,
we show that setting up such equations in a manner such that the
orthogonality/immunization condition $$\partial_\eta M(\alpha, \eta) = 0$$ at
the true parameter values is satisfied, coupled with plausible conditions on
the smoothness of $M$ and the quality of the estimator $\hat \eta$, guarantees
that inference on for the main parameter $\alpha$ based on testing or point
estimation methods discussed below will be regular despite selection or
regularization biases occurring in estimation of $\eta$. In particular, the
estimator of $\alpha$ will often be uniformly consistent at the root-$n$ rate
and uniformly asymptotically normal even though estimators $\hat \eta$ will
generally not be asymptotically linear and regular. The uniformity holds over
large classes of models that do not impose highly implausible ""beta-min""
conditions. We also show that inference can be carried out by inverting tests
formed from Neyman's $C(\alpha)$ (orthogonal score) statistics.
",present expository general analysis valid post selection post regularization inference low dimensional target parameter $ \alpha$ presence high dimensional nuisance parameter $ \eta$ estimate modern selection regularization method analysis rely high level easy interpret condition allow clearly structure need achieve valid post regularization inference simple readily verifiable sufficient condition provide class affine quadratic model focus discussion estimation inference procedure base empirical analog theoretical equation $ $ m(\alpha \eta)=0$$ identify $ \alpha$. structure set equation manner orthogonality immunization condition $ $ \partial_\eta m(\alpha \eta = 0$$ true parameter value satisfied couple plausible condition smoothness $ m$ quality estimator $ \hat \eta$ guarantee inference main parameter $ \alpha$ base testing point estimation method discuss regular despite selection regularization bias occur estimation $ \eta$. particular estimator $ \alpha$ uniformly consistent root-$n$ rate uniformly asymptotically normal estimator $ \hat \eta$ generally asymptotically linear regular uniformity hold large class model impose highly implausible beta min condition inference carry invert test form neyman $ c(\alpha)$ orthogonal score statistic
econ,A lava attack on the recovery of sums of dense and sparse signals,"  Common high-dimensional methods for prediction rely on having either a sparse
signal model, a model in which most parameters are zero and there are a small
number of non-zero parameters that are large in magnitude, or a dense signal
model, a model with no large parameters and very many small non-zero
parameters. We consider a generalization of these two basic models, termed here
a ""sparse+dense"" model, in which the signal is given by the sum of a sparse
signal and a dense signal. Such a structure poses problems for traditional
sparse estimators, such as the lasso, and for traditional dense estimation
methods, such as ridge estimation. We propose a new penalization-based method,
called lava, which is computationally efficient. With suitable choices of
penalty parameters, the proposed method strictly dominates both lasso and
ridge. We derive analytic expressions for the finite-sample risk function of
the lava estimator in the Gaussian sequence model. We also provide an deviation
bound for the prediction risk in the Gaussian regression model with fixed
design. In both cases, we provide Stein's unbiased estimator for lava's
prediction risk. A simulation example compares the performance of lava to
lasso, ridge, and elastic net in a regression example using feasible,
data-dependent penalty parameters and illustrates lava's improved performance
relative to these benchmarks.
",common high dimensional method prediction rely have sparse signal model model parameter zero small number non zero parameter large magnitude dense signal model model large parameter small non zero parameter consider generalization basic model term sparse+dense model signal give sum sparse signal dense signal structure pose problem traditional sparse estimator lasso traditional dense estimation method ridge estimation propose new penalization base method call lava computationally efficient suitable choice penalty parameter propose method strictly dominate lasso ridge derive analytic expression finite sample risk function lava estimator gaussian sequence model provide deviation bind prediction risk gaussian regression model fix design case provide stein unbiased estimator lava prediction risk simulation example compare performance lava lasso ridge elastic net regression example feasible data dependent penalty parameter illustrate lava improved performance relative benchmark
econ,Equilibrium in Misspecified Markov Decision Processes,"  We study Markov decision problems where the agent does not know the
transition probability function mapping current states and actions to future
states. The agent has a prior belief over a set of possible transition
functions and updates beliefs using Bayes' rule. We allow her to be
misspecified in the sense that the true transition probability function is not
in the support of her prior. This problem is relevant in many economic settings
but is usually not amenable to analysis by the researcher. We make the problem
tractable by studying asymptotic behavior. We propose an equilibrium notion and
provide conditions under which it characterizes steady state behavior. In the
special case where the problem is static, equilibrium coincides with the
single-agent version of Berk-Nash equilibrium (Esponda and Pouzo (2016)). We
also discuss subtle issues that arise exclusively in dynamic settings due to
the possibility of a negative value of experimentation.
",study markov decision problem agent know transition probability function mapping current state action future state agent prior belief set possible transition function update belief bayes rule allow misspecifie sense true transition probability function support prior problem relevant economic setting usually amenable analysis researcher problem tractable study asymptotic behavior propose equilibrium notion provide condition characterize steady state behavior special case problem static equilibrium coincide single agent version berk nash equilibrium esponda pouzo 2016 discuss subtle issue arise exclusively dynamic setting possibility negative value experimentation
econ,Recursive Partitioning for Heterogeneous Causal Effects,"  In this paper we study the problems of estimating heterogeneity in causal
effects in experimental or observational studies and conducting inference about
the magnitude of the differences in treatment effects across subsets of the
population. In applications, our method provides a data-driven approach to
determine which subpopulations have large or small treatment effects and to
test hypotheses about the differences in these effects. For experiments, our
method allows researchers to identify heterogeneity in treatment effects that
was not specified in a pre-analysis plan, without concern about invalidating
inference due to multiple testing. In most of the literature on supervised
machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal
is to build a model of the relationship between a unit's attributes and an
observed outcome. A prominent role in these methods is played by
cross-validation which compares predictions to actual outcomes in test samples,
in order to select the level of complexity of the model that provides the best
predictive power. Our method is closely related, but it differs in that it is
tailored for predicting causal effects of a treatment rather than a unit's
outcome. The challenge is that the ""ground truth"" for a causal effect is not
observed for any individual unit: we observe the unit with the treatment, or
without the treatment, but not both at the same time. Thus, it is not obvious
how to use cross-validation to determine whether a causal effect has been
accurately predicted. We propose several novel cross-validation criteria for
this problem and demonstrate through simulations the conditions under which
they perform better than standard methods for the problem of causal effects. We
then apply the method to a large-scale field experiment re-ranking results on a
search engine.
",paper study problem estimate heterogeneity causal effect experimental observational study conduct inference magnitude difference treatment effect subset population application method provide data drive approach determine subpopulation large small treatment effect test hypothesis difference effect experiment method allow researcher identify heterogeneity treatment effect specify pre analysis plan concern invalidate inference multiple testing literature supervised machine learning e.g. regression tree random forest lasso etc goal build model relationship unit attribute observe outcome prominent role method play cross validation compare prediction actual outcome test sample order select level complexity model provide good predictive power method closely related differ tailor predict causal effect treatment unit outcome challenge ground truth causal effect observe individual unit observe unit treatment treatment time obvious use cross validation determine causal effect accurately predict propose novel cross validation criterion problem demonstrate simulation condition perform well standard method problem causal effect apply method large scale field experiment rank result search engine
econ,"Alternative Asymptotics and the Partially Linear Model with Many
  Regressors","  Non-standard distributional approximations have received considerable
attention in recent years. They often provide more accurate approximations in
small samples, and theoretical improvements in some cases. This paper shows
that the seemingly unrelated ""many instruments asymptotics"" and ""small
bandwidth asymptotics"" share a common structure, where the object determining
the limiting distribution is a V-statistic with a remainder that is an
asymptotically normal degenerate U-statistic. We illustrate how this general
structure can be used to derive new results by obtaining a new asymptotic
distribution of a series estimator of the partially linear model when the
number of terms in the series approximation possibly grows as fast as the
sample size, which we call ""many terms asymptotics"".
",non standard distributional approximation receive considerable attention recent year provide accurate approximation small sample theoretical improvement case paper show seemingly unrelated instrument asymptotic small bandwidth asymptotic share common structure object determine limit distribution v statistic remainder asymptotically normal degenerate u statistic illustrate general structure derive new result obtain new asymptotic distribution series estimator partially linear model number term series approximation possibly grow fast sample size term asymptotic
econ,"Enhanced Gravity Model of trade: reconciling macroeconomic and network
  models","  The structure of the International Trade Network (ITN), whose nodes and links
represent world countries and their trade relations respectively, affects key
economic processes worldwide, including globalization, economic integration,
industrial production, and the propagation of shocks and instabilities.
Characterizing the ITN via a simple yet accurate model is an open problem. The
traditional Gravity Model (GM) successfully reproduces the volume of trade
between connected countries, using macroeconomic properties such as GDP,
geographic distance, and possibly other factors. However, it predicts a network
with complete or homogeneous topology, thus failing to reproduce the highly
heterogeneous structure of the ITN. On the other hand, recent maximum-entropy
network models successfully reproduce the complex topology of the ITN, but
provide no information about trade volumes. Here we integrate these two
currently incompatible approaches via the introduction of an Enhanced Gravity
Model (EGM) of trade. The EGM is the simplest model combining the GM with the
network approach within a maximum-entropy framework. Via a unified and
principled mechanism that is transparent enough to be generalized to any
economic network, the EGM provides a new econometric framework wherein trade
probabilities and trade volumes can be separately controlled by any combination
of dyadic and country-specific macroeconomic variables. The model successfully
reproduces both the global topology and the local link weights of the ITN,
parsimoniously reconciling the conflicting approaches. It also indicates that
the probability that any two countries trade a certain volume should follow a
geometric or exponential distribution with an additional point mass at zero
volume.
",structure international trade network itn node link represent world country trade relation respectively affect key economic process worldwide include globalization economic integration industrial production propagation shock instability characterize itn simple accurate model open problem traditional gravity model gm successfully reproduce volume trade connected country macroeconomic property gdp geographic distance possibly factor predict network complete homogeneous topology fail reproduce highly heterogeneous structure itn hand recent maximum entropy network model successfully reproduce complex topology itn provide information trade volume integrate currently incompatible approach introduction enhanced gravity model egm trade egm simple model combine gm network approach maximum entropy framework unified principle mechanism transparent generalize economic network egm provide new econometric framework trade probability trade volume separately control combination dyadic country specific macroeconomic variable model successfully reproduce global topology local link weight itn parsimoniously reconcile conflict approach indicate probability country trade certain volume follow geometric exponential distribution additional point mass zero volume
econ,"On Game-Theoretic Risk Management (Part One) -- Towards a Theory of
  Games with Payoffs that are Probability-Distributions","  Optimal behavior in (competitive) situation is traditionally determined with
the help of utility functions that measure the payoff of different actions.
Given an ordering on the space of revenues (payoffs), the classical axiomatic
approach of von Neumann and Morgenstern establishes the existence of suitable
utility functions, and yields to game-theory as the most prominent
materialization of a theory to determine optimal behavior. Although this
appears to be a most natural approach to risk management too, applications in
critical infrastructures often violate the implicit assumption of actions
leading to deterministic consequences. In that sense, the gameplay in a
critical infrastructure risk control competition is intrinsically random in the
sense of actions having uncertain consequences. Mathematically, this takes us
to utility functions that are probability-distribution-valued, in which case we
loose the canonic (in fact every possible) ordering on the space of payoffs,
and the original techniques of von Neumann and Morgenstern no longer apply.
  This work introduces a new kind of game in which uncertainty applies to the
payoff functions rather than the player's actions (a setting that has been
widely studied in the literature, yielding to celebrated notions like the
trembling hands equilibrium or the purification theorem). In detail, we show
how to fix the non-existence of a (canonic) ordering on the space of
probability distributions by only mildly restricting the full set to a subset
that can be totally ordered. Our vehicle to define the ordering and establish
basic game-theory is non-standard analysis and hyperreal numbers.
",optimal behavior competitive situation traditionally determined help utility function measure payoff different action give ordering space revenue payoff classical axiomatic approach von neumann morgenstern establish existence suitable utility function yield game theory prominent materialization theory determine optimal behavior appear natural approach risk management application critical infrastructure violate implicit assumption action lead deterministic consequence sense gameplay critical infrastructure risk control competition intrinsically random sense action have uncertain consequence mathematically take utility function probability distribution value case loose canonic fact possible order space payoff original technique von neumann morgenstern long apply work introduce new kind game uncertainty apply payoff function player action setting widely study literature yield celebrate notion like tremble hand equilibrium purification theorem detail fix non existence canonic ordering space probability distribution mildly restrict set subset totally order vehicle define ordering establish basic game theory non standard analysis hyperreal number
econ,"Inference in Linear Regression Models with Many Covariates and
  Heteroskedasticity","  The linear regression model is widely used in empirical work in Economics,
Statistics, and many other disciplines. Researchers often include many
covariates in their linear model specification in an attempt to control for
confounders. We give inference methods that allow for many covariates and
heteroskedasticity. Our results are obtained using high-dimensional
approximations, where the number of included covariates are allowed to grow as
fast as the sample size. We find that all of the usual versions of Eicker-White
heteroskedasticity consistent standard error estimators for linear models are
inconsistent under this asymptotics. We then propose a new heteroskedasticity
consistent standard error formula that is fully automatic and robust to both
(conditional)\ heteroskedasticity of unknown form and the inclusion of possibly
many covariates. We apply our findings to three settings: parametric linear
models with many covariates, linear panel models with many fixed effects, and
semiparametric semi-linear models with many technical regressors. Simulation
evidence consistent with our theoretical results is also provided. The proposed
methods are also illustrated with an empirical application.
",linear regression model widely empirical work economics statistic discipline researcher include covariate linear model specification attempt control confounder inference method allow covariate heteroskedasticity result obtain high dimensional approximation number include covariate allow grow fast sample size find usual version eicker white heteroskedasticity consistent standard error estimator linear model inconsistent asymptotic propose new heteroskedasticity consistent standard error formula fully automatic robust conditional)\ heteroskedasticity unknown form inclusion possibly covariate apply finding setting parametric linear model covariate linear panel model fix effect semiparametric semi linear model technical regressor simulation evidence consistent theoretical result provide propose method illustrate empirical application
econ,Nonparametric instrumental variable estimation under monotonicity,"  The ill-posedness of the inverse problem of recovering a regression function
in a nonparametric instrumental variable model leads to estimators that may
suffer from a very slow, logarithmic rate of convergence. In this paper, we
show that restricting the problem to models with monotone regression functions
and monotone instruments significantly weakens the ill-posedness of the
problem. In stark contrast to the existing literature, the presence of a
monotone instrument implies boundedness of our measure of ill-posedness when
restricted to the space of monotone functions. Based on this result we derive a
novel non-asymptotic error bound for the constrained estimator that imposes
monotonicity of the regression function. For a given sample size, the bound is
independent of the degree of ill-posedness as long as the regression function
is not too steep. As an implication, the bound allows us to show that the
constrained estimator converges at a fast, polynomial rate, independently of
the degree of ill-posedness, in a large, but slowly shrinking neighborhood of
constant functions. Our simulation study demonstrates significant finite-sample
performance gains from imposing monotonicity even when the regression function
is rather far from being a constant. We apply the constrained estimator to the
problem of estimating gasoline demand functions from U.S. data.
",ill posedness inverse problem recover regression function nonparametric instrumental variable model lead estimator suffer slow logarithmic rate convergence paper restrict problem model monotone regression function monotone instrument significantly weaken ill posedness problem stark contrast exist literature presence monotone instrument imply boundedness measure ill posedness restrict space monotone function base result derive novel non asymptotic error bind constrained estimator impose monotonicity regression function give sample size bind independent degree ill posedness long regression function steep implication bound allow constrain estimator converge fast polynomial rate independently degree ill posedness large slowly shrink neighborhood constant function simulation study demonstrate significant finite sample performance gain impose monotonicity regression function far constant apply constrain estimator problem estimate gasoline demand function u.s. datum
econ,"On the Effect of Bias Estimation on Coverage Accuracy in Nonparametric
  Inference","  Nonparametric methods play a central role in modern empirical work. While
they provide inference procedures that are more robust to parametric
misspecification bias, they may be quite sensitive to tuning parameter choices.
We study the effects of bias correction on confidence interval coverage in the
context of kernel density and local polynomial regression estimation, and prove
that bias correction can be preferred to undersmoothing for minimizing coverage
error and increasing robustness to tuning parameter choice. This is achieved
using a novel, yet simple, Studentization, which leads to a new way of
constructing kernel-based bias-corrected confidence intervals. In addition, for
practical cases, we derive coverage error optimal bandwidths and discuss
easy-to-implement bandwidth selectors. For interior points, we show that the
MSE-optimal bandwidth for the original point estimator (before bias correction)
delivers the fastest coverage error decay rate after bias correction when
second-order (equivalent) kernels are employed, but is otherwise suboptimal
because it is too ""large"". Finally, for odd-degree local polynomial regression,
we show that, as with point estimation, coverage error adapts to boundary
points automatically when appropriate Studentization is used; however, the
MSE-optimal bandwidth for the original point estimator is suboptimal. All the
results are established using valid Edgeworth expansions and illustrated with
simulated data. Our findings have important consequences for empirical work as
they indicate that bias-corrected confidence intervals, coupled with
appropriate standard errors, have smaller coverage error and are less sensitive
to tuning parameter choices in practically relevant cases where additional
smoothness is available.
",nonparametric method play central role modern empirical work provide inference procedure robust parametric misspecification bias sensitive tune parameter choice study effect bias correction confidence interval coverage context kernel density local polynomial regression estimation prove bias correction prefer undersmoothe minimize coverage error increase robustness tune parameter choice achieve novel simple studentization lead new way construct kernel base bias correct confidence interval addition practical case derive coverage error optimal bandwidth discuss easy implement bandwidth selector interior point mse optimal bandwidth original point estimator bias correction deliver fast coverage error decay rate bias correction second order equivalent kernel employ suboptimal large finally odd degree local polynomial regression point estimation coverage error adapt boundary point automatically appropriate studentization mse optimal bandwidth original point estimator suboptimal result establish valid edgeworth expansion illustrate simulated datum finding important consequence empirical work indicate bias correct confidence interval couple appropriate standard error small coverage error sensitive tune parameter choice practically relevant case additional smoothness available
econ,"Optimal Sup-norm Rates and Uniform Inference on Nonlinear Functionals of
  Nonparametric IV Regression","  This paper makes several important contributions to the literature about
nonparametric instrumental variables (NPIV) estimation and inference on a
structural function $h_0$ and its functionals. First, we derive sup-norm
convergence rates for computationally simple sieve NPIV (series 2SLS)
estimators of $h_0$ and its derivatives. Second, we derive a lower bound that
describes the best possible (minimax) sup-norm rates of estimating $h_0$ and
its derivatives, and show that the sieve NPIV estimator can attain the minimax
rates when $h_0$ is approximated via a spline or wavelet sieve. Our optimal
sup-norm rates surprisingly coincide with the optimal root-mean-squared rates
for severely ill-posed problems, and are only a logarithmic factor slower than
the optimal root-mean-squared rates for mildly ill-posed problems. Third, we
use our sup-norm rates to establish the uniform Gaussian process strong
approximations and the score bootstrap uniform confidence bands (UCBs) for
collections of nonlinear functionals of $h_0$ under primitive conditions,
allowing for mildly and severely ill-posed problems. Fourth, as applications,
we obtain the first asymptotic pointwise and uniform inference results for
plug-in sieve t-statistics of exact consumer surplus (CS) and deadweight loss
(DL) welfare functionals under low-level conditions when demand is estimated
via sieve NPIV. Empiricists could read our real data application of UCBs for
exact CS and DL functionals of gasoline demand that reveals interesting
patterns and is applicable to other markets.
",paper make important contribution literature nonparametric instrumental variable npiv estimation inference structural function $ h_0 $ functional derive sup norm convergence rate computationally simple sieve npiv series 2sls estimator $ h_0 $ derivative second derive lower bind describe well possible minimax sup norm rate estimate $ h_0 $ derivative sieve npiv estimator attain minimax rate $ h_0 $ approximate spline wavelet sieve optimal sup norm rate surprisingly coincide optimal root mean square rate severely ill pose problem logarithmic factor slow optimal root mean square rate mildly ill pose problem use sup norm rate establish uniform gaussian process strong approximation score bootstrap uniform confidence band ucb collection nonlinear functional $ h_0 $ primitive condition allow mildly severely ill pose problem fourth application obtain asymptotic pointwise uniform inference result plug sieve t statistic exact consumer surplus cs deadweight loss dl welfare functional low level condition demand estimate sieve npiv empiricist read real data application ucb exact cs dl functional gasoline demand reveal interesting pattern applicable market
econ,Trading Networks with Bilateral Contracts,"  We consider a model of matching in trading networks in which firms can enter
into bilateral contracts. In trading networks, stable outcomes, which are
immune to deviations of arbitrary sets of firms, may not exist. We define a new
solution concept called trail stability. Trail-stable outcomes are immune to
consecutive, pairwise deviations between linked firms. We show that any trading
network with bilateral contracts has a trail-stable outcome whenever firms'
choice functions satisfy the full substitutability condition. For trail-stable
outcomes, we prove results on the lattice structure, the rural hospitals
theorem, strategy-proofness, and comparative statics of firm entry and exit. We
also introduce weak trail stability which is implied by trail stability under
full substitutability. We describe relationships between the solution concepts.
",consider model matching trading network firm enter bilateral contract trading network stable outcome immune deviation arbitrary set firm exist define new solution concept call trail stability trail stable outcome immune consecutive pairwise deviation link firm trading network bilateral contract trail stable outcome firm choice function satisfy substitutability condition trail stable outcome prove result lattice structure rural hospital theorem strategy proofness comparative static firm entry exit introduce weak trail stability imply trail stability substitutability describe relationship solution concept
econ,Deriving Priorities From Inconsistent PCM using the Network Algorithms,"  In several multiobjective decision problems Pairwise Comparison Matrices
(PCM) are applied to evaluate the decision variants. The problem that arises
very often is the inconsistency of a given PCM. In such a situation it is
important to approximate the PCM with a consistent one. The most common way is
to minimize the Euclidean distance between the matrices. In the paper we
consider the problem of minimizing the maximum distance. After applying the
logarithmic transformation we are able to formulate the obtained subproblem as
a Shortest Path Problem and solve it more efficiently. We analyze and
completely characterize the form of the set of optimal solutions and provide an
algorithm that results in a unique, Pareto-efficient solution.
",multiobjective decision problem pairwise comparison matrices pcm apply evaluate decision variant problem arise inconsistency give pcm situation important approximate pcm consistent common way minimize euclidean distance matrix paper consider problem minimize maximum distance apply logarithmic transformation able formulate obtain subproblem shortest path problem solve efficiently analyze completely characterize form set optimal solution provide algorithm result unique pareto efficient solution
econ,"My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em
  Competition","  The first ever human vs. computer no-limit Texas hold 'em competition took
place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this
article I present my thoughts on the competition design, agent architecture,
and lessons learned.
",human vs. computer limit texas hold them competition take place april 24 8 2015 river casino pittsburgh pa article present thought competition design agent architecture lesson learn
econ,"Estimating the effect of treatments allocated by randomized waiting
  lists","  Oversubscribed treatments are often allocated using randomized waiting lists.
Applicants are ranked randomly, and treatment offers are made following that
ranking until all seats are filled. To estimate causal effects, researchers
often compare applicants getting and not getting an offer. We show that those
two groups are not statistically comparable. Therefore, the estimator arising
from that comparison is inconsistent. We propose a new estimator, and show that
it is consistent. Finally, we revisit an application, and we show that using
our estimator can lead to sizably different results from those obtained using
the commonly used estimator.
",oversubscribed treatment allocate randomized waiting list applicants rank randomly treatment offer follow rank seat fill estimate causal effect researcher compare applicant get get offer group statistically comparable estimator arise comparison inconsistent propose new estimator consistent finally revisit application estimator lead sizably different result obtain commonly estimator
econ,Adaptive estimation for some nonparametric instrumental variable models,"  The problem of endogeneity in statistics and econometrics is often handled by
introducing instrumental variables (IV) which fulfill the mean independence
assumption, i.e. the unobservable is mean independent of the instruments. When
full independence of IV's and the unobservable is assumed, nonparametric IV
regression models and nonparametric demand models lead to nonlinear integral
equations with unknown integral kernels. We prove convergence rates for the
mean integrated square error of the iteratively regularized Newton method
applied to these problems. Compared to related results we derive stronger
convergence results that rely on weaker nonlinearity restrictions. We
demonstrate in numerical simulations for a nonparametric IV regression that the
method produces better results than the standard model.
",problem endogeneity statistic econometric handle introduce instrumental variable iv fulfill mean independence assumption i.e. unobservable mean independent instrument independence iv unobservable assume nonparametric iv regression model nonparametric demand model lead nonlinear integral equation unknown integral kernel prove convergence rate mean integrate square error iteratively regularize newton method apply problem compare relate result derive strong convergence result rely weak nonlinearity restriction demonstrate numerical simulation nonparametric iv regression method produce well result standard model
econ,"On Game-Theoretic Risk Management (Part Two) -- Algorithms to Compute
  Nash-Equilibria in Games with Distributions as Payoffs","  The game-theoretic risk management framework put forth in the precursor work
""Towards a Theory of Games with Payoffs that are Probability-Distributions""
(arXiv:1506.07368 [q-fin.EC]) is herein extended by algorithmic details on how
to compute equilibria in games where the payoffs are probability distributions.
Our approach is ""data driven"" in the sense that we assume empirical data
(measurements, simulation, etc.) to be available that can be compiled into
distribution models, which are suitable for efficient decisions about
preferences, and setting up and solving games using these as payoffs. While
preferences among distributions turn out to be quite simple if nonparametric
methods (kernel density estimates) are used, computing Nash-equilibria in games
using such models is discovered as inefficient (if not impossible). In fact, we
give a counterexample in which fictitious play fails to converge for the
(specifically unfortunate) choice of payoff distributions in the game, and
introduce a suitable tail approximation of the payoff densities to tackle the
issue. The overall procedure is essentially a modified version of fictitious
play, and is herein described for standard and multicriteria games, to
iteratively deliver an (approximate) Nash-equilibrium. An exact method using
linear programming is also given.
",game theoretic risk management framework forth precursor work theory games payoffs probability distributions arxiv:1506.07368 q fin ec extend algorithmic detail compute equilibria game payoff probability distribution approach datum drive sense assume empirical datum measurement simulation etc available compile distribution model suitable efficient decision preference set solve game payoff preference distribution turn simple nonparametric method kernel density estimate compute nash equilibria game model discover inefficient impossible fact counterexample fictitious play fail converge specifically unfortunate choice payoff distribution game introduce suitable tail approximation payoff density tackle issue overall procedure essentially modify version fictitious play describe standard multicriteria games iteratively deliver approximate nash equilibrium exact method linear programming give
econ,Money as Minimal Complexity,"  We consider mechanisms that provide traders the opportunity to exchange
commodity $i$ for commodity $j$, for certain ordered pairs $ij$. Given any
connected graph $G$ of opportunities, we show that there is a unique mechanism
$M_{G}$ that satisfies some natural conditions of ""fairness"" and ""convenience"".
Let $\mathfrak{M}(m)$ denote the class of mechanisms $M_{G}$ obtained by
varying $G$ on the commodity set $\left\{1,\ldots,m\right\} $. We define the
complexity of a mechanism $M$ in $\mathfrak{M(m)}$ to be a certain pair of
integers $\tau(M),\pi(M)$ which represent the time required to exchange $i$ for
$j$ and the information needed to determine the exchange ratio (each in the
worst case scenario, across all $i\neq j$). This induces a quasiorder $\preceq$
on $\mathfrak{M}(m)$ by the rule \[ M\preceq
M^{\prime}\text{if}\tau(M)\leq\tau(M^{\prime})\text{and}\pi(M)\leq\pi(M^{\prime}).
\] We show that, for $m>3$, there are precisely three $\preceq$-minimal
mechanisms $M_{G}$ in $\mathfrak{M}(m)$, where $G$ corresponds to the star,
cycle and complete graphs. The star mechanism has a distinguished commodity --
the money -- that serves as the sole medium of exchange and mediates trade
between decentralized markets for the other commodities.
  Our main result is that, for any weights $\lambda,\mu>0,$ the star mechanism
is the unique minimizer of $\lambda\tau(M)+\mu\pi(M)$ on $\mathfrak{M}(m)$ for
large enough $m$.
","consider mechanism provide trader opportunity exchange commodity $ i$ commodity $ j$ certain order pair $ ij$. give connect graph $ g$ opportunity unique mechanism $ m_{g}$ satisfy natural condition fairness convenience let $ \mathfrak{m}(m)$ denote class mechanism $ m_{g}$ obtain vary $ g$ commodity set $ \left\{1,\ldot m\right\ $ define complexity mechanism $ m$ $ \mathfrak{m(m)}$ certain pair integer $ \tau(m),\pi(m)$ represent time require exchange $ i$ $ j$ information need determine exchange ratio bad case scenario $ i\neq j$ induce quasiorder $ \preceq$ $ \mathfrak{m}(m)$ rule m\preceq m^{\prime}\text{if}\tau(m)\leq\tau(m^{\prime})\text{and}\pi(m)\leq\pi(m^{\prime $ m>3 $ precisely $ \preceq$-minimal mechanism $ m_{g}$ $ \mathfrak{m}(m)$ $ g$ correspond star cycle complete graph star mechanism distinguished commodity money serve sole medium exchange mediate trade decentralized market commodity main result weight $ \lambda,\mu>0,$ star mechanism unique minimizer $ \lambda\tau(m)+\mu\pi(m)$ $ \mathfrak{m}(m)$ large $ m$."
econ,Graphical Exchange Mechanisms,"  Consider an exchange mechanism which accepts diversified offers of various
commodities and redistributes everything it receives. We impose certain
conditions of fairness and convenience on such a mechanism and show that it
admits unique prices, which equalize the value of offers and returns for each
individual.
  We next define the complexity of a mechanism in terms of certain integers
$\tau_{ij},\pi_{ij}$ and $k_{i}$ that represent the time required to exchange
$i$ for $j$, the difficulty in determining the exchange ratio, and the
dimension of the message space. We show that there are a finite number of
minimally complex mechanisms, in each of which all trade is conducted through
markets for commodity pairs.
  Finally we consider minimal mechanisms with smallest worst-case complexities
$\tau=\max\tau_{ij}$ and $\pi=\max\pi_{ij}$. For $m>3$ commodities, there are
precisely three such mechanisms, one of which has a distinguished commodity --
the money -- that serves as the sole medium of exchange. As $m\rightarrow
\infty$ the money mechanism is the only one with bounded $\left( \pi
,\tau\right) $.
","consider exchange mechanism accept diversified offer commodity redistribute receive impose certain condition fairness convenience mechanism admit unique price equalize value offer return individual define complexity mechanism term certain integer $ \tau_{ij},\pi_{ij}$ $ k_{i}$ represent time require exchange $ i$ $ j$ difficulty determine exchange ratio dimension message space finite number minimally complex mechanism trade conduct market commodity pair finally consider minimal mechanism small bad case complexity $ \tau=\max\tau_{ij}$ $ \pi=\max\pi_{ij}$. $ m>3 $ commodity precisely mechanism distinguished commodity money serve sole medium exchange $ m\rightarrow \infty$ money mechanism bound $ \left \pi \tau\right $"
econ,"The Sorted Effects Method: Discovering Heterogeneous Effects Beyond
  Their Averages","  The partial (ceteris paribus) effects of interest in nonlinear and
interactive linear models are heterogeneous as they can vary dramatically with
the underlying observed or unobserved covariates. Despite the apparent
importance of heterogeneity, a common practice in modern empirical work is to
largely ignore it by reporting average partial effects (or, at best, average
effects for some groups). While average effects provide very convenient scalar
summaries of typical effects, by definition they fail to reflect the entire
variety of the heterogeneous effects. In order to discover these effects much
more fully, we propose to estimate and report sorted effects -- a collection of
estimated partial effects sorted in increasing order and indexed by
percentiles. By construction the sorted effect curves completely represent and
help visualize the range of the heterogeneous effects in one plot. They are as
convenient and easy to report in practice as the conventional average partial
effects. They also serve as a basis for classification analysis, where we
divide the observational units into most or least affected groups and summarize
their characteristics. We provide a quantification of uncertainty (standard
errors and confidence bands) for the estimated sorted effects and related
classification analysis, and provide confidence sets for the most and least
affected groups. The derived statistical results rely on establishing key, new
mathematical results on Hadamard differentiability of a multivariate sorting
operator and a related classification operator, which are of independent
interest. We apply the sorted effects method and classification analysis to
demonstrate several striking patterns in the gender wage gap.
",partial ceteris paribus effect interest nonlinear interactive linear model heterogeneous vary dramatically underlie observed unobserved covariate despite apparent importance heterogeneity common practice modern empirical work largely ignore report average partial effect good average effect group average effect provide convenient scalar summary typical effect definition fail reflect entire variety heterogeneous effect order discover effect fully propose estimate report sorted effect collection estimate partial effect sort increase order index percentile construction sorted effect curve completely represent help visualize range heterogeneous effect plot convenient easy report practice conventional average partial effect serve basis classification analysis divide observational unit affected group summarize characteristic provide quantification uncertainty standard error confidence band estimate sorted effect related classification analysis provide confidence set affected group derive statistical result rely establish key new mathematical result hadamard differentiability multivariate sort operator related classification operator independent interest apply sorted effect method classification analysis demonstrate strike pattern gender wage gap
econ,On the Non-Asymptotic Properties of Regularized M-estimators,"  We propose a general framework for regularization in M-estimation problems
under time dependent (absolutely regular-mixing) data which encompasses many of
the existing estimators. We derive non-asymptotic concentration bounds for the
regularized M-estimator. Our results exhibit a variance-bias trade-off, with
the variance term being governed by a novel measure of the complexity of the
parameter set. We also show that the mixing structure affect the variance term
by scaling the number of observations; depending on the decay rate of the
mixing coefficients, this scaling can even affect the asymptotic behavior.
Finally, we propose a data-driven method for choosing the tuning parameters of
the regularized estimator which yield the same (up to constants) concentration
bound as one that optimally balances the (squared) bias and variance terms. We
illustrate the results with several canonical examples.
",propose general framework regularization m estimation problem time dependent absolutely regular mix datum encompass exist estimator derive non asymptotic concentration bound regularize m estimator result exhibit variance bias trade variance term govern novel measure complexity parameter set mixing structure affect variance term scale number observation depend decay rate mixing coefficient scaling affect asymptotic behavior finally propose data drive method choose tuning parameter regularized estimator yield constant concentration bind optimally balance square bias variance term illustrate result canonical example
econ,Confidence Intervals for Projections of Partially Identified Parameters,"  We propose a bootstrap-based calibrated projection procedure to build
confidence intervals for single components and for smooth functions of a
partially identified parameter vector in moment (in)equality models. The method
controls asymptotic coverage uniformly over a large class of data generating
processes. The extreme points of the calibrated projection confidence interval
are obtained by extremizing the value of the function of interest subject to a
proper relaxation of studentized sample analogs of the moment (in)equality
conditions. The degree of relaxation, or critical level, is calibrated so that
the function of theta, not theta itself, is uniformly asymptotically covered
with prespecified probability. This calibration is based on repeatedly checking
feasibility of linear programming problems, rendering it computationally
attractive.
  Nonetheless, the program defining an extreme point of the confidence interval
is generally nonlinear and potentially intricate. We provide an algorithm,
based on the response surface method for global optimization, that approximates
the solution rapidly and accurately, and we establish its rate of convergence.
The algorithm is of independent interest for optimization problems with simple
objectives and complicated constraints. An empirical application estimating an
entry game illustrates the usefulness of the method. Monte Carlo simulations
confirm the accuracy of the solution algorithm, the good statistical as well as
computational performance of calibrated projection (including in comparison to
other methods), and the algorithm's potential to greatly accelerate computation
of other confidence intervals.
",propose bootstrap base calibrated projection procedure build confidence interval single component smooth function partially identify parameter vector moment in)equality model method control asymptotic coverage uniformly large class datum generating process extreme point calibrate projection confidence interval obtain extremize value function interest subject proper relaxation studentized sample analog moment in)equality condition degree relaxation critical level calibrate function theta theta uniformly asymptotically cover prespecified probability calibration base repeatedly check feasibility linear programming problem render computationally attractive nonetheless program define extreme point confidence interval generally nonlinear potentially intricate provide algorithm base response surface method global optimization approximate solution rapidly accurately establish rate convergence algorithm independent interest optimization problem simple objective complicated constraint empirical application estimate entry game illustrate usefulness method monte carlo simulation confirm accuracy solution algorithm good statistical computational performance calibrate projection include comparison method algorithm potential greatly accelerate computation confidence interval
econ,"Doubly Robust Uniform Confidence Band for the Conditional Average
  Treatment Effect Function","  In this paper, we propose a doubly robust method to present the heterogeneity
of the average treatment effect with respect to observed covariates of
interest. We consider a situation where a large number of covariates are needed
for identifying the average treatment effect but the covariates of interest for
analyzing heterogeneity are of much lower dimension. Our proposed estimator is
doubly robust and avoids the curse of dimensionality. We propose a uniform
confidence band that is easy to compute, and we illustrate its usefulness via
Monte Carlo experiments and an application to the effects of smoking on birth
weights.
",paper propose doubly robust method present heterogeneity average treatment effect respect observe covariate interest consider situation large number covariate need identify average treatment effect covariate interest analyze heterogeneity low dimension propose estimator doubly robust avoid curse dimensionality propose uniform confidence band easy compute illustrate usefulness monte carlo experiment application effect smoking birth weight
econ,"Efficient Bayesian Inference for Multivariate Factor Stochastic
  Volatility Models","  We discuss efficient Bayesian estimation of dynamic covariance matrices in
multivariate time series through a factor stochastic volatility model. In
particular, we propose two interweaving strategies (Yu and Meng, Journal of
Computational and Graphical Statistics, 20(3), 531-570, 2011) to substantially
accelerate convergence and mixing of standard MCMC approaches. Similar to
marginal data augmentation techniques, the proposed acceleration procedures
exploit non-identifiability issues which frequently arise in factor models. Our
new interweaving strategies are easy to implement and come at almost no extra
computational cost; nevertheless, they can boost estimation efficiency by
several orders of magnitude as is shown in extensive simulation studies. To
conclude, the application of our algorithm to a 26-dimensional exchange rate
data set illustrates the superior performance of the new approach for
real-world data.
",discuss efficient bayesian estimation dynamic covariance matrix multivariate time series factor stochastic volatility model particular propose interweave strategy yu meng journal computational graphical statistics 20(3 531 570 2011 substantially accelerate convergence mixing standard mcmc approach similar marginal datum augmentation technique propose acceleration procedure exploit non identifiability issue frequently arise factor model new interweave strategy easy implement come extra computational cost boost estimation efficiency order magnitude show extensive simulation study conclude application algorithm 26 dimensional exchange rate datum set illustrate superior performance new approach real world datum
econ,High-Dimensional $L_2$Boosting: Rate of Convergence,"  Boosting is one of the most significant developments in machine learning.
This paper studies the rate of convergence of $L_2$Boosting, which is tailored
for regression, in a high-dimensional setting. Moreover, we introduce so-called
\textquotedblleft post-Boosting\textquotedblright. This is a post-selection
estimator which applies ordinary least squares to the variables selected in the
first stage by $L_2$Boosting. Another variant is \textquotedblleft Orthogonal
Boosting\textquotedblright\ where after each step an orthogonal projection is
conducted. We show that both post-$L_2$Boosting and the orthogonal boosting
achieve the same rate of convergence as LASSO in a sparse, high-dimensional
setting. We show that the rate of convergence of the classical $L_2$Boosting
depends on the design matrix described by a sparse eigenvalue constant. To show
the latter results, we derive new approximation results for the pure greedy
algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also
introduce feasible rules for early stopping, which can be easily implemented
and used in applied work. Our results also allow a direct comparison between
LASSO and boosting which has been missing from the literature. Finally, we
present simulation studies and applications to illustrate the relevance of our
theoretical results and to provide insights into the practical aspects of
boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms
LASSO.
",boost significant development machine learning paper study rate convergence $ l_2$boosting tailor regression high dimensional setting introduce call \textquotedblleft post boosting\textquotedblright post selection estimator apply ordinary square variable select stage $ l_2$boosting variant \textquotedblleft orthogonal boosting\textquotedblright\ step orthogonal projection conduct post-$l_2$boosting orthogonal boosting achieve rate convergence lasso sparse high dimensional setting rate convergence classical $ l_2$booste depend design matrix describe sparse eigenvalue constant result derive new approximation result pure greedy algorithm base analyze revisit behavior $ l_2$booste introduce feasible rule early stopping easily implement apply work result allow direct comparison lasso boost miss literature finally present simulation study application illustrate relevance theoretical result provide insight practical aspect boost simulation study post-$l_2$booste clearly outperform lasso
econ,"Oracle Estimation of a Change Point in High Dimensional Quantile
  Regression","  In this paper, we consider a high-dimensional quantile regression model where
the sparsity structure may differ between two sub-populations. We develop
$\ell_1$-penalized estimators of both regression coefficients and the threshold
parameter. Our penalized estimators not only select covariates but also
discriminate between a model with homogeneous sparsity and a model with a
change point. As a result, it is not necessary to know or pretest whether the
change point is present, or where it occurs. Our estimator of the change point
achieves an oracle property in the sense that its asymptotic distribution is
the same as if the unknown active sets of regression coefficients were known.
Importantly, we establish this oracle property without a perfect covariate
selection, thereby avoiding the need for the minimum level condition on the
signals of active covariates. Dealing with high-dimensional quantile regression
with an unknown change point calls for a new proof technique since the quantile
loss function is non-smooth and furthermore the corresponding objective
function is non-convex with respect to the change point. The technique
developed in this paper is applicable to a general M-estimation framework with
a change point, which may be of independent interest. The proposed methods are
then illustrated via Monte Carlo experiments and an application to tipping in
the dynamics of racial segregation.
",paper consider high dimensional quantile regression model sparsity structure differ sub population develop $ \ell_1$-penalize estimator regression coefficient threshold parameter penalize estimator select covariate discriminate model homogeneous sparsity model change point result necessary know pretest change point present occur estimator change point achieve oracle property sense asymptotic distribution unknown active set regression coefficient know importantly establish oracle property perfect covariate selection avoid need minimum level condition signal active covariate deal high dimensional quantile regression unknown change point call new proof technique quantile loss function non smooth furthermore corresponding objective function non convex respect change point technique develop paper applicable general m estimation framework change point independent interest propose method illustrate monte carlo experiment application tipping dynamic racial segregation
econ,"Coordination Event Detection and Initiator Identification in Time Series
  Data","  Behavior initiation is a form of leadership and is an important aspect of
social organization that affects the processes of group formation, dynamics,
and decision-making in human societies and other social animal species. In this
work, we formalize the ""Coordination Initiator Inference Problem"" and propose a
simple yet powerful framework for extracting periods of coordinated activity
and determining individuals who initiated this coordination, based solely on
the activity of individuals within a group during those periods. The proposed
approach, given arbitrary individual time series, automatically (1) identifies
times of coordinated group activity, (2) determines the identities of
initiators of those activities, and (3) classifies the likely mechanism by
which the group coordination occurred, all of which are novel computational
tasks. We demonstrate our framework on both simulated and real-world data:
trajectories tracking of animals as well as stock market data. Our method is
competitive with existing global leadership inference methods but provides the
first approaches for local leadership and coordination mechanism
classification. Our results are consistent with ground-truthed biological data
and the framework finds many known events in financial data which are not
otherwise reflected in the aggregate NASDAQ index. Our method is easily
generalizable to any coordinated time-series data from interacting entities.
",behavior initiation form leadership important aspect social organization affect process group formation dynamic decision making human society social animal specie work formalize coordination initiator inference problem propose simple powerful framework extract period coordinated activity determine individual initiate coordination base solely activity individual group period propose approach give arbitrary individual time series automatically 1 identify time coordinated group activity 2 determine identity initiator activity 3 classify likely mechanism group coordination occur novel computational task demonstrate framework simulated real world datum trajectory tracking animal stock market datum method competitive exist global leadership inference method provide approach local leadership coordination mechanism classification result consistent ground truthe biological datum framework find know event financial datum reflect aggregate nasdaq index method easily generalizable coordinated time series datum interact entity
econ,High-Dimensional Metrics in R,"  The package High-dimensional Metrics (\Rpackage{hdm}) is an evolving
collection of statistical methods for estimation and quantification of
uncertainty in high-dimensional approximately sparse models. It focuses on
providing confidence intervals and significance testing for (possibly many)
low-dimensional subcomponents of the high-dimensional parameter vector.
Efficient estimators and uniformly valid confidence intervals for regression
coefficients on target variables (e.g., treatment or policy variable) in a
high-dimensional approximately sparse regression model, for average treatment
effect (ATE) and average treatment effect for the treated (ATET), as well for
extensions of these parameters to the endogenous setting are provided. Theory
grounded, data-driven methods for selecting the penalization parameter in Lasso
regressions under heteroscedastic and non-Gaussian errors are implemented.
Moreover, joint/ simultaneous confidence intervals for regression coefficients
of a high-dimensional sparse regression are implemented, including a joint
significance test for Lasso regression. Data sets which have been used in the
literature and might be useful for classroom demonstration and for testing new
estimators are included. \R and the package \Rpackage{hdm} are open-source
software projects and can be freely downloaded from CRAN:
\texttt{http://cran.r-project.org}.
",package high dimensional metrics \rpackage{hdm evolve collection statistical method estimation quantification uncertainty high dimensional approximately sparse model focus provide confidence interval significance testing possibly low dimensional subcomponent high dimensional parameter vector efficient estimator uniformly valid confidence interval regression coefficient target variable e.g. treatment policy variable high dimensional approximately sparse regression model average treatment effect ate average treatment effect treated atet extension parameter endogenous setting provide theory ground data drive method select penalization parameter lasso regression heteroscedastic non gaussian error implement joint/ simultaneous confidence interval regression coefficient high dimensional sparse regression implement include joint significance test lasso regression datum set literature useful classroom demonstration test new estimator include \r package \rpackage{hdm open source software project freely download cran \texttt{http://cran.r project.org
econ,Optimal Data Collection for Randomized Control Trials,"  In a randomized control trial, the precision of an average treatment effect
estimator can be improved either by collecting data on additional individuals,
or by collecting additional covariates that predict the outcome variable. We
propose the use of pre-experimental data such as a census, or a household
survey, to inform the choice of both the sample size and the covariates to be
collected. Our procedure seeks to minimize the resulting average treatment
effect estimator's mean squared error, subject to the researcher's budget
constraint. We rely on a modification of an orthogonal greedy algorithm that is
conceptually simple and easy to implement in the presence of a large number of
potential covariates, and does not require any tuning parameters. In two
empirical applications, we show that our procedure can lead to substantial
gains of up to 58%, measured either in terms of reductions in data collection
costs or in terms of improvements in the precision of the treatment effect
estimator.
",randomized control trial precision average treatment effect estimator improve collect datum additional individual collect additional covariate predict outcome variable propose use pre experimental datum census household survey inform choice sample size covariate collect procedure seek minimize result average treatment effect estimator mean square error subject researcher budget constraint rely modification orthogonal greedy algorithm conceptually simple easy implement presence large number potential covariate require tuning parameter empirical application procedure lead substantial gain 58 measure term reduction datum collection cost term improvement precision treatment effect estimator
econ,"Augmented Factor Models with Applications to Validating Market Risk
  Factors and Forecasting Bond Risk Premia","  We study factor models augmented by observed covariates that have explanatory
powers on the unknown factors. In financial factor models, the unknown factors
can be reasonably well explained by a few observable proxies, such as the
Fama-French factors. In diffusion index forecasts, identified factors are
strongly related to several directly measurable economic variables such as
consumption-wealth variable, financial ratios, and term spread. With those
covariates, both the factors and loadings are identifiable up to a rotation
matrix even only with a finite dimension. To incorporate the explanatory power
of these covariates, we propose a smoothed principal component analysis (PCA):
(i) regress the data onto the observed covariates, and (ii) take the principal
components of the fitted data to estimate the loadings and factors. This allows
us to accurately estimate the percentage of both explained and unexplained
components in factors and thus to assess the explanatory power of covariates.
We show that both the estimated factors and loadings can be estimated with
improved rates of convergence compared to the benchmark method. The degree of
improvement depends on the strength of the signals, representing the
explanatory power of the covariates on the factors. The proposed estimator is
robust to possibly heavy-tailed distributions. We apply the model to forecast
US bond risk premia, and find that the observed macroeconomic characteristics
contain strong explanatory powers of the factors. The gain of forecast is more
substantial when the characteristics are incorporated to estimate the common
factors than directly used for forecasts.
",study factor model augment observed covariate explanatory power unknown factor financial factor model unknown factor reasonably explain observable proxy fama french factor diffusion index forecast identify factor strongly related directly measurable economic variable consumption wealth variable financial ratio term spread covariate factor loading identifiable rotation matrix finite dimension incorporate explanatory power covariate propose smoothed principal component analysis pca regress datum observed covariate ii principal component fit datum estimate loading factor allow accurately estimate percentage explain unexplained component factor assess explanatory power covariate estimate factor loading estimate improve rate convergence compare benchmark method degree improvement depend strength signal represent explanatory power covariate factor propose estimator robust possibly heavy tail distribution apply model forecast bond risk premia find observed macroeconomic characteristic contain strong explanatory power factor gain forecast substantial characteristic incorporate estimate common factor directly forecast
econ,"Estimating Treatment Effects using Multiple Surrogates: The Role of the
  Surrogate Score and the Surrogate Index","  Estimating the long-term effects of treatments is of interest in many fields.
A common challenge in estimating such treatment effects is that long-term
outcomes are unobserved in the time frame needed to make policy decisions. One
approach to overcome this missing data problem is to analyze treatments effects
on an intermediate outcome, often called a statistical surrogate, if it
satisfies the condition that treatment and outcome are independent conditional
on the statistical surrogate. The validity of the surrogacy condition is often
controversial. Here we exploit that fact that in modern datasets, researchers
often observe a large number, possibly hundreds or thousands, of intermediate
outcomes, thought to lie on or close to the causal chain between the treatment
and the long-term outcome of interest. Even if none of the individual proxies
satisfies the statistical surrogacy criterion by itself, using multiple proxies
can be useful in causal inference. We focus primarily on a setting with two
samples, an experimental sample containing data about the treatment indicator
and the surrogates and an observational sample containing information about the
surrogates and the primary outcome. We state assumptions under which the
average treatment effect be identified and estimated with a high-dimensional
vector of proxies that collectively satisfy the surrogacy assumption, and
derive the bias from violations of the surrogacy assumption, and show that even
if the primary outcome is also observed in the experimental sample, there is
still information to be gained from using surrogates.
",estimate long term effect treatment interest field common challenge estimate treatment effect long term outcome unobserve time frame need policy decision approach overcome miss datum problem analyze treatment effect intermediate outcome call statistical surrogate satisfy condition treatment outcome independent conditional statistical surrogate validity surrogacy condition controversial exploit fact modern dataset researcher observe large number possibly hundred thousand intermediate outcome think lie close causal chain treatment long term outcome interest individual proxy satisfy statistical surrogacy criterion multiple proxy useful causal inference focus primarily setting sample experimental sample contain datum treatment indicator surrogate observational sample contain information surrogate primary outcome state assumption average treatment effect identify estimate high dimensional vector proxy collectively satisfy surrogacy assumption derive bias violation surrogacy assumption primary outcome observe experimental sample information gain surrogate
econ,The Mittag-Leffler Fitting of the Phillips Curve,"  In this paper, a mathematical model based on the one-parameter Mittag-Leffler
function is proposed to be used for the first time to describe the relation
between unemployment rate and inflation rate, also known as the Phillips curve.
The Phillips curve is in the literature often represented by an
exponential-like shape. On the other hand, Phillips in his fundamental paper
used a power function in the model definition. Considering that the ordinary as
well as generalised Mittag-Leffler function behaves between a purely
exponential function and a power function it is natural to implement it in the
definition of the model used to describe the relation between the data
representing the Phillips curve. For the modelling purposes the data of two
different European economies, France and Switzerland, were used and an
""out-of-sample"" forecast was done to compare the performance of the
Mittag-Leffler model to the performance of the power-type and exponential-type
model. The results demonstrate that the ability of the Mittag-Leffler function
to fit data that manifest signs of stretched exponentials, oscillations or even
damped oscillations can be of use when describing economic relations and
phenomenons, such as the Phillips curve.
",paper mathematical model base parameter mittag leffler function propose time describe relation unemployment rate inflation rate know phillips curve phillips curve literature represent exponential like shape hand phillips fundamental paper power function model definition consider ordinary generalise mittag leffler function behave purely exponential function power function natural implement definition model describe relation datum represent phillips curve modelling purpose datum different european economy france switzerland sample forecast compare performance mittag leffler model performance power type exponential type model result demonstrate ability mittag leffler function fit datum manifest sign stretched exponential oscillation damp oscillation use describe economic relation phenomenon phillips curve
econ,Program Evaluation with Right-Censored Data,"  In a unified framework, we provide estimators and confidence bands for a
variety of treatment effects when the outcome of interest, typically a
duration, is subjected to right censoring. Our methodology accommodates
average, distributional, and quantile treatment effects under different
identifying assumptions including unconfoundedness, local treatment effects,
and nonlinear differences-in-differences. The proposed estimators are easy to
implement, have close-form representation, are fully data-driven upon
estimation of nuisance parameters, and do not rely on parametric distributional
assumptions, shape restrictions, or on restricting the potential treatment
effect heterogeneity across different subpopulations. These treatment effects
results are obtained as a consequence of more general results on two-step
Kaplan-Meier estimators that are of independent interest: we provide conditions
for applying (i) uniform law of large numbers, (ii) functional central limit
theorems, and (iii) we prove the validity of the ordinary nonparametric
bootstrap in a two-step estimation procedure where the outcome of interest may
be randomly censored.
",unified framework provide estimator confidence band variety treatment effect outcome interest typically duration subject right censoring methodology accommodate average distributional quantile treatment effect different identify assumption include unconfoundedness local treatment effect nonlinear difference difference propose estimator easy implement close form representation fully datum drive estimation nuisance parameter rely parametric distributional assumption shape restriction restrict potential treatment effect heterogeneity different subpopulation treatment effect result obtain consequence general result step kaplan meier estimator independent interest provide condition apply uniform law large number ii functional central limit theorem iii prove validity ordinary nonparametric bootstrap step estimation procedure outcome interest randomly censor
econ,"Approximate Residual Balancing: De-Biased Inference of Average Treatment
  Effects in High Dimensions","  There are many settings where researchers are interested in estimating
average treatment effects and are willing to rely on the unconfoundedness
assumption, which requires that the treatment assignment be as good as random
conditional on pre-treatment variables. The unconfoundedness assumption is
often more plausible if a large number of pre-treatment variables are included
in the analysis, but this can worsen the performance of standard approaches to
treatment effect estimation. In this paper, we develop a method for de-biasing
penalized regression adjustments to allow sparse regression methods like the
lasso to be used for sqrt{n}-consistent inference of average treatment effects
in high-dimensional linear models. Given linearity, we do not need to assume
that the treatment propensities are estimable, or that the average treatment
effect is a sparse contrast of the outcome model parameters. Rather, in
addition standard assumptions used to make lasso regression on the outcome
model consistent under 1-norm error, we only require overlap, i.e., that the
propensity score be uniformly bounded away from 0 and 1. Procedurally, our
method combines balancing weights with a regularized regression adjustment.
",setting researcher interested estimate average treatment effect willing rely unconfoundedness assumption require treatment assignment good random conditional pre treatment variable unconfoundedness assumption plausible large number pre treatment variable include analysis worsen performance standard approach treatment effect estimation paper develop method de biasing penalize regression adjustment allow sparse regression method like lasso sqrt{n}-consistent inference average treatment effect high dimensional linear model give linearity need assume treatment propensity estimable average treatment effect sparse contrast outcome model parameter addition standard assumption lasso regression outcome model consistent 1 norm error require overlap i.e. propensity score uniformly bound away 0 1 procedurally method combine balance weight regularized regression adjustment
econ,Monte Carlo Confidence Sets for Identified Sets,"  In complicated/nonlinear parametric models, it is generally hard to know
whether the model parameters are point identified. We provide computationally
attractive procedures to construct confidence sets (CSs) for identified sets of
full parameters and of subvectors in models defined through a likelihood or a
vector of moment equalities or inequalities. These CSs are based on level sets
of optimal sample criterion functions (such as likelihood or optimally-weighted
or continuously-updated GMM criterions). The level sets are constructed using
cutoffs that are computed via Monte Carlo (MC) simulations directly from the
quasi-posterior distributions of the criterions. We establish new Bernstein-von
Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions
of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified
regular models and some non-regular models. These results imply that our MC CSs
have exact asymptotic frequentist coverage for identified sets of full
parameters and of subvectors in partially-identified regular models, and have
valid but potentially conservative coverage in models with reduced-form
parameters on the boundary. Our MC CSs for identified sets of subvectors are
shown to have exact asymptotic coverage in models with singularities. We also
provide results on uniform validity of our CSs over classes of DGPs that
include point and partially identified models. We demonstrate good
finite-sample coverage properties of our procedures in two simulation
experiments. Finally, our procedures are applied to two non-trivial empirical
examples: an airline entry game and a model of trade flows.
",complicated nonlinear parametric model generally hard know model parameter point identify provide computationally attractive procedure construct confidence set cs identify set parameter subvector model define likelihood vector moment equality inequality cs base level set optimal sample criterion function likelihood optimally weight continuously update gmm criterion level set construct cutoff compute monte carlo mc simulation directly quasi posterior distribution criterion establish new bernstein von mises bayesian wilks type theorem quasi posterior distribution quasi likelihood ratio qlr profile qlr partially identify regular model non regular model result imply mc cs exact asymptotic frequentist coverage identify set parameter subvector partially identify regular model valid potentially conservative coverage model reduced form parameter boundary mc cs identify set subvector show exact asymptotic coverage model singularity provide result uniform validity cs class dgps include point partially identify model demonstrate good finite sample coverage property procedure simulation experiment finally procedure apply non trivial empirical example airline entry game model trade flow
econ,Economic Development and Inequality: a complex system analysis,"  By borrowing methods from complex system analysis, in this paper we analyze
the features of the complex relationship that links the development and the
industrialization of a country to economic inequality. In order to do this, we
identify industrialization as a combination of a monetary index, the GDP per
capita, and a recently introduced measure of the complexity of an economy, the
Fitness. At first we explore these relations on a global scale over the time
period 1990--2008 focusing on two different dimensions of inequality: the
capital share of income and a Theil measure of wage inequality. In both cases,
the movement of inequality follows a pattern similar to the one theorized by
Kuznets in the fifties. We then narrow down the object of study ad we
concentrate on wage inequality within the United States. By employing data on
wages and employment on the approximately 3100 US counties for the time
interval 1990--2014, we generalize the Fitness-Complexity algorithm for
counties and NAICS sectors, and we investigate wage inequality between
industrial sectors within counties. At this scale, in the early nineties we
recover a behavior similar to the global one. While, in more recent years, we
uncover a trend reversal: wage inequality monotonically increases as
industrialization levels grow. Hence at a county level, at net of the social
and institutional factors that differ among countries, we not only observe an
upturn in inequality but also a change in the structure of the relation between
wage inequality and development.
",borrow method complex system analysis paper analyze feature complex relationship link development industrialization country economic inequality order identify industrialization combination monetary index gdp capita recently introduce measure complexity economy fitness explore relation global scale time period 1990 -2008 focus different dimension inequality capital share income theil measure wage inequality case movement inequality follow pattern similar theorize kuznets fifty narrow object study ad concentrate wage inequality united states employ datum wage employment approximately 3100 county time interval 1990 -2014 generalize fitness complexity algorithm county naics sector investigate wage inequality industrial sector county scale early ninety recover behavior similar global recent year uncover trend reversal wage inequality monotonically increase industrialization level grow county level net social institutional factor differ country observe upturn inequality change structure relation wage inequality development
econ,Testing for Common Breaks in a Multiple Equations System,"  The issue addressed in this paper is that of testing for common breaks across
or within equations of a multivariate system. Our framework is very general and
allows integrated regressors and trends as well as stationary regressors. The
null hypothesis is that breaks in different parameters occur at common
locations and are separated by some positive fraction of the sample size unless
they occur across different equations. Under the alternative hypothesis, the
break dates across parameters are not the same and also need not be separated
by a positive fraction of the sample size whether within or across equations.
The test considered is the quasi-likelihood ratio test assuming normal errors,
though as usual the limit distribution of the test remains valid with
non-normal errors. Of independent interest, we provide results about the rate
of convergence of the estimates when searching over all possible partitions
subject only to the requirement that each regime contains at least as many
observations as some positive fraction of the sample size, allowing break dates
not separated by a positive fraction of the sample size across equations.
Simulations show that the test has good finite sample properties. We also
provide an application to issues related to level shifts and persistence for
various measures of inflation to illustrate its usefulness.
",issue address paper testing common break equation multivariate system framework general allow integrated regressor trend stationary regressor null hypothesis break different parameter occur common location separate positive fraction sample size occur different equation alternative hypothesis break date parameter need separate positive fraction sample size equation test consider quasi likelihood ratio test assume normal error usual limit distribution test remain valid non normal error independent interest provide result rate convergence estimate search possible partition subject requirement regime contain observation positive fraction sample size allow break date separate positive fraction sample size equation simulation test good finite sample property provide application issue relate level shift persistence measure inflation illustrate usefulness
econ,A/B Testing of Auctions,"  For many application areas A/B testing, which partitions users of a system
into an A (control) and B (treatment) group to experiment between several
application designs, enables Internet companies to optimize their services to
the behavioral patterns of their users. Unfortunately, the A/B testing
framework cannot be applied in a straightforward manner to applications like
auctions where the users (a.k.a., bidders) submit bids before the partitioning
into the A and B groups is made. This paper combines auction theoretic modeling
with the A/B testing framework to develop methodology for A/B testing auctions.
The accuracy of our method %, assuming the auction is directly comparable to
ideal A/B testing where there is no interference between A and B. Our results
are based on an extension and improved analysis of the inference method of
Chawla et al. (2014).
",application area b testing partition user system control b treatment group experiment application design enable internet company optimize service behavioral pattern user unfortunately b testing framework apply straightforward manner application like auction user a.k.a bidder submit bid partitioning b group paper combine auction theoretic modeling b testing framework develop methodology b testing auction accuracy method assume auction directly comparable ideal b testing interference b. result base extension improve analysis inference method chawla et al 2014
econ,Nonparametric Analysis of Random Utility Models,"  This paper develops and implements a nonparametric test of Random Utility
Models. The motivating application is to test the null hypothesis that a sample
of cross-sectional demand distributions was generated by a population of
rational consumers. We test a necessary and sufficient condition for this that
does not rely on any restriction on unobserved heterogeneity or the number of
goods. We also propose and implement a control function approach to account for
endogenous expenditure. An econometric result of independent interest is a test
for linear inequality constraints when these are represented as the vertices of
a polyhedron rather than its faces. An empirical application to the U.K.
Household Expenditure Survey illustrates computational feasibility of the
method in demand problems with 5 goods.
",paper develop implement nonparametric test random utility model motivating application test null hypothesis sample cross sectional demand distribution generate population rational consumer test necessary sufficient condition rely restriction unobserved heterogeneity number good propose implement control function approach account endogenous expenditure econometric result independent interest test linear inequality constraint represent vertex polyhedron face empirical application u.k. household expenditure survey illustrate computational feasibility method demand problem 5 good
econ,"Quantile Graphical Models: Prediction and Conditional Independence with
  Applications to Systemic Risk","  We propose two types of Quantile Graphical Models (QGMs) --- Conditional
Independence Quantile Graphical Models (CIQGMs) and Prediction Quantile
Graphical Models (PQGMs). CIQGMs characterize the conditional independence of
distributions by evaluating the distributional dependence structure at each
quantile index. As such, CIQGMs can be used for validation of the graph
structure in the causal graphical models (\cite{pearl2009causality,
robins1986new, heckman2015causal}). One main advantage of these models is that
we can apply them to large collections of variables driven by non-Gaussian and
non-separable shocks. PQGMs characterize the statistical dependencies through
the graphs of the best linear predictors under asymmetric loss functions. PQGMs
make weaker assumptions than CIQGMs as they allow for misspecification. Because
of QGMs' ability to handle large collections of variables and focus on specific
parts of the distributions, we could apply them to quantify tail
interdependence. The resulting tail risk network can be used for measuring
systemic risk contributions that help make inroads in understanding
international financial contagion and dependence structures of returns under
downside market movements.
  We develop estimation and inference methods for QGMs focusing on the
high-dimensional case, where the number of variables in the graph is large
compared to the number of observations. For CIQGMs, these methods and results
include valid simultaneous choices of penalty functions, uniform rates of
convergence, and confidence regions that are simultaneously valid. We also
derive analogous results for PQGMs, which include new results for penalized
quantile regressions in high-dimensional settings to handle misspecification,
many controls, and a continuum of additional conditioning events.
",propose type quantile graphical models qgms conditional independence quantile graphical models ciqgms prediction quantile graphical models pqgms ciqgm characterize conditional independence distribution evaluate distributional dependence structure quantile index ciqgm validation graph structure causal graphical model \cite{pearl2009causality robins1986new heckman2015causal main advantage model apply large collection variable drive non gaussian non separable shock pqgm characterize statistical dependency graph good linear predictor asymmetric loss function pqgm weak assumption ciqgm allow misspecification qgm ability handle large collection variable focus specific part distribution apply quantify tail interdependence result tail risk network measure systemic risk contribution help inroad understand international financial contagion dependence structure return downside market movement develop estimation inference method qgm focus high dimensional case number variable graph large compare number observation ciqgm method result include valid simultaneous choice penalty function uniform rate convergence confidence region simultaneously valid derive analogous result pqgm include new result penalize quantile regression high dimensional setting handle misspecification control continuum additional conditioning event
